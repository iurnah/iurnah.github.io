<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><link href=https://ruihan.org/courses/machine-learning-coursera/notes/ rel=canonical><link rel="shortcut icon" href=../../../assets/images/favicon.png><meta name=generator content="mkdocs-1.1.2, mkdocs-material-6.0.2"><title>Machine Learning (Coursera) - RUIHAN.ORG</title><link rel=stylesheet href=../../../assets/stylesheets/main.38780c08.min.css><link rel=stylesheet href=../../../assets/stylesheets/palette.3f72e892.min.css><meta name=theme-color content=#000000><link href=https://fonts.gstatic.com rel=preconnect crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback"><style>body,input{font-family:"Roboto",-apple-system,BlinkMacSystemFont,Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono",SFMono-Regular,Consolas,Menlo,monospace}</style></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=black data-md-color-accent=black> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#machine-learning-coursera class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class=md-header data-md-component=header> <nav class="md-header-nav md-grid" aria-label=Header> <a href=https://ruihan.org title=RUIHAN.ORG class="md-header-nav__button md-logo" aria-label=RUIHAN.ORG> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 003-3 3 3 0 00-3-3 3 3 0 00-3 3 3 3 0 003 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg> </a> <label class="md-header-nav__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg> </label> <div class=md-header-nav__title data-md-component=header-title> <div class=md-header-nav__ellipsis> <span class="md-header-nav__topic md-ellipsis"> RUIHAN.ORG </span> <span class="md-header-nav__topic md-ellipsis"> Machine Learning (Coursera) </span> </div> </div> <label class="md-header-nav__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query data-md-state=active> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </label> <button type=reset class="md-search__icon md-icon" aria-label=Clear data-md-component=search-reset tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg> </button> </form> <div class=md-search__output> <div class=md-search__scrollwrap data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list></ol> </div> </div> </div> </div> </div> <div class=md-header-nav__source> <a href=https://github.com/iurnah/ruihan.org title="Go to repository" class=md-source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 00-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 01-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 000 40.81l195.61 195.6a28.86 28.86 0 0040.8 0l194.69-194.69a28.86 28.86 0 000-40.81z"/></svg> </div> <div class=md-source__repository> iurnah/ruihan.org </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class="md-tabs md-tabs--active" aria-label=Tabs data-md-component=tabs> <div class="md-tabs__inner md-grid"> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../../../books/ class=md-tabs__link> Books </a> </li> <li class=md-tabs__item> <a href=../../ class="md-tabs__link md-tabs__link--active"> Courses </a> </li> <li class=md-tabs__item> <a href=../../../leetcode/ class=md-tabs__link> Leetcode </a> </li> <li class=md-tabs__item> <a href=../../../research/ class=md-tabs__link> Research </a> </li> <li class=md-tabs__item> <a href=../../../seedlabs/ class=md-tabs__link> SEED Labs </a> </li> <li class=md-tabs__item> <a href=../../../system-design/ class=md-tabs__link> System Design </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=https://ruihan.org title=RUIHAN.ORG class="md-nav__button md-logo" aria-label=RUIHAN.ORG> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 003-3 3 3 0 00-3-3 3 3 0 00-3 3 3 3 0 003 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg> </a> RUIHAN.ORG </label> <div class=md-nav__source> <a href=https://github.com/iurnah/ruihan.org title="Go to repository" class=md-source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 00-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 01-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 000 40.81l195.61 195.6a28.86 28.86 0 0040.8 0l194.69-194.69a28.86 28.86 0 000-40.81z"/></svg> </div> <div class=md-source__repository> iurnah/ruihan.org </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1 type=checkbox id=nav-1> <label class=md-nav__link for=nav-1> Books <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Books data-md-level=1> <label class=md-nav__title for=nav-1> <span class="md-nav__icon md-icon"></span> Books </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../books/ class=md-nav__link> Index </a> </li> <li class=md-nav__item> <a href=../../../books/accelerated-cpp/notes/ class=md-nav__link> Accelerated C++ </a> </li> <li class=md-nav__item> <a href=../../../books/mining-massive-datasets/notes/ class=md-nav__link> Mining Massive Datasets </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-2 type=checkbox id=nav-2 checked> <label class=md-nav__link for=nav-2> Courses <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Courses data-md-level=1> <label class=md-nav__title for=nav-2> <span class="md-nav__icon md-icon"></span> Courses </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../ class=md-nav__link> Index </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" data-md-toggle=toc type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> Machine Learning (Coursera) <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> Machine Learning (Coursera) </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=#week-1 class=md-nav__link> Week 1 </a> <nav class=md-nav aria-label="Week 1"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#introduction class=md-nav__link> Introduction </a> </li> <li class=md-nav__item> <a href=#cost-function class=md-nav__link> Cost Function </a> </li> <li class=md-nav__item> <a href=#cost-function-intuition-i class=md-nav__link> Cost Function Intuition I </a> </li> <li class=md-nav__item> <a href=#cost-function-intuition-ii class=md-nav__link> Cost Function Intuition II </a> </li> <li class=md-nav__item> <a href=#gradient-descent class=md-nav__link> Gradient Descent </a> </li> <li class=md-nav__item> <a href=#gradient-descent-intuition class=md-nav__link> Gradient Descent Intuition </a> </li> <li class=md-nav__item> <a href=#gradient-descent-and-linear-regression class=md-nav__link> Gradient Descent and Linear Regression </a> </li> <li class=md-nav__item> <a href=#linear-algebra-review class=md-nav__link> Linear Algebra Review </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#week-2 class=md-nav__link> Week 2 </a> <nav class=md-nav aria-label="Week 2"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#multiple-featuresvariables-linear-regression class=md-nav__link> Multiple features(variables) linear regression </a> </li> <li class=md-nav__item> <a href=#feature-scaling class=md-nav__link> Feature scaling </a> </li> <li class=md-nav__item> <a href=#learning-rate class=md-nav__link> Learning rate </a> </li> <li class=md-nav__item> <a href=#polynomial-regression class=md-nav__link> Polynomial Regression </a> </li> <li class=md-nav__item> <a href=#normal-equation class=md-nav__link> Normal Equation </a> </li> <li class=md-nav__item> <a href=#normal-equation-and-non-invertibility class=md-nav__link> Normal equation and non-invertibility </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#week-3 class=md-nav__link> Week 3 </a> <nav class=md-nav aria-label="Week 3"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#logistic-regression-model class=md-nav__link> Logistic regression model </a> <nav class=md-nav aria-label="Logistic regression model"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#classification-problem class=md-nav__link> Classification problem </a> </li> <li class=md-nav__item> <a href=#hypothesis-representation class=md-nav__link> Hypothesis representation </a> </li> <li class=md-nav__item> <a href=#interpretation class=md-nav__link> Interpretation </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#logistic-regression-decision-boundary class=md-nav__link> Logistic regression decision boundary </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#logistic-regression-cost-function class=md-nav__link> Logistic regression cost function </a> <nav class=md-nav aria-label="Logistic regression cost function"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#problem-description class=md-nav__link> Problem description </a> </li> <li class=md-nav__item> <a href=#logistic-regression-cost-function_1 class=md-nav__link> Logistic Regression Cost Function </a> </li> <li class=md-nav__item> <a href=#gradient-descent-and-cost-function class=md-nav__link> Gradient descent and cost function </a> <nav class=md-nav aria-label="Gradient descent and cost function"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#gradient-descent-for-logistic-regression class=md-nav__link> Gradient Descent for logistic regression </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#advanced-optimization class=md-nav__link> Advanced Optimization </a> <nav class=md-nav aria-label="Advanced Optimization"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#function-fminunc class=md-nav__link> Function fminunc() </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#multiclass-classification class=md-nav__link> Multiclass classification </a> </li> <li class=md-nav__item> <a href=#regularization class=md-nav__link> Regularization </a> <nav class=md-nav aria-label=Regularization> <ul class=md-nav__list> <li class=md-nav__item> <a href=#address-overfitting-problems class=md-nav__link> Address overfitting problems </a> </li> <li class=md-nav__item> <a href=#regularization-intuition class=md-nav__link> Regularization Intuition </a> </li> <li class=md-nav__item> <a href=#regularized-linear-regression class=md-nav__link> Regularized linear regression </a> </li> <li class=md-nav__item> <a href=#regularized-logistic-regression class=md-nav__link> Regularized logistic regression </a> </li> <li class=md-nav__item> <a href=#advanced-optimization_1 class=md-nav__link> Advanced optimization </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#week-4-neural-networks-model class=md-nav__link> Week 4 Neural networks model </a> <nav class=md-nav aria-label="Week 4 Neural networks model"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#model-representation class=md-nav__link> Model representation </a> </li> <li class=md-nav__item> <a href=#neural-network class=md-nav__link> Neural Network </a> </li> <li class=md-nav__item> <a href=#forward-propagation-implementation class=md-nav__link> Forward propagation implementation </a> </li> <li class=md-nav__item> <a href=#learning-its-own-features class=md-nav__link> Learning its own features </a> </li> <li class=md-nav__item> <a href=#xnor-example class=md-nav__link> XNOR example </a> </li> <li class=md-nav__item> <a href=#multiclass-classification_1 class=md-nav__link> Multiclass classification </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../cs224n/lec-notes/ class=md-nav__link> CS224N Lecture Notes </a> </li> <li class=md-nav__item> <a href=../../cs224n/write-up/ class=md-nav__link> CS224N Write-up </a> </li> <li class=md-nav__item> <a href=../../coursera-dl4-cnn/notes/ class=md-nav__link> Convolutional Neural Networks </a> </li> <li class=md-nav__item> <a href=../../mining-massive-datasets/notes/ class=md-nav__link> Mining Massive Data Sets </a> </li> <li class=md-nav__item> <a href=../../6.431-probability/notes/ class=md-nav__link> 6.431 Probability </a> </li> <li class=md-nav__item> <a href=../../learning-from-data/notes.md class=md-nav__link> Learning From Data </a> </li> <li class=md-nav__item> <a href=../../9chap-system-design/notes/ class=md-nav__link> Nine Chapter System Design </a> </li> <li class=md-nav__item> <a href=../../9chap-dynamic-prog/notes/ class=md-nav__link> Nine Chapter Dynamic Programming </a> </li> <li class=md-nav__item> <a href=../../func-prog-in-scala/notes/ class=md-nav__link> Functional Programming Principles in Scala </a> </li> <li class=md-nav__item> <a href=../../applied-scrum-for-agile/notes/ class=md-nav__link> Applied Scrum for Agile Project Management </a> </li> <li class=md-nav__item> <a href=../../concurrent-prog-java/notes/ class=md-nav__link> Concurrent Programming in Java </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-3 type=checkbox id=nav-3> <label class=md-nav__link for=nav-3> Leetcode <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Leetcode data-md-level=1> <label class=md-nav__title for=nav-3> <span class="md-nav__icon md-icon"></span> Leetcode </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../leetcode/ class=md-nav__link> Index </a> </li> <li class=md-nav__item> <a href=../../../leetcode/array/notes/ class=md-nav__link> Array </a> </li> <li class=md-nav__item> <a href=../../../leetcode/backtracking/notes/ class=md-nav__link> Backtracking </a> </li> <li class=md-nav__item> <a href=../../../leetcode/binary-search/notes/ class=md-nav__link> Binary Search </a> </li> <li class=md-nav__item> <a href=../../../leetcode/breadth-first-search/notes/ class=md-nav__link> Breadth-First Search (BFS) </a> </li> <li class=md-nav__item> <a href=../../../leetcode/depth-first-search/notes/ class=md-nav__link> Depth-First Search (DFS) </a> </li> <li class=md-nav__item> <a href=../../../leetcode/dynamic-programming/notes/ class=md-nav__link> Dynamic Programming </a> </li> <li class=md-nav__item> <a href=../../../leetcode/graph/notes/ class=md-nav__link> Graph </a> </li> <li class=md-nav__item> <a href=../../../leetcode/heap/notes/ class=md-nav__link> Heap </a> </li> <li class=md-nav__item> <a href=../../../leetcode/interval/notes/ class=md-nav__link> Interval </a> </li> <li class=md-nav__item> <a href=../../../leetcode/linked-list/notes/ class=md-nav__link> Linked List </a> </li> <li class=md-nav__item> <a href=../../../leetcode/math/notes/ class=md-nav__link> Math </a> </li> <li class=md-nav__item> <a href=../../../leetcode/stack/notes/ class=md-nav__link> Stack </a> </li> <li class=md-nav__item> <a href=../../../leetcode/string/notes/ class=md-nav__link> String </a> </li> <li class=md-nav__item> <a href=../../../leetcode/tree/notes/ class=md-nav__link> Tree </a> </li> <li class=md-nav__item> <a href=../../../leetcode/trie/notes/ class=md-nav__link> Trie </a> </li> <li class=md-nav__item> <a href=../../../leetcode/union-find/notes/ class=md-nav__link> Union Find </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-4 type=checkbox id=nav-4> <label class=md-nav__link for=nav-4> Research <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Research data-md-level=1> <label class=md-nav__title for=nav-4> <span class="md-nav__icon md-icon"></span> Research </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../research/ class=md-nav__link> Index </a> </li> <li class=md-nav__item> <a href=../../../research/paper-reading/notes/ class=md-nav__link> Paper Reading </a> </li> <li class=md-nav__item> <a href=../../../research/coalition-game/notes/ class=md-nav__link> Coalition Game </a> </li> <li class=md-nav__item> <a href=../../../research/contextual-bandit/notes/ class=md-nav__link> Contextual Multi-Armed Bandit </a> </li> <li class=md-nav__item> <a href=../../../research/tfidf-score/notes/ class=md-nav__link> TF-IDF for Information Retrieval </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-5 type=checkbox id=nav-5> <label class=md-nav__link for=nav-5> SEED Labs <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="SEED Labs" data-md-level=1> <label class=md-nav__title for=nav-5> <span class="md-nav__icon md-icon"></span> SEED Labs </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../seedlabs/ class=md-nav__link> Index </a> </li> <li class=md-nav__item> <a href=../../../seedlabs/applied-crypto/notes/ class=md-nav__link> Applied Cryptograph Notes </a> </li> <li class=md-nav__item> <a href=../../../seedlabs/public-key-cryptography-and-pki/notes/ class=md-nav__link> Public Key Cryptography and PKI </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-6 type=checkbox id=nav-6> <label class=md-nav__link for=nav-6> System Design <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="System Design" data-md-level=1> <label class=md-nav__title for=nav-6> <span class="md-nav__icon md-icon"></span> System Design </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../system-design/ class=md-nav__link> Index </a> </li> <li class=md-nav__item> <a href=../../../system-design/browser/notes.md class=md-nav__link> How Browser Works </a> </li> <li class=md-nav__item> <a href=../../../system-design/concurrency/notes/ class=md-nav__link> Concurrency and Synchronization </a> </li> <li class=md-nav__item> <a href=../../../system-design/concepts/notes/ class=md-nav__link> Distributed System Concepts </a> </li> <li class=md-nav__item> <a href=../../../system-design/patterns/notes/ class=md-nav__link> Design Patterns </a> </li> <li class=md-nav__item> <a href=../../../system-design/problems/tinyurl/notes/ class=md-nav__link> How to Design TinyUrl </a> </li> <li class=md-nav__item> <a href=../../../system-design/problems/twitter/notes/ class=md-nav__link> How to Design Twitter </a> </li> <li class=md-nav__item> <a href=../../../system-design/problems/ticketmaster/notes/ class=md-nav__link> How to Design Ticketmaster </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=#week-1 class=md-nav__link> Week 1 </a> <nav class=md-nav aria-label="Week 1"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#introduction class=md-nav__link> Introduction </a> </li> <li class=md-nav__item> <a href=#cost-function class=md-nav__link> Cost Function </a> </li> <li class=md-nav__item> <a href=#cost-function-intuition-i class=md-nav__link> Cost Function Intuition I </a> </li> <li class=md-nav__item> <a href=#cost-function-intuition-ii class=md-nav__link> Cost Function Intuition II </a> </li> <li class=md-nav__item> <a href=#gradient-descent class=md-nav__link> Gradient Descent </a> </li> <li class=md-nav__item> <a href=#gradient-descent-intuition class=md-nav__link> Gradient Descent Intuition </a> </li> <li class=md-nav__item> <a href=#gradient-descent-and-linear-regression class=md-nav__link> Gradient Descent and Linear Regression </a> </li> <li class=md-nav__item> <a href=#linear-algebra-review class=md-nav__link> Linear Algebra Review </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#week-2 class=md-nav__link> Week 2 </a> <nav class=md-nav aria-label="Week 2"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#multiple-featuresvariables-linear-regression class=md-nav__link> Multiple features(variables) linear regression </a> </li> <li class=md-nav__item> <a href=#feature-scaling class=md-nav__link> Feature scaling </a> </li> <li class=md-nav__item> <a href=#learning-rate class=md-nav__link> Learning rate </a> </li> <li class=md-nav__item> <a href=#polynomial-regression class=md-nav__link> Polynomial Regression </a> </li> <li class=md-nav__item> <a href=#normal-equation class=md-nav__link> Normal Equation </a> </li> <li class=md-nav__item> <a href=#normal-equation-and-non-invertibility class=md-nav__link> Normal equation and non-invertibility </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#week-3 class=md-nav__link> Week 3 </a> <nav class=md-nav aria-label="Week 3"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#logistic-regression-model class=md-nav__link> Logistic regression model </a> <nav class=md-nav aria-label="Logistic regression model"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#classification-problem class=md-nav__link> Classification problem </a> </li> <li class=md-nav__item> <a href=#hypothesis-representation class=md-nav__link> Hypothesis representation </a> </li> <li class=md-nav__item> <a href=#interpretation class=md-nav__link> Interpretation </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#logistic-regression-decision-boundary class=md-nav__link> Logistic regression decision boundary </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#logistic-regression-cost-function class=md-nav__link> Logistic regression cost function </a> <nav class=md-nav aria-label="Logistic regression cost function"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#problem-description class=md-nav__link> Problem description </a> </li> <li class=md-nav__item> <a href=#logistic-regression-cost-function_1 class=md-nav__link> Logistic Regression Cost Function </a> </li> <li class=md-nav__item> <a href=#gradient-descent-and-cost-function class=md-nav__link> Gradient descent and cost function </a> <nav class=md-nav aria-label="Gradient descent and cost function"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#gradient-descent-for-logistic-regression class=md-nav__link> Gradient Descent for logistic regression </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#advanced-optimization class=md-nav__link> Advanced Optimization </a> <nav class=md-nav aria-label="Advanced Optimization"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#function-fminunc class=md-nav__link> Function fminunc() </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#multiclass-classification class=md-nav__link> Multiclass classification </a> </li> <li class=md-nav__item> <a href=#regularization class=md-nav__link> Regularization </a> <nav class=md-nav aria-label=Regularization> <ul class=md-nav__list> <li class=md-nav__item> <a href=#address-overfitting-problems class=md-nav__link> Address overfitting problems </a> </li> <li class=md-nav__item> <a href=#regularization-intuition class=md-nav__link> Regularization Intuition </a> </li> <li class=md-nav__item> <a href=#regularized-linear-regression class=md-nav__link> Regularized linear regression </a> </li> <li class=md-nav__item> <a href=#regularized-logistic-regression class=md-nav__link> Regularized logistic regression </a> </li> <li class=md-nav__item> <a href=#advanced-optimization_1 class=md-nav__link> Advanced optimization </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#week-4-neural-networks-model class=md-nav__link> Week 4 Neural networks model </a> <nav class=md-nav aria-label="Week 4 Neural networks model"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#model-representation class=md-nav__link> Model representation </a> </li> <li class=md-nav__item> <a href=#neural-network class=md-nav__link> Neural Network </a> </li> <li class=md-nav__item> <a href=#forward-propagation-implementation class=md-nav__link> Forward propagation implementation </a> </li> <li class=md-nav__item> <a href=#learning-its-own-features class=md-nav__link> Learning its own features </a> </li> <li class=md-nav__item> <a href=#xnor-example class=md-nav__link> XNOR example </a> </li> <li class=md-nav__item> <a href=#multiclass-classification_1 class=md-nav__link> Multiclass classification </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class=md-content> <article class="md-content__inner md-typeset"> <h1 id=machine-learning-coursera>Machine Learning (Coursera)<a class=headerlink href=#machine-learning-coursera title="Permanent link">&para;</a></h1> <h2 id=week-1>Week 1<a class=headerlink href=#week-1 title="Permanent link">&para;</a></h2> <h3 id=introduction>Introduction<a class=headerlink href=#introduction title="Permanent link">&para;</a></h3> <div class="admonition qoute"> <p class=admonition-title>Qoute</p> <p>Machine Learning is a field of study that gives computers the ability to learn without being explicitly programmed. -- Arthur Samuel (1959)</p> </div> <div class="admonition qoute"> <p class=admonition-title>Qoute</p> <p>A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E. -- Tom Mitchell (1998)</p> </div> <ul> <li>Well-posed Learning Problem<ul> <li>Experience <strong>E</strong></li> <li>Task <strong>T</strong></li> <li>Performance <strong>P</strong></li> </ul> </li> <li>Supervised learning: give the "right answer" of existing</li> <li>Unsupervised learning</li> <li>Reinforcement learning</li> <li>Recommendation system</li> <li> <p><strong>Regression V.S. classification</strong></p> <ol> <li>Regression, predict a continuous value across range<ul> <li>a function of independent variable</li> </ul> </li> <li>Classification, predict a discrete value</li> </ol> </li> </ul> <h3 id=cost-function>Cost Function<a class=headerlink href=#cost-function title="Permanent link">&para;</a></h3> <ul> <li>Cost function (squared error function)</li> </ul> <div> <div class=MathJax_Preview>J(\theta_0, \theta_1) = \frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})^2</div> <script type="math/tex; mode=display">J(\theta_0, \theta_1) = \frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})^2</script> </div> <ul> <li>Goal</li> </ul> <div> <div class=MathJax_Preview>\underset{\theta_0, \theta_1}{\text{minimize}} J(\theta_0, \theta_1)</div> <script type="math/tex; mode=display">\underset{\theta_0, \theta_1}{\text{minimize}} J(\theta_0, \theta_1)</script> </div> <ul> <li> <p>Some Intuitions</p> <ol> <li>when the sample is <code>{1, 1}, {2, 2}, and {3, 3}</code>. <span><span class=MathJax_Preview>h_\theta(x) = x, (\theta_0 = 0, \theta_1 = 1)</span><script type=math/tex>h_\theta(x) =  x, (\theta_0 = 0, \theta_1 = 1)</script></span>, we can calculate <span><span class=MathJax_Preview>J(\theta_0, \theta_1) = 0.</span><script type=math/tex>J(\theta_0, \theta_1) = 0.</script></span></li> <li>when the sample is <code>{1, 1}, {2, 2}, and {3, 3}</code>. <span><span class=MathJax_Preview>h_\theta(x) = 0.5 x, (\theta_0 = 0, \theta_1 = 0.5)</span><script type=math/tex>h_\theta(x) =  0.5 x, (\theta_0 = 0, \theta_1 = 0.5)</script></span>, we can calculate <span><span class=MathJax_Preview>J(\theta_1) = \frac{3.5}{6}.</span><script type=math/tex>J(\theta_1) = \frac{3.5}{6}.</script></span></li> <li>we can plot <span><span class=MathJax_Preview>J(\theta_1)</span><script type=math/tex>J(\theta_1)</script></span> vs. <span><span class=MathJax_Preview>\theta_1</span><script type=math/tex>\theta_1</script></span>, the function plotted would be a quadratic curve.</li> </ol> </li> </ul> <h3 id=cost-function-intuition-i>Cost Function Intuition I<a class=headerlink href=#cost-function-intuition-i title="Permanent link">&para;</a></h3> <ul> <li>Andrew plot the hypothesis <span><span class=MathJax_Preview>\textstyle h_\theta(x)</span><script type=math/tex>\textstyle h_\theta(x)</script></span> and the cost function <span><span class=MathJax_Preview>\textstyle {J(\theta_1)}</span><script type=math/tex>\textstyle {J(\theta_1)}</script></span>（setting <span><span class=MathJax_Preview>\theta_0 = 0</span><script type=math/tex>\theta_0 = 0</script></span>）, The function <span><span class=MathJax_Preview>\textstyle J(\theta_1)</span><script type=math/tex>\textstyle J(\theta_1)</script></span> is a curve have a minimum value.</li> </ul> <h3 id=cost-function-intuition-ii>Cost Function Intuition II<a class=headerlink href=#cost-function-intuition-ii title="Permanent link">&para;</a></h3> <ul> <li> <p>The plot of <span><span class=MathJax_Preview>h(x)</span><script type=math/tex>h(x)</script></span> and <span><span class=MathJax_Preview>J(\theta_0, \theta_1)</span><script type=math/tex>J(\theta_0, \theta_1)</script></span> show that <span><span class=MathJax_Preview>J(\theta_0, \theta_1)</span><script type=math/tex>J(\theta_0, \theta_1)</script></span> has to be contour plot.</p> </li> <li> <p>Our goal is to <span><span class=MathJax_Preview>\min_{\theta_0, \theta_1}J(\theta_0, \theta_1)</span><script type=math/tex>\min_{\theta_0, \theta_1}J(\theta_0, \theta_1)</script></span></p> </li> </ul> <p>To summarize, in this lecture, we formulate the hypothesis function and defined the cost function. We also plotted them to get some insight into the two functions. The machine learning problem we learn from this lecture is a minimization problem. Given a hypothesis, we looking for the <span><span class=MathJax_Preview>\theta_0</span><script type=math/tex>\theta_0</script></span> and <span><span class=MathJax_Preview>\theta_1</span><script type=math/tex>\theta_1</script></span> which minimize the cost function. Then we solve the problem. We use the Gradient Descent algorithm to search this minimum value.</p> <h3 id=gradient-descent>Gradient Descent<a class=headerlink href=#gradient-descent title="Permanent link">&para;</a></h3> <ul> <li>It could solve general minimization problems<ul> <li>Given function <span><span class=MathJax_Preview>J(\theta_0, \theta_1)</span><script type=math/tex>J(\theta_0, \theta_1)</script></span></li> <li>Objective: <span><span class=MathJax_Preview>\min_{\theta_0, \theta_1}J(\theta_0, \theta_1)</span><script type=math/tex>\min_{\theta_0, \theta_1}J(\theta_0, \theta_1)</script></span></li> </ul> </li> </ul> <div class="admonition example"> <p class=admonition-title>Gradient Descent Algorithm</p> <ul> <li>Start with some <span><span class=MathJax_Preview>\theta_0, \theta_1</span><script type=math/tex>\theta_0, \theta_1</script></span><ul> <li>Keep changing <span><span class=MathJax_Preview>\theta_0, \theta_1</span><script type=math/tex>\theta_0, \theta_1</script></span> to reduce <span><span class=MathJax_Preview>J(\theta_0, \theta_1)</span><script type=math/tex>J(\theta_0, \theta_1)</script></span><ul> <li>Simultianeously update: <span><span class=MathJax_Preview>\theta_j := \theta_j - \alpha \frac{\partial }{\partial \theta_j}J(\theta_0, \theta_1)</span><script type=math/tex>\theta_j := \theta_j - \alpha \frac{\partial }{\partial \theta_j}J(\theta_0, \theta_1)</script></span>, for <span><span class=MathJax_Preview>( j = 0, j = 1)</span><script type=math/tex>( j = 0, j = 1)</script></span></li> </ul> </li> </ul> </li> <li>Untill we hopefully end up at the minimum.(convergence)</li> </ul> </div> <ul> <li>Question: How to implement the algorithm.</li> </ul> <h3 id=gradient-descent-intuition>Gradient Descent Intuition<a class=headerlink href=#gradient-descent-intuition title="Permanent link">&para;</a></h3> <ul> <li>learning rate<ol> <li>don't have to adjust the learning rate</li> </ol> </li> <li>derivative<ol> <li>would reduce automatically</li> </ol> </li> </ul> <h3 id=gradient-descent-and-linear-regression>Gradient Descent and Linear Regression<a class=headerlink href=#gradient-descent-and-linear-regression title="Permanent link">&para;</a></h3> <ul> <li>Take the linear regression cost function and apply gradient descent algorithm.</li> <li>Model: <span><span class=MathJax_Preview>h_\theta(x) = \theta_0 + \theta_1x</span><script type=math/tex>h_\theta(x) = \theta_0 + \theta_1x</script></span></li> <li>Cost function: <span><span class=MathJax_Preview>J(\theta_0, \theta_1) = \frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})^2</span><script type=math/tex>J(\theta_0, \theta_1) = \frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})^2</script></span></li> <li>Gradient Descent in Linear Regression.</li> </ul> <p>repeat <span><span class=MathJax_Preview>\begin{Bmatrix} \theta_0 := \theta_0 - \alpha \frac{1}{m} \sum_{i = 1}^{m}(h_\theta(x^{(i)}) - y^{(i)}) \\ \theta_1 := \theta_1 - \alpha \frac{1}{m} \sum_{i = 1}^{m}(h_\theta(x^{(i)}) - y^{(i)}) x^{(i)} \end{Bmatrix}</span><script type=math/tex>\begin{Bmatrix}
\theta_0 := \theta_0 - \alpha \frac{1}{m} \sum_{i = 1}^{m}(h_\theta(x^{(i)}) - y^{(i)})  \\
\theta_1 := \theta_1 - \alpha \frac{1}{m} \sum_{i = 1}^{m}(h_\theta(x^{(i)}) - y^{(i)}) x^{(i)}
\end{Bmatrix}</script></span> simultaneously.</p> <ul> <li>The linear regression cost function is always a <strong>convex function</strong> - always has a single minimum (bowl Shaped).</li> </ul> <h3 id=linear-algebra-review>Linear Algebra Review<a class=headerlink href=#linear-algebra-review title="Permanent link">&para;</a></h3> <ol> <li><strong>Not commutative:</strong> A x B != B x A except that matrix B is identity matrix.</li> <li><strong>Associativity:</strong> (A x B) x C = A x (B x C)</li> <li><strong>Identity Matrix:</strong><ul> <li>Any matrix <strong>A</strong> which can be multiplied by an identity matrix gives you matrix <strong>A</strong> back</li> <li>I[m, m] x A[m, n] = A[m, n]</li> <li>A[m, n] x I[n, n] = A[m,n]</li> </ul> </li> <li><strong>Inverse</strong><ul> <li>Multiplicative inverse(reciprocal): a number multiply it by to get the identify element. i.e. if you have <code>x</code>, <code>x * 1/x = 1</code>.</li> <li>Additive inverse: -3 is 3's additive inverse.</li> </ul> </li> <li><strong>Matrix inverse</strong><ul> <li><span><span class=MathJax_Preview>A \dot A^{-1} = I</span><script type=math/tex>A \dot A^{-1} = I</script></span></li> <li>Only square matrix, but not all square matrix have inverse.</li> </ul> </li> <li><strong>singularity</strong><ul> <li>If A is all zeros then there is no inverse matrix</li> <li>Some others don\'t, intuition should be matrices that don\'t have an inverse are a <strong>singular matrix</strong> or a <strong>degenerate matrix</strong> (i.e. when it\'s too close to 0)</li> <li>So if all the values of a matrix reach zero, this can be described as reaching <strong>singularity</strong></li> </ul> </li> </ol> <h2 id=week-2>Week 2<a class=headerlink href=#week-2 title="Permanent link">&para;</a></h2> <h3 id=multiple-featuresvariables-linear-regression>Multiple features(variables) linear regression<a class=headerlink href=#multiple-featuresvariables-linear-regression title="Permanent link">&para;</a></h3> <ol> <li>From feature <span><span class=MathJax_Preview>x^{(i)}</span><script type=math/tex>x^{(i)}</script></span>to features <span><span class=MathJax_Preview>x_j^{(i)}</span><script type=math/tex>x_j^{(i)}</script></span>, means the <span><span class=MathJax_Preview>j</span><script type=math/tex>j</script></span>th feature element of the <span><span class=MathJax_Preview>i</span><script type=math/tex>i</script></span>th feature vector.</li> <li>Hypothesis becomes: <span><span class=MathJax_Preview>\textstyle h_\theta(x) = \theta^{T}x = \theta_0x_0 + \theta_1x_1 + ... + \theta_nx_n</span><script type=math/tex>\textstyle h_\theta(x) = \theta^{T}x = \theta_0x_0 + \theta_1x_1 + ... + \theta_nx_n</script></span>.</li> <li>Cost Function: <span><span class=MathJax_Preview>J(\theta_0, \theta_1, ..., \theta_n) = \frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})^2</span><script type=math/tex>J(\theta_0, \theta_1, ..., \theta_n) = \frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})^2</script></span></li> <li>Gradient descent: <span><span class=MathJax_Preview>\theta_j := \theta_j - \alpha \frac{\partial }{\partial \theta_j}J(\theta_0, \theta_1, ..., \theta_n)</span><script type=math/tex>\theta_j := \theta_j - \alpha \frac{\partial }{\partial \theta_j}J(\theta_0, \theta_1, ..., \theta_n)</script></span>, simultaneously update for every <span><span class=MathJax_Preview>j = 0, 1, 2, ..., n</span><script type=math/tex>j = 0, 1, 2, ..., n</script></span>.</li> <li>Compare of Gradient descent for multiple variables: <img alt="Gradient Descent" src=../fig/mutivariable_gradient_descent.png></li> </ol> <h3 id=feature-scaling>Feature scaling<a class=headerlink href=#feature-scaling title="Permanent link">&para;</a></h3> <p>One of the idea to improve the result in gradient descent is to make different features in the same scale, i.e. <span><span class=MathJax_Preview>-1 \leq x_i \leq 1</span><script type=math/tex>-1 \leq x_i \leq 1</script></span> range .</p> <p>For example, in the housing price prediction problem, we have to make the size of the house (0-2000 square feet), and the number of rooms in the hourse (1-5 rooms) in the same scale. We do this by the method of <strong>mean normalization.</strong></p> <ul> <li><strong>Mean normalization</strong> Replace <span><span class=MathJax_Preview>x_i</span><script type=math/tex>x_i</script></span> with <span><span class=MathJax_Preview>x_i - \mu_i</span><script type=math/tex>x_i - \mu_i</script></span> to make features have approximately zero mean (Do not apply to <span><span class=MathJax_Preview>x_0 = 1</span><script type=math/tex>x_0 = 1</script></span>)<ul> <li>E.g. <span><span class=MathJax_Preview>x_1 = \frac{size - 1000}{2000}</span><script type=math/tex>x_1 = \frac{size - 1000}{2000}</script></span>, <span><span class=MathJax_Preview>x_2 = \frac{\#rooms - 2}{5(range)}</span><script type=math/tex>x_2 = \frac{\#rooms - 2}{5(range)}</script></span> to make <span><span class=MathJax_Preview>-0.5 \le x_1 \le 0.5, -0.5 \le x_2 \le 0.5,</span><script type=math/tex>-0.5 \le x_1 \le 0.5, -0.5 \le x_2 \le 0.5,</script></span></li> </ul> </li> </ul> <h3 id=learning-rate>Learning rate<a class=headerlink href=#learning-rate title="Permanent link">&para;</a></h3> <p>Another idea to improve gradient descent algorithm is to select the learning rate <span><span class=MathJax_Preview>\alpha</span><script type=math/tex>\alpha</script></span>to make the algorithm works correctly.</p> <ul> <li><strong>Convergence test:</strong> we can declare convergence if <span><span class=MathJax_Preview>J(\theta)</span><script type=math/tex>J(\theta)</script></span> decreases by less than <span><span class=MathJax_Preview>10^{-3}</span><script type=math/tex>10^{-3}</script></span>in one iteration.</li> </ul> <p>For sufficient small <span><span class=MathJax_Preview>\alpha</span><script type=math/tex>\alpha</script></span>, <span><span class=MathJax_Preview>J(\theta)</span><script type=math/tex>J(\theta)</script></span> should decrease on every iteration, but if <span><span class=MathJax_Preview>\alpha</span><script type=math/tex>\alpha</script></span> is too small, gradient descent can be slow to converge. If <span><span class=MathJax_Preview>\alpha</span><script type=math/tex>\alpha</script></span> is too large: <span><span class=MathJax_Preview>J(\theta)</span><script type=math/tex>J(\theta)</script></span> may not decrease on every iteration, may not converge.</p> <ul> <li><strong>Rule of Thumb:</strong> to choose <span><span class=MathJax_Preview>\alpha</span><script type=math/tex>\alpha</script></span>, try these numbers <code>..., 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, ...</code></li> </ul> <h3 id=polynomial-regression>Polynomial Regression<a class=headerlink href=#polynomial-regression title="Permanent link">&para;</a></h3> <p>In the house price prediction proble. if two features selected are frontage and depth. we can also take the polynomial problem, take the area as a single feature, thusly reduce the problem to a linear regression. We could even take the higher polynomials of the size feature, like second order, third order, and so on. for example,</p> <p><span><span class=MathJax_Preview>h_\theta = \theta_0 + \theta_1x_1 + \theta_2x_2 + \theta_3x_3 = \theta_0 + \theta_1(size) + \theta_2(size)^2 + \theta_3(size)^3</span><script type=math/tex>h_\theta = \theta_0 + \theta_1x_1 + \theta_2x_2 + \theta_3x_3 = \theta_0 + \theta_1(size) + \theta_2(size)^2 + \theta_3(size)^3</script></span></p> <p>In this case, the polynomial regression problem will fit in a curve instead of a line.</p> <h3 id=normal-equation>Normal Equation<a class=headerlink href=#normal-equation title="Permanent link">&para;</a></h3> <p>Normal Equation: Method to solve for θ analytically. The intuition comes that we can mathematically sovle <span><span class=MathJax_Preview>\frac{\partial}{\partial\theta_j}J(\Theta) = 0</span><script type=math/tex>\frac{\partial}{\partial\theta_j}J(\Theta) = 0</script></span> for <span><span class=MathJax_Preview>\theta_0, \theta_1, \dots, \theta_n</span><script type=math/tex>\theta_0, \theta_1, \dots, \theta_n</script></span>, given the <span><span class=MathJax_Preview>J(\Theta)</span><script type=math/tex>J(\Theta)</script></span>.</p> <p>The equation to get the value of <span><span class=MathJax_Preview>\Theta</span><script type=math/tex>\Theta</script></span> is <span><span class=MathJax_Preview>\Theta = (X^TX)^{-1}X^Ty</span><script type=math/tex>\Theta = (X^TX)^{-1}X^Ty</script></span>, in Octave, you can calculate by the command <code>pinv(X'*X)*X'*y)</code>, pay attention to the dimention of the the matrix <code>X</code> and <code>y</code>.</p> <h3 id=normal-equation-and-non-invertibility>Normal equation and non-invertibility<a class=headerlink href=#normal-equation-and-non-invertibility title="Permanent link">&para;</a></h3> <p>TBD</p> <h2 id=week-3>Week 3<a class=headerlink href=#week-3 title="Permanent link">&para;</a></h2> <h3 id=logistic-regression-model>Logistic regression model<a class=headerlink href=#logistic-regression-model title="Permanent link">&para;</a></h3> <h4 id=classification-problem>Classification problem<a class=headerlink href=#classification-problem title="Permanent link">&para;</a></h4> <ul> <li>Email: spam / Not spam?</li> <li>Tumor: Malignant / Benign?</li> <li> <p>Example:</p> <p><img alt="Tumor Example" src=../fig/tumor-example.png></p> </li> <li> <p>y is either 0 or 1</p> </li> <li>0: Negative class</li> <li>1: Positive class.</li> <li>With linear regression and a threshold value, we can use linear regression to solve classification problem.</li> <li>However linear regression could result in the case when <span><span class=MathJax_Preview>h_\theta(x)</span><script type=math/tex>h_\theta(x)</script></span> is &gt; 1 or &lt; 0.</li> <li>Need a different method which will make <span><span class=MathJax_Preview>0 \le h_\theta(x) \le 1</span><script type=math/tex>0 \le h_\theta(x) \le 1</script></span>. This is why logistic regression comes in.</li> </ul> <h4 id=hypothesis-representation>Hypothesis representation<a class=headerlink href=#hypothesis-representation title="Permanent link">&para;</a></h4> <p>Because we want to have <span><span class=MathJax_Preview>0 \le h_\theta(x) \le 1</span><script type=math/tex>0 \le h_\theta(x) \le 1</script></span>, the domain of sigmoid function is in the range. From linear regression <span><span class=MathJax_Preview>h_\theta(x) = \theta^Tx</span><script type=math/tex>h_\theta(x) = \theta^Tx</script></span>, we can have <span><span class=MathJax_Preview>h_\theta(x)</span><script type=math/tex>h_\theta(x)</script></span> for logistic regression:</p> <div> <div class=MathJax_Preview>h_{\theta}(x) = g_\theta(\theta^Tx) = \frac{1}{1 + e^{-\theta^Tx}}, 0 \le h_\theta(x) \le 1</div> <script type="math/tex; mode=display">h_{\theta}(x) = g_\theta(\theta^Tx) = \frac{1}{1 + e^{-\theta^Tx}}, 0 \le h_\theta(x) \le 1</script> </div> <p><span><span class=MathJax_Preview>g_\theta(z) = \frac{1}{1 + e^{-z}}</span><script type=math/tex>g_\theta(z) = \frac{1}{1 + e^{-z}}</script></span> is <a href=https://en.wikipedia.org/wiki/Sigmoid_function>Sigmoid function</a>, also called logistic function. now in logistic regress model, we can write</p> <div> <div class=MathJax_Preview>h_\theta(x) = \frac{1}{1 + e^{-\theta^Tx}}.</div> <script type="math/tex; mode=display">h_\theta(x) = \frac{1}{1 + e^{-\theta^Tx}}.</script> </div> <h4 id=interpretation>Interpretation<a class=headerlink href=#interpretation title="Permanent link">&para;</a></h4> <ul> <li>Logistic regression model <span><span class=MathJax_Preview>h_\theta(x) = \frac{1}{1 + e^{-\theta^Tx}},</span><script type=math/tex>h_\theta(x) = \frac{1}{1 + e^{-\theta^Tx}},</script></span> is the estimated probability that y = 1 on input x. i.e. <span><span class=MathJax_Preview>h_\theta(x) = 0.7</span><script type=math/tex>h_\theta(x) = 0.7</script></span> tell patient that 70% chance of tumore being malignant.</li> <li><span><span class=MathJax_Preview>h_\theta(x) = P(y = 1|x;\theta)</span><script type=math/tex>h_\theta(x) = P(y = 1|x;\theta)</script></span>,</li> <li><span><span class=MathJax_Preview>P(y=0|x; \theta) + P(y=1|x;\theta) = 1</span><script type=math/tex>P(y=0|x; \theta) + P(y=1|x;\theta) = 1</script></span></li> </ul> <h3 id=logistic-regression-decision-boundary>Logistic regression decision boundary<a class=headerlink href=#logistic-regression-decision-boundary title="Permanent link">&para;</a></h3> <ol> <li>Gives a better sense of what the hypothesis function is computing</li> <li>Better understanding of what the hypothesis function looks like</li> <li>One way of using the sigmoid function is:<ul> <li>When the probability of y being 1 is greater than 0.5 then we can predict y = 1</li> <li>Else we predict y = 0</li> </ul> </li> <li>When is it exactly that <span><span class=MathJax_Preview>h_\theta(x)</span><script type=math/tex>h_\theta(x)</script></span> is greater than 0.5?<ul> <li>Look at sigmoid function<ul> <li>g(z) is greater than or equal to 0.5 when z is greater than or equal to 0.</li> </ul> </li> <li>So if z is positive, g(z) is greater than 0.5<ul> <li><span><span class=MathJax_Preview>z = (\theta^T x)</span><script type=math/tex>z = (\theta^T x)</script></span></li> </ul> </li> <li>So when<ul> <li><span><span class=MathJax_Preview>\theta^T x &gt;= 0</span><script type=math/tex>\theta^T x >= 0</script></span></li> </ul> </li> <li>Then <span><span class=MathJax_Preview>h_\theta(x) &gt;= 0.5</span><script type=math/tex>h_\theta(x) >= 0.5</script></span></li> </ul> </li> <li>So what we've shown is that the hypothesis predicts y = 1 when <span><span class=MathJax_Preview>\theta^T x &gt;= 0</span><script type=math/tex>\theta^T x >= 0</script></span><ul> <li>The corollary of that when <span><span class=MathJax_Preview>\theta^T x &lt;= 0</span><script type=math/tex>\theta^T x <= 0</script></span> then the hypothesis predicts y = 0</li> <li>Let's use this to better understand how the hypothesis makes its predictions</li> </ul> </li> <li>Linear Decision boundary and non-linear decision boundary <img alt="Linear Decision Boundary" src=../fig/linear-decision-boundary.png> <img alt="Non Linear Decision Boundary" src=../fig/non-linear-decision-boundary.png></li> </ol> <h2 id=logistic-regression-cost-function>Logistic regression cost function<a class=headerlink href=#logistic-regression-cost-function title="Permanent link">&para;</a></h2> <h3 id=problem-description>Problem description<a class=headerlink href=#problem-description title="Permanent link">&para;</a></h3> <p><img alt="Logistic Regression" src=../fig/logistic-regression-problem-statement.png></p> <h3 id=logistic-regression-cost-function_1>Logistic Regression Cost Function<a class=headerlink href=#logistic-regression-cost-function_1 title="Permanent link">&para;</a></h3> <p>Out goal to solve a logistic regression problem is to reduce the cost incurred when predict wrong result. In linear regression, we minimize the cost function with respect to vector <span><span class=MathJax_Preview>\theta</span><script type=math/tex>\theta</script></span>. Generally, we can think the cost function as a penalty of the incorrect classification. It is a qualitative measure of such penalty. For example, we use the squared error cost function in linear regression:</p> <p><span><span class=MathJax_Preview>J(\theta) = \frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})^2</span><script type=math/tex>J(\theta) = \frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})^2</script></span>.</p> <p>In logistic regression, the <span><span class=MathJax_Preview>h_\theta(x)</span><script type=math/tex>h_\theta(x)</script></span> is a much complex function, so the cost function <span><span class=MathJax_Preview>J(\theta)</span><script type=math/tex>J(\theta)</script></span> used in linear regression will be a non-convex function in logisitic regress. This will produce a hard problem to solve logistic problems numerically. So we define a convex logistic regression cost function</p> <p><span><span class=MathJax_Preview>Cost(h_\theta(x), y) =\begin{cases} -log(h_\theta(x)) &amp; \text{if}\ y = 1\\ -log(1-h_\theta(x)) &amp; \text{if}\ y = 0 \end{cases}</span><script type=math/tex>Cost(h_\theta(x), y) =\begin{cases}
               -log(h_\theta(x))    & \text{if}\  y = 1\\
               -log(1-h_\theta(x)) & \text{if}\ y = 0
            \end{cases}</script></span></p> <p>It is the logistic regression cost function, it can be interpreted as the penalty the algorithm pays.</p> <p>We can plot the function as following:</p> <p><img alt="Logistic Regression Cost Function" src=../fig/logistic-regression-cost-function.png></p> <p>Some intuition/properties about the simplified logistic regression cost function:</p> <ul> <li>X axis is what we predict</li> <li>Y axis is the cost associated with that prediction.</li> <li>If y = 1, and <span><span class=MathJax_Preview>h_\theta(x) = 1</span><script type=math/tex>h_\theta(x) = 1</script></span>, hypothesis predicts exactly 1 (exactly correct) then cost corresponds to 0 (no cost for correct predict)</li> <li>If y = 1, and <span><span class=MathJax_Preview>h_\theta(x) = 0</span><script type=math/tex>h_\theta(x) = 0</script></span>, predict <span><span class=MathJax_Preview>P(y = 1|x; \theta) = 0</span><script type=math/tex>P(y = 1|x; \theta) = 0</script></span>, this is wrong, and it penalized with a massive cost (cost approach positive infinity.</li> <li>Similar reasoning holds for y = 0.</li> </ul> <h3 id=gradient-descent-and-cost-function>Gradient descent and cost function<a class=headerlink href=#gradient-descent-and-cost-function title="Permanent link">&para;</a></h3> <p>We can neatly write the logistic regression function in the following format:</p> <p><span><span class=MathJax_Preview>cost(h_\theta(x), y) = -y\log(h_\theta(x)) - (1 - y)\log(1-h_\theta(x))</span><script type=math/tex>cost(h_\theta(x), y) = -y\log(h_\theta(x)) - (1 - y)\log(1-h_\theta(x))</script></span></p> <p>We can take this cost function and obtained the logistic regression cost function <span><span class=MathJax_Preview>J(\theta)</span><script type=math/tex>J(\theta)</script></span>:</p> <p><span><span class=MathJax_Preview>J(\theta) = \frac{1}{m}\sum_{i=1}^{m}Cost(h_\theta(x^{(i)}),y^{(i)}) = -\frac{1}{m}\Big[\sum_{i=1}^{m}y^{(i)}\log(h_\theta(x^{(i)})) + (1 - y^{(i)})\log(1-h_\theta(x^{(i)}))\Big]</span><script type=math/tex>J(\theta) = \frac{1}{m}\sum_{i=1}^{m}Cost(h_\theta(x^{(i)}),y^{(i)}) =
 -\frac{1}{m}\Big[\sum_{i=1}^{m}y^{(i)}\log(h_\theta(x^{(i)})) + (1 - y^{(i)})\log(1-h_\theta(x^{(i)}))\Big]</script></span></p> <div class="admonition note"> <p class=admonition-title>Why do we chose this function when other cost functions exist?</p> <ul> <li>This cost function can be derived from statistics using the principle of <strong><em>maximum likelihood estimation</em></strong>.<ul> <li>Note this does mean there's an underlying Gaussian assumption relating to the distribution of features.</li> </ul> </li> <li>Also has the nice property that it's <strong><em>convex</em></strong></li> </ul> </div> <h4 id=gradient-descent-for-logistic-regression>Gradient Descent for logistic regression<a class=headerlink href=#gradient-descent-for-logistic-regression title="Permanent link">&para;</a></h4> <p>The algorithm looks identical to linear regression except the hypothesis function is Sigmoid function and not linear any more.</p> <p><img alt="Logistic Regression Gradient Descent" src=../fig/logistic-regression-gradient-descent.png></p> <h3 id=advanced-optimization>Advanced Optimization<a class=headerlink href=#advanced-optimization title="Permanent link">&para;</a></h3> <p>Beside gradient descent algorithm, there are many other optimization algorithm such as conjugate gradient, BFGS, and L-BFGS, can minimization the cost function for solving logistic regression problem. Most of those advanced algorithms are more efficient to compute, you don't have to select the convergent rate <span><span class=MathJax_Preview>\alpha</span><script type=math/tex>\alpha</script></span>.</p> <p>Here is one function in Octave library, possibly also in Matlab, can be used for finding the <span><span class=MathJax_Preview>\theta</span><script type=math/tex>\theta</script></span></p> <h4 id=function-fminunc>Function fminunc()<a class=headerlink href=#function-fminunc title="Permanent link">&para;</a></h4> <p>Octave have a function <code>fminunc()</code>.<a href=https://www.gnu.org/software/octave/doc/interpreter/Minimizers.html>^1</a> To use it, we should first call <code>optimset()</code>. There is three steps we need to take care to solve optimization problem using these functions</p> <ol> <li>calculate the cost function <span><span class=MathJax_Preview>J(\theta)</span><script type=math/tex>J(\theta)</script></span></li> <li>calculate the gradient functions <span><span class=MathJax_Preview>\frac{\partial}{\partial\theta_j}J(\theta)</span><script type=math/tex>\frac{\partial}{\partial\theta_j}J(\theta)</script></span></li> <li>give initial <span><span class=MathJax_Preview>\theta</span><script type=math/tex>\theta</script></span> value</li> <li> <p>call the function <code>optimset()</code> and <code>fminunc()</code> as following ```Octave tab="Octave" function [jVal, gradient] = costFunction(theta) jVal = (theta(1)-5)^2 + (theta(2)-5)^2; gradient = zeros(2,1); gradient(1) = 2<em>(theta(1)-5); gradient(2) = 2</em>(theta(2)-5);</p> <p>options = optimset(‘GradObj’, ‘on’, ‘MaxIter’, ‘100’); initialTheta = zeros(2,1); [optTheta, functionVal, exitFlag] = fminunc(@costFunction, initialTheta, options); ```</p> </li> <li> <p>Note we have <code>n+1</code> parameters, we implement the code in the following way. <img alt="Logistic Regression Cost Function Optimization" src=../fig/logistic-regression-cost-function-optimization.png></p> </li> </ol> <h3 id=multiclass-classification>Multiclass classification<a class=headerlink href=#multiclass-classification title="Permanent link">&para;</a></h3> <p><img alt="Multi Class One versus All" src=../fig/logistic-regression-one-over-all-class.png></p> <ul> <li>Train a logistic regression classifier <span><span class=MathJax_Preview>h_{\theta}^{(i)}(x)</span><script type=math/tex>h_{\theta}^{(i)}(x)</script></span> for each class <span><span class=MathJax_Preview>i</span><script type=math/tex>i</script></span> to predict the probability that <span><span class=MathJax_Preview>y = i</span><script type=math/tex>y = i</script></span>.</li> <li>On a new input <span><span class=MathJax_Preview>x</span><script type=math/tex>x</script></span>, to make a prediction, pick the class <span><span class=MathJax_Preview>i</span><script type=math/tex>i</script></span> that maximize the <span><span class=MathJax_Preview>\max_i h_{\theta}^{(i)}(x)</span><script type=math/tex>\max_i h_{\theta}^{(i)}(x)</script></span>.</li> </ul> <h3 id=regularization>Regularization<a class=headerlink href=#regularization title="Permanent link">&para;</a></h3> <div class="admonition note"> <p class=admonition-title>Overfitting</p> <p>If we have too many features, the learned hypothesis may fit the training set very well ( <span><span class=MathJax_Preview>J(\theta) \approx 0</span><script type=math/tex>J(\theta) \approx 0</script></span> ), but fail to generalize to new examples (predict prices on new examples).</p> </div> <h4 id=address-overfitting-problems>Address overfitting problems<a class=headerlink href=#address-overfitting-problems title="Permanent link">&para;</a></h4> <ol> <li>Reduce the number of features<ol> <li>Manually select what features to keep</li> <li>feature selection algorithm</li> </ol> </li> <li>Regularization<ol> <li>Keep all the features, reduce the value of the parameter <span><span class=MathJax_Preview>\theta_j</span><script type=math/tex>\theta_j</script></span>.</li> <li>work well when we have a lot of data, when each of the feature contribute to the algorithm a bit to predict <span><span class=MathJax_Preview>y</span><script type=math/tex>y</script></span>.</li> </ol> </li> </ol> <h4 id=regularization-intuition>Regularization Intuition<a class=headerlink href=#regularization-intuition title="Permanent link">&para;</a></h4> <p>To penalize the higher order of polynomial by adding extra terms of high coefficient for <span><span class=MathJax_Preview>\theta^n</span><script type=math/tex>\theta^n</script></span> terms. i.e. from the cost function minimization problem, <span><span class=MathJax_Preview>J(\theta) = \frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})^2</span><script type=math/tex>J(\theta) = \frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})^2</script></span>, we can instead use the regularized form <span><span class=MathJax_Preview>J(\theta) = \frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})^2 + \lambda\sum_{j = 1}^{n}\theta_j^2</span><script type=math/tex>J(\theta) = \frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})^2 + \lambda\sum_{j = 1}^{n}\theta_j^2</script></span>. Small values for parameters <span><span class=MathJax_Preview>\theta_j</span><script type=math/tex>\theta_j</script></span> will make "Simpler" hypothesis and Less prone to overfitting. Please note that the <span><span class=MathJax_Preview>\theta_0</span><script type=math/tex>\theta_0</script></span> is <strong>excluded</strong> from the regularization term <span><span class=MathJax_Preview>\lambda\sum_{n = 1}^{n}\theta_j^2</span><script type=math/tex>\lambda\sum_{n = 1}^{n}\theta_j^2</script></span>.</p> <p><img alt=Regularization src=../fig/regularization.png> <img alt="Regularization Question" src=../fig/regularization_question.png></p> <h4 id=regularized-linear-regression>Regularized linear regression<a class=headerlink href=#regularized-linear-regression title="Permanent link">&para;</a></h4> <p>Gradient Descent:</p> <p><img alt="Regularized Linear Regression" src=../fig/regularized_linear_regression.png></p> <ul> <li>We don't regularize <span><span class=MathJax_Preview>\theta_0</span><script type=math/tex>\theta_0</script></span>, so we explicitly update it in the formula and it is the same with the non-regularized linear regression gradient descent. All other <span><span class=MathJax_Preview>\theta_j</span><script type=math/tex>\theta_j</script></span>, <span><span class=MathJax_Preview>j = 1, ..., n</span><script type=math/tex>j = 1, ..., n</script></span> will update differently.</li> </ul> <h4 id=regularized-logistic-regression>Regularized logistic regression<a class=headerlink href=#regularized-logistic-regression title="Permanent link">&para;</a></h4> <p><img alt="Regularized Logistic Regression" src=../fig/regularized_logistic_regression.png></p> <ul> <li>Doesn't regularize the <span><span class=MathJax_Preview>\theta_0</span><script type=math/tex>\theta_0</script></span></li> <li>The cost function is different from the linear regression</li> </ul> <h4 id=advanced-optimization_1>Advanced optimization<a class=headerlink href=#advanced-optimization_1 title="Permanent link">&para;</a></h4> <p><img alt="Regularized Advanced Optimization" src=../fig/regularized_advanced_optimization.png></p> <ul> <li>Add the regularized term in the cost function and gradient calculation</li> </ul> <h2 id=week-4-neural-networks-model>Week 4 Neural networks model<a class=headerlink href=#week-4-neural-networks-model title="Permanent link">&para;</a></h2> <p>Neural Networks is originated when people try to mimic the functionality of brain by algorithm.</p> <h4 id=model-representation>Model representation<a class=headerlink href=#model-representation title="Permanent link">&para;</a></h4> <h4 id=neural-network>Neural Network<a class=headerlink href=#neural-network title="Permanent link">&para;</a></h4> <h4 id=forward-propagation-implementation>Forward propagation implementation<a class=headerlink href=#forward-propagation-implementation title="Permanent link">&para;</a></h4> <h4 id=learning-its-own-features>Learning its own features<a class=headerlink href=#learning-its-own-features title="Permanent link">&para;</a></h4> <h4 id=xnor-example>XNOR example<a class=headerlink href=#xnor-example title="Permanent link">&para;</a></h4> <h4 id=multiclass-classification_1>Multiclass classification<a class=headerlink href=#multiclass-classification_1 title="Permanent link">&para;</a></h4> </article> </div> </div> </main> <footer class=md-footer> <div class=md-footer-nav> <nav class="md-footer-nav__inner md-grid" aria-label=Footer> <a href=../../ class="md-footer-nav__link md-footer-nav__link--prev" rel=prev> <div class="md-footer-nav__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </div> <div class=md-footer-nav__title> <div class=md-ellipsis> <span class=md-footer-nav__direction> Previous </span> Index </div> </div> </a> <a href=../../cs224n/lec-notes/ class="md-footer-nav__link md-footer-nav__link--next" rel=next> <div class=md-footer-nav__title> <div class=md-ellipsis> <span class=md-footer-nav__direction> Next </span> CS224N Lecture Notes </div> </div> <div class="md-footer-nav__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg> </div> </a> </nav> </div> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-footer-copyright> <div class=md-footer-copyright__highlight> Copyright &copy; 2020 Rui Han </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> </div> </div> </footer> </div> <script src=../../../assets/javascripts/vendor.77e55a48.min.js></script> <script src=../../../assets/javascripts/bundle.9554a270.min.js></script><script id=__lang type=application/json>{"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing"}</script> <script>
        app = initialize({
          base: "../../..",
          features: ['navigation.tabs'],
          search: Object.assign({
            worker: "../../../assets/javascripts/worker/search.4ac00218.min.js"
          }, typeof search !== "undefined" && search)
        })
      </script> <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script> </body> </html>