<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><link href=https://ruihan.org/courses/machine-learning-coursera/notes/ rel=canonical><link rel="shortcut icon" href=../../../assets/images/favicon.png><meta name=generator content="mkdocs-1.1.2, mkdocs-material-6.0.2"><title>Machine Learning (Coursera) - RUIHAN.ORG</title><link rel=stylesheet href=../../../assets/stylesheets/main.38780c08.min.css><link rel=stylesheet href=../../../assets/stylesheets/palette.3f72e892.min.css><meta name=theme-color content=#000000><link href=https://fonts.gstatic.com rel=preconnect crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback"><style>body,input{font-family:"Roboto",-apple-system,BlinkMacSystemFont,Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono",SFMono-Regular,Consolas,Menlo,monospace}</style></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=black data-md-color-accent=black> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#machine-learning-coursera class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class=md-header data-md-component=header> <nav class="md-header-nav md-grid" aria-label=Header> <a href=https://ruihan.org title=RUIHAN.ORG class="md-header-nav__button md-logo" aria-label=RUIHAN.ORG> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 003-3 3 3 0 00-3-3 3 3 0 00-3 3 3 3 0 003 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg> </a> <label class="md-header-nav__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg> </label> <div class=md-header-nav__title data-md-component=header-title> <div class=md-header-nav__ellipsis> <span class="md-header-nav__topic md-ellipsis"> RUIHAN.ORG </span> <span class="md-header-nav__topic md-ellipsis"> Machine Learning (Coursera) </span> </div> </div> <label class="md-header-nav__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query data-md-state=active> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </label> <button type=reset class="md-search__icon md-icon" aria-label=Clear data-md-component=search-reset tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg> </button> </form> <div class=md-search__output> <div class=md-search__scrollwrap data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list></ol> </div> </div> </div> </div> </div> <div class=md-header-nav__source> <a href=https://github.com/iurnah/ruihan.org title="Go to repository" class=md-source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 00-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 01-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 000 40.81l195.61 195.6a28.86 28.86 0 0040.8 0l194.69-194.69a28.86 28.86 0 000-40.81z"/></svg> </div> <div class=md-source__repository> iurnah/ruihan.org </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class="md-tabs md-tabs--active" aria-label=Tabs data-md-component=tabs> <div class="md-tabs__inner md-grid"> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../../../books/ class=md-tabs__link> Books </a> </li> <li class=md-tabs__item> <a href=../../ class="md-tabs__link md-tabs__link--active"> Courses </a> </li> <li class=md-tabs__item> <a href=../../../leetcode/ class=md-tabs__link> Leetcode </a> </li> <li class=md-tabs__item> <a href=../../../research/ class=md-tabs__link> Research </a> </li> <li class=md-tabs__item> <a href=../../../seedlabs/ class=md-tabs__link> SEED Labs </a> </li> <li class=md-tabs__item> <a href=../../../system-design/ class=md-tabs__link> System Design </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=https://ruihan.org title=RUIHAN.ORG class="md-nav__button md-logo" aria-label=RUIHAN.ORG> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 003-3 3 3 0 00-3-3 3 3 0 00-3 3 3 3 0 003 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg> </a> RUIHAN.ORG </label> <div class=md-nav__source> <a href=https://github.com/iurnah/ruihan.org title="Go to repository" class=md-source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 00-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 01-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 000 40.81l195.61 195.6a28.86 28.86 0 0040.8 0l194.69-194.69a28.86 28.86 0 000-40.81z"/></svg> </div> <div class=md-source__repository> iurnah/ruihan.org </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1 type=checkbox id=nav-1> <label class=md-nav__link for=nav-1> Books <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Books data-md-level=1> <label class=md-nav__title for=nav-1> <span class="md-nav__icon md-icon"></span> Books </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../books/ class=md-nav__link> Index </a> </li> <li class=md-nav__item> <a href=../../../books/accelerated-cpp/notes/ class=md-nav__link> Accelerated C++ </a> </li> <li class=md-nav__item> <a href=../../../books/mining-massive-datasets/notes/ class=md-nav__link> Mining Massive Datasets </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-2 type=checkbox id=nav-2 checked> <label class=md-nav__link for=nav-2> Courses <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Courses data-md-level=1> <label class=md-nav__title for=nav-2> <span class="md-nav__icon md-icon"></span> Courses </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../ class=md-nav__link> Index </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" data-md-toggle=toc type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> Machine Learning (Coursera) <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> Machine Learning (Coursera) </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=#week-1-introduction class=md-nav__link> Week 1 Introduction </a> <nav class=md-nav aria-label="Week 1 Introduction"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#cost-function class=md-nav__link> Cost Function </a> </li> <li class=md-nav__item> <a href=#cost-function-intuition-i class=md-nav__link> Cost Function Intuition I </a> </li> <li class=md-nav__item> <a href=#cost-function-intuition-ii class=md-nav__link> Cost Function Intuition II </a> </li> <li class=md-nav__item> <a href=#gradient-descent class=md-nav__link> Gradient Descent </a> </li> <li class=md-nav__item> <a href=#gradient-descent-intuition class=md-nav__link> Gradient Descent Intuition </a> </li> <li class=md-nav__item> <a href=#gradient-descent-and-linear-regression class=md-nav__link> Gradient Descent and Linear Regression </a> </li> <li class=md-nav__item> <a href=#linear-algebra-review class=md-nav__link> Linear Algebra Review </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#week-2-multiple-featuresvariables-linear-regression class=md-nav__link> Week 2 Multiple features(variables) linear regression </a> <nav class=md-nav aria-label="Week 2 Multiple features(variables) linear regression"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#feature-scaling class=md-nav__link> Feature scaling </a> </li> <li class=md-nav__item> <a href=#learning-rate class=md-nav__link> Learning rate </a> </li> <li class=md-nav__item> <a href=#polynomial-regression class=md-nav__link> Polynomial Regression </a> </li> <li class=md-nav__item> <a href=#normal-equation class=md-nav__link> Normal Equation </a> </li> <li class=md-nav__item> <a href=#normal-equation-and-non-invertibility class=md-nav__link> Normal equation and non-invertibility </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#week-3-logistic-regression-model class=md-nav__link> Week 3 Logistic regression model </a> <nav class=md-nav aria-label="Week 3 Logistic regression model"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#classification-problem class=md-nav__link> Classification problem </a> </li> <li class=md-nav__item> <a href=#hypothesis-representation class=md-nav__link> Hypothesis representation </a> </li> <li class=md-nav__item> <a href=#interpretation class=md-nav__link> Interpretation </a> </li> <li class=md-nav__item> <a href=#logistic-regression-decision-boundary class=md-nav__link> Logistic regression decision boundary </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#logistic-regression-cost-function class=md-nav__link> Logistic regression cost function </a> <nav class=md-nav aria-label="Logistic regression cost function"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#problem-description class=md-nav__link> Problem description </a> </li> <li class=md-nav__item> <a href=#logistic-regression-cost-function_1 class=md-nav__link> Logistic Regression Cost Function </a> </li> <li class=md-nav__item> <a href=#gradient-descent-and-cost-function class=md-nav__link> Gradient descent and cost function </a> <nav class=md-nav aria-label="Gradient descent and cost function"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#gradient-descent-for-logistic-regression class=md-nav__link> Gradient Descent for logistic regression </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#advanced-optimization class=md-nav__link> Advanced Optimization </a> <nav class=md-nav aria-label="Advanced Optimization"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#function-fminunc class=md-nav__link> Function fminunc() </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#multiclass-classification class=md-nav__link> Multiclass classification </a> </li> <li class=md-nav__item> <a href=#regularization class=md-nav__link> Regularization </a> <nav class=md-nav aria-label=Regularization> <ul class=md-nav__list> <li class=md-nav__item> <a href=#address-overfitting-problems class=md-nav__link> Address overfitting problems </a> </li> <li class=md-nav__item> <a href=#regularization-intuition class=md-nav__link> Regularization Intuition </a> </li> <li class=md-nav__item> <a href=#regularized-linear-regression class=md-nav__link> Regularized linear regression </a> </li> <li class=md-nav__item> <a href=#regularized-logistic-regression class=md-nav__link> Regularized logistic regression </a> </li> <li class=md-nav__item> <a href=#advanced-optimization_1 class=md-nav__link> Advanced optimization </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#week-4-neural-networks-model class=md-nav__link> Week 4 Neural networks model </a> <nav class=md-nav aria-label="Week 4 Neural networks model"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#model-representation class=md-nav__link> Model representation </a> </li> <li class=md-nav__item> <a href=#neural-network class=md-nav__link> Neural Network </a> </li> <li class=md-nav__item> <a href=#forward-propagation-implementation class=md-nav__link> Forward propagation implementation </a> </li> <li class=md-nav__item> <a href=#learning-its-own-features class=md-nav__link> Learning its own features </a> </li> <li class=md-nav__item> <a href=#xnor-example class=md-nav__link> XNOR example </a> </li> <li class=md-nav__item> <a href=#multiclass-classification_1 class=md-nav__link> Multiclass classification </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#week-5-neural-networks-learning class=md-nav__link> Week 5 Neural networks learning </a> <nav class=md-nav aria-label="Week 5 Neural networks learning"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#neural-network-classification class=md-nav__link> Neural network classification </a> </li> <li class=md-nav__item> <a href=#cost-function_1 class=md-nav__link> Cost Function </a> <nav class=md-nav aria-label="Cost Function"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#logistic-regression class=md-nav__link> Logistic regression </a> </li> <li class=md-nav__item> <a href=#neural-network_1 class=md-nav__link> Neural Network </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#gradient-computation class=md-nav__link> Gradient computation </a> <nav class=md-nav aria-label="Gradient computation"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#forward-propagation class=md-nav__link> Forward propagation </a> </li> <li class=md-nav__item> <a href=#back-propagation class=md-nav__link> Back propagation </a> </li> <li class=md-nav__item> <a href=#how-to-derive-the-back-prop class=md-nav__link> How to derive the back-prop </a> </li> <li class=md-nav__item> <a href=#backpropagation-algorithm class=md-nav__link> Backpropagation Algorithm </a> </li> <li class=md-nav__item> <a href=#math-representation class=md-nav__link> Math representation </a> </li> <li class=md-nav__item> <a href=#a-high-level-description class=md-nav__link> A high level description </a> </li> <li class=md-nav__item> <a href=#what-we-need-to-watch-out class=md-nav__link> What we need to watch out? </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#backpropagation-intuition class=md-nav__link> Backpropagation Intuition </a> </li> <li class=md-nav__item> <a href=#unrolling-parameters class=md-nav__link> Unrolling parameters </a> </li> <li class=md-nav__item> <a href=#gradient-checking class=md-nav__link> Gradient Checking </a> </li> <li class=md-nav__item> <a href=#random-initialization class=md-nav__link> Random Initialization </a> </li> <li class=md-nav__item> <a href=#putting-it-together class=md-nav__link> Putting it Together </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#week-6-advice-for-applying-machine-learning class=md-nav__link> Week 6 Advice for applying machine learning </a> <nav class=md-nav aria-label="Week 6 Advice for applying machine learning"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#deciding-what-to-try-next class=md-nav__link> Deciding what to try next </a> </li> <li class=md-nav__item> <a href=#machine-learning-diagnostic class=md-nav__link> Machine learning diagnostic </a> </li> <li class=md-nav__item> <a href=#evaluating-a-hypothesis class=md-nav__link> Evaluating a hypothesis </a> </li> <li class=md-nav__item> <a href=#model-selection-and-trainvalidationtest-sets class=md-nav__link> Model selection and Train/Validation/Test sets </a> </li> <li class=md-nav__item> <a href=#diagnosing-bias-vs-variance class=md-nav__link> Diagnosing bias vs. variance </a> </li> <li class=md-nav__item> <a href=#regularization-and-biasvariance class=md-nav__link> Regularization and bias/variance </a> </li> <li class=md-nav__item> <a href=#learning-curve class=md-nav__link> Learning curve </a> <nav class=md-nav aria-label="Learning curve"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#high-bias class=md-nav__link> High bias </a> </li> <li class=md-nav__item> <a href=#high-variance class=md-nav__link> High variance </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#deciding-what-to-do-next-revisited class=md-nav__link> Deciding what to do next revisited </a> </li> <li class=md-nav__item> <a href=#prioritizing-what-to-work-on class=md-nav__link> Prioritizing what to work on </a> </li> <li class=md-nav__item> <a href=#building-a-spam-classifier class=md-nav__link> Building a spam classifier </a> </li> <li class=md-nav__item> <a href=#error-analysis class=md-nav__link> Error analysis </a> </li> <li class=md-nav__item> <a href=#error-metrics-for-skewed-classes class=md-nav__link> Error metrics for skewed classes </a> <nav class=md-nav aria-label="Error metrics for skewed classes"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#precision-and-recall class=md-nav__link> Precision and Recall </a> </li> <li class=md-nav__item> <a href=#trading-off-precision-and-recall class=md-nav__link> Trading off precision and recall </a> </li> <li class=md-nav__item> <a href=#f-score-f_score class=md-nav__link> F score {#f_score} </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#data-for-machine-learning class=md-nav__link> Data for machine learning </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#week-7-support-vector-machine class=md-nav__link> Week 7 Support Vector Machine </a> <nav class=md-nav aria-label="Week 7 Support Vector Machine"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#optimization-objective class=md-nav__link> Optimization objective </a> <nav class=md-nav aria-label="Optimization objective"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#svm-cost-function class=md-nav__link> SVM cost function </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#large-margin-intuition class=md-nav__link> Large margin intuition </a> </li> <li class=md-nav__item> <a href=#mathematics-behind-large-margin-classification class=md-nav__link> Mathematics behind large margin classification </a> </li> <li class=md-nav__item> <a href=#kernel class=md-nav__link> Kernel </a> <nav class=md-nav aria-label=Kernel> <ul class=md-nav__list> <li class=md-nav__item> <a href=#non-linear-decision-boundary class=md-nav__link> Non-linear decision boundary </a> </li> <li class=md-nav__item> <a href=#landmarks class=md-nav__link> Landmarks </a> </li> <li class=md-nav__item> <a href=#gaussian-kernel class=md-nav__link> Gaussian kernel </a> </li> <li class=md-nav__item> <a href=#choosing-the-landmarks class=md-nav__link> Choosing the landmarks </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#using-svm class=md-nav__link> Using SVM </a> <nav class=md-nav aria-label="Using SVM"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#logistic-regression-vs-smvs class=md-nav__link> Logistic regression vs. SMVs </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#week-8-unsupervised-algorithms class=md-nav__link> Week 8 Unsupervised Algorithms </a> <nav class=md-nav aria-label="Week 8 Unsupervised Algorithms"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#k-means-algorithm class=md-nav__link> K-Means Algorithm </a> </li> <li class=md-nav__item> <a href=#optimization-objective_1 class=md-nav__link> Optimization objective </a> </li> <li class=md-nav__item> <a href=#random-initialization_1 class=md-nav__link> Random initialization </a> </li> <li class=md-nav__item> <a href=#choosing-the-k class=md-nav__link> Choosing the K </a> </li> <li class=md-nav__item> <a href=#dimensionality-reduction class=md-nav__link> Dimensionality reduction </a> </li> <li class=md-nav__item> <a href=#principal-component-analysis-pca class=md-nav__link> Principal Component Analysis (PCA) </a> </li> <li class=md-nav__item> <a href=#pca-algorithm class=md-nav__link> PCA algorithm </a> </li> <li class=md-nav__item> <a href=#reconstruction-from-pca-compressed-data class=md-nav__link> Reconstruction from PCA compressed data </a> </li> <li class=md-nav__item> <a href=#choosing-the-number-of-principal-components class=md-nav__link> Choosing the number of principal components </a> </li> <li class=md-nav__item> <a href=#advice-for-applying-pca class=md-nav__link> Advice for applying PCA </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#week-9-anomaly-detection class=md-nav__link> Week 9 Anomaly Detection </a> <nav class=md-nav aria-label="Week 9 Anomaly Detection"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#anomaly-detection class=md-nav__link> Anomaly Detection </a> </li> <li class=md-nav__item> <a href=#developing-and-evaluating class=md-nav__link> Developing and evaluating </a> </li> <li class=md-nav__item> <a href=#anomaly-detection-vs-supervised-learning class=md-nav__link> Anomaly detection vs. supervised learning </a> </li> <li class=md-nav__item> <a href=#multivariate-gaussian-distribution class=md-nav__link> Multivariate Gaussian distribution </a> </li> <li class=md-nav__item> <a href=#predicting-movie-ratings class=md-nav__link> Predicting movie ratings </a> <nav class=md-nav aria-label="Predicting movie ratings"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#problem-formulation class=md-nav__link> Problem formulation </a> </li> <li class=md-nav__item> <a href=#content-based-recommendations class=md-nav__link> Content based recommendations </a> </li> <li class=md-nav__item> <a href=#collaborate-filtering class=md-nav__link> Collaborate filtering </a> </li> <li class=md-nav__item> <a href=#collaborative-filtering-algorithm class=md-nav__link> Collaborative filtering algorithm </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../cs224n/lec-notes/ class=md-nav__link> CS224N Lecture Notes </a> </li> <li class=md-nav__item> <a href=../../cs224n/write-up/ class=md-nav__link> CS224N Write-up </a> </li> <li class=md-nav__item> <a href=../../coursera-dl4-cnn/notes/ class=md-nav__link> Convolutional Neural Networks </a> </li> <li class=md-nav__item> <a href=../../mining-massive-datasets/notes/ class=md-nav__link> Mining Massive Data Sets </a> </li> <li class=md-nav__item> <a href=../../6.431-probability/notes/ class=md-nav__link> 6.431 Probability </a> </li> <li class=md-nav__item> <a href=../../learning-from-data/notes.md class=md-nav__link> Learning From Data </a> </li> <li class=md-nav__item> <a href=../../9chap-algorithms/notes/ class=md-nav__link> Nine Chapter Algorithms </a> </li> <li class=md-nav__item> <a href=../../9chap-system-design/notes/ class=md-nav__link> Nine Chapter System Design </a> </li> <li class=md-nav__item> <a href=../../9chap-dynamic-prog/notes/ class=md-nav__link> Nine Chapter Dynamic Programming </a> </li> <li class=md-nav__item> <a href=../../func-prog-in-scala/notes/ class=md-nav__link> Functional Programming Principles in Scala </a> </li> <li class=md-nav__item> <a href=../../applied-scrum-for-agile/notes/ class=md-nav__link> Applied Scrum for Agile Project Management </a> </li> <li class=md-nav__item> <a href=../../concurrent-prog-java/notes/ class=md-nav__link> Concurrent Programming in Java </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-3 type=checkbox id=nav-3> <label class=md-nav__link for=nav-3> Leetcode <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Leetcode data-md-level=1> <label class=md-nav__title for=nav-3> <span class="md-nav__icon md-icon"></span> Leetcode </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../leetcode/ class=md-nav__link> Index </a> </li> <li class=md-nav__item> <a href=../../../leetcode/favorite/notes/ class=md-nav__link> Favorite </a> </li> <li class=md-nav__item> <a href=../../../leetcode/array/notes/ class=md-nav__link> Array </a> </li> <li class=md-nav__item> <a href=../../../leetcode/backtracking/notes/ class=md-nav__link> Backtracking </a> </li> <li class=md-nav__item> <a href=../../../leetcode/binary-search/notes/ class=md-nav__link> Binary Search </a> </li> <li class=md-nav__item> <a href=../../../leetcode/breadth-first-search/notes/ class=md-nav__link> Breadth-First Search (BFS) </a> </li> <li class=md-nav__item> <a href=../../../leetcode/depth-first-search/notes/ class=md-nav__link> Depth-First Search (DFS) </a> </li> <li class=md-nav__item> <a href=../../../leetcode/dynamic-programming/notes/ class=md-nav__link> Dynamic Programming </a> </li> <li class=md-nav__item> <a href=../../../leetcode/graph/notes/ class=md-nav__link> Graph </a> </li> <li class=md-nav__item> <a href=../../../leetcode/heap/notes/ class=md-nav__link> Heap </a> </li> <li class=md-nav__item> <a href=../../../leetcode/interval/notes/ class=md-nav__link> Interval </a> </li> <li class=md-nav__item> <a href=../../../leetcode/linked-list/notes/ class=md-nav__link> Linked List </a> </li> <li class=md-nav__item> <a href=../../../leetcode/math/notes/ class=md-nav__link> Math </a> </li> <li class=md-nav__item> <a href=../../../leetcode/reservoir-sampling/notes/ class=md-nav__link> Reservoir Sampling </a> </li> <li class=md-nav__item> <a href=../../../leetcode/stack/notes/ class=md-nav__link> Stack </a> </li> <li class=md-nav__item> <a href=../../../leetcode/string/notes/ class=md-nav__link> String </a> </li> <li class=md-nav__item> <a href=../../../leetcode/tree/notes/ class=md-nav__link> Tree </a> </li> <li class=md-nav__item> <a href=../../../leetcode/trie/notes/ class=md-nav__link> Trie </a> </li> <li class=md-nav__item> <a href=../../../leetcode/union-find/notes/ class=md-nav__link> Union Find </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-4 type=checkbox id=nav-4> <label class=md-nav__link for=nav-4> Research <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Research data-md-level=1> <label class=md-nav__title for=nav-4> <span class="md-nav__icon md-icon"></span> Research </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../research/ class=md-nav__link> Index </a> </li> <li class=md-nav__item> <a href=../../../research/paper-reading/notes/ class=md-nav__link> Paper Reading </a> </li> <li class=md-nav__item> <a href=../../../research/coalition-game/notes/ class=md-nav__link> Coalition Game </a> </li> <li class=md-nav__item> <a href=../../../research/contextual-bandit/notes/ class=md-nav__link> Contextual Multi-Armed Bandit </a> </li> <li class=md-nav__item> <a href=../../../research/tfidf-score/notes/ class=md-nav__link> TF-IDF for Information Retrieval </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-5 type=checkbox id=nav-5> <label class=md-nav__link for=nav-5> SEED Labs <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="SEED Labs" data-md-level=1> <label class=md-nav__title for=nav-5> <span class="md-nav__icon md-icon"></span> SEED Labs </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../seedlabs/ class=md-nav__link> Index </a> </li> <li class=md-nav__item> <a href=../../../seedlabs/applied-crypto/notes/ class=md-nav__link> Applied Cryptograph Notes </a> </li> <li class=md-nav__item> <a href=../../../seedlabs/public-key-cryptography-and-pki/notes/ class=md-nav__link> Public Key Cryptography and PKI </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-6 type=checkbox id=nav-6> <label class=md-nav__link for=nav-6> System Design <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="System Design" data-md-level=1> <label class=md-nav__title for=nav-6> <span class="md-nav__icon md-icon"></span> System Design </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../system-design/ class=md-nav__link> Index </a> </li> <li class=md-nav__item> <a href=../../../system-design/browser/notes.md class=md-nav__link> How Browser Works </a> </li> <li class=md-nav__item> <a href=../../../system-design/concurrency/notes/ class=md-nav__link> Concurrency and Synchronization </a> </li> <li class=md-nav__item> <a href=../../../system-design/concepts/notes/ class=md-nav__link> Distributed System Concepts </a> </li> <li class=md-nav__item> <a href=../../../system-design/patterns/notes/ class=md-nav__link> Design Patterns </a> </li> <li class=md-nav__item> <a href=../../../system-design/problems/crawler/notes/ class=md-nav__link> How to Design Crawler </a> </li> <li class=md-nav__item> <a href=../../../system-design/problems/tinyurl/notes/ class=md-nav__link> How to Design TinyUrl </a> </li> <li class=md-nav__item> <a href=../../../system-design/problems/twitter/notes/ class=md-nav__link> How to Design Twitter </a> </li> <li class=md-nav__item> <a href=../../../system-design/problems/scheduler/notes/ class=md-nav__link> How to Deisgn Scheduler </a> </li> <li class=md-nav__item> <a href=../../../system-design/problems/ticketmaster/notes/ class=md-nav__link> How to Design Ticketmaster </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=#week-1-introduction class=md-nav__link> Week 1 Introduction </a> <nav class=md-nav aria-label="Week 1 Introduction"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#cost-function class=md-nav__link> Cost Function </a> </li> <li class=md-nav__item> <a href=#cost-function-intuition-i class=md-nav__link> Cost Function Intuition I </a> </li> <li class=md-nav__item> <a href=#cost-function-intuition-ii class=md-nav__link> Cost Function Intuition II </a> </li> <li class=md-nav__item> <a href=#gradient-descent class=md-nav__link> Gradient Descent </a> </li> <li class=md-nav__item> <a href=#gradient-descent-intuition class=md-nav__link> Gradient Descent Intuition </a> </li> <li class=md-nav__item> <a href=#gradient-descent-and-linear-regression class=md-nav__link> Gradient Descent and Linear Regression </a> </li> <li class=md-nav__item> <a href=#linear-algebra-review class=md-nav__link> Linear Algebra Review </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#week-2-multiple-featuresvariables-linear-regression class=md-nav__link> Week 2 Multiple features(variables) linear regression </a> <nav class=md-nav aria-label="Week 2 Multiple features(variables) linear regression"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#feature-scaling class=md-nav__link> Feature scaling </a> </li> <li class=md-nav__item> <a href=#learning-rate class=md-nav__link> Learning rate </a> </li> <li class=md-nav__item> <a href=#polynomial-regression class=md-nav__link> Polynomial Regression </a> </li> <li class=md-nav__item> <a href=#normal-equation class=md-nav__link> Normal Equation </a> </li> <li class=md-nav__item> <a href=#normal-equation-and-non-invertibility class=md-nav__link> Normal equation and non-invertibility </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#week-3-logistic-regression-model class=md-nav__link> Week 3 Logistic regression model </a> <nav class=md-nav aria-label="Week 3 Logistic regression model"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#classification-problem class=md-nav__link> Classification problem </a> </li> <li class=md-nav__item> <a href=#hypothesis-representation class=md-nav__link> Hypothesis representation </a> </li> <li class=md-nav__item> <a href=#interpretation class=md-nav__link> Interpretation </a> </li> <li class=md-nav__item> <a href=#logistic-regression-decision-boundary class=md-nav__link> Logistic regression decision boundary </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#logistic-regression-cost-function class=md-nav__link> Logistic regression cost function </a> <nav class=md-nav aria-label="Logistic regression cost function"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#problem-description class=md-nav__link> Problem description </a> </li> <li class=md-nav__item> <a href=#logistic-regression-cost-function_1 class=md-nav__link> Logistic Regression Cost Function </a> </li> <li class=md-nav__item> <a href=#gradient-descent-and-cost-function class=md-nav__link> Gradient descent and cost function </a> <nav class=md-nav aria-label="Gradient descent and cost function"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#gradient-descent-for-logistic-regression class=md-nav__link> Gradient Descent for logistic regression </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#advanced-optimization class=md-nav__link> Advanced Optimization </a> <nav class=md-nav aria-label="Advanced Optimization"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#function-fminunc class=md-nav__link> Function fminunc() </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#multiclass-classification class=md-nav__link> Multiclass classification </a> </li> <li class=md-nav__item> <a href=#regularization class=md-nav__link> Regularization </a> <nav class=md-nav aria-label=Regularization> <ul class=md-nav__list> <li class=md-nav__item> <a href=#address-overfitting-problems class=md-nav__link> Address overfitting problems </a> </li> <li class=md-nav__item> <a href=#regularization-intuition class=md-nav__link> Regularization Intuition </a> </li> <li class=md-nav__item> <a href=#regularized-linear-regression class=md-nav__link> Regularized linear regression </a> </li> <li class=md-nav__item> <a href=#regularized-logistic-regression class=md-nav__link> Regularized logistic regression </a> </li> <li class=md-nav__item> <a href=#advanced-optimization_1 class=md-nav__link> Advanced optimization </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#week-4-neural-networks-model class=md-nav__link> Week 4 Neural networks model </a> <nav class=md-nav aria-label="Week 4 Neural networks model"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#model-representation class=md-nav__link> Model representation </a> </li> <li class=md-nav__item> <a href=#neural-network class=md-nav__link> Neural Network </a> </li> <li class=md-nav__item> <a href=#forward-propagation-implementation class=md-nav__link> Forward propagation implementation </a> </li> <li class=md-nav__item> <a href=#learning-its-own-features class=md-nav__link> Learning its own features </a> </li> <li class=md-nav__item> <a href=#xnor-example class=md-nav__link> XNOR example </a> </li> <li class=md-nav__item> <a href=#multiclass-classification_1 class=md-nav__link> Multiclass classification </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#week-5-neural-networks-learning class=md-nav__link> Week 5 Neural networks learning </a> <nav class=md-nav aria-label="Week 5 Neural networks learning"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#neural-network-classification class=md-nav__link> Neural network classification </a> </li> <li class=md-nav__item> <a href=#cost-function_1 class=md-nav__link> Cost Function </a> <nav class=md-nav aria-label="Cost Function"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#logistic-regression class=md-nav__link> Logistic regression </a> </li> <li class=md-nav__item> <a href=#neural-network_1 class=md-nav__link> Neural Network </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#gradient-computation class=md-nav__link> Gradient computation </a> <nav class=md-nav aria-label="Gradient computation"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#forward-propagation class=md-nav__link> Forward propagation </a> </li> <li class=md-nav__item> <a href=#back-propagation class=md-nav__link> Back propagation </a> </li> <li class=md-nav__item> <a href=#how-to-derive-the-back-prop class=md-nav__link> How to derive the back-prop </a> </li> <li class=md-nav__item> <a href=#backpropagation-algorithm class=md-nav__link> Backpropagation Algorithm </a> </li> <li class=md-nav__item> <a href=#math-representation class=md-nav__link> Math representation </a> </li> <li class=md-nav__item> <a href=#a-high-level-description class=md-nav__link> A high level description </a> </li> <li class=md-nav__item> <a href=#what-we-need-to-watch-out class=md-nav__link> What we need to watch out? </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#backpropagation-intuition class=md-nav__link> Backpropagation Intuition </a> </li> <li class=md-nav__item> <a href=#unrolling-parameters class=md-nav__link> Unrolling parameters </a> </li> <li class=md-nav__item> <a href=#gradient-checking class=md-nav__link> Gradient Checking </a> </li> <li class=md-nav__item> <a href=#random-initialization class=md-nav__link> Random Initialization </a> </li> <li class=md-nav__item> <a href=#putting-it-together class=md-nav__link> Putting it Together </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#week-6-advice-for-applying-machine-learning class=md-nav__link> Week 6 Advice for applying machine learning </a> <nav class=md-nav aria-label="Week 6 Advice for applying machine learning"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#deciding-what-to-try-next class=md-nav__link> Deciding what to try next </a> </li> <li class=md-nav__item> <a href=#machine-learning-diagnostic class=md-nav__link> Machine learning diagnostic </a> </li> <li class=md-nav__item> <a href=#evaluating-a-hypothesis class=md-nav__link> Evaluating a hypothesis </a> </li> <li class=md-nav__item> <a href=#model-selection-and-trainvalidationtest-sets class=md-nav__link> Model selection and Train/Validation/Test sets </a> </li> <li class=md-nav__item> <a href=#diagnosing-bias-vs-variance class=md-nav__link> Diagnosing bias vs. variance </a> </li> <li class=md-nav__item> <a href=#regularization-and-biasvariance class=md-nav__link> Regularization and bias/variance </a> </li> <li class=md-nav__item> <a href=#learning-curve class=md-nav__link> Learning curve </a> <nav class=md-nav aria-label="Learning curve"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#high-bias class=md-nav__link> High bias </a> </li> <li class=md-nav__item> <a href=#high-variance class=md-nav__link> High variance </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#deciding-what-to-do-next-revisited class=md-nav__link> Deciding what to do next revisited </a> </li> <li class=md-nav__item> <a href=#prioritizing-what-to-work-on class=md-nav__link> Prioritizing what to work on </a> </li> <li class=md-nav__item> <a href=#building-a-spam-classifier class=md-nav__link> Building a spam classifier </a> </li> <li class=md-nav__item> <a href=#error-analysis class=md-nav__link> Error analysis </a> </li> <li class=md-nav__item> <a href=#error-metrics-for-skewed-classes class=md-nav__link> Error metrics for skewed classes </a> <nav class=md-nav aria-label="Error metrics for skewed classes"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#precision-and-recall class=md-nav__link> Precision and Recall </a> </li> <li class=md-nav__item> <a href=#trading-off-precision-and-recall class=md-nav__link> Trading off precision and recall </a> </li> <li class=md-nav__item> <a href=#f-score-f_score class=md-nav__link> F score {#f_score} </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#data-for-machine-learning class=md-nav__link> Data for machine learning </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#week-7-support-vector-machine class=md-nav__link> Week 7 Support Vector Machine </a> <nav class=md-nav aria-label="Week 7 Support Vector Machine"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#optimization-objective class=md-nav__link> Optimization objective </a> <nav class=md-nav aria-label="Optimization objective"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#svm-cost-function class=md-nav__link> SVM cost function </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#large-margin-intuition class=md-nav__link> Large margin intuition </a> </li> <li class=md-nav__item> <a href=#mathematics-behind-large-margin-classification class=md-nav__link> Mathematics behind large margin classification </a> </li> <li class=md-nav__item> <a href=#kernel class=md-nav__link> Kernel </a> <nav class=md-nav aria-label=Kernel> <ul class=md-nav__list> <li class=md-nav__item> <a href=#non-linear-decision-boundary class=md-nav__link> Non-linear decision boundary </a> </li> <li class=md-nav__item> <a href=#landmarks class=md-nav__link> Landmarks </a> </li> <li class=md-nav__item> <a href=#gaussian-kernel class=md-nav__link> Gaussian kernel </a> </li> <li class=md-nav__item> <a href=#choosing-the-landmarks class=md-nav__link> Choosing the landmarks </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#using-svm class=md-nav__link> Using SVM </a> <nav class=md-nav aria-label="Using SVM"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#logistic-regression-vs-smvs class=md-nav__link> Logistic regression vs. SMVs </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#week-8-unsupervised-algorithms class=md-nav__link> Week 8 Unsupervised Algorithms </a> <nav class=md-nav aria-label="Week 8 Unsupervised Algorithms"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#k-means-algorithm class=md-nav__link> K-Means Algorithm </a> </li> <li class=md-nav__item> <a href=#optimization-objective_1 class=md-nav__link> Optimization objective </a> </li> <li class=md-nav__item> <a href=#random-initialization_1 class=md-nav__link> Random initialization </a> </li> <li class=md-nav__item> <a href=#choosing-the-k class=md-nav__link> Choosing the K </a> </li> <li class=md-nav__item> <a href=#dimensionality-reduction class=md-nav__link> Dimensionality reduction </a> </li> <li class=md-nav__item> <a href=#principal-component-analysis-pca class=md-nav__link> Principal Component Analysis (PCA) </a> </li> <li class=md-nav__item> <a href=#pca-algorithm class=md-nav__link> PCA algorithm </a> </li> <li class=md-nav__item> <a href=#reconstruction-from-pca-compressed-data class=md-nav__link> Reconstruction from PCA compressed data </a> </li> <li class=md-nav__item> <a href=#choosing-the-number-of-principal-components class=md-nav__link> Choosing the number of principal components </a> </li> <li class=md-nav__item> <a href=#advice-for-applying-pca class=md-nav__link> Advice for applying PCA </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#week-9-anomaly-detection class=md-nav__link> Week 9 Anomaly Detection </a> <nav class=md-nav aria-label="Week 9 Anomaly Detection"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#anomaly-detection class=md-nav__link> Anomaly Detection </a> </li> <li class=md-nav__item> <a href=#developing-and-evaluating class=md-nav__link> Developing and evaluating </a> </li> <li class=md-nav__item> <a href=#anomaly-detection-vs-supervised-learning class=md-nav__link> Anomaly detection vs. supervised learning </a> </li> <li class=md-nav__item> <a href=#multivariate-gaussian-distribution class=md-nav__link> Multivariate Gaussian distribution </a> </li> <li class=md-nav__item> <a href=#predicting-movie-ratings class=md-nav__link> Predicting movie ratings </a> <nav class=md-nav aria-label="Predicting movie ratings"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#problem-formulation class=md-nav__link> Problem formulation </a> </li> <li class=md-nav__item> <a href=#content-based-recommendations class=md-nav__link> Content based recommendations </a> </li> <li class=md-nav__item> <a href=#collaborate-filtering class=md-nav__link> Collaborate filtering </a> </li> <li class=md-nav__item> <a href=#collaborative-filtering-algorithm class=md-nav__link> Collaborative filtering algorithm </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class=md-content> <article class="md-content__inner md-typeset"> <h1 id=machine-learning-coursera>Machine Learning (Coursera)<a class=headerlink href=#machine-learning-coursera title="Permanent link">&para;</a></h1> <h2 id=week-1-introduction>Week 1 Introduction<a class=headerlink href=#week-1-introduction title="Permanent link">&para;</a></h2> <div class="admonition quote"> <p class=admonition-title>Quote</p> <p>Machine Learning is a field of study that gives computers the ability to learn without being explicitly programmed. -- Arthur Samuel (1959)</p> </div> <div class="admonition quote"> <p class=admonition-title>Quote</p> <p>A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E. -- Tom Mitchell (1998)</p> </div> <ul> <li>Well-posed Learning Problem</li> <li>Experience <strong>E</strong></li> <li>Task <strong>T</strong></li> <li>Performance <strong>P</strong></li> <li>Supervised learning: give the "right answer" of existing</li> <li>Unsupervised learning</li> <li>Reinforcement learning</li> <li>Recommendation system</li> <li> <p><strong>Regression V.S. classification</strong></p> <ol> <li>Regression, predict a continuous value across range<ul> <li>a function of independent variable</li> </ul> </li> <li>Classification, predict a discrete value</li> </ol> </li> </ul> <h3 id=cost-function>Cost Function<a class=headerlink href=#cost-function title="Permanent link">&para;</a></h3> <ul> <li>Cost function (squared error function)</li> </ul> <div> <div class=MathJax_Preview>J(\theta_0, \theta_1) = \frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})^2</div> <script type="math/tex; mode=display">J(\theta_0, \theta_1) = \frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})^2</script> </div> <ul> <li>Goal</li> </ul> <div> <div class=MathJax_Preview>\underset{\theta_0, \theta_1}{\text{minimize}} J(\theta_0, \theta_1)</div> <script type="math/tex; mode=display">\underset{\theta_0, \theta_1}{\text{minimize}} J(\theta_0, \theta_1)</script> </div> <ul> <li> <p>Some Intuitions</p> <ol> <li>when the sample is <code>{1, 1}, {2, 2}, and {3, 3}</code>. <span><span class=MathJax_Preview>h_\theta(x) = x, (\theta_0 = 0, \theta_1 = 1)</span><script type=math/tex>h_\theta(x) =  x, (\theta_0 = 0, \theta_1 = 1)</script></span>, we can calculate <span><span class=MathJax_Preview>J(\theta_0, \theta_1) = 0.</span><script type=math/tex>J(\theta_0, \theta_1) = 0.</script></span></li> <li>when the sample is <code>{1, 1}, {2, 2}, and {3, 3}</code>. <span><span class=MathJax_Preview>h_\theta(x) = 0.5 x, (\theta_0 = 0, \theta_1 = 0.5)</span><script type=math/tex>h_\theta(x) =  0.5 x, (\theta_0 = 0, \theta_1 = 0.5)</script></span>, we can calculate <span><span class=MathJax_Preview>J(\theta_1) = \frac{3.5}{6}.</span><script type=math/tex>J(\theta_1) = \frac{3.5}{6}.</script></span></li> <li>we can plot <span><span class=MathJax_Preview>J(\theta_1)</span><script type=math/tex>J(\theta_1)</script></span> vs. <span><span class=MathJax_Preview>\theta_1</span><script type=math/tex>\theta_1</script></span>, the function plotted would be a quadratic curve.</li> </ol> </li> </ul> <h3 id=cost-function-intuition-i>Cost Function Intuition I<a class=headerlink href=#cost-function-intuition-i title="Permanent link">&para;</a></h3> <ul> <li>Andrew plot the hypothesis <span><span class=MathJax_Preview>\textstyle h_\theta(x)</span><script type=math/tex>\textstyle h_\theta(x)</script></span> and the cost function <span><span class=MathJax_Preview>\textstyle {J(\theta_1)}</span><script type=math/tex>\textstyle {J(\theta_1)}</script></span>（setting <span><span class=MathJax_Preview>\theta_0 = 0</span><script type=math/tex>\theta_0 = 0</script></span>）, The function <span><span class=MathJax_Preview>\textstyle J(\theta_1)</span><script type=math/tex>\textstyle J(\theta_1)</script></span> is a curve have a minimum value.</li> </ul> <h3 id=cost-function-intuition-ii>Cost Function Intuition II<a class=headerlink href=#cost-function-intuition-ii title="Permanent link">&para;</a></h3> <ul> <li> <p>The plot of <span><span class=MathJax_Preview>h(x)</span><script type=math/tex>h(x)</script></span> and <span><span class=MathJax_Preview>J(\theta_0, \theta_1)</span><script type=math/tex>J(\theta_0, \theta_1)</script></span> show that <span><span class=MathJax_Preview>J(\theta_0, \theta_1)</span><script type=math/tex>J(\theta_0, \theta_1)</script></span> has to be contour plot.</p> </li> <li> <p>Our goal is to <span><span class=MathJax_Preview>\min_{\theta_0, \theta_1}J(\theta_0, \theta_1)</span><script type=math/tex>\min_{\theta_0, \theta_1}J(\theta_0, \theta_1)</script></span></p> </li> </ul> <p>To summarize, in this lecture, we formulate the hypothesis function and defined the cost function. We also plotted them to get some insight into the two functions. The machine learning problem we learn from this lecture is a minimization problem. Given a hypothesis, we looking for the <span><span class=MathJax_Preview>\theta_0</span><script type=math/tex>\theta_0</script></span> and <span><span class=MathJax_Preview>\theta_1</span><script type=math/tex>\theta_1</script></span> which minimize the cost function. Then we solve the problem. We use the Gradient Descent algorithm to search this minimum value.</p> <h3 id=gradient-descent>Gradient Descent<a class=headerlink href=#gradient-descent title="Permanent link">&para;</a></h3> <ul> <li>It could solve general minimization problems<ul> <li>Given function <span><span class=MathJax_Preview>J(\theta_0, \theta_1)</span><script type=math/tex>J(\theta_0, \theta_1)</script></span></li> <li>Objective: <span><span class=MathJax_Preview>\min_{\theta_0, \theta_1}J(\theta_0, \theta_1)</span><script type=math/tex>\min_{\theta_0, \theta_1}J(\theta_0, \theta_1)</script></span></li> </ul> </li> </ul> <div class="admonition example"> <p class=admonition-title>Gradient Descent Algorithm</p> <ul> <li>Start with some <span><span class=MathJax_Preview>\theta_0, \theta_1</span><script type=math/tex>\theta_0, \theta_1</script></span><ul> <li>Keep changing <span><span class=MathJax_Preview>\theta_0, \theta_1</span><script type=math/tex>\theta_0, \theta_1</script></span> to reduce <span><span class=MathJax_Preview>J(\theta_0, \theta_1)</span><script type=math/tex>J(\theta_0, \theta_1)</script></span><ul> <li>Simultianeously update: <span><span class=MathJax_Preview>\theta_j := \theta_j - \alpha \frac{\partial }{\partial \theta_j}J(\theta_0, \theta_1)</span><script type=math/tex>\theta_j := \theta_j - \alpha \frac{\partial }{\partial \theta_j}J(\theta_0, \theta_1)</script></span>, for <span><span class=MathJax_Preview>( j = 0, j = 1)</span><script type=math/tex>( j = 0, j = 1)</script></span></li> </ul> </li> </ul> </li> <li>Untill we hopefully end up at the minimum.(convergence)</li> </ul> </div> <ul> <li>Question: How to implement the algorithm.</li> </ul> <h3 id=gradient-descent-intuition>Gradient Descent Intuition<a class=headerlink href=#gradient-descent-intuition title="Permanent link">&para;</a></h3> <ul> <li>learning rate<ol> <li>don't have to adjust the learning rate</li> </ol> </li> <li>derivative<ol> <li>would reduce automatically</li> </ol> </li> </ul> <h3 id=gradient-descent-and-linear-regression>Gradient Descent and Linear Regression<a class=headerlink href=#gradient-descent-and-linear-regression title="Permanent link">&para;</a></h3> <ul> <li>Take the linear regression cost function and apply gradient descent algorithm.</li> <li>Model: <span><span class=MathJax_Preview>h_\theta(x) = \theta_0 + \theta_1x</span><script type=math/tex>h_\theta(x) = \theta_0 + \theta_1x</script></span></li> <li>Cost function: <span><span class=MathJax_Preview>J(\theta_0, \theta_1) = \frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})^2</span><script type=math/tex>J(\theta_0, \theta_1) = \frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})^2</script></span></li> <li>Gradient Descent in Linear Regression.</li> </ul> <p>repeat <span><span class=MathJax_Preview>\begin{Bmatrix} \theta_0 := \theta_0 - \alpha \frac{1}{m} \sum_{i = 1}^{m}(h_\theta(x^{(i)}) - y^{(i)}) \\ \theta_1 := \theta_1 - \alpha \frac{1}{m} \sum_{i = 1}^{m}(h_\theta(x^{(i)}) - y^{(i)}) x^{(i)} \end{Bmatrix}</span><script type=math/tex>\begin{Bmatrix}
\theta_0 := \theta_0 - \alpha \frac{1}{m} \sum_{i = 1}^{m}(h_\theta(x^{(i)}) - y^{(i)})  \\
\theta_1 := \theta_1 - \alpha \frac{1}{m} \sum_{i = 1}^{m}(h_\theta(x^{(i)}) - y^{(i)}) x^{(i)}
\end{Bmatrix}</script></span> simultaneously.</p> <ul> <li>The linear regression cost function is always a <strong>convex function</strong> - always has a single minimum (bowl Shaped).</li> </ul> <h3 id=linear-algebra-review>Linear Algebra Review<a class=headerlink href=#linear-algebra-review title="Permanent link">&para;</a></h3> <ol> <li><strong>Not commutative:</strong> A x B != B x A except that matrix B is identity matrix.</li> <li><strong>Associativity:</strong> (A x B) x C = A x (B x C)</li> <li><strong>Identity Matrix:</strong><ul> <li>Any matrix <strong>A</strong> which can be multiplied by an identity matrix gives you matrix <strong>A</strong> back</li> <li>I[m, m] x A[m, n] = A[m, n]</li> <li>A[m, n] x I[n, n] = A[m,n]</li> </ul> </li> <li><strong>Inverse</strong><ul> <li>Multiplicative inverse(reciprocal): a number multiply it by to get the identify element. i.e. if you have <code>x</code>, <code>x * 1/x = 1</code>.</li> <li>Additive inverse: -3 is 3's additive inverse.</li> </ul> </li> <li><strong>Matrix inverse</strong><ul> <li><span><span class=MathJax_Preview>A \cdot A^{-1} = I</span><script type=math/tex>A \cdot A^{-1} = I</script></span></li> <li>Only square matrix, but not all square matrix have an inverse.</li> </ul> </li> <li><strong>singularity</strong><ul> <li>If A is all zeros then there is no inverse matrix</li> <li>Some others don't, intuition should be matrices that don't have an inverse are a <strong>singular matrix</strong> or a <strong>degenerate matrix</strong> (i.e. when it's too close to 0)</li> <li>So if all the values of a matrix reach zero, this can be described as reaching <strong>singularity</strong></li> </ul> </li> </ol> <h2 id=week-2-multiple-featuresvariables-linear-regression>Week 2 Multiple features(variables) linear regression<a class=headerlink href=#week-2-multiple-featuresvariables-linear-regression title="Permanent link">&para;</a></h2> <ol> <li>From feature <span><span class=MathJax_Preview>x^{(i)}</span><script type=math/tex>x^{(i)}</script></span>to features <span><span class=MathJax_Preview>x_j^{(i)}</span><script type=math/tex>x_j^{(i)}</script></span>, means the <span><span class=MathJax_Preview>j</span><script type=math/tex>j</script></span>th feature element of the <span><span class=MathJax_Preview>i</span><script type=math/tex>i</script></span>th feature vector.</li> <li>Hypothesis becomes: <span><span class=MathJax_Preview>\textstyle h_\theta(x) = \theta^{T}x = \theta_0x_0 + \theta_1x_1 + ... + \theta_nx_n</span><script type=math/tex>\textstyle h_\theta(x) = \theta^{T}x = \theta_0x_0 + \theta_1x_1 + ... + \theta_nx_n</script></span>.</li> <li>Cost Function: <span><span class=MathJax_Preview>J(\theta_0, \theta_1, ..., \theta_n) = \frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})^2</span><script type=math/tex>J(\theta_0, \theta_1, ..., \theta_n) = \frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})^2</script></span></li> <li>Gradient descent: <span><span class=MathJax_Preview>\theta_j := \theta_j - \alpha \frac{\partial }{\partial \theta_j}J(\theta_0, \theta_1, ..., \theta_n)</span><script type=math/tex>\theta_j := \theta_j - \alpha \frac{\partial }{\partial \theta_j}J(\theta_0, \theta_1, ..., \theta_n)</script></span>, simultaneously update for every <span><span class=MathJax_Preview>j = 0, 1, 2, ..., n</span><script type=math/tex>j = 0, 1, 2, ..., n</script></span>.</li> <li>Compare of Gradient descent for multiple variables: <img alt="Gradient Descent" src=../fig/mutivariable_gradient_descent.png></li> </ol> <h3 id=feature-scaling>Feature scaling<a class=headerlink href=#feature-scaling title="Permanent link">&para;</a></h3> <p>One of the idea to improve the result in gradient descent is to make different features in the same scale, i.e. <span><span class=MathJax_Preview>-1 \leq x_i \leq 1</span><script type=math/tex>-1 \leq x_i \leq 1</script></span> range .</p> <p>For example, in the housing price prediction problem, we have to make the size of the house (0-2000 square feet), and the number of rooms in the house (1-5 rooms) in the same scale. We do this by the method of <strong>mean normalization.</strong></p> <ul> <li><strong>Mean normalization</strong> Replace <span><span class=MathJax_Preview>x_i</span><script type=math/tex>x_i</script></span> with <span><span class=MathJax_Preview>x_i - \mu_i</span><script type=math/tex>x_i - \mu_i</script></span> to make features have approximately zero mean (Do not apply to <span><span class=MathJax_Preview>x_0 = 1</span><script type=math/tex>x_0 = 1</script></span>)<ul> <li>E.g. <span><span class=MathJax_Preview>x_1 = \frac{size - 1000}{2000}</span><script type=math/tex>x_1 = \frac{size - 1000}{2000}</script></span>, <span><span class=MathJax_Preview>x_2 = \frac{\#rooms - 2}{5(range)}</span><script type=math/tex>x_2 = \frac{\#rooms - 2}{5(range)}</script></span> to make <span><span class=MathJax_Preview>-0.5 \le x_1 \le 0.5, -0.5 \le x_2 \le 0.5,</span><script type=math/tex>-0.5 \le x_1 \le 0.5, -0.5 \le x_2 \le 0.5,</script></span></li> </ul> </li> </ul> <h3 id=learning-rate>Learning rate<a class=headerlink href=#learning-rate title="Permanent link">&para;</a></h3> <p>Another idea to improve the gradient descent algorithm is to select the learning rate <span><span class=MathJax_Preview>\alpha</span><script type=math/tex>\alpha</script></span> to make the algorithm works correctly.</p> <ul> <li><strong>Convergence test:</strong> we can declare convergence if <span><span class=MathJax_Preview>J(\theta)</span><script type=math/tex>J(\theta)</script></span> decreases by less than <span><span class=MathJax_Preview>10^{-3}</span><script type=math/tex>10^{-3}</script></span> in one iteration.</li> </ul> <p>For sufficient small <span><span class=MathJax_Preview>\alpha</span><script type=math/tex>\alpha</script></span>, <span><span class=MathJax_Preview>J(\theta)</span><script type=math/tex>J(\theta)</script></span> should decrease on every iteration, but if <span><span class=MathJax_Preview>\alpha</span><script type=math/tex>\alpha</script></span> is too small, gradient descent can be slow to converge. If <span><span class=MathJax_Preview>\alpha</span><script type=math/tex>\alpha</script></span> is too large: <span><span class=MathJax_Preview>J(\theta)</span><script type=math/tex>J(\theta)</script></span> may not decrease on every iteration, may not converge.</p> <ul> <li><strong>Rule of Thumb:</strong> to choose <span><span class=MathJax_Preview>\alpha</span><script type=math/tex>\alpha</script></span>, try these numbers <code>..., 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, ...</code></li> </ul> <h3 id=polynomial-regression>Polynomial Regression<a class=headerlink href=#polynomial-regression title="Permanent link">&para;</a></h3> <p>In the house price prediction proble. if two features selected are frontage and depth. we can also take the polynomial problem, take the area as a single feature, thusly reduce the problem to a linear regression. We could even take the higher polynomials of the size feature, like second order, third order, and so on. for example,</p> <p><span><span class=MathJax_Preview>h_\theta = \theta_0 + \theta_1x_1 + \theta_2x_2 + \theta_3x_3 = \theta_0 + \theta_1(size) + \theta_2(size)^2 + \theta_3(size)^3</span><script type=math/tex>h_\theta = \theta_0 + \theta_1x_1 + \theta_2x_2 + \theta_3x_3 = \theta_0 + \theta_1(size) + \theta_2(size)^2 + \theta_3(size)^3</script></span></p> <p>In this case, the polynomial regression problem will fit in a curve instead of a line.</p> <h3 id=normal-equation>Normal Equation<a class=headerlink href=#normal-equation title="Permanent link">&para;</a></h3> <p>Normal Equation: Method to solve for θ analytically. The intuition comes that we can mathematically sovle <span><span class=MathJax_Preview>\frac{\partial}{\partial\theta_j}J(\Theta) = 0</span><script type=math/tex>\frac{\partial}{\partial\theta_j}J(\Theta) = 0</script></span> for <span><span class=MathJax_Preview>\theta_0, \theta_1, \dots, \theta_n</span><script type=math/tex>\theta_0, \theta_1, \dots, \theta_n</script></span>, given the <span><span class=MathJax_Preview>J(\Theta)</span><script type=math/tex>J(\Theta)</script></span>.</p> <p>The equation to get the value of <span><span class=MathJax_Preview>\Theta</span><script type=math/tex>\Theta</script></span> is <span><span class=MathJax_Preview>\Theta = (X^TX)^{-1}X^Ty</span><script type=math/tex>\Theta = (X^TX)^{-1}X^Ty</script></span>, in Octave, you can calculate by the command <code>pinv(X'*X)*X'*y)</code>, pay attention to the dimention of the the matrix <code>X</code> and <code>y</code>.</p> <h3 id=normal-equation-and-non-invertibility>Normal equation and non-invertibility<a class=headerlink href=#normal-equation-and-non-invertibility title="Permanent link">&para;</a></h3> <p>TBD</p> <h2 id=week-3-logistic-regression-model>Week 3 Logistic regression model<a class=headerlink href=#week-3-logistic-regression-model title="Permanent link">&para;</a></h2> <h4 id=classification-problem>Classification problem<a class=headerlink href=#classification-problem title="Permanent link">&para;</a></h4> <ul> <li>Email: spam / Not spam?</li> <li>Tumor: Malignant / Benign?</li> <li> <p>Example:</p> <p><img alt="Tumor Example" src=../fig/tumor-example.png></p> </li> <li> <p>y is either 0 or 1</p> </li> <li>0: Negative class</li> <li>1: Positive class.</li> <li>With linear regression and a threshold value, we can use linear regression to solve classification problem.</li> <li>However linear regression could result in the case when <span><span class=MathJax_Preview>h_\theta(x)</span><script type=math/tex>h_\theta(x)</script></span> is &gt; 1 or &lt; 0.</li> <li>Need a different method which will make <span><span class=MathJax_Preview>0 \le h_\theta(x) \le 1</span><script type=math/tex>0 \le h_\theta(x) \le 1</script></span>. This is why logistic regression comes in.</li> </ul> <h4 id=hypothesis-representation>Hypothesis representation<a class=headerlink href=#hypothesis-representation title="Permanent link">&para;</a></h4> <p>Because we want to have <span><span class=MathJax_Preview>0 \le h_\theta(x) \le 1</span><script type=math/tex>0 \le h_\theta(x) \le 1</script></span>, the domain of sigmoid function is in the range. From linear regression <span><span class=MathJax_Preview>h_\theta(x) = \theta^Tx</span><script type=math/tex>h_\theta(x) = \theta^Tx</script></span>, we can have <span><span class=MathJax_Preview>h_\theta(x)</span><script type=math/tex>h_\theta(x)</script></span> for logistic regression:</p> <div> <div class=MathJax_Preview>h_{\theta}(x) = g_\theta(\theta^Tx) = \frac{1}{1 + e^{-\theta^Tx}}, 0 \le h_\theta(x) \le 1</div> <script type="math/tex; mode=display">h_{\theta}(x) = g_\theta(\theta^Tx) = \frac{1}{1 + e^{-\theta^Tx}}, 0 \le h_\theta(x) \le 1</script> </div> <p><span><span class=MathJax_Preview>g_\theta(z) = \frac{1}{1 + e^{-z}}</span><script type=math/tex>g_\theta(z) = \frac{1}{1 + e^{-z}}</script></span> is <a href=https://en.wikipedia.org/wiki/Sigmoid_function>Sigmoid function</a>, also called logistic function. now in logistic regress model, we can write</p> <div> <div class=MathJax_Preview>h_\theta(x) = \frac{1}{1 + e^{-\theta^Tx}}.</div> <script type="math/tex; mode=display">h_\theta(x) = \frac{1}{1 + e^{-\theta^Tx}}.</script> </div> <h4 id=interpretation>Interpretation<a class=headerlink href=#interpretation title="Permanent link">&para;</a></h4> <ul> <li>Logistic regression model <span><span class=MathJax_Preview>h_\theta(x) = \frac{1}{1 + e^{-\theta^Tx}},</span><script type=math/tex>h_\theta(x) = \frac{1}{1 + e^{-\theta^Tx}},</script></span> is the estimated probability that y = 1 on input x. i.e. <span><span class=MathJax_Preview>h_\theta(x) = 0.7</span><script type=math/tex>h_\theta(x) = 0.7</script></span> tell patient that 70% chance of tumore being malignant.</li> <li><span><span class=MathJax_Preview>h_\theta(x) = P(y = 1|x;\theta)</span><script type=math/tex>h_\theta(x) = P(y = 1|x;\theta)</script></span>,</li> <li><span><span class=MathJax_Preview>P(y=0|x; \theta) + P(y=1|x;\theta) = 1</span><script type=math/tex>P(y=0|x; \theta) + P(y=1|x;\theta) = 1</script></span></li> </ul> <h3 id=logistic-regression-decision-boundary>Logistic regression decision boundary<a class=headerlink href=#logistic-regression-decision-boundary title="Permanent link">&para;</a></h3> <ol> <li>Gives a better sense of what the hypothesis function is computing</li> <li>Better understanding of what the hypothesis function looks like</li> <li>One way of using the sigmoid function is:<ul> <li>When the probability of y being 1 is greater than 0.5 then we can predict y = 1</li> <li>Else we predict y = 0</li> </ul> </li> <li>When is it exactly that <span><span class=MathJax_Preview>h_\theta(x)</span><script type=math/tex>h_\theta(x)</script></span> is greater than 0.5?<ul> <li>Look at sigmoid function<ul> <li>g(z) is greater than or equal to 0.5 when z is greater than or equal to 0.</li> </ul> </li> <li>So if z is positive, g(z) is greater than 0.5<ul> <li><span><span class=MathJax_Preview>z = (\theta^T x)</span><script type=math/tex>z = (\theta^T x)</script></span></li> </ul> </li> <li>So when<ul> <li><span><span class=MathJax_Preview>\theta^T x &gt;= 0</span><script type=math/tex>\theta^T x >= 0</script></span></li> </ul> </li> <li>Then <span><span class=MathJax_Preview>h_\theta(x) &gt;= 0.5</span><script type=math/tex>h_\theta(x) >= 0.5</script></span></li> </ul> </li> <li>So what we've shown is that the hypothesis predicts y = 1 when <span><span class=MathJax_Preview>\theta^T x &gt;= 0</span><script type=math/tex>\theta^T x >= 0</script></span><ul> <li>The corollary of that when <span><span class=MathJax_Preview>\theta^T x &lt;= 0</span><script type=math/tex>\theta^T x <= 0</script></span> then the hypothesis predicts y = 0</li> <li>Let's use this to better understand how the hypothesis makes its predictions</li> </ul> </li> <li>Linear Decision boundary and non-linear decision boundary <img alt="Linear Decision Boundary" src=../fig/linear-decision-boundary.png> <img alt="Non Linear Decision Boundary" src=../fig/non-linear-decision-boundary.png></li> </ol> <h2 id=logistic-regression-cost-function>Logistic regression cost function<a class=headerlink href=#logistic-regression-cost-function title="Permanent link">&para;</a></h2> <h3 id=problem-description>Problem description<a class=headerlink href=#problem-description title="Permanent link">&para;</a></h3> <p><img alt="Logistic Regression" src=../fig/logistic-regression-problem-statement.png></p> <h3 id=logistic-regression-cost-function_1>Logistic Regression Cost Function<a class=headerlink href=#logistic-regression-cost-function_1 title="Permanent link">&para;</a></h3> <p>Out goal to solve a logistic regression problem is to reduce the cost incurred when predict wrong result. In linear regression, we minimize the cost function with respect to vector <span><span class=MathJax_Preview>\theta</span><script type=math/tex>\theta</script></span>. Generally, we can think the cost function as a penalty of the incorrect classification. It is a qualitative measure of such penalty. For example, we use the squared error cost function in linear regression:</p> <div> <div class=MathJax_Preview> J(\theta) = \frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})^2. </div> <script type="math/tex; mode=display">
J(\theta) = \frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})^2.
</script> </div> <p>In logistic regression, the <span><span class=MathJax_Preview>h_\theta(x)</span><script type=math/tex>h_\theta(x)</script></span> is a much complex function, so the cost function <span><span class=MathJax_Preview>J(\theta)</span><script type=math/tex>J(\theta)</script></span> used in linear regression will be a non-convex function in logisitic regress. This will produce a hard problem to solve logistic problems numerically. So we define a convex logistic regression cost function</p> <div> <div class=MathJax_Preview> Cost(h_\theta(x), y) =\begin{cases} -log(h_\theta(x)) &amp; \text{if}\ y = 1\\ -log(1-h_\theta(x)) &amp; \text{if}\ y = 0 \end{cases} </div> <script type="math/tex; mode=display">
Cost(h_\theta(x), y) =\begin{cases}
               -log(h_\theta(x))    & \text{if}\  y = 1\\
               -log(1-h_\theta(x)) & \text{if}\ y = 0
            \end{cases}
</script> </div> <p>It is the logistic regression cost function, it can be interpreted as the penalty the algorithm pays.</p> <p>We can plot the function as following:</p> <p><img alt="Logistic Regression Cost Function" src=../fig/logistic-regression-cost-function.png></p> <p>Some intuition/properties about the simplified logistic regression cost function:</p> <ul> <li>X axis is what we predict</li> <li>Y axis is the cost associated with that prediction.</li> <li>If y = 1, and <span><span class=MathJax_Preview>h_\theta(x) = 1</span><script type=math/tex>h_\theta(x) = 1</script></span>, hypothesis predicts exactly 1 (exactly correct) then cost corresponds to 0 (no cost for correct predict)</li> <li>If y = 1, and <span><span class=MathJax_Preview>h_\theta(x) = 0</span><script type=math/tex>h_\theta(x) = 0</script></span>, predict <span><span class=MathJax_Preview>P(y = 1|x; \theta) = 0</span><script type=math/tex>P(y = 1|x; \theta) = 0</script></span>, this is wrong, and it penalized with a massive cost (the cost approach positive infinity).</li> <li>Similar reasoning holds for y = 0.</li> </ul> <h3 id=gradient-descent-and-cost-function>Gradient descent and cost function<a class=headerlink href=#gradient-descent-and-cost-function title="Permanent link">&para;</a></h3> <p>We can neatly write the logistic regression function in the following format:</p> <div> <div class=MathJax_Preview> cost(h_\theta(x), y) = -y\log(h_\theta(x)) - (1 - y)\log(1-h_\theta(x)) </div> <script type="math/tex; mode=display">
cost(h_\theta(x), y) = -y\log(h_\theta(x)) - (1 - y)\log(1-h_\theta(x))
</script> </div> <p>We can take this cost function and obtained the logistic regression cost function <span><span class=MathJax_Preview>J(\theta)</span><script type=math/tex>J(\theta)</script></span>:</p> <div> <div class=MathJax_Preview> \begin{align*} J(\theta) &amp; = \frac{1}{m}\sum_{i=1}^{m}Cost(h_\theta(x^{(i)}),y^{(i)}) \\ &amp; = -\frac{1}{m}\Big[\sum_{i=1}^{m}y^{(i)}\log(h_\theta(x^{(i)})) + (1 - y^{(i)})\log(1-h_\theta(x^{(i)}))\Big] \end{align*} </div> <script type="math/tex; mode=display">
\begin{align*}
J(\theta) & = \frac{1}{m}\sum_{i=1}^{m}Cost(h_\theta(x^{(i)}),y^{(i)}) \\
 & = -\frac{1}{m}\Big[\sum_{i=1}^{m}y^{(i)}\log(h_\theta(x^{(i)})) + (1 - y^{(i)})\log(1-h_\theta(x^{(i)}))\Big]
\end{align*}
</script> </div> <div class="admonition note"> <p class=admonition-title>Why do we chose this function when other cost functions exist?</p> <ul> <li>This cost function can be derived from statistics using the principle of <strong><em>maximum likelihood estimation</em></strong>.<ul> <li>Note this does mean there's an underlying Gaussian assumption relating to the distribution of features.</li> </ul> </li> <li>Also has the nice property that it's <strong><em>convex</em></strong> function.</li> </ul> </div> <h4 id=gradient-descent-for-logistic-regression>Gradient Descent for logistic regression<a class=headerlink href=#gradient-descent-for-logistic-regression title="Permanent link">&para;</a></h4> <p>The algorithm looks identical to linear regression except the hypothesis function is Sigmoid function and not linear any more.</p> <p><img alt="Logistic Regression Gradient Descent" src=../fig/logistic-regression-gradient-descent.png></p> <h3 id=advanced-optimization>Advanced Optimization<a class=headerlink href=#advanced-optimization title="Permanent link">&para;</a></h3> <p>Beside gradient descent algorithm, there are many other optimization algorithm such as conjugate gradient, BFGS, and L-BFGS, can minimization the cost function for solving logistic regression problem. Most of those advanced algorithms are more efficient to compute, you don't have to select the convergent rate <span><span class=MathJax_Preview>\alpha</span><script type=math/tex>\alpha</script></span>.</p> <p>Here is one function in Octave library, possibly also in Matlab, can be used for finding the <span><span class=MathJax_Preview>\theta</span><script type=math/tex>\theta</script></span></p> <h4 id=function-fminunc>Function fminunc()<a class=headerlink href=#function-fminunc title="Permanent link">&para;</a></h4> <p>Octave have a function <code>fminunc()</code>.<a href=https://www.gnu.org/software/octave/doc/interpreter/Minimizers.html>^1</a> To use it, we should first call <code>optimset()</code>. There is three steps we need to take care to solve optimization problem using these functions</p> <ol> <li>calculate the cost function <span><span class=MathJax_Preview>J(\theta)</span><script type=math/tex>J(\theta)</script></span></li> <li>calculate the gradient functions <span><span class=MathJax_Preview>\frac{\partial}{\partial\theta_j}J(\theta)</span><script type=math/tex>\frac{\partial}{\partial\theta_j}J(\theta)</script></span></li> <li>give initial <span><span class=MathJax_Preview>\theta</span><script type=math/tex>\theta</script></span> value</li> <li> <p>call the function <code>optimset()</code> and <code>fminunc()</code> as following <div class=highlight><pre><span></span><code><span class=k>function</span><span class=w> </span>[jVal, gradient] <span class=p>=</span><span class=w> </span><span class=nf>costFunction</span><span class=p>(</span>theta<span class=p>)</span><span class=w></span>
<span class=w>        </span><span class=n>jVal</span> <span class=p>=</span> <span class=p>(</span><span class=n>theta</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span><span class=o>-</span><span class=mi>5</span><span class=p>)</span>^<span class=mi>2</span> <span class=o>+</span> <span class=p>(</span><span class=n>theta</span><span class=p>(</span><span class=mi>2</span><span class=p>)</span><span class=o>-</span><span class=mi>5</span><span class=p>)</span>^<span class=mi>2</span><span class=p>;</span>
        <span class=nb>gradient</span> <span class=p>=</span> <span class=nb>zeros</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span><span class=mi>1</span><span class=p>);</span>
        <span class=nb>gradient</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span> <span class=p>=</span> <span class=mi>2</span><span class=o>*</span><span class=p>(</span><span class=n>theta</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span><span class=o>-</span><span class=mi>5</span><span class=p>);</span>
        <span class=nb>gradient</span><span class=p>(</span><span class=mi>2</span><span class=p>)</span> <span class=p>=</span> <span class=mi>2</span><span class=o>*</span><span class=p>(</span><span class=n>theta</span><span class=p>(</span><span class=mi>2</span><span class=p>)</span><span class=o>-</span><span class=mi>5</span><span class=p>);</span>

<span class=n>options</span> <span class=p>=</span> <span class=nb>optimset</span><span class=p>(</span>‘<span class=n>GradObj</span>’<span class=p>,</span> ‘<span class=n>on</span>’<span class=p>,</span> ‘<span class=n>MaxIter</span>’<span class=p>,</span> ‘<span class=mi>100</span>’<span class=p>);</span>
<span class=n>initialTheta</span> <span class=p>=</span> <span class=nb>zeros</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span><span class=mi>1</span><span class=p>);</span>
<span class=p>[</span><span class=n>optTheta</span><span class=p>,</span> <span class=n>functionVal</span><span class=p>,</span> <span class=n>exitFlag</span><span class=p>]</span> <span class=p>=</span> <span class=n>fminunc</span><span class=p>(@</span><span class=n>costFunction</span><span class=p>,</span> <span class=n>initialTheta</span><span class=p>,</span> <span class=n>options</span><span class=p>);</span>
</code></pre></div></p> </li> <li> <p>Note we have <code>n+1</code> parameters, we implement the code in the following way. <img alt="Logistic Regression Cost Function Optimization" src=../fig/logistic-regression-cost-function-optimization.png></p> </li> </ol> <h3 id=multiclass-classification>Multiclass classification<a class=headerlink href=#multiclass-classification title="Permanent link">&para;</a></h3> <p><img alt="Multi Class One versus All" src=../fig/logistic-regression-one-over-all-class.png></p> <ul> <li>Train a logistic regression classifier <span><span class=MathJax_Preview>h_{\theta}^{(i)}(x)</span><script type=math/tex>h_{\theta}^{(i)}(x)</script></span> for each class <span><span class=MathJax_Preview>i</span><script type=math/tex>i</script></span> to predict the probability that <span><span class=MathJax_Preview>y = i</span><script type=math/tex>y = i</script></span>.</li> <li>On a new input <span><span class=MathJax_Preview>x</span><script type=math/tex>x</script></span>, to make a prediction, pick the class <span><span class=MathJax_Preview>i</span><script type=math/tex>i</script></span> that maximize the <span><span class=MathJax_Preview>h</span><script type=math/tex>h</script></span>, <span><span class=MathJax_Preview>\operatorname*{argmax}_i h_{\theta}^{(i)}(x)</span><script type=math/tex>\operatorname*{argmax}_i h_{\theta}^{(i)}(x)</script></span></li> </ul> <h3 id=regularization>Regularization<a class=headerlink href=#regularization title="Permanent link">&para;</a></h3> <div class="admonition note"> <p class=admonition-title>Overfitting</p> <p>If we have too many features, the learned hypothesis may fit the training set very well ( <span><span class=MathJax_Preview>J(\theta) \approx 0</span><script type=math/tex>J(\theta) \approx 0</script></span> ), but fail to generalize to new examples (predict prices on new examples).</p> </div> <h4 id=address-overfitting-problems>Address overfitting problems<a class=headerlink href=#address-overfitting-problems title="Permanent link">&para;</a></h4> <ol> <li>Reduce the number of features<ol> <li>Manually select what features to keep</li> <li>feature selection algorithm</li> </ol> </li> <li>Regularization<ol> <li>Keep all the features, reduce the value of the parameter <span><span class=MathJax_Preview>\theta_j</span><script type=math/tex>\theta_j</script></span>.</li> <li>work well when we have a lot of data when each of the features contribute to the algorithm a bit to predict <span><span class=MathJax_Preview>y</span><script type=math/tex>y</script></span>.</li> </ol> </li> </ol> <h4 id=regularization-intuition>Regularization Intuition<a class=headerlink href=#regularization-intuition title="Permanent link">&para;</a></h4> <p>To penalize the higher order of polynomial by adding extra terms of high coefficient for <span><span class=MathJax_Preview>\theta^n</span><script type=math/tex>\theta^n</script></span> terms. i.e. from the cost function minimization problem, <span><span class=MathJax_Preview>J(\theta) = \frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})^2</span><script type=math/tex>J(\theta) = \frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})^2</script></span>, we can instead use the regularized form <span><span class=MathJax_Preview>J(\theta) = \frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})^2 + \lambda\sum_{j = 1}^{n}\theta_j^2</span><script type=math/tex>J(\theta) = \frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})^2 + \lambda\sum_{j = 1}^{n}\theta_j^2</script></span>. Small values for parameters <span><span class=MathJax_Preview>\theta_j</span><script type=math/tex>\theta_j</script></span> will make "Simpler" hypothesis and Less prone to overfitting. Please note that the <span><span class=MathJax_Preview>\theta_0</span><script type=math/tex>\theta_0</script></span> is <strong>excluded</strong> from the regularization term <span><span class=MathJax_Preview>\lambda\sum_{n = 1}^{n}\theta_j^2</span><script type=math/tex>\lambda\sum_{n = 1}^{n}\theta_j^2</script></span>.</p> <p><img alt=Regularization src=../fig/regularization.png> <img alt="Regularization Question" src=../fig/regularization_question.png></p> <h4 id=regularized-linear-regression>Regularized linear regression<a class=headerlink href=#regularized-linear-regression title="Permanent link">&para;</a></h4> <p>Gradient Descent:</p> <p><img alt="Regularized Linear Regression" src=../fig/regularized_linear_regression.png></p> <ul> <li>We don't regularize <span><span class=MathJax_Preview>\theta_0</span><script type=math/tex>\theta_0</script></span>, so we explicitly update it in the formula and it is the same with the non-regularized linear regression gradient descent. All other <span><span class=MathJax_Preview>\theta_j</span><script type=math/tex>\theta_j</script></span>, <span><span class=MathJax_Preview>j = 1, ..., n</span><script type=math/tex>j = 1, ..., n</script></span> will update differently.</li> </ul> <h4 id=regularized-logistic-regression>Regularized logistic regression<a class=headerlink href=#regularized-logistic-regression title="Permanent link">&para;</a></h4> <p><img alt="Regularized Logistic Regression" src=../fig/regularized_logistic_regression.png></p> <ul> <li>Doesn't regularize the <span><span class=MathJax_Preview>\theta_0</span><script type=math/tex>\theta_0</script></span></li> <li>The cost function is different from the linear regression</li> </ul> <h4 id=advanced-optimization_1>Advanced optimization<a class=headerlink href=#advanced-optimization_1 title="Permanent link">&para;</a></h4> <p><img alt="Regularized Advanced Optimization" src=../fig/regularized_advanced_optimization.png></p> <ul> <li>Add the regularized term in the cost function and gradient calculation</li> </ul> <h2 id=week-4-neural-networks-model>Week 4 Neural networks model<a class=headerlink href=#week-4-neural-networks-model title="Permanent link">&para;</a></h2> <p>Neural Networks is originated when people try to mimic the functionality of brain by an algorithm.</p> <h3 id=model-representation>Model representation<a class=headerlink href=#model-representation title="Permanent link">&para;</a></h3> <p>In an artificial neural network, a neuron is a logistic unit that</p> <ol> <li>Feed input via input wires</li> <li>Logistic unit does the computation</li> <li>Sends output down output wires</li> </ol> <p>The logistic computation is just like our previous logistic regression hypothesis calculation.</p> <p><img alt="Logistic Unit" src=../fig/logistic-unit.png></p> <h3 id=neural-network>Neural Network<a class=headerlink href=#neural-network title="Permanent link">&para;</a></h3> <p><img alt="Neural Netowrk" src=../fig/neural-network.png></p> <p>Use the following notation convention:</p> <ul> <li><span><span class=MathJax_Preview>a_i^{(j)}</span><script type=math/tex>a_i^{(j)}</script></span> to represent the \"activation\" of unit <span><span class=MathJax_Preview>i</span><script type=math/tex>i</script></span> in layer <span><span class=MathJax_Preview>j</span><script type=math/tex>j</script></span></li> <li><span><span class=MathJax_Preview>\Theta^{(j)}</span><script type=math/tex>\Theta^{(j)}</script></span> to represent the matrix of weights controlling function mapping from layer <span><span class=MathJax_Preview>j</span><script type=math/tex>j</script></span> to layer <span><span class=MathJax_Preview>j+1</span><script type=math/tex>j+1</script></span></li> </ul> <p>The value at each node can be calculated as</p> <div> <div class=MathJax_Preview> a_1^{(2)} = g(\theta_{10}^{(1)}x_0 + \theta_{11}^{(1)}x_1 + \theta_{12}^{(1)}x_2 + \theta_{13}^{(1)}x_3) \\ a_2^{(2)} = g(\theta_{20}^{(1)}x_0 + \theta_{21}^{(1)}x_1 + \theta_{22}^{(1)}x_2 + \theta_{23}^{(1)}x_3) \\ a_3^{(2)} = g(\theta_{30}^{(1)}x_0 + \theta_{31}^{(1)}x_1 + \theta_{32}^{(1)}x_2 + \theta_{33}^{(1)}x_3) \\ h_{\Theta}(x) = a_1^{(3)} = g(\theta_{10}^{(2)}a_0^{(2)} + \theta_{11}^{(2)}a_1^{(2)} + \theta_{12}^{(2)}a_2^{(2)} + \theta_{13}^{(2)}a_3^{(2)}) </div> <script type="math/tex; mode=display">
a_1^{(2)} = g(\theta_{10}^{(1)}x_0 + \theta_{11}^{(1)}x_1 + \theta_{12}^{(1)}x_2 + \theta_{13}^{(1)}x_3) \\
a_2^{(2)} = g(\theta_{20}^{(1)}x_0 + \theta_{21}^{(1)}x_1 + \theta_{22}^{(1)}x_2 + \theta_{23}^{(1)}x_3) \\
a_3^{(2)} = g(\theta_{30}^{(1)}x_0 + \theta_{31}^{(1)}x_1 + \theta_{32}^{(1)}x_2 + \theta_{33}^{(1)}x_3) \\
h_{\Theta}(x) = a_1^{(3)} = g(\theta_{10}^{(2)}a_0^{(2)} + \theta_{11}^{(2)}a_1^{(2)} + \theta_{12}^{(2)}a_2^{(2)} + \theta_{13}^{(2)}a_3^{(2)})
</script> </div> <p>If the network has <span><span class=MathJax_Preview>s_j</span><script type=math/tex>s_j</script></span> units in layer <code>j</code>, <span><span class=MathJax_Preview>s_{j+1}</span><script type=math/tex>s_{j+1}</script></span> units in layer <code>j+1</code>, then <span><span class=MathJax_Preview>\Theta^{(j)}</span><script type=math/tex>\Theta^{(j)}</script></span> will be of dimension <span><span class=MathJax_Preview>s_{j+1} \times (s_j + 1)</span><script type=math/tex>s_{j+1} \times (s_j + 1)</script></span>. It could be interpreted as <strong>"the dimension of <span><span class=MathJax_Preview>\Theta^{(j)}</span><script type=math/tex>\Theta^{(j)}</script></span> is number of nodes in the next layer <span><span class=MathJax_Preview>\times</span><script type=math/tex>\times</script></span> the current layer's node + 1".</strong></p> <h3 id=forward-propagation-implementation>Forward propagation implementation<a class=headerlink href=#forward-propagation-implementation title="Permanent link">&para;</a></h3> <p>By defining <span><span class=MathJax_Preview>z_1^{(2)}</span><script type=math/tex>z_1^{(2)}</script></span>, <span><span class=MathJax_Preview>z_1^{(2)}</span><script type=math/tex>z_1^{(2)}</script></span>, and <span><span class=MathJax_Preview>z_1^{(2)}</span><script type=math/tex>z_1^{(2)}</script></span>, we could obtain <span><span class=MathJax_Preview>a_1^{(2)} = g(z_1^{(2)})</span><script type=math/tex>a_1^{(2)} = g(z_1^{(2)})</script></span>, where <span><span class=MathJax_Preview>z_1^{(2)} = \theta_{10}^{(1)}x_0 + \theta_{11}^{(1)}x_1 + \theta_{12}^{(1)}x_2 + \theta_{13}^{(1)}x_3</span><script type=math/tex>z_1^{(2)} = \theta_{10}^{(1)}x_0 + \theta_{11}^{(1)}x_1 + \theta_{12}^{(1)}x_2 + \theta_{13}^{(1)}x_3</script></span>. To make it more compact, we define <span><span class=MathJax_Preview>x = \left[ \begin{array}{c} x_0 \\ x_1 \\ x_2 \\ x_3 \end{array} \right]</span><script type=math/tex>x = \left[ \begin{array}{c} x_0 \\ x_1 \\ x_2 \\ x_3 \end{array} \right]</script></span> and <span><span class=MathJax_Preview>z^{(2)} = \left[ \begin{array}{c}z_1^{(2)} \\ z_2^{(2)} \\ z_3^{(2)} \end{array} \right]</span><script type=math/tex>z^{(2)} = \left[ \begin{array}{c}z_1^{(2)} \\ z_2^{(2)} \\ z_3^{(2)} \end{array} \right]</script></span>, so we have reached the following vectorized implementation of neural network. It is also called <strong>forward propagation algorithm</strong>.</p> <p>Input: <span><span class=MathJax_Preview>x</span><script type=math/tex>x</script></span></p> <p>Forward Propagation Algorithm:</p> <p><span><span class=MathJax_Preview>z^{(2)} = \Theta^{(1)}x</span><script type=math/tex>z^{(2)} = \Theta^{(1)}x</script></span> <span><span class=MathJax_Preview>a^{(2)} = g(z^{(2)})</span><script type=math/tex>a^{(2)} = g(z^{(2)})</script></span> and <span><span class=MathJax_Preview>a_0^{(2)} =1</span><script type=math/tex>a_0^{(2)} =1</script></span>. <span><span class=MathJax_Preview>z^{(3)} = \Theta^{(3)}a^{(2)}</span><script type=math/tex>z^{(3)} = \Theta^{(3)}a^{(2)}</script></span> <span><span class=MathJax_Preview>h_{\Theta}(x) = a^{(3)} = g(z^{(3)})</span><script type=math/tex>h_{\Theta}(x) = a^{(3)} = g(z^{(3)})</script></span></p> <h3 id=learning-its-own-features>Learning its own features<a class=headerlink href=#learning-its-own-features title="Permanent link">&para;</a></h3> <p>What neural network is doing is it just like logistic regression, rather than using hte original feature, <span><span class=MathJax_Preview>x_1</span><script type=math/tex>x_1</script></span>, <span><span class=MathJax_Preview>x_2</span><script type=math/tex>x_2</script></span>, <span><span class=MathJax_Preview>x_3</span><script type=math/tex>x_3</script></span>, it use new feature <span><span class=MathJax_Preview>a_1^{(2)}</span><script type=math/tex>a_1^{(2)}</script></span>, <span><span class=MathJax_Preview>a_1^{(2)}</span><script type=math/tex>a_1^{(2)}</script></span>, <span><span class=MathJax_Preview>a_1^{(2)}</span><script type=math/tex>a_1^{(2)}</script></span>. Those new feature are learned from the original input <span><span class=MathJax_Preview>x_1</span><script type=math/tex>x_1</script></span>, <span><span class=MathJax_Preview>x_2</span><script type=math/tex>x_2</script></span>, <span><span class=MathJax_Preview>x_3</span><script type=math/tex>x_3</script></span>.</p> <p><img alt="Learning Own Features" src=../fig/learning-own-features.png></p> <h3 id=xnor-example>XNOR example<a class=headerlink href=#xnor-example title="Permanent link">&para;</a></h3> <p>We calculate a Non-linear classification example: XOR/XNOR, where <span><span class=MathJax_Preview>x_1</span><script type=math/tex>x_1</script></span> and <span><span class=MathJax_Preview>x_2</span><script type=math/tex>x_2</script></span> are binary 0 or 1. <span><span class=MathJax_Preview>y = x_1 \text{ XNOR } x_2 = \text{ NOT } (x_1 \text{ XOR } x_2)</span><script type=math/tex>y = x_1 \text{ XNOR } x_2 = \text{ NOT } (x_1 \text{ XOR } x_2)</script></span>. But those are all build on the basic non-linear operation <code>AND</code>, <code>OR</code>, and <code>NOT</code>.</p> <p><img alt="Neural Network AND" src=../fig/neural-network-and.png> <img alt="Neural Network OR" src=../fig/neural-network-or.png> <img alt="Neural Network NOT" src=../fig/neural-network-not.png> <img alt="Neural Network ALL" src=../fig/neural-network-all.png></p> <p>Don't worry about how we get the <span><span class=MathJax_Preview>\theta</span><script type=math/tex>\theta</script></span>. This example just to give some intuitions of how neural network problem can be solved.</p> <h3 id=multiclass-classification_1>Multiclass classification<a class=headerlink href=#multiclass-classification_1 title="Permanent link">&para;</a></h3> <p>Suppose our algorithm is to recognize pedestrian, car, motorbike or truck, we need to build a neural network with four output units. We could use a vector to do this When image is a pedestrian get <code>[1,0,0,0]</code> and so on.</p> <ul> <li>1 is 0/1 pedestrain</li> <li>2 is 0/1 car</li> <li>3 is 0/1 motorcycle</li> <li>4 is 0/1 truck</li> </ul> <p>Just like one vs. all classifier described earlier. here we have four logistic regression classifiers</p> <p><img alt="Nerual Network Multiclass" src=../fig/neural-network-multiclass.png></p> <p>Contrast to the previous notation we wrote <span><span class=MathJax_Preview>y</span><script type=math/tex>y</script></span> as an integer <span><span class=MathJax_Preview>{1, 2, 3, 4}</span><script type=math/tex>{1, 2, 3, 4}</script></span> to represent four classes. Now we use the following notation to represent <span><span class=MathJax_Preview>y^{(i)}</span><script type=math/tex>y^{(i)}</script></span> as one of <span><span class=MathJax_Preview>\left[ \begin{array}{c} 1 \\ 0 \\ 0 \\ 0 \end{array} \right]</span><script type=math/tex>\left[ \begin{array}{c} 1 \\ 0 \\ 0 \\ 0 \end{array} \right]</script></span>, <span><span class=MathJax_Preview>\left[ \begin{array}{c} 0 \\ 1 \\ 0 \\ 0 \end{array} \right]</span><script type=math/tex>\left[ \begin{array}{c} 0 \\ 1 \\ 0 \\ 0 \end{array} \right]</script></span>, <span><span class=MathJax_Preview>\left[ \begin{array}{c} 0 \\ 0 \\ 1 \\ 0 \end{array} \right]</span><script type=math/tex>\left[ \begin{array}{c} 0 \\ 0 \\ 1 \\ 0 \end{array} \right]</script></span>, <span><span class=MathJax_Preview>\left[ \begin{array}{c} 0 \\ 0 \\ 0 \\ 1 \end{array} \right]</span><script type=math/tex>\left[ \begin{array}{c} 0 \\ 0 \\ 0 \\ 1 \end{array} \right]</script></span>.</p> <h2 id=week-5-neural-networks-learning>Week 5 Neural networks learning<a class=headerlink href=#week-5-neural-networks-learning title="Permanent link">&para;</a></h2> <h3 id=neural-network-classification>Neural network classification<a class=headerlink href=#neural-network-classification title="Permanent link">&para;</a></h3> <p>The cost function of neural network would be a generalized form of the regularized logistic regression cost function as shown below.</p> <h3 id=cost-function_1>Cost Function<a class=headerlink href=#cost-function_1 title="Permanent link">&para;</a></h3> <h4 id=logistic-regression>Logistic regression<a class=headerlink href=#logistic-regression title="Permanent link">&para;</a></h4> <div> <div class=MathJax_Preview> J(\theta) = -\frac{1}{m}\Bigg[\sum_{i=1}^{n}y^{(i)}\log(h_\theta(x^{(i)})) + (1 - y^{(i)})\log(1-h_\theta(x^{(i)}))\Bigg] + \frac{\lambda}{2m}\sum_{j=1}^{n}\theta_j^2 </div> <script type="math/tex; mode=display">
J(\theta) = -\frac{1}{m}\Bigg[\sum_{i=1}^{n}y^{(i)}\log(h_\theta(x^{(i)})) + (1 - y^{(i)})\log(1-h_\theta(x^{(i)}))\Bigg] + \frac{\lambda}{2m}\sum_{j=1}^{n}\theta_j^2
</script> </div> <h4 id=neural-network_1>Neural Network<a class=headerlink href=#neural-network_1 title="Permanent link">&para;</a></h4> <p>We have the hypothesis in the K dimensional Euclidean space, denoted as <span><span class=MathJax_Preview>h_{\Theta}(x) \in \mathbb{R}^K</span><script type=math/tex>h_{\Theta}(x) \in \mathbb{R}^K</script></span>, K is the number of output units. <span><span class=MathJax_Preview>(h_{\Theta}(x))_i = i^{th}</span><script type=math/tex>(h_{\Theta}(x))_i = i^{th}</script></span> output.</p> <div> <div class=MathJax_Preview> J(\Theta) = -\frac{1}{m}\Bigg[\sum_{i=1}^{m}\sum_{k=1}^Ky_k^{(i)}\log(h_\Theta(x^{(i)}))_k + (1 - y_k^{(i)})\log(1-(h_\Theta(x^{(i)}))_k)\Bigg] + \frac{\lambda}{2m}\sum_{l=1}^{L-1}\sum_{i=1}^{s_l}\sum_{j=1}^{s_{l+1}}(\Theta_{ji}^{(l)})^2 </div> <script type="math/tex; mode=display">
J(\Theta) = -\frac{1}{m}\Bigg[\sum_{i=1}^{m}\sum_{k=1}^Ky_k^{(i)}\log(h_\Theta(x^{(i)}))_k + (1 - y_k^{(i)})\log(1-(h_\Theta(x^{(i)}))_k)\Bigg] + \frac{\lambda}{2m}\sum_{l=1}^{L-1}\sum_{i=1}^{s_l}\sum_{j=1}^{s_{l+1}}(\Theta_{ji}^{(l)})^2
</script> </div> <p>Noticed that all the summation is start from <code>1</code> not <code>0</code>. For example, the index <code>k</code> and <code>j</code>. Because we don't regularize the bias unit which is with subscript <code>0</code>.</p> <h3 id=gradient-computation>Gradient computation<a class=headerlink href=#gradient-computation title="Permanent link">&para;</a></h3> <p>We have the cost function for the neural network. Next step we need to calculate the gradient of the cost function. As we can see that the cost function is too complicated to derive its gradient. Here we will use <strong>back propagation</strong> method to calculate the partial derivative. Before dive into the details of the back propagation, we need to summarize some of the ideas we had for the neural network so far.</p> <h4 id=forward-propagation>Forward propagation<a class=headerlink href=#forward-propagation title="Permanent link">&para;</a></h4> <ul> <li>Forward propagation takes initial input into that neural network and pushes the input through the neural network</li> <li>It leads to the generation of an output hypothesis, which may be a single real number, but can be a vector.</li> </ul> <h4 id=back-propagation>Back propagation<a class=headerlink href=#back-propagation title="Permanent link">&para;</a></h4> <p>The appearance of the back propagation algorithm isn't very natural. Instead, it is a very smart way to solve the optimization problem in neural network. Because it is difficult to follow the method we used in linear regression or logistic regression to analytically get the derivative of the cost function. Back propagation come to help solving the optimization problem numerically.</p> <p>To compute the partial derivative <span><span class=MathJax_Preview>\frac{\partial}{\partial\Theta_{i,j}^{(l)}}J(\Theta)</span><script type=math/tex>\frac{\partial}{\partial\Theta_{i,j}^{(l)}}J(\Theta)</script></span> (the gradient of cost function in neural network), thus allow us to find the parameter <span><span class=MathJax_Preview>\Theta</span><script type=math/tex>\Theta</script></span> that minimize the cost <span><span class=MathJax_Preview>J(\Theta)</span><script type=math/tex>J(\Theta)</script></span> (using gradient descent or advanced optimization algorithm), which will result in a hypothesis with parameters learn from the existing data.</p> <h4 id=how-to-derive-the-back-prop>How to derive the back-prop<a class=headerlink href=#how-to-derive-the-back-prop title="Permanent link">&para;</a></h4> <ol> <li>We define <span><span class=MathJax_Preview>\delta_{j}^{(l)} =</span><script type=math/tex>\delta_{j}^{(l)} =</script></span> \"error\" of node <span><span class=MathJax_Preview>j</span><script type=math/tex>j</script></span> in layer <span><span class=MathJax_Preview>l</span><script type=math/tex>l</script></span></li> <li>We calculate <span><span class=MathJax_Preview>\delta^{(L)} = a^{(L)} - y</span><script type=math/tex>\delta^{(L)} = a^{(L)} - y</script></span>, which is the vector of errors defined above at last layer <span><span class=MathJax_Preview>L</span><script type=math/tex>L</script></span>. Note <span><span class=MathJax_Preview>\delta^{(L)}</span><script type=math/tex>\delta^{(L)}</script></span> is vectorized representation of <span><span class=MathJax_Preview>\delta_{j}^{(l)}</span><script type=math/tex>\delta_{j}^{(l)}</script></span>. So our "error values" for the last layer are simply the differences of our actual results <span><span class=MathJax_Preview>a^{(L)}</span><script type=math/tex>a^{(L)}</script></span> in the last layer and the correct outputs in <span><span class=MathJax_Preview>y</span><script type=math/tex>y</script></span>.</li> <li>To get the delta values of the layers before the last layer, we can use an equation that steps us back from right to left, thereby <strong>back propagate</strong>, by <span><span class=MathJax_Preview>\delta^{(l)} = ((\Theta^{(l)})^T\delta^{(l+1)}).*g'(z^{(l)})</span><script type=math/tex>\delta^{(l)} = ((\Theta^{(l)})^T\delta^{(l+1)}).*g'(z^{(l)})</script></span>. It can be interpreted as the error value of layer <span><span class=MathJax_Preview>l</span><script type=math/tex>l</script></span> are calculated by multiplying the error values in the next layer with theta matrix of layer <span><span class=MathJax_Preview>l</span><script type=math/tex>l</script></span>. We then element wise multiply that with the derivative of the activation function <span><span class=MathJax_Preview>a^{(l)} = g(z^{(l)})</span><script type=math/tex>a^{(l)} = g(z^{(l)})</script></span>， namely <span><span class=MathJax_Preview>g'(z^{(l)}) = a^{(l)}.*(1-a^{(l)}).</span><script type=math/tex>g'(z^{(l)}) = a^{(l)}.*(1-a^{(l)}).</script></span> The derivation of this deviative can be found at the course wiki.</li> <li><strong>Partial derivative.</strong> With all the calculated <span><span class=MathJax_Preview>\delta^{(l)}</span><script type=math/tex>\delta^{(l)}</script></span> values, We can compute our partial derivative terms by multiplying our activation values and our error values for each training example <span><span class=MathJax_Preview>t</span><script type=math/tex>t</script></span>, <span><span class=MathJax_Preview>\dfrac{\partial}{\partial \Theta_{i,j}^{(l)}}J(\Theta) = \frac{1}{m}\sum_{t=1}^m a_j^{(t)(l)} {\delta}_i^{(t)(l+1)}</span><script type=math/tex>\dfrac{\partial}{\partial \Theta_{i,j}^{(l)}}J(\Theta) = \frac{1}{m}\sum_{t=1}^m a_j^{(t)(l)} {\delta}_i^{(t)(l+1)}</script></span>. This is ignoring regularization terms.</li> <li>Notations:<ul> <li><span><span class=MathJax_Preview>t</span><script type=math/tex>t</script></span> is the index of the training examples, we have total of <span><span class=MathJax_Preview>m</span><script type=math/tex>m</script></span> samples.</li> <li><span><span class=MathJax_Preview>l</span><script type=math/tex>l</script></span> is the layer, we have total layer of <span><span class=MathJax_Preview>L</span><script type=math/tex>L</script></span>.</li> <li><span><span class=MathJax_Preview>i</span><script type=math/tex>i</script></span> is the error affected node in the target layer, layer <span><span class=MathJax_Preview>l+1</span><script type=math/tex>l+1</script></span>.</li> <li><span><span class=MathJax_Preview>j</span><script type=math/tex>j</script></span> is the node in the current layer <span><span class=MathJax_Preview>l</span><script type=math/tex>l</script></span>.</li> <li><span><span class=MathJax_Preview>\delta^{(l+1)}</span><script type=math/tex>\delta^{(l+1)}</script></span> and <span><span class=MathJax_Preview>\ a^{(l+1)}</span><script type=math/tex>\ a^{(l+1)}</script></span> are vectors with <span><span class=MathJax_Preview>s_{l+1}</span><script type=math/tex>s_{l+1}</script></span> elements. Similarly, <span><span class=MathJax_Preview>\ a^{(l)}</span><script type=math/tex>\ a^{(l)}</script></span> is a vector with <span><span class=MathJax_Preview>s_{l}</span><script type=math/tex>s_{l}</script></span> elements. Multiplying them produces a matrix that is <span><span class=MathJax_Preview>s_{l+1}</span><script type=math/tex>s_{l+1}</script></span> by <span><span class=MathJax_Preview>s_{l}</span><script type=math/tex>s_{l}</script></span> which is the same dimension as <span><span class=MathJax_Preview>\Theta^{(l)}</span><script type=math/tex>\Theta^{(l)}</script></span>. That is, the process produces a gradient term for every element in <span><span class=MathJax_Preview>\Theta^{(l)}</span><script type=math/tex>\Theta^{(l)}</script></span>. |</li> </ul> </li> </ol> <h4 id=backpropagation-algorithm>Backpropagation Algorithm<a class=headerlink href=#backpropagation-algorithm title="Permanent link">&para;</a></h4> <ul> <li>Given training set <span><span class=MathJax_Preview>\lbrace (x^{(1)}, y^{(1)}) \cdots (x^{(m)}, y^{(m)})\rbrace</span><script type=math/tex>\lbrace (x^{(1)}, y^{(1)}) \cdots (x^{(m)}, y^{(m)})\rbrace</script></span></li> <li>Set <span><span class=MathJax_Preview>\Delta^{(l)}_{i,j} := 0</span><script type=math/tex>\Delta^{(l)}_{i,j} := 0</script></span> for all <span><span class=MathJax_Preview>(l,i,j)</span><script type=math/tex>(l,i,j)</script></span></li> <li>For training example <span><span class=MathJax_Preview>t = 1</span><script type=math/tex>t = 1</script></span> to <span><span class=MathJax_Preview>m</span><script type=math/tex>m</script></span>:<ul> <li>Set <span><span class=MathJax_Preview>a^{(1)} := x^{(t)}</span><script type=math/tex>a^{(1)} := x^{(t)}</script></span></li> <li>Perform forward propagation to compute <span><span class=MathJax_Preview>a^{(l)}</span><script type=math/tex>a^{(l)}</script></span> for <span><span class=MathJax_Preview>l = 2,3,\dots ,L</span><script type=math/tex>l = 2,3,\dots ,L</script></span></li> <li>Using <span><span class=MathJax_Preview>y^{(t)}</span><script type=math/tex>y^{(t)}</script></span>, compute <span><span class=MathJax_Preview>\delta^{(L)} = a^{(L)} - y^{(t)}</span><script type=math/tex>\delta^{(L)} = a^{(L)} - y^{(t)}</script></span></li> <li>Compute <span><span class=MathJax_Preview>\delta^{(L-1)}, \delta^{(L-2)},\dots,\delta^{(2)}</span><script type=math/tex>\delta^{(L-1)}, \delta^{(L-2)},\dots,\delta^{(2)}</script></span> using <span><span class=MathJax_Preview>\delta^{(l)} = ((\Theta^{(l)})^T \delta^{(l+1)})\ .*\ a^{(l)}\ .*\ (1 - a^{(l)})</span><script type=math/tex>\delta^{(l)} = ((\Theta^{(l)})^T \delta^{(l+1)})\ .*\ a^{(l)}\ .*\ (1 - a^{(l)})</script></span></li> <li><span><span class=MathJax_Preview>\Delta^{(l)}_{i,j} := \Delta^{(l)}_{i,j} + a_j^{(l)} \delta_i^{(l+1)}</span><script type=math/tex>\Delta^{(l)}_{i,j} := \Delta^{(l)}_{i,j} + a_j^{(l)} \delta_i^{(l+1)}</script></span> or with vectorization, <span><span class=MathJax_Preview>\Delta^{(l)} := \Delta^{(l)} + \delta^{(l+1)}(a^{(l)})^T</span><script type=math/tex>\Delta^{(l)} := \Delta^{(l)} + \delta^{(l+1)}(a^{(l)})^T</script></span> (programmatic calculation of <span><span class=MathJax_Preview>\frac{1}{m}\sum_{t=1}^m a_j^{(t)(l)} {\delta}_i^{(t)(l+1)}</span><script type=math/tex>\frac{1}{m}\sum_{t=1}^m a_j^{(t)(l)} {\delta}_i^{(t)(l+1)}</script></span>)</li> </ul> </li> <li><span><span class=MathJax_Preview>D^{(l)}_{i,j} := \dfrac{1}{m}\left(\Delta^{(l)}_{i,j} + \lambda\Theta^{(l)}_{i,j}\right)</span><script type=math/tex>D^{(l)}_{i,j} := \dfrac{1}{m}\left(\Delta^{(l)}_{i,j} + \lambda\Theta^{(l)}_{i,j}\right)</script></span> If <span><span class=MathJax_Preview>j \neq 0</span><script type=math/tex>j \neq 0</script></span> (NOTE: Typo in lecture slide omits outside parentheses. This version is correct.)</li> <li><span><span class=MathJax_Preview>D^{(l)}_{i,j} := \dfrac{1}{m}\Delta^{(l)}_{i,j}</span><script type=math/tex>D^{(l)}_{i,j} := \dfrac{1}{m}\Delta^{(l)}_{i,j}</script></span> If <span><span class=MathJax_Preview>j = 0</span><script type=math/tex>j = 0</script></span></li> </ul> <p>The capital-delta matrix is used as an \"accumulator\" to add up our values as we go along and eventually compute our partial derivative.</p> <p>The actual proof is quite involved, but, the <span><span class=MathJax_Preview>D^{(l)}_{i,j}</span><script type=math/tex>D^{(l)}_{i,j}</script></span> terms are the partial derivatives and the results we are looking for: <span><span class=MathJax_Preview>D_{i,j}^{(l)} = \dfrac{\partial J(\Theta)}{\partial \Theta_{i,j}^{(l)}}.</span><script type=math/tex>D_{i,j}^{(l)} = \dfrac{\partial J(\Theta)}{\partial \Theta_{i,j}^{(l)}}.</script></span></p> <h4 id=math-representation>Math representation<a class=headerlink href=#math-representation title="Permanent link">&para;</a></h4> <ul> <li>There is a <span><span class=MathJax_Preview>\Theta</span><script type=math/tex>\Theta</script></span> matrix for each layer in the network<ul> <li>This has each node in layer <span><span class=MathJax_Preview>l</span><script type=math/tex>l</script></span> as one dimension and each node in <span><span class=MathJax_Preview>l+1</span><script type=math/tex>l+1</script></span> as the other dimension</li> </ul> </li> <li>Similarly, there is going to be a <span><span class=MathJax_Preview>\Delta</span><script type=math/tex>\Delta</script></span> matrix for each layer<ul> <li>This has each node as one dimension and each training data example as the other</li> </ul> </li> </ul> <h4 id=a-high-level-description>A high level description<a class=headerlink href=#a-high-level-description title="Permanent link">&para;</a></h4> <ul> <li>Back propagation basically takes the output you got from your network, compares it to the real value (y) and calculates how wrong the network was (i.e. how wrong the parameters were)</li> <li>It then, using the error you\'ve just calculated, back-calculates the error associated with each unit from right to left.</li> <li>This goes on until you reach the input layer (where obviously there is no error, as the activation is the input)</li> <li>These \"error\" measurements for each unit can be used to calculate the <strong>partial derivatives</strong><ul> <li>Partial derivatives are the bomb, because gradient descent needs them to minimize the cost function</li> </ul> </li> <li>We use the partial derivatives with gradient descent to try minimize the cost function and update all the <span><span class=MathJax_Preview>\Theta</span><script type=math/tex>\Theta</script></span> values</li> <li>This repeats until gradient descent reports convergence</li> </ul> <h4 id=what-we-need-to-watch-out>What we need to watch out?<a class=headerlink href=#what-we-need-to-watch-out title="Permanent link">&para;</a></h4> <ul> <li>We don\'t have <span><span class=MathJax_Preview>\delta^{(1)}</span><script type=math/tex>\delta^{(1)}</script></span> because we have all <span><span class=MathJax_Preview>x</span><script type=math/tex>x</script></span>, it is zero.</li> <li>We don\'t calculate the bias node.</li> </ul> <h3 id=backpropagation-intuition>Backpropagation Intuition<a class=headerlink href=#backpropagation-intuition title="Permanent link">&para;</a></h3> <p>Backpropagation very similar to the forward propagation, we can use a example to calculate the <span><span class=MathJax_Preview>\lambda_i^{(\ell)}</span><script type=math/tex>\lambda_i^{(\ell)}</script></span> or the <span><span class=MathJax_Preview>z^{(\ell)}</span><script type=math/tex>z^{(\ell)}</script></span> as bellow. To further understand the algorithm, refer to the code in assignment 4.</p> <p><img alt="Foward Prop Intuition" src=../fig/forwardprop-intuition.png> <img alt="Back Prop Intuition" src=../fig/backprop-intuition.png></p> <p>As it is shown in the slide, <span><span class=MathJax_Preview>z_1^{(2)}</span><script type=math/tex>z_1^{(2)}</script></span> is calculate by multiply the corresponding <span><span class=MathJax_Preview>\Theta^{(2)}_{ij}</span><script type=math/tex>\Theta^{(2)}_{ij}</script></span> value. the index <span><span class=MathJax_Preview>i</span><script type=math/tex>i</script></span> is the same as in <span><span class=MathJax_Preview>z_i^{(2)}</span><script type=math/tex>z_i^{(2)}</script></span>. The index <span><span class=MathJax_Preview>j</span><script type=math/tex>j</script></span> means the <span><span class=MathJax_Preview>j</span><script type=math/tex>j</script></span>th node in the previous layer.</p> <h3 id=unrolling-parameters>Unrolling parameters<a class=headerlink href=#unrolling-parameters title="Permanent link">&para;</a></h3> <p>With neural networks, we are working with sets of matrices: <span><span class=MathJax_Preview>\Theta^{(1)}</span><script type=math/tex>\Theta^{(1)}</script></span>, <span><span class=MathJax_Preview>\Theta^{(2)}</span><script type=math/tex>\Theta^{(2)}</script></span>, <span><span class=MathJax_Preview>\Theta^{(3)}, \dots</span><script type=math/tex>\Theta^{(3)}, \dots</script></span>. <span><span class=MathJax_Preview>D^{(1)}, D^{(1)}, D^{(3)}, \dots</span><script type=math/tex>D^{(1)}, D^{(1)}, D^{(3)}, \dots</script></span></p> <p>In order to use optimizing functions such as "fminunc()", we will want to "unroll" all the elements and put them into one long vector</p> <div class=highlight><pre><span></span><code><span class=c># unroll the matrices</span>
<span class=n>thetaVector</span> <span class=p>=</span> <span class=p>[</span> <span class=n>Theta1</span><span class=p>(:);</span> <span class=n>Theta2</span><span class=p>(:);</span> <span class=n>Theta3</span><span class=p>(:);</span> <span class=p>]</span>
<span class=n>deltaVector</span> <span class=p>=</span> <span class=p>[</span> <span class=n>D1</span><span class=p>(:);</span> <span class=n>D2</span><span class=p>(:);</span> <span class=n>D3</span><span class=p>(:)</span> <span class=p>]</span>
</code></pre></div> <p>If the dimensions of Theta1 is 10x11, Theta2 is 10x11 and Theta3 is 1x11, then we can get back our original matrices from the "unrolled" versions as follows</p> <div class=highlight><pre><span></span><code><span class=c># using reshape function to build matrix from vectors</span>
<span class=n>Theta1</span> <span class=p>=</span> <span class=nb>reshape</span><span class=p>(</span><span class=n>thetaVector</span><span class=p>(</span><span class=mi>1</span><span class=p>:</span><span class=mi>110</span><span class=p>),</span><span class=mi>10</span><span class=p>,</span><span class=mi>11</span><span class=p>)</span>
<span class=n>Theta2</span> <span class=p>=</span> <span class=nb>reshape</span><span class=p>(</span><span class=n>thetaVector</span><span class=p>(</span><span class=mi>111</span><span class=p>:</span><span class=mi>220</span><span class=p>),</span><span class=mi>10</span><span class=p>,</span><span class=mi>11</span><span class=p>)</span>
<span class=n>Theta3</span> <span class=p>=</span> <span class=nb>reshape</span><span class=p>(</span><span class=n>thetaVector</span><span class=p>(</span><span class=mi>221</span><span class=p>:</span><span class=mi>231</span><span class=p>),</span><span class=mi>1</span><span class=p>,</span><span class=mi>11</span><span class=p>)</span>
</code></pre></div> <p>In Octave, if we want to implement the back propagation algorithm, we have to do the unrolling and reshaping back and forth in order to calculate the <code>gradientVec</code>. Notice that the last two lines are shifted in the picture, it should read "Use forward propagation and back propagation to compute <span><span class=MathJax_Preview>D^{(1)}, D^{(1)}, D^{(3)}</span><script type=math/tex>D^{(1)}, D^{(1)}, D^{(3)}</script></span> and <span><span class=MathJax_Preview>J(\Theta)</span><script type=math/tex>J(\Theta)</script></span>. Unroll <span><span class=MathJax_Preview>D^{(1)}, D^{(1)}, D^{(3)}</span><script type=math/tex>D^{(1)}, D^{(1)}, D^{(3)}</script></span> to get <code>gradientVec</code>.</p> <p><img alt="NN Unroll" src=../fig/neural-networks-unrolling-reshape.png></p> <h3 id=gradient-checking>Gradient Checking<a class=headerlink href=#gradient-checking title="Permanent link">&para;</a></h3> <p>To ensure a bug free back propagation implementation, one method we can use is gradient checking. It is a way of numerically estimate the gradient value in each computation point. It works as a reference value to check the back propagation is correct.</p> <p><img alt="NN Gradient Checking" src=../fig/neural-network-gradient-checking.png> <img alt="NN Gradient Checking Vector" src=../fig/neural-network-gradient-checking-vector.png></p> <p>In Octave, we can implement this gradient checking as follows. We then want to check that gradApprox <span><span class=MathJax_Preview>\approx</span><script type=math/tex>\approx</script></span> deltaVector. Remove the gradient checking when you checking is done, otherwise the code is going to be very slow.</p> <div class=highlight><pre><span></span><code><span class=n>epsilon</span> <span class=p>=</span> <span class=mf>1e-4</span><span class=p>;</span>
<span class=k>for</span> <span class=n>i</span> <span class=p>=</span> <span class=mi>1</span><span class=p>:</span><span class=n>n</span><span class=p>,</span>
  <span class=n>thetaPlus</span> <span class=p>=</span> <span class=n>theta</span><span class=p>;</span>
  <span class=n>thetaPlus</span><span class=p>(</span><span class=n>i</span><span class=p>)</span> <span class=o>+=</span> <span class=n>epsilon</span><span class=p>;</span>
  <span class=n>thetaMinus</span> <span class=p>=</span> <span class=n>theta</span><span class=p>;</span>
  <span class=n>thetaMinus</span><span class=p>(</span><span class=n>i</span><span class=p>)</span> <span class=o>-=</span> <span class=n>epsilon</span><span class=p>;</span>
  <span class=n>gradApprox</span><span class=p>(</span><span class=n>i</span><span class=p>)</span> <span class=p>=</span> <span class=p>(</span><span class=n>J</span><span class=p>(</span><span class=n>thetaPlus</span><span class=p>)</span> <span class=o>-</span> <span class=n>J</span><span class=p>(</span><span class=n>thetaMinus</span><span class=p>))</span><span class=o>/</span><span class=p>(</span><span class=mi>2</span><span class=o>*</span><span class=n>epsilon</span><span class=p>)</span>
<span class=k>end</span><span class=p>;</span>
</code></pre></div> <h3 id=random-initialization>Random Initialization<a class=headerlink href=#random-initialization title="Permanent link">&para;</a></h3> <p>Initializing all theta weights to zero does not work with neural networks. When we back propagate, all nodes will update to the same value repeatedly.</p> <p>Instead we can randomly initialize our weights: initialize each <span><span class=MathJax_Preview>\Theta_{ij}^{(l)}</span><script type=math/tex>\Theta_{ij}^{(l)}</script></span> to a random value between <span><span class=MathJax_Preview>[-\epsilon, \epsilon]</span><script type=math/tex>[-\epsilon, \epsilon]</script></span>.</p> <p>In Octave, we can use the following code to generate the initial theta values.</p> <div class=highlight><pre><span></span><code><span class=c>#If the dimensions of Theta1 is 10x11, Theta2 is 10x11 and Theta3 is 1x11.</span>

<span class=n>Theta1</span> <span class=p>=</span> <span class=nb>rand</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span><span class=mi>11</span><span class=p>)</span> <span class=o>*</span> <span class=p>(</span><span class=mi>2</span> <span class=o>*</span> <span class=n>INIT_EPSILON</span><span class=p>)</span> <span class=o>-</span> <span class=n>INIT_EPSILON</span><span class=p>;</span>
<span class=n>Theta2</span> <span class=p>=</span> <span class=nb>rand</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span><span class=mi>11</span><span class=p>)</span> <span class=o>*</span> <span class=p>(</span><span class=mi>2</span> <span class=o>*</span> <span class=n>INIT_EPSILON</span><span class=p>)</span> <span class=o>-</span> <span class=n>INIT_EPSILON</span><span class=p>;</span>
<span class=n>Theta3</span> <span class=p>=</span> <span class=nb>rand</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span><span class=mi>11</span><span class=p>)</span> <span class=o>*</span> <span class=p>(</span><span class=mi>2</span> <span class=o>*</span> <span class=n>INIT_EPSILON</span><span class=p>)</span> <span class=o>-</span> <span class=n>INIT_EPSILON</span><span class=p>;</span>
</code></pre></div> <p><code>rand(x,y)</code> will initialize a matrix of random real numbers between 0 and 1. (Note: this <span><span class=MathJax_Preview>\epsilon</span><script type=math/tex>\epsilon</script></span> is unrelated to the <span><span class=MathJax_Preview>\epsilon</span><script type=math/tex>\epsilon</script></span> from Gradient Checking)</p> <h3 id=putting-it-together>Putting it Together<a class=headerlink href=#putting-it-together title="Permanent link">&para;</a></h3> <p>First, pick a network architecture; choose the layout of your neural network, including how many hidden units in each layer and how many layers total.</p> <ul> <li>Number of input units = dimension of features <span><span class=MathJax_Preview>x^{(i)}</span><script type=math/tex>x^{(i)}</script></span></li> <li>Number of output units = number of classes</li> <li>Number of hidden units per layer = usually more the better (must balance with the cost of computation as it increases with more hidden units)</li> </ul> <p>Defaults: 1 hidden layer. If more than 1 hidden layer, then the same number of units in every hidden layer.</p> <p><img alt="NN ALL 1" src=../fig/neural-networks-all-steps-1.png> <img alt="NN ALL 2" src=../fig/neural-networks-all-steps-2.png></p> <h2 id=week-6-advice-for-applying-machine-learning>Week 6 Advice for applying machine learning<a class=headerlink href=#week-6-advice-for-applying-machine-learning title="Permanent link">&para;</a></h2> <h3 id=deciding-what-to-try-next>Deciding what to try next<a class=headerlink href=#deciding-what-to-try-next title="Permanent link">&para;</a></h3> <ol> <li>Get more training examples</li> <li>Try smaller sets\'of features</li> <li>Try getting additional features</li> <li>Try adding polynomial features</li> <li>Try decreasing <span><span class=MathJax_Preview>\lambda</span><script type=math/tex>\lambda</script></span></li> <li>Try increasing <span><span class=MathJax_Preview>\lambda</span><script type=math/tex>\lambda</script></span></li> </ol> <h3 id=machine-learning-diagnostic>Machine learning diagnostic<a class=headerlink href=#machine-learning-diagnostic title="Permanent link">&para;</a></h3> <p><strong>Diagnostic</strong>: A test that you can run to gain insight what is/isn\'t working with a learning algorithm, and gain guidance as to how best to improve its performance. It can take time to implement, but doing so can be a very good use of your time.</p> <h3 id=evaluating-a-hypothesis>Evaluating a hypothesis<a class=headerlink href=#evaluating-a-hypothesis title="Permanent link">&para;</a></h3> <p>Diagnostic over-fitting by splitting the training set and test set</p> <ol> <li>Training set (70%)</li> <li>Test set (30%)</li> </ol> <p>If the cost function <span><span class=MathJax_Preview>J(\theta)</span><script type=math/tex>J(\theta)</script></span> calculated for training set is low, it is high for test set, over-fitting happens. This is apply to both logistic regression and linear regression.</p> <h3 id=model-selection-and-trainvalidationtest-sets>Model selection and Train/Validation/Test sets<a class=headerlink href=#model-selection-and-trainvalidationtest-sets title="Permanent link">&para;</a></h3> <p>Normally we choose a model from a series of different degree of polynomial hypothesis, We choose the model that have the least cost for testing data. More specifically, we learned the parameter <span><span class=MathJax_Preview>\theta^{(d)}</span><script type=math/tex>\theta^{(d)}</script></span> (d is the degree of polynomial) from the training set and calculate the cost for the test dataset with the learned <span><span class=MathJax_Preview>\theta</span><script type=math/tex>\theta</script></span>. This is not the best practice for the reason that the hypothesis we selected has been fit to the test set. It is very likely the model will not fit new data very well.</p> <p>We can improve the model selection by introduce the cross validation set. We divide our available data as</p> <ol> <li>Training set (60%)</li> <li>Cross validation set (20%)</li> <li>test set (20%)</li> </ol> <p>We use the cross validation data to help us to select the model, and using the test data, which the model never see before to evaluate generalization error.</p> <div class="admonition warning"> <p class=admonition-title>Warning</p> <p>Avoid to use the test data to select the model (select the least cost model calculated on the test data set) and then report the generalization error using the test data.</p> </div> <h3 id=diagnosing-bias-vs-variance>Diagnosing bias vs. variance<a class=headerlink href=#diagnosing-bias-vs-variance title="Permanent link">&para;</a></h3> <p>The important point to understand bias and variance is to understand how the cost for training data, cross validation data (or test data) changes.</p> <p><img alt="Bias and Variance I" src=../fig/bias-variance.png> <img alt="Bias and Variance II" src=../fig/bias-variance-2.png></p> <h3 id=regularization-and-biasvariance>Regularization and bias/variance<a class=headerlink href=#regularization-and-biasvariance title="Permanent link">&para;</a></h3> <p>The regularization parameter <span><span class=MathJax_Preview>\lambda</span><script type=math/tex>\lambda</script></span> could play important role in selecting the best model to fit the data (both cross validation data and test data). This section will study how <span><span class=MathJax_Preview>\lambda</span><script type=math/tex>\lambda</script></span> affect the cost function both for the training data and cross validation data.</p> <p><img alt="Linear Regression Regularization" src=../fig/lr-regularization.png> <img alt="Choosing Lambda" src=../fig/choosing-lambda.png> <img alt="Cost vs Lambda" src=../fig/cost-vs-lambda.png></p> <h3 id=learning-curve>Learning curve<a class=headerlink href=#learning-curve title="Permanent link">&para;</a></h3> <p>This section mainly discuss how the sample size affect the cost function. There are two kinds of problem we face, high bias and high variance</p> <h4 id=high-bias>High bias<a class=headerlink href=#high-bias title="Permanent link">&para;</a></h4> <p><img alt="High Bias" src=../fig/high-bias.png></p> <h4 id=high-variance>High variance<a class=headerlink href=#high-variance title="Permanent link">&para;</a></h4> <p><img alt="High Variance" src=../fig/high-variance.png></p> <h3 id=deciding-what-to-do-next-revisited>Deciding what to do next revisited<a class=headerlink href=#deciding-what-to-do-next-revisited title="Permanent link">&para;</a></h3> <table> <thead> <tr> <th align=center>bias/variance</th> <th align=center>underfitting/overfitting</th> <th align=center>regularization</th> <th align=center>to improve</th> </tr> </thead> <tbody> <tr> <td align=center>High bias</td> <td align=center>underfitting</td> <td align=center>bigger <span><span class=MathJax_Preview>lambda</span><script type=math/tex>lambda</script></span></td> <td align=center>more sample will not help much</td> </tr> <tr> <td align=center>High variance</td> <td align=center>overfitting (high polynomial)</td> <td align=center>smaller <span><span class=MathJax_Preview>lambda</span><script type=math/tex>lambda</script></span></td> <td align=center>more sample will likely to help</td> </tr> </tbody> </table> <p><img alt="Debugging Learning" src=../fig/debugging-learning.png> <img alt="NN Overfitting" src=../fig/neural-network-overfitting.png></p> <h3 id=prioritizing-what-to-work-on>Prioritizing what to work on<a class=headerlink href=#prioritizing-what-to-work-on title="Permanent link">&para;</a></h3> <h3 id=building-a-spam-classifier>Building a spam classifier<a class=headerlink href=#building-a-spam-classifier title="Permanent link">&para;</a></h3> <h3 id=error-analysis>Error analysis<a class=headerlink href=#error-analysis title="Permanent link">&para;</a></h3> <ol> <li>Start simple algorithm, implement it and test it on cross-validation data.</li> <li>Plot learning curves to decide if more data, more features, etc. are like to help.</li> <li>Error analysis: Manually examine the examples (in cross validation set) that you algorithm make errors on. See if you spot any systematic trend in what type of examples it it making error on.</li> </ol> <h3 id=error-metrics-for-skewed-classes>Error metrics for skewed classes<a class=headerlink href=#error-metrics-for-skewed-classes title="Permanent link">&para;</a></h3> <p>Problem arise in error analysis when only very small(less than 0.5%) negative samples possibly exist. Take the cancer classification as a example.</p> <p><img alt="Skew Data Cancer" src=../fig/skew-data-cancer.png></p> <h4 id=precision-and-recall>Precision and Recall<a class=headerlink href=#precision-and-recall title="Permanent link">&para;</a></h4> <p><img alt="Precision & Recall" src=../fig/precision-recall.png></p> <h4 id=trading-off-precision-and-recall>Trading off precision and recall<a class=headerlink href=#trading-off-precision-and-recall title="Permanent link">&para;</a></h4> <p><img alt="Precison Recall Trade Off" src=../fig/trading-off-precision-and-recall.png></p> <h4 id=f-score-f_score>F score {#f_score}<a class=headerlink href=#f-score-f_score title="Permanent link">&para;</a></h4> <p><img alt="F Score" src=../fig/F-score.png></p> <h3 id=data-for-machine-learning>Data for machine learning<a class=headerlink href=#data-for-machine-learning title="Permanent link">&para;</a></h3> <p><img alt="Bank and Brill" src=../fig/bank-and-brill-2001.png> <img alt="Large Data Rational I" src=../fig/large-data-rationale-0.png> <img alt="Large Data Rational II" src=../fig/large-data-rationale-1.png></p> <h2 id=week-7-support-vector-machine>Week 7 Support Vector Machine<a class=headerlink href=#week-7-support-vector-machine title="Permanent link">&para;</a></h2> <h3 id=optimization-objective>Optimization objective<a class=headerlink href=#optimization-objective title="Permanent link">&para;</a></h3> <p>SVM will be derived from logistic regression. There are not that much of logic, just following the derivation in the lecture. Let's first review the hypothesis of the logistic regress and its cost function:</p> <p><img alt="Logistic hypothesis" src=../fig/logistic-hypothesis-review.png> <img alt="Alternative Logistic hypothesis" src=../fig/logistic-cost-func-review.png></p> <p>Notice in the second figure above, we define approximate functions <span><span class=MathJax_Preview>cost_0(z)</span><script type=math/tex>cost_0(z)</script></span> and <span><span class=MathJax_Preview>cost_1(z)</span><script type=math/tex>cost_1(z)</script></span>. These functions will be used in the SVM cost function we will see next.</p> <h4 id=svm-cost-function>SVM cost function<a class=headerlink href=#svm-cost-function title="Permanent link">&para;</a></h4> <p>Compare to logistic regression, SVM use a different parameters <span><span class=MathJax_Preview>C</span><script type=math/tex>C</script></span>. As in logistic regression, we use <span><span class=MathJax_Preview>\lambda</span><script type=math/tex>\lambda</script></span> to control the trade off optimizing the first term and the second term in the objective function.</p> <p><img alt="SVM Objective Function" src=../fig/svm-objective-func.png></p> <p>What's more, the SVM hypothesis doesn\'t give us a probability prediction, it give use either <code>1</code> or <code>0</code>. Such as <span><span class=MathJax_Preview>h_\theta = \begin{cases} 1 \qquad \text{if}\ \Theta^Tx \geq 0\\ 0 \qquad \text{otherwise} \end{cases}</span><script type=math/tex>h_\theta = \begin{cases}
          1 \qquad \text{if}\  \Theta^Tx \geq 0\\
          0 \qquad \text{otherwise}
            \end{cases}</script></span></p> <h3 id=large-margin-intuition>Large margin intuition<a class=headerlink href=#large-margin-intuition title="Permanent link">&para;</a></h3> <p>Suppose we make the parameter <span><span class=MathJax_Preview>C</span><script type=math/tex>C</script></span> very large, we want to make the first term in the summation approximate <code>0</code>. We can derive the problem as a optimization problem subject to some condition. This optimization problem will lead us to discuss the decision boundary. when <span><span class=MathJax_Preview>C</span><script type=math/tex>C</script></span> is very large, the decision boundary will have a large margin, thus large margin classifier. Later we will see that using different kernel and parameters can control this margin.</p> <div class="admonition question"> <p class=admonition-title>Question</p> <p>How the large margin classifier related to the optimization problem when <code>C</code> is very large?</p> </div> <p><img alt="Large Margin Optimization" src=../fig/large-margin-optimization.png> <img alt="Large Margin Outlier" src=../fig/large-margin-outlier.png></p> <p>When <span><span class=MathJax_Preview>C</span><script type=math/tex>C</script></span> is very large, we have the magneto decision boundary, if <span><span class=MathJax_Preview>C</span><script type=math/tex>C</script></span> is not very large, we will have the black decision boundary, which is having the large margin properties.</p> <h3 id=mathematics-behind-large-margin-classification>Mathematics behind large margin classification<a class=headerlink href=#mathematics-behind-large-margin-classification title="Permanent link">&para;</a></h3> <p>Using the vector inner product to transform the constrain from <span><span class=MathJax_Preview>\theta^T x^{(i)} \geq 1</span><script type=math/tex>\theta^T x^{(i)} \geq 1</script></span> to <span><span class=MathJax_Preview>p^{(i)}\Vert\theta \Vert \geq 1</span><script type=math/tex>p^{(i)}\Vert\theta \Vert \geq 1</script></span> for <span><span class=MathJax_Preview>y^{(i)} = 1</span><script type=math/tex>y^{(i)} = 1</script></span></p> <p><img alt="SVM Decision Boundary Optimization" src=../fig/svm-decision-boundary-optimization.png> <img alt="SVM Decision Boundary Optimization Translate" src=../fig/svm-decision-boundary-opti-translate.png></p> <p>The point of this section is to discuss the mathematical insight why SVM choose the large margin decision boundary. (because, the large margin decision boundary will produce a larger <span><span class=MathJax_Preview>p^{(i)}</span><script type=math/tex>p^{(i)}</script></span>, thus a smaller <span><span class=MathJax_Preview>\Vert\theta \Vert</span><script type=math/tex>\Vert\theta \Vert</script></span> in the constrain of the optimization problem, so as to minimize the objective function <span><span class=MathJax_Preview>\min_\theta \frac{1}{2} \sum_{j=1}^{n} \theta_j^2</span><script type=math/tex>\min_\theta \frac{1}{2} \sum_{j=1}^{n} \theta_j^2</script></span>)</p> <h3 id=kernel>Kernel<a class=headerlink href=#kernel title="Permanent link">&para;</a></h3> <h4 id=non-linear-decision-boundary>Non-linear decision boundary<a class=headerlink href=#non-linear-decision-boundary title="Permanent link">&para;</a></h4> <p>Is there a different/better choice of the features <span><span class=MathJax_Preview>f_1, f_2, f_3, \dots,</span><script type=math/tex>f_1, f_2, f_3, \dots,</script></span></p> <p><img alt="svm features" src=../fig/svm-features.png></p> <h4 id=landmarks>Landmarks<a class=headerlink href=#landmarks title="Permanent link">&para;</a></h4> <p>To respond the above question, we use the following way to generate the new features <span><span class=MathJax_Preview>f_1, f_2, f_3, \dots,</span><script type=math/tex>f_1, f_2, f_3, \dots,</script></span>. Compare the similarity between the examples and the landmarks using a similarity function.</p> <h4 id=gaussian-kernel>Gaussian kernel<a class=headerlink href=#gaussian-kernel title="Permanent link">&para;</a></h4> <p>We use the Guassian kernel <span><span class=MathJax_Preview>k(x, \ell^{(i)}) = \exp(-\frac{\Vert x - \ell^{(i)}\Vert^2}{2\delta^2})</span><script type=math/tex>k(x, \ell^{(i)}) = \exp(-\frac{\Vert x - \ell^{(i)}\Vert^2}{2\delta^2})</script></span> as the similarity function to generate the new feature.</p> <p><img alt="svm guassian kernel" src=../fig/svm-guassian-kernel.png> <img alt="svm kernel similarity" src=../fig/svm-kernel-similarity.png> <img alt="svm guassian kernel delta example" src=../fig/svm-guassian-kernel-delta-example.png> <img alt="svm guassian kernel predict" src=../fig/svm-guassian-kernel-predict.png></p> <h4 id=choosing-the-landmarks>Choosing the landmarks<a class=headerlink href=#choosing-the-landmarks title="Permanent link">&para;</a></h4> <p>Naturally, we are going to choose the example itself as the landmark, thus we are able to transform the feature <span><span class=MathJax_Preview>x^{(i)}</span><script type=math/tex>x^{(i)}</script></span> to feature vector <span><span class=MathJax_Preview>f^{(i)}</span><script type=math/tex>f^{(i)}</script></span> for training example <span><span class=MathJax_Preview>(x^{(i)}, y^{(i)})</span><script type=math/tex>(x^{(i)}, y^{(i)})</script></span>.</p> <p><img alt="svm choose landmarks" src=../fig/svm-choose-landmarks.png> <img alt="svm kernel" src=../fig/svm-kernel.png> <img alt="svm kernel hypothesis training" src=../fig/svm-kernel-hypothesis-training.png> <img alt="svm kernel bias variance" src=../fig/svm-kernel-bias-variance.png></p> <h3 id=using-svm>Using SVM<a class=headerlink href=#using-svm title="Permanent link">&para;</a></h3> <ul> <li>Choice of parameter <span><span class=MathJax_Preview>C</span><script type=math/tex>C</script></span>.</li> <li>Choice of kernel (similarity function):<ul> <li>No kernel ("linear Kernel")</li> <li>Gaussian kernel (need to choose <span><span class=MathJax_Preview>\delta^2</span><script type=math/tex>\delta^2</script></span>)</li> </ul> </li> </ul> <p>Because the similarity function is apply to each feature, we\'d better to perform feature scaling before using the Gaussian kernel. (take the house price features as an example, the areas and the number of rooms are not in the same scale.)</p> <div class="admonition note"> <p class=admonition-title>Note</p> <p>Not all similarity functions <span><span class=MathJax_Preview>similarity(x, \ell)</span><script type=math/tex>similarity(x, \ell)</script></span> make valid kernels. Need to satisfy technical condition called "Mercer's Theorem" to make sure SVM packagesl' optimizations run correctly, and do not diverge).</p> </div> <p>Many off-the-shelf kernels available:</p> <ul> <li>Polynomial kernel: <span><span class=MathJax_Preview>k(X, \ell) = (X^T\ell + c)^{p}</span><script type=math/tex>k(X, \ell) = (X^T\ell + c)^{p}</script></span>, such as <span><span class=MathJax_Preview>k(x, \ell) = (X^T\ell + 1)^2.</span><script type=math/tex>k(x, \ell) = (X^T\ell + 1)^2.</script></span></li> <li>More esoteric: <strong>String kernel, chi-square kernel, histogram intersection kernel,</strong> ...</li> </ul> <div class="admonition note svm multi-class classification"> <p class=admonition-title>Note</p> <p>Many SVM packages already have built-in multi-class classification functionality. Otherwise, we can use one-vs.-all method. Specifically, train <span><span class=MathJax_Preview>K</span><script type=math/tex>K</script></span> SVMs, one to distinguish <span><span class=MathJax_Preview>y = i</span><script type=math/tex>y = i</script></span> from the rest, for <span><span class=MathJax_Preview>i = 1, 2, \dots, K</span><script type=math/tex>i = 1, 2, \dots, K</script></span>, get <span><span class=MathJax_Preview>\theta^{(1)}, \theta^{(2)}, \dots, \theta^{(K)}</span><script type=math/tex>\theta^{(1)}, \theta^{(2)}, \dots, \theta^{(K)}</script></span>, pick class <span><span class=MathJax_Preview>i</span><script type=math/tex>i</script></span> with largest <span><span class=MathJax_Preview>(\theta^{(i)})^Tx</span><script type=math/tex>(\theta^{(i)})^Tx</script></span>.</p> </div> <h4 id=logistic-regression-vs-smvs>Logistic regression vs. SMVs<a class=headerlink href=#logistic-regression-vs-smvs title="Permanent link">&para;</a></h4> <p>This section mainly discuss when to use logistic regression vs. SVM from the high level. I summarize the content of the slides in the table.</p> <table> <thead> <tr> <th align=center>n</th> <th align=center>m</th> <th align=center>logistic regression vs. SVMs</th> </tr> </thead> <tbody> <tr> <td align=center>large (relative to mm) i.e. 10,000</td> <td align=center>10 - 1000</td> <td align=center>use logistic regression, or SVM without a kerenl ("linear kernel")</td> </tr> <tr> <td align=center>small (i.e. 1-1000)</td> <td align=center>intermediate (10-10,000)</td> <td align=center>use SVM with Gaussian kernel</td> </tr> <tr> <td align=center>small (i.e. 1-1000)</td> <td align=center>large (50,000+)</td> <td align=center>create/add more features, then use logistic regression or SVM without a kernel</td> </tr> </tbody> </table> <p><img alt=svm-vs-logistic-regression src=../fig/svm-vs-logistic-regression.png></p> <h2 id=week-8-unsupervised-algorithms>Week 8 Unsupervised Algorithms<a class=headerlink href=#week-8-unsupervised-algorithms title="Permanent link">&para;</a></h2> <h3 id=k-means-algorithm>K-Means Algorithm<a class=headerlink href=#k-means-algorithm title="Permanent link">&para;</a></h3> <p>In the k-means clustering algorithm. We have input <span><span class=MathJax_Preview>K</span><script type=math/tex>K</script></span> (number of clusters) and traning set <span><span class=MathJax_Preview>\{x^{(1)}, x^{(1)}, \dots, x^{(m)}\}</span><script type=math/tex>\{x^{(1)}, x^{(1)}, \dots, x^{(m)}\}</script></span>, for each of the training example <span><span class=MathJax_Preview>x^{(i)}\in \R^n</span><script type=math/tex>x^{(i)}\in \R^n</script></span>. Note here we drop the <span><span class=MathJax_Preview>x_0 = 1</span><script type=math/tex>x_0 = 1</script></span> convention. To visualize the algorithm, we have the following three picture.</p> <p><img alt="K Means 1" src=../fig/K-means-1.png> <img alt="K Means 2" src=../fig/K-means-2.png> <img alt="K Means 3" src=../fig/K-means-3.png></p> <p>Concretely, the k-means algorithm is:</p> <ul> <li>Input: training set <span><span class=MathJax_Preview>\! \{x^{(i)}\}_{i=1}^m,</span><script type=math/tex>\! \{x^{(i)}\}_{i=1}^m,</script></span> and number of clusters <span><span class=MathJax_Preview>K</span><script type=math/tex>K</script></span>.</li> <li>Algorith:<ol> <li>Randomly initialize <span><span class=MathJax_Preview>K</span><script type=math/tex>K</script></span> cluster centroids <span><span class=MathJax_Preview>\mu_1, \mu_1, \dots, \mu_K\in \R^n</span><script type=math/tex>\mu_1, \mu_1, \dots,  \mu_K\in \R^n</script></span>.</li> <li>Repeat:<ul> <li>for <span><span class=MathJax_Preview>i = 1</span><script type=math/tex>i = 1</script></span> to <span><span class=MathJax_Preview>m</span><script type=math/tex>m</script></span>: // cluster assignment step<ul> <li><span><span class=MathJax_Preview>c^{(i)}</span><script type=math/tex>c^{(i)}</script></span> := index (from 1 to <span><span class=MathJax_Preview>K</span><script type=math/tex>K</script></span>) of cluster centroid closest to <span><span class=MathJax_Preview>x^{(i)}</span><script type=math/tex>x^{(i)}</script></span></li> </ul> </li> <li>for <span><span class=MathJax_Preview>k = 1</span><script type=math/tex>k = 1</script></span> to <span><span class=MathJax_Preview>K</span><script type=math/tex>K</script></span>: // move centroid step<ul> <li><span><span class=MathJax_Preview>\mu_k</span><script type=math/tex>\mu_k</script></span> := average (mean) of points assigned to cluster <span><span class=MathJax_Preview>k</span><script type=math/tex>k</script></span></li> </ul> </li> </ul> </li> <li>Until convergence</li> </ol> </li> </ul> <h3 id=optimization-objective_1>Optimization objective<a class=headerlink href=#optimization-objective_1 title="Permanent link">&para;</a></h3> <p><img alt="K Means Objective" src=../fig/K-means-objective.png></p> <h3 id=random-initialization_1>Random initialization<a class=headerlink href=#random-initialization_1 title="Permanent link">&para;</a></h3> <p>We can random initialize the centroid by randomly pick <span><span class=MathJax_Preview>K</span><script type=math/tex>K</script></span> training examples and set <span><span class=MathJax_Preview>\mu_1, \dots, \mu_K</span><script type=math/tex>\mu_1, \dots, \mu_K</script></span> equal to these <span><span class=MathJax_Preview>K</span><script type=math/tex>K</script></span> examples. Obviously, we should have <span><span class=MathJax_Preview>K &lt; m</span><script type=math/tex>K < m</script></span>. However, some of the random initialization would lead the algorithm to achieve local optima, which is shown below in the pictures. To solve this problem, we can random initialize multiple times then to pick clustering that gave lowest cost <span><span class=MathJax_Preview>J(c^{(1)}, \dots, c^{(m)}, \mu_1, \dots \mu_K)</span><script type=math/tex>J(c^{(1)}, \dots, c^{(m)}, \mu_1, \dots \mu_K)</script></span>.</p> <p><img alt="K Means Rand Init Prob" src=../fig/K-means-rand-init-prob.png> <img alt="K Means Rand Init Algo" src=../fig/K-means-rand-init-algo.png></p> <h3 id=choosing-the-k>Choosing the K<a class=headerlink href=#choosing-the-k title="Permanent link">&para;</a></h3> <p>It depends on the problem we are trying to solve. One straightforward problem is using a method called the Elbow method as the following picture. Secondly, the K value should be picked to maximize some practical utility function, for example, we cluster the size of different T-shirts, whether we want to have 3 sizes or 5 sizes is depends on the profitability.</p> <p><img alt="Elbow Method" src=../fig/elbow-method.png></p> <h3 id=dimensionality-reduction>Dimensionality reduction<a class=headerlink href=#dimensionality-reduction title="Permanent link">&para;</a></h3> <p>The two motivation of Dimensionality Reduction are:</p> <ol> <li>Data compression, for example, reduce data from 2D to 1D.</li> <li>Visualization, we can reduce the high dimensionality data to 2 or 3 dimensions, and visualize them. For example, to visualize some complex economical data for different countries.</li> </ol> <h3 id=principal-component-analysis-pca>Principal Component Analysis (PCA)<a class=headerlink href=#principal-component-analysis-pca title="Permanent link">&para;</a></h3> <p>To reduce from 2-dimension to 1-dimension: Find a direction (a vector <span><span class=MathJax_Preview>u^{(1)}\in \R^n</span><script type=math/tex>u^{(1)}\in \R^n</script></span>) onto which to project the data so as to minimize the projection error. To reduce from n-dimension to k-dimension: Find k direction vectors <span><span class=MathJax_Preview>u^{(1)}, u^{(2)}, \dots, u^{(k)}</span><script type=math/tex>u^{(1)}, u^{(2)}, \dots, u^{(k)}</script></span> onto which to project the data so as to minimize the projection error.</p> <div class="admonition note"> <p class=admonition-title>Note</p> <p>Note the difference between PCA and linear regression.</p> </div> <p><img alt="PCA Formulation" src=../fig/pca-formulation.png></p> <h3 id=pca-algorithm>PCA algorithm<a class=headerlink href=#pca-algorithm title="Permanent link">&para;</a></h3> <p>Before carry out the PCA, we\'d better to do some feature scaling/mean normalization. As in the slides below, the <span><span class=MathJax_Preview>S_i</span><script type=math/tex>S_i</script></span> could be the maximum/minimum value or other values such as variance <span><span class=MathJax_Preview>\sigma</span><script type=math/tex>\sigma</script></span>.</p> <p><img alt="PCA Preprocessing" src=../fig/pca-preprocessing.png></p> <p>once the pre-proccessing steps are done we can run the following algorithm in Octave to implement PCA.</p> <p><img alt="PCA Algo I" src=../fig/pca-algorithm-0.png> <img alt="PCA Algo II" src=../fig/pca-algorithm-1.png> <img alt="PCA Algo III" src=../fig/pca-algorithm-2.png> <img alt="PCA Algo IV" src=../fig/pca-algorithm-3.png></p> <h3 id=reconstruction-from-pca-compressed-data>Reconstruction from PCA compressed data<a class=headerlink href=#reconstruction-from-pca-compressed-data title="Permanent link">&para;</a></h3> <p><img alt="PCA Reconstruction" src=../fig/pca-reconstruction.png></p> <h3 id=choosing-the-number-of-principal-components>Choosing the number of principal components<a class=headerlink href=#choosing-the-number-of-principal-components title="Permanent link">&para;</a></h3> <p>Two concepts we should define here:</p> <p>Average squared projection error:</p> <ul> <li><span><span class=MathJax_Preview>\frac{1}{m}\sum_{i=1}^{m}\lVert x^{(i)} - x_{approx}^{(i)}\rVert^2</span><script type=math/tex>\frac{1}{m}\sum_{i=1}^{m}\lVert x^{(i)} - x_{approx}^{(i)}\rVert^2</script></span></li> </ul> <p>Total variation in the data:</p> <ul> <li><span><span class=MathJax_Preview>\frac{1}{m}\sum_{i=1}^{m}\lVert x^{(i)} \rVert^2</span><script type=math/tex>\frac{1}{m}\sum_{i=1}^{m}\lVert x^{(i)} \rVert^2</script></span></li> </ul> <p>Typically, we choose <span><span class=MathJax_Preview>k</span><script type=math/tex>k</script></span> to be smallest value so that ration of the two quality should be less than 1%.</p> <p><img alt="PCA Choose K" src=../fig/pca-choose-k.png> <img alt="PCA Choose K Algorithm" src=../fig/pca-choose-k-algorithm.png> <img alt="PCA Choose K Algorithm 1" src=../fig/pca-choose-k-algorithm-1.png></p> <h3 id=advice-for-applying-pca>Advice for applying PCA<a class=headerlink href=#advice-for-applying-pca title="Permanent link">&para;</a></h3> <p>One important point should keep in mind when using PCA:</p> <ul> <li>don't try to use PCA to prevent overfitting, use regularization instead.</li> </ul> <p><img alt="PCA APPs" src=../fig/pca-apps.png> <img alt="PCA APPs Bad Use I" src=../fig/pca-apps-bad-use0.png> <img alt="PCA APPs Bad Use II" src=../fig/pca-apps-bad-use1.png></p> <h2 id=week-9-anomaly-detection>Week 9 Anomaly Detection<a class=headerlink href=#week-9-anomaly-detection title="Permanent link">&para;</a></h2> <h3 id=anomaly-detection>Anomaly Detection<a class=headerlink href=#anomaly-detection title="Permanent link">&para;</a></h3> <p>Given a model <span><span class=MathJax_Preview>p(x)</span><script type=math/tex>p(x)</script></span>, test on the example <span><span class=MathJax_Preview>x^{(i)}</span><script type=math/tex>x^{(i)}</script></span> to check whether we have <span><span class=MathJax_Preview>p(x) &lt; \epsilon</span><script type=math/tex>p(x) < \epsilon</script></span>. Generally, the anomaly detection system apply to the scenario that we don't have much anomaly example, such as a dataset about aircraft engines.</p> <p>In this particular lecture, the <span><span class=MathJax_Preview>p(x)</span><script type=math/tex>p(x)</script></span> is <a href=https://en.wikipedia.org/wiki/Normal_distribution>multivariate Gaussian distribution</a>. The algorithm is to find the parameter <span><span class=MathJax_Preview>\mu</span><script type=math/tex>\mu</script></span> and <span><span class=MathJax_Preview>\sigma^2</span><script type=math/tex>\sigma^2</script></span>. Once we fit the data with a multivariate Gaussian distribution, we are able to obtain a probability value, which can be use to detect the anomaly by select a probability threshold <span><span class=MathJax_Preview>\epsilon</span><script type=math/tex>\epsilon</script></span>.</p> <p>Here is the anomaly detection algorithm:</p> <p><img alt="Anomaly Detection Algorithm" src=../fig/anomaly-detection-algorithm.png></p> <h3 id=developing-and-evaluating>Developing and evaluating<a class=headerlink href=#developing-and-evaluating title="Permanent link">&para;</a></h3> <p><img alt="Anomaly Detection Develop" src=../fig/anomaly-detection-develop.png> <img alt="Anomaly Detection Develop Example" src=../fig/anomaly-detection-develop-example.png> <img alt="Anomaly Detection Develop Evaluation" src=../fig/anomaly-detection-develop-evaluation.png></p> <h3 id=anomaly-detection-vs-supervised-learning>Anomaly detection vs. supervised learning<a class=headerlink href=#anomaly-detection-vs-supervised-learning title="Permanent link">&para;</a></h3> <p><img alt="Anomaly vs Supervised I" src=../fig/anomaly-vs-supervised-1.png> <img alt="Anomaly vs Supervised II" src=../fig/anomaly-vs-supervised-2.png></p> <h3 id=multivariate-gaussian-distribution>Multivariate Gaussian distribution<a class=headerlink href=#multivariate-gaussian-distribution title="Permanent link">&para;</a></h3> <p>There are plots of different multivariate Gaussian distributions with different mean and covariance matrix. It intuitively show how changes in the mean and covariance matrix can change the shape of the plot of the distribution. It also compared the original model (multiple single variate Gaussian distribution) to multivariate Gaussian distribution. Generally, if we have multiple features, we tend to use multivariate Gaussian distribution to fit the data, even thought the original model is more computationally cheaper.</p> <p><img alt="Original Multivariate Gaussian" src=../fig/original-multivariate-gaussian.png></p> <h3 id=predicting-movie-ratings>Predicting movie ratings<a class=headerlink href=#predicting-movie-ratings title="Permanent link">&para;</a></h3> <h4 id=problem-formulation>Problem formulation<a class=headerlink href=#problem-formulation title="Permanent link">&para;</a></h4> <p><img alt="Rating Problem Example Movie" src=../fig/rating-problem-example-movie.png></p> <h4 id=content-based-recommendations>Content based recommendations<a class=headerlink href=#content-based-recommendations title="Permanent link">&para;</a></h4> <p>Suppose we have a feature vector for each of the movie, combining with the rating values, we can solve the minimization problem to get <span><span class=MathJax_Preview>\theta^{(j)}</span><script type=math/tex>\theta^{(j)}</script></span>, which is the parameter vector of user <span><span class=MathJax_Preview>j</span><script type=math/tex>j</script></span>. With this parameter vector, we can predicting the rating of the movie by <span><span class=MathJax_Preview>(\theta^{(j)})^T x^{(i)}</span><script type=math/tex>(\theta^{(j)})^T x^{(i)}</script></span>.</p> <p><img alt="Rating Problem Formulation" src=../fig/rating-problem-formulation.png> <img alt="Rating Problem Notation" src=../fig/rating-problem-notation.png> <img alt="Rating Problem Optimization Objective" src=../fig/rating-problem-optimization-objective.png> <img alt="Rating Problem Gradient Descent" src=../fig/rating-problem-gradient-descent.png></p> <h4 id=collaborate-filtering>Collaborate filtering<a class=headerlink href=#collaborate-filtering title="Permanent link">&para;</a></h4> <p>In collaborate filtering, we don\'t have the feature, we only have the parameter vector <span><span class=MathJax_Preview>\theta^{(j)}</span><script type=math/tex>\theta^{(j)}</script></span> for user <span><span class=MathJax_Preview>j</span><script type=math/tex>j</script></span>. We can solve a optimization problem in regarding to the feature vector <span><span class=MathJax_Preview>x^{(i)}</span><script type=math/tex>x^{(i)}</script></span> through the rating values.</p> <p>What interesting about this if we don\'t have the initial <span><span class=MathJax_Preview>\theta^{(j)}</span><script type=math/tex>\theta^{(j)}</script></span>, we can generate a small random value of <span><span class=MathJax_Preview>\theta^{(j)}</span><script type=math/tex>\theta^{(j)}</script></span>, and repetitively to get the feature vector <span><span class=MathJax_Preview>x^{(i)}</span><script type=math/tex>x^{(i)}</script></span>. As later we can say, we can solve a optimization problem with regarding to both <span><span class=MathJax_Preview>\theta^{(j)}</span><script type=math/tex>\theta^{(j)}</script></span> and <span><span class=MathJax_Preview>x^{(i)}</span><script type=math/tex>x^{(i)}</script></span> all at once. See the slides for details.</p> <p><img alt="Collaborating Filtering Problem" src=../fig/collaborating-filtering-problem.png> <img alt="Collaborating Filtering Optimization" src=../fig/collaborating-filtering-optimization.png> <img alt="Collaborating Filtering Algorithm" src=../fig/collaborating-filtering-algorithm-1.png></p> <h4 id=collaborative-filtering-algorithm>Collaborative filtering algorithm<a class=headerlink href=#collaborative-filtering-algorithm title="Permanent link">&para;</a></h4> <p>As discussed above, in practice, we solve the optimization problem respect to both <span><span class=MathJax_Preview>\theta^{(j)}</span><script type=math/tex>\theta^{(j)}</script></span> and <span><span class=MathJax_Preview>x^{(i)}</span><script type=math/tex>x^{(i)}</script></span>. See the slides for the optimization problem we are trying to solve, and the gradient descent algorithm to solve it.</p> <p><img alt="Collaborating Filtering Optimization Problem" src=../fig/collaborating-filtering-optimization-problem.png> <img alt="Collaborating Filtering Algorithm" src=../fig/collaborating-filtering-algorithm-2.png></p> </article> </div> </div> </main> <footer class=md-footer> <div class=md-footer-nav> <nav class="md-footer-nav__inner md-grid" aria-label=Footer> <a href=../../ class="md-footer-nav__link md-footer-nav__link--prev" rel=prev> <div class="md-footer-nav__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </div> <div class=md-footer-nav__title> <div class=md-ellipsis> <span class=md-footer-nav__direction> Previous </span> Index </div> </div> </a> <a href=../../cs224n/lec-notes/ class="md-footer-nav__link md-footer-nav__link--next" rel=next> <div class=md-footer-nav__title> <div class=md-ellipsis> <span class=md-footer-nav__direction> Next </span> CS224N Lecture Notes </div> </div> <div class="md-footer-nav__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg> </div> </a> </nav> </div> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-footer-copyright> <div class=md-footer-copyright__highlight> Copyright &copy; 2020 Rui Han </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> </div> </div> </footer> </div> <script src=../../../assets/javascripts/vendor.77e55a48.min.js></script> <script src=../../../assets/javascripts/bundle.9554a270.min.js></script><script id=__lang type=application/json>{"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing"}</script> <script>
        app = initialize({
          base: "../../..",
          features: ['navigation.tabs'],
          search: Object.assign({
            worker: "../../../assets/javascripts/worker/search.4ac00218.min.js"
          }, typeof search !== "undefined" && search)
        })
      </script> <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script> </body> </html>