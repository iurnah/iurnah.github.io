<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><link href=https://ruihan.org/courses/cs224n/Part%202%20Neural%20Networks%20and%20Backpropagation/ rel=canonical><meta name=lang:clipboard.copy content="Copy to clipboard"><meta name=lang:clipboard.copied content="Copied to clipboard"><meta name=lang:search.language content=en><meta name=lang:search.pipeline.stopwords content=True><meta name=lang:search.pipeline.trimmer content=True><meta name=lang:search.result.none content="No matching documents"><meta name=lang:search.result.one content="1 matching document"><meta name=lang:search.result.other content="# matching documents"><meta name=lang:search.tokenizer content=[\s\-]+><link rel="shortcut icon" href=../../../assets/images/favicon.png><meta name=generator content="mkdocs-1.1, mkdocs-material-4.6.3"><title>Part 2 Neural Networks and Backpropagation - RUIHAN.ORG</title><link rel=stylesheet href=../../../assets/stylesheets/application.adb8469c.css><link rel=stylesheet href=../../../assets/stylesheets/application-palette.a8b3c06d.css><meta name=theme-color content><script src=../../../assets/javascripts/modernizr.86422ebf.js></script><link href=https://fonts.gstatic.com rel=preconnect crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback"><style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style><link rel=stylesheet href=../../../assets/fonts/material-icons.css></head> <body dir=ltr data-md-color-primary=black data-md-color-accent=black> <svg class=md-svg> <defs> <svg xmlns=http://www.w3.org/2000/svg width=416 height=448 viewbox="0 0 416 448" id=__github><path fill=currentColor d="M160 304q0 10-3.125 20.5t-10.75 19T128 352t-18.125-8.5-10.75-19T96 304t3.125-20.5 10.75-19T128 256t18.125 8.5 10.75 19T160 304zm160 0q0 10-3.125 20.5t-10.75 19T288 352t-18.125-8.5-10.75-19T256 304t3.125-20.5 10.75-19T288 256t18.125 8.5 10.75 19T320 304zm40 0q0-30-17.25-51T296 232q-10.25 0-48.75 5.25Q229.5 240 208 240t-39.25-2.75Q130.75 232 120 232q-29.5 0-46.75 21T56 304q0 22 8 38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0 37.25-1.75t35-7.375 30.5-15 20.25-25.75T360 304zm56-44q0 51.75-15.25 82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5T212 416q-19.5 0-35.5-.75t-36.875-3.125-38.125-7.5-34.25-12.875T37 371.5t-21.5-28.75Q0 312 0 260q0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25 30.875Q171.5 96 212 96q37 0 70 8 26.25-20.5 46.75-30.25T376 64q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34 99.5z"/></svg> </defs> </svg> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay data-md-component=overlay for=__drawer></label> <a href=#part-2-neural-networks-and-backpropagation tabindex=0 class=md-skip> Skip to content </a> <header class=md-header data-md-component=header> <nav class="md-header-nav md-grid"> <div class=md-flex> <div class="md-flex__cell md-flex__cell--shrink"> <a href=https://ruihan.org title=RUIHAN.ORG aria-label=RUIHAN.ORG class="md-header-nav__button md-logo"> <i class=md-icon></i> </a> </div> <div class="md-flex__cell md-flex__cell--shrink"> <label class="md-icon md-icon--menu md-header-nav__button" for=__drawer></label> </div> <div class="md-flex__cell md-flex__cell--stretch"> <div class="md-flex__ellipsis md-header-nav__title" data-md-component=title> <span class=md-header-nav__topic> RUIHAN.ORG </span> <span class=md-header-nav__topic> Part 2 Neural Networks and Backpropagation </span> </div> </div> <div class="md-flex__cell md-flex__cell--shrink"> <label class="md-icon md-icon--search md-header-nav__button" for=__search></label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input aria-label=search name=query placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=query data-md-state=active> <label class="md-icon md-search__icon" for=__search></label> <button type=reset class="md-icon md-search__icon" data-md-component=reset tabindex=-1> &#xE5CD; </button> </form> <div class=md-search__output> <div class=md-search__scrollwrap data-md-scrollfix> <div class=md-search-result data-md-component=result> <div class=md-search-result__meta> Type to start searching </div> <ol class=md-search-result__list></ol> </div> </div> </div> </div> </div> </div> <div class="md-flex__cell md-flex__cell--shrink"> <div class=md-header-nav__source> <a href=https://github.com/iurnah/iurnah.github.io title="Go to repository" class=md-source data-md-source=github> <div class=md-source__icon> <svg viewbox="0 0 24 24" width=24 height=24> <use xlink:href=#__github width=24 height=24></use> </svg> </div> <div class=md-source__repository> iurnah/iurnah.github.io </div> </a> </div> </div> </div> </nav> </header> <div class=md-container> <nav class=md-tabs data-md-component=tabs> <div class="md-tabs__inner md-grid"> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../../../leetcode/binary-search/ class=md-tabs__link> Leetcode </a> </li> <li class=md-tabs__item> <a href=../cs224n-notes/ class=md-tabs__link> Course Notes </a> </li> <li class=md-tabs__item> <a href=../../../seed/Public-Key-Cryptography-and-PKI/ class=md-tabs__link> SEED Labs </a> </li> </ul> </div> </nav> <main class=md-main role=main> <div class="md-main__inner md-grid" data-md-component=container> <div class="md-sidebar md-sidebar--primary" data-md-component=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary" data-md-level=0> <label class="md-nav__title md-nav__title--site" for=__drawer> <a href=https://ruihan.org title=RUIHAN.ORG class="md-nav__button md-logo"> <i class=md-icon></i> </a> RUIHAN.ORG </label> <div class=md-nav__source> <a href=https://github.com/iurnah/iurnah.github.io title="Go to repository" class=md-source data-md-source=github> <div class=md-source__icon> <svg viewbox="0 0 24 24" width=24 height=24> <use xlink:href=#__github width=24 height=24></use> </svg> </div> <div class=md-source__repository> iurnah/iurnah.github.io </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-toggle md-nav__toggle" data-md-toggle=nav-1 type=checkbox id=nav-1> <label class=md-nav__link for=nav-1> Leetcode </label> <nav class=md-nav data-md-component=collapsible data-md-level=1> <label class=md-nav__title for=nav-1> Leetcode </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../leetcode/binary-search/ title="Binary Search" class=md-nav__link> Binary Search </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-toggle md-nav__toggle" data-md-toggle=nav-2 type=checkbox id=nav-2> <label class=md-nav__link for=nav-2> Course Notes </label> <nav class=md-nav data-md-component=collapsible data-md-level=1> <label class=md-nav__title for=nav-2> Course Notes </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../cs224n-notes/ title="CS224N NLP with Deep Learning" class=md-nav__link> CS224N NLP with Deep Learning </a> </li> <li class=md-nav__item> <a href=../../6.431-probability/6.431-probability-notes/ title="6.431 Probability" class=md-nav__link> 6.431 Probability </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-toggle md-nav__toggle" data-md-toggle=nav-3 type=checkbox id=nav-3> <label class=md-nav__link for=nav-3> SEED Labs </label> <nav class=md-nav data-md-component=collapsible data-md-level=1> <label class=md-nav__title for=nav-3> SEED Labs </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../seed/Public-Key-Cryptography-and-PKI/ title="Public Key Cryptography and PKI" class=md-nav__link> Public Key Cryptography and PKI </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary"> <label class=md-nav__title for=__toc>Table of contents</label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=#intro-to-neural-networks class=md-nav__link> Intro to Neural Networks </a> </li> <li class=md-nav__item> <a href=#forward-propagation-in-matrix-notation class=md-nav__link> Forward Propagation in Matrix Notation </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content> <article class="md-content__inner md-typeset"> <h1 id=part-2-neural-networks-and-backpropagation>Part 2 Neural Networks and Backpropagation<a class=headerlink href=#part-2-neural-networks-and-backpropagation title="Permanent link">&para;</a></h1> <p>Date: April 15th 2019 We discussed the softmax classifier in Part 1 and its major drawback that the classifier only gives linear decision boundaries. In Part 2, Neural Networks will be introduced to demonstrate that it can learn much more complex functions and nonlinear decision boundaries.</p> <h2 id=intro-to-neural-networks>Intro to Neural Networks<a class=headerlink href=#intro-to-neural-networks title="Permanent link">&para;</a></h2> <table> <thead> <tr> <th align=center>Biological Neuron</th> <th align=center>Mathematical Model</th> <th align=center>Simplified Neuron</th> </tr> </thead> <tbody> <tr> <td align=center><img alt="Biological Neuron" src=http://cs231n.github.io/assets/nn1/neuron.png></td> <td align=center><img alt="Mathematical model" src=http://cs231n.github.io/assets/nn1/neuron_model.jpeg></td> <td align=center><img alt="simplified neuron" src=http://static.zybuluo.com/iurnah/vwr6yjxddccv1qhl0g9sdcro/simplified%20neuron.png></td> </tr> </tbody> </table> <p>The neuron can be modeled as a binary logistic regression unit as in the right figure above. It can be further simplified as following functions,</p> <div> <div class=MathJax_Preview>\begin{equation*} h_{w, b} (\boldsymbol{x})=f(\boldsymbol{w}^\mathsf{T} \boldsymbol{x} + b) \\ f(z) = \frac{1}{1 + e^{-z}}. \end{equation*}</div> <script type="math/tex; mode=display">\begin{equation*} 
h_{w, b} (\boldsymbol{x})=f(\boldsymbol{w}^\mathsf{T} \boldsymbol{x} + b) \\
f(z) = \frac{1}{1 + e^{-z}}.
\end{equation*}</script> </div> <ul> <li><span><span class=MathJax_Preview>\boldsymbol{x}</span><script type=math/tex>\boldsymbol{x}</script></span> is the inputs</li> <li><span><span class=MathJax_Preview>\boldsymbol{w}</span><script type=math/tex>\boldsymbol{w}</script></span> is the weights</li> <li><span><span class=MathJax_Preview>b</span><script type=math/tex>b</script></span> is a bias term</li> <li><span><span class=MathJax_Preview>h</span><script type=math/tex>h</script></span> is a hidden layer function</li> <li><span><span class=MathJax_Preview>f</span><script type=math/tex>f</script></span> is a nonlinear activation function (sigmoid, tanh, etc.)</li> </ul> <p>If we feed a vector of inputs through a bunch of logistic regression (sigmoid) functions, then we get a vector of outputs. The <a href=https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1174/lecture_notes/cs224n-2017-notes3.pdf>CS224N lecture note 3 section 1.2</a> has a complete derivation on multiple sigmoid units. But we don’t have to decide ahead of time what variables these logistic regressions are trying to predict! It is the loss function that will direct what the intermediate hidden variables should be, to do a good job at predicting the targets for the next layer, etc.</p> <h2 id=forward-propagation-in-matrix-notation>Forward Propagation in Matrix Notation<a class=headerlink href=#forward-propagation-in-matrix-notation title="Permanent link">&para;</a></h2> <p>In a multilayer neural network, not only did we have multiple sigmoid units, but we also have more than one layer. Let's explicitly write down the signal transformation (aka forward propagation) from one layer to another referring to this network from Andrew's ML course.</p> <p><img alt=Neural_network.png src=http://static.zybuluo.com/iurnah/m3g37lozjn6dq9sqa1x05ph0/Neural_network.png></p> <p>We will use the following notation convention</p> <ul> <li><span><span class=MathJax_Preview>a^{(j)}_i</span><script type=math/tex>a^{(j)}_i</script></span> to represent the "activation" of unit <span><span class=MathJax_Preview>i</span><script type=math/tex>i</script></span> in layer <span><span class=MathJax_Preview>j</span><script type=math/tex>j</script></span>.</li> <li><span><span class=MathJax_Preview>W^{(j)}</span><script type=math/tex>W^{(j)}</script></span> to represent the matrix of weights controlling function mapping from layer <span><span class=MathJax_Preview>j</span><script type=math/tex>j</script></span> to layer <span><span class=MathJax_Preview>j + 1</span><script type=math/tex>j + 1</script></span>.</li> </ul> <p>The value at each node can be calculated as</p> <div> <div class=MathJax_Preview>\begin{equation*} a^{(2)}_1 = f(w^{(1)}_{10} x_0 + w^{(1)}_{11} x_1 + w^{(1)}_{12} x_2 + w^{(1)}_{13} x_3) \\ a^{(2)}_2 = f(w^{(1)}_{20} x_0 + w^{(1)}_{21} x_1 + w^{(1)}_{22} x_2 + w^{(1)}_{23} x_3) \\ a^{(2)}_3 = f(w^{(1)}_{30} x_0 + w^{(1)}_{31} x_1 + w^{(1)}_{32} x_2 + w^{(1)}_{33} x_3) \\ \text{and} \\ h_{w} (\boldsymbol{x}) = a^{(3)}_1 = f(w^{(2)}_{10} a^{(2)}_0 + w^{(2)}_{11} a^{(2)}_1 + w^{(2)}_{12} a^{(2)}_2 + w^{(2)}_{13} a^{(2)}_3) \end{equation*}</div> <script type="math/tex; mode=display">\begin{equation*}
a^{(2)}_1 = f(w^{(1)}_{10} x_0 + w^{(1)}_{11} x_1 + w^{(1)}_{12} x_2 + w^{(1)}_{13} x_3) \\
a^{(2)}_2 = f(w^{(1)}_{20} x_0 + w^{(1)}_{21} x_1 + w^{(1)}_{22} x_2 + w^{(1)}_{23} x_3) \\
a^{(2)}_3 = f(w^{(1)}_{30} x_0 + w^{(1)}_{31} x_1 + w^{(1)}_{32} x_2 + w^{(1)}_{33} x_3) \\
\text{and} \\
h_{w} (\boldsymbol{x}) = a^{(3)}_1 = f(w^{(2)}_{10} a^{(2)}_0 + w^{(2)}_{11} a^{(2)}_1 + w^{(2)}_{12} a^{(2)}_2 + w^{(2)}_{13} a^{(2)}_3)
\end{equation*}</script> </div> <p>write the matrix <span><span class=MathJax_Preview>W^{(j)}</span><script type=math/tex>W^{(j)}</script></span> explicity,</p> <div> <div class=MathJax_Preview>\begin{equation*} W^{(1)} = \begin{bmatrix} w^{(1)}_{10} &amp; w^{(1)}_{11} &amp; w^{(1)}_{12} &amp; w^{(1)}_{13} \\ w^{(1)}_{20} &amp; w^{(1)}_{21} &amp; w^{(1)}_{22} &amp; w^{(1)}_{23} \\ w^{(1)}_{30} &amp; w^{(1)}_{31} &amp; w^{(1)}_{32} &amp; w^{(1)}_{33} \end{bmatrix} \end{equation*}</div> <script type="math/tex; mode=display">\begin{equation*}
W^{(1)} =
\begin{bmatrix}
w^{(1)}_{10} & w^{(1)}_{11} & w^{(1)}_{12} & w^{(1)}_{13} \\
w^{(1)}_{20} & w^{(1)}_{21} & w^{(1)}_{22} & w^{(1)}_{23} \\
w^{(1)}_{30} & w^{(1)}_{31} & w^{(1)}_{32} & w^{(1)}_{33}
\end{bmatrix}
\end{equation*}</script> </div> <div> <div class=MathJax_Preview>\begin{equation*} W^{(2)} = \begin{bmatrix} w^{(2)}_{10} &amp; w^{(2)}_{11} &amp; w^{(2)}_{12} &amp; w^{(2)}_{13} \end{bmatrix} \end{equation*}</div> <script type="math/tex; mode=display">\begin{equation*}
W^{(2)} =
\begin{bmatrix}
w^{(2)}_{10} & w^{(2)}_{11} & w^{(2)}_{12} & w^{(2)}_{13}
\end{bmatrix}
\end{equation*}</script> </div> <p>With the above form, we can use matrix notation as</p> <div> <div class=MathJax_Preview>\begin{align*} &amp; \boldsymbol{a}^{(2)} = f(\boldsymbol{W}^{(1)} \boldsymbol{x}) \\ h_{w} (\boldsymbol{x}) = &amp; \boldsymbol{a}^{(3)} = f(\boldsymbol{W}^{(2)} \boldsymbol{a}^{(2)}) \end{align*}</div> <script type="math/tex; mode=display">\begin{align*}
& \boldsymbol{a}^{(2)} = f(\boldsymbol{W}^{(1)} \boldsymbol{x}) \\
h_{w} (\boldsymbol{x})  = & \boldsymbol{a}^{(3)} = f(\boldsymbol{W}^{(2)} \boldsymbol{a}^{(2)})
\end{align*}</script> </div> <p>We can see from above notations that if the network has <span><span class=MathJax_Preview>s_j</span><script type=math/tex>s_j</script></span> units in layer <span><span class=MathJax_Preview>j</span><script type=math/tex>j</script></span> and <span><span class=MathJax_Preview>s_{j+1}</span><script type=math/tex>s_{j+1}</script></span> in layer <span><span class=MathJax_Preview>j + 1</span><script type=math/tex>j + 1</script></span>, the matrix <span><span class=MathJax_Preview>W^{(j)}</span><script type=math/tex>W^{(j)}</script></span> will be of dimention <span><span class=MathJax_Preview>s_{j+1} \times s_{j}+1</span><script type=math/tex>s_{j+1} \times s_{j}+1</script></span>. It could be interpreted as "the dimention of <span><span class=MathJax_Preview>W^{(j)}</span><script type=math/tex>W^{(j)}</script></span> is the number of nodes in the next layer (layer <span><span class=MathJax_Preview>j + 1</span><script type=math/tex>j + 1</script></span>) <span><span class=MathJax_Preview>\times</span><script type=math/tex>\times</script></span> the number of nodes in the current layer <span><span class=MathJax_Preview>+</span><script type=math/tex>+</script></span> 1.</p> <blockquote> <p>Note that in cs224n the matrix notation is slightly different, \begin{align<em>} &amp; \boldsymbol{a}^{(2)} = f(\boldsymbol{W}^{(1)} \boldsymbol{x} + \boldsymbol{b}^{(1)}) \ h_{w} (\boldsymbol{x}) = &amp; \boldsymbol{a}^{(3)} = f(\boldsymbol{W}^{(2)} \boldsymbol{a}^{(2)} + \boldsymbol{b}^{(2)}) \end{align</em>}</p> </blockquote> <p>the difference is from how we denote the bias. The two are enssentially the same, but be cautions that the matrix dimentions are different.</p> <h1 id=word-window-classification-using-neural-networks>Word Window Classification Using Neural Networks<a class=headerlink href=#word-window-classification-using-neural-networks title="Permanent link">&para;</a></h1> <p>From now on, let's switch the notation to cs224n so that we can derive the backpropagation algorithm for word window classification and get more intuition about the backprop. The drawing of the neural nets in cs224n for word window classification is less dynamic and slitly different from the drawing from Andrew's ML class. The figure from cs224n may be slightly confusing at first, but it is good to understand it from this particular application, word window classification.</p> <h2 id=forward-propagation>Forward propagation<a class=headerlink href=#forward-propagation title="Permanent link">&para;</a></h2> <p>Firstly, the goal of this classification task is to classify whether the center word is a location. Similar to word2vec, we will go over all positions in a corpus. But this time, it will be supervised and only some positions should get a high score.</p> <p><img alt="Screen Shot 2019-04-18 at 10.50.59 PM.png-57.6kB" src=http://static.zybuluo.com/iurnah/jtmvh9jagsxznbfd4bscavpb/Screen%20Shot%202019-04-18%20at%2010.50.59%20PM.png></p> <p>The figure above illustrate the feed-forward process. We use the method by Collobert &amp; Weston (2008, 2011). An unnormalized score will be calculated from the activation <span><span class=MathJax_Preview>\boldsymbol{a} = [a_1, a_2, a_3, \cdots]^\mathsf{T}</span><script type=math/tex>\boldsymbol{a} = [a_1, a_2, a_3, \cdots]^\mathsf{T}</script></span>.</p> <div> <div class=MathJax_Preview>\begin{equation*} \text{scroe}(\boldsymbol{x}) = U^\mathsf{T} \boldsymbol{a} \in \mathbb{R} \end{equation*}</div> <script type="math/tex; mode=display">\begin{equation*}
\text{scroe}(\boldsymbol{x}) = U^\mathsf{T} \boldsymbol{a} \in \mathbb{R}
\end{equation*}</script> </div> <p>We will use max-margin loss as our loss function. The training is essentially to find the optimal weights <span><span class=MathJax_Preview>W</span><script type=math/tex>W</script></span> by minimize the max-margin loss \begin{equation<em>} J = \max(0, 1 - s + s_c) \end{equation</em>}</p> <p><span><span class=MathJax_Preview>s</span><script type=math/tex>s</script></span> is the score of a window that have a location in the center. <span><span class=MathJax_Preview>s_c</span><script type=math/tex>s_c</script></span> is the score of a window that doesn't have a location in the center. For full objective function: Sample several corrupt windows per true one. Sum over all training windows. It is similar to negative sampling in word2vec.</p> <p>We will use gradient descent to update the parameter so as to minimize the loss function. The key is how to calculate the gradient with respect to the model parameters, namely <span><span class=MathJax_Preview>\nabla_{\theta} J(\theta)</span><script type=math/tex>\nabla_{\theta} J(\theta)</script></span>. Here we use <span><span class=MathJax_Preview>\theta</span><script type=math/tex>\theta</script></span> to represent the hyperthetic parameters, it can include the <span><span class=MathJax_Preview>W</span><script type=math/tex>W</script></span> and other parameters of the model. \begin{equation<em>} \theta^{\text{new}} = \theta^{\text{old}} - \alpha\nabla_{\theta} J(\theta) \end{equation</em>}</p> <h2 id=gradients-and-jacobians-matrix>Gradients and Jacobians Matrix<a class=headerlink href=#gradients-and-jacobians-matrix title="Permanent link">&para;</a></h2> <p>At first, let us layout the input and all the equations in this simple neural network. | Input Layer | Hidden Layer | Output Layer | |:---:|:---:|:---:| | <span><span class=MathJax_Preview>\boldsymbol{x}</span><script type=math/tex>\boldsymbol{x}</script></span> | <span><span class=MathJax_Preview>\boldsymbol{z} = \boldsymbol{W} \boldsymbol{x} + \boldsymbol{b}, \\ \boldsymbol{h} = f(\boldsymbol{z})</span><script type=math/tex>\boldsymbol{z} = \boldsymbol{W} \boldsymbol{x} + \boldsymbol{b}, \\ \boldsymbol{h} = f(\boldsymbol{z})</script></span> | $ s = \boldsymbol{u}^{\mathsf{T}}\boldsymbol{h}$ |</p> <p>To update the parameters in this model, namely <span><span class=MathJax_Preview>\boldsymbol{x}, \boldsymbol{W}, \boldsymbol{b}</span><script type=math/tex>\boldsymbol{x}, \boldsymbol{W}, \boldsymbol{b}</script></span>, we would like to compute the derivitavies of <span><span class=MathJax_Preview>s</span><script type=math/tex>s</script></span> with respect to all these parameters. We have to use the chain rule to compute it. What chain rule says is that we can compute the partial derivatives of each individual functions and then multiply them together to get the derivative with respect the specific variable. For example, <span><span class=MathJax_Preview>\frac{\partial s}{\partial \boldsymbol{b}}</span><script type=math/tex>\frac{\partial s}{\partial \boldsymbol{b}}</script></span> is computed as <span><span class=MathJax_Preview>\frac{\partial s}{\partial \boldsymbol{b}} = \frac{\partial s}{\partial \boldsymbol{h}} \cdot \frac{\partial \boldsymbol{h}}{\partial \boldsymbol{z}} \cdot \frac{\partial \boldsymbol{z}}{\partial \boldsymbol{b}}</span><script type=math/tex>\frac{\partial s}{\partial \boldsymbol{b}} = \frac{\partial s}{\partial \boldsymbol{h}} \cdot \frac{\partial \boldsymbol{h}}{\partial \boldsymbol{z}} \cdot \frac{\partial \boldsymbol{z}}{\partial \boldsymbol{b}}</script></span>. This seems very easy to understand, but when it comes to implemenation in vectorized format, it become confusing for those who doesn't work on matrix calculus for quite a while like me. I want to get the points straight here. What exactly is <span><span class=MathJax_Preview>\frac{\partial s}{\partial \boldsymbol{b}}</span><script type=math/tex>\frac{\partial s}{\partial \boldsymbol{b}}</script></span> and <span><span class=MathJax_Preview>\frac{\partial \boldsymbol{h}}{\partial \boldsymbol{z}}</span><script type=math/tex>\frac{\partial \boldsymbol{h}}{\partial \boldsymbol{z}}</script></span>. Note both <span><span class=MathJax_Preview>\boldsymbol{h}</span><script type=math/tex>\boldsymbol{h}</script></span> and <span><span class=MathJax_Preview>\boldsymbol{z}</span><script type=math/tex>\boldsymbol{z}</script></span> are vectors. To calculate these two gradient, simply remember the following two rules:</p> <ol> <li> <p>Given a function with 1 output and <span><span class=MathJax_Preview>n</span><script type=math/tex>n</script></span> inputs <span><span class=MathJax_Preview>f(\boldsymbol{x}) = f(x_1, x_2, \cdots, x_n)</span><script type=math/tex>f(\boldsymbol{x}) = f(x_1, x_2, \cdots, x_n)</script></span>, it's gradient is a vector of partial derivatives with respect to each input(take the gradient element wise). \begin{equation<em>} \frac{\partial f}{\partial \boldsymbol{x}} = \Bigg [ \frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \cdots, \frac{\partial f}{\partial x_n}, \Bigg ] \end{equation</em>}</p> </li> <li> <p>Given a function with <span><span class=MathJax_Preview>m</span><script type=math/tex>m</script></span> output and <span><span class=MathJax_Preview>n</span><script type=math/tex>n</script></span> inputs <span><span class=MathJax_Preview>\boldsymbol{f}(\boldsymbol{x}) = \big [f_1(x_1, x_2, \cdots, x_n), \cdots, f_m(x_1, x_2, \cdots, x_n) \big ]</span><script type=math/tex>\boldsymbol{f}(\boldsymbol{x}) = \big [f_1(x_1, x_2, \cdots, x_n), \cdots, f_m(x_1, x_2, \cdots, x_n) \big ]</script></span>, it's gradient is an <span><span class=MathJax_Preview>m \times n</span><script type=math/tex>m \times n</script></span> matrix of partial derivatives <span><span class=MathJax_Preview>(\frac{\partial \boldsymbol{f}}{\partial \boldsymbol{x}})_{ij} = \dfrac{\partial f_i}{\partial x_j}</span><script type=math/tex>(\frac{\partial \boldsymbol{f}}{\partial \boldsymbol{x}})_{ij} = \dfrac{\partial f_i}{\partial x_j}</script></span>. This matrix is also called Jacobian matrix. </p> </li> </ol> <div> <div class=MathJax_Preview>\begin{equation*} \frac{\partial \boldsymbol{f}}{\partial \boldsymbol{x}} = \begin{bmatrix} \dfrac{\partial \boldsymbol{f}}{\partial x_1} &amp; \cdots &amp; \dfrac{\partial \boldsymbol{f}}{\partial x_n} \end{bmatrix} = \begin{bmatrix} \dfrac{\partial f_1}{\partial x_1} &amp; \cdots &amp; \dfrac{\partial f_1}{\partial x_n}\\ \vdots &amp; \ddots &amp; \vdots\\ \dfrac{\partial f_m}{\partial x_1} &amp; \cdots &amp; \dfrac{\partial f_m}{\partial x_n} \end{bmatrix} \end{equation*}</div> <script type="math/tex; mode=display">\begin{equation*}
\frac{\partial \boldsymbol{f}}{\partial \boldsymbol{x}} 
= \begin{bmatrix}
\dfrac{\partial \boldsymbol{f}}{\partial x_1} & \cdots & \dfrac{\partial \boldsymbol{f}}{\partial x_n} 
\end{bmatrix}
= \begin{bmatrix}
\dfrac{\partial f_1}{\partial x_1} & \cdots & \dfrac{\partial f_1}{\partial x_n}\\
\vdots & \ddots & \vdots\\
\dfrac{\partial f_m}{\partial x_1} & \cdots & \dfrac{\partial f_m}{\partial x_n} \end{bmatrix}
\end{equation*}</script> </div> <h2 id=computing-gradients-with-the-chain-rule>Computing gradients with the chain rule<a class=headerlink href=#computing-gradients-with-the-chain-rule title="Permanent link">&para;</a></h2> <p>With these two rules we can calculate the partials. We will use <span><span class=MathJax_Preview>\frac{\partial s}{\partial \boldsymbol{b}}</span><script type=math/tex>\frac{\partial s}{\partial \boldsymbol{b}}</script></span> as an example.</p> <div> <div class=MathJax_Preview>\begin{equation*} \frac{\partial s}{\partial \boldsymbol{b}} = \frac{\partial s}{\partial \boldsymbol{h}} \frac{\partial \boldsymbol{h}}{\partial \boldsymbol{z}} \frac{\partial \boldsymbol{z}}{\partial \boldsymbol{b}} \end{equation*}</div> <script type="math/tex; mode=display">\begin{equation*}
\frac{\partial s}{\partial \boldsymbol{b}} = \frac{\partial s}{\partial \boldsymbol{h}} \frac{\partial \boldsymbol{h}}{\partial \boldsymbol{z}} \frac{\partial \boldsymbol{z}}{\partial \boldsymbol{b}}
\end{equation*}</script> </div> <ul> <li><span><span class=MathJax_Preview>\frac{\partial s}{\partial \boldsymbol{h}} = \frac{\partial}{\partial \boldsymbol{h}}(\boldsymbol{u}^{\mathsf{T}}\boldsymbol{h}) = \boldsymbol{u}^{\mathsf{T}}</span><script type=math/tex>\frac{\partial s}{\partial \boldsymbol{h}} = \frac{\partial}{\partial \boldsymbol{h}}(\boldsymbol{u}^{\mathsf{T}}\boldsymbol{h}) = \boldsymbol{u}^{\mathsf{T}}</script></span></li> <li><span><span class=MathJax_Preview>\frac{\partial \boldsymbol{h}}{\partial \boldsymbol{z}} = \frac{\partial}{\partial \boldsymbol{z}}(f(\boldsymbol{z})) = \begin{pmatrix} f'(z_1) &amp; &amp; 0 \\ &amp; \ddots &amp; \\ 0 &amp; &amp; f'(z_n) \end{pmatrix} = \mathrm{diag} (f' (\boldsymbol{z}))</span><script type=math/tex>\frac{\partial \boldsymbol{h}}{\partial \boldsymbol{z}} = \frac{\partial}{\partial \boldsymbol{z}}(f(\boldsymbol{z})) = \begin{pmatrix}
f'(z_1) & & 0 \\
& \ddots & \\
0 & & f'(z_n) 
\end{pmatrix} = \mathrm{diag} (f' (\boldsymbol{z}))</script></span></li> <li><span><span class=MathJax_Preview>\frac{\partial \boldsymbol{z}}{\partial \boldsymbol{b}} = \frac{\partial}{\partial \boldsymbol{b}}(\boldsymbol{W}\boldsymbol{x} + \boldsymbol{b}) = \boldsymbol{I}</span><script type=math/tex>\frac{\partial \boldsymbol{z}}{\partial \boldsymbol{b}} = \frac{\partial}{\partial \boldsymbol{b}}(\boldsymbol{W}\boldsymbol{x} + \boldsymbol{b}) = \boldsymbol{I}</script></span></li> </ul> <p>Therefore, \begin{equation<em>} \frac{\partial s}{\partial \boldsymbol{b}} = \boldsymbol{u}^{\mathsf{T}} \mathrm{diag}(f'(\boldsymbol{z})) \boldsymbol{I} = \boldsymbol{u}^{\mathsf{T}} \circ f'(\boldsymbol{z}) \end{equation</em>}</p> <p>Similarly we can calculate the partial with respect to <span><span class=MathJax_Preview>\boldsymbol{W}</span><script type=math/tex>\boldsymbol{W}</script></span> and <span><span class=MathJax_Preview>\boldsymbol{x}</span><script type=math/tex>\boldsymbol{x}</script></span>. Since the top layer partials are already calculated, we can reuese the results. We denote those reusable partials as <span><span class=MathJax_Preview>\delta</span><script type=math/tex>\delta</script></span> meaning local error signal.</p> <div> <div class=MathJax_Preview>\begin{align*} \frac{\partial s}{\partial \boldsymbol{W}} &amp; = \frac{\partial s}{\partial \boldsymbol{h}} \frac{\partial \boldsymbol{h}}{\partial \boldsymbol{z}} \frac{\partial \boldsymbol{z}}{\partial \boldsymbol{W}} \\ \\ \frac{\partial s}{\partial \boldsymbol{x}} &amp; = \frac{\partial s}{\partial \boldsymbol{h}} \frac{\partial \boldsymbol{h}}{\partial \boldsymbol{z}} \frac{\partial \boldsymbol{z}}{\partial \boldsymbol{x}} \\ \\ \frac{\partial s}{\partial \boldsymbol{b}} &amp; = \underbrace{\frac{\partial s}{\partial \boldsymbol{h}} \frac{\partial \boldsymbol{h}}{\partial \boldsymbol{z}}}_{\mathrm{reuse}} \frac{\partial \boldsymbol{z}}{\partial \boldsymbol{b}} = \boldsymbol{\delta} \frac{\partial \boldsymbol{z}}{\partial \boldsymbol{b}} = \boldsymbol{\delta} = \boldsymbol{u}^{\mathsf{T}} \circ f'(\boldsymbol{z}) \end{align*}</div> <script type="math/tex; mode=display">\begin{align*}
\frac{\partial s}{\partial \boldsymbol{W}} & =  \frac{\partial s}{\partial \boldsymbol{h}} \frac{\partial \boldsymbol{h}}{\partial \boldsymbol{z}} \frac{\partial \boldsymbol{z}}{\partial \boldsymbol{W}} \\ \\
\frac{\partial s}{\partial \boldsymbol{x}} & = \frac{\partial s}{\partial \boldsymbol{h}} \frac{\partial \boldsymbol{h}}{\partial \boldsymbol{z}} \frac{\partial \boldsymbol{z}}{\partial \boldsymbol{x}} \\ \\
\frac{\partial s}{\partial \boldsymbol{b}} & =  \underbrace{\frac{\partial s}{\partial \boldsymbol{h}}
\frac{\partial \boldsymbol{h}}{\partial \boldsymbol{z}}}_{\mathrm{reuse}} \frac{\partial \boldsymbol{z}}{\partial \boldsymbol{b}} = \boldsymbol{\delta} \frac{\partial \boldsymbol{z}}{\partial \boldsymbol{b}} = \boldsymbol{\delta} = \boldsymbol{u}^{\mathsf{T}} \circ f'(\boldsymbol{z})
\end{align*}</script> </div> <h2 id=shape-convention>Shape convention<a class=headerlink href=#shape-convention title="Permanent link">&para;</a></h2> <p>What does the shape of the derivatives looks like in practice? How can we make the chain rule computation efficient? According to the aforementioned gradient calculation rules, <span><span class=MathJax_Preview>\frac{\partial s}{\partial \boldsymbol{W}}</span><script type=math/tex>\frac{\partial s}{\partial \boldsymbol{W}}</script></span> is a row vector. The chain rule gave </p> <div> <div class=MathJax_Preview>\begin{equation*} \frac{\partial s}{\partial \boldsymbol{W}} = \boldsymbol{\delta} \frac{\partial \boldsymbol{z}}{\partial \boldsymbol{W}}. \end{equation*}</div> <script type="math/tex; mode=display">\begin{equation*}
\frac{\partial s}{\partial \boldsymbol{W}}  =  \boldsymbol{\delta} \frac{\partial \boldsymbol{z}}{\partial \boldsymbol{W}}.
\end{equation*}</script> </div> <p>We know from Jacobians that <span><span class=MathJax_Preview>\frac{\partial\boldsymbol{z}}{\partial \boldsymbol{W}} = \frac{\partial}{\partial \boldsymbol{W}}(\boldsymbol{W}\boldsymbol{x} + \boldsymbol{b}) = \boldsymbol{x}^{\mathsf{T}}</span><script type=math/tex>\frac{\partial\boldsymbol{z}}{\partial \boldsymbol{W}} = \frac{\partial}{\partial \boldsymbol{W}}(\boldsymbol{W}\boldsymbol{x} + \boldsymbol{b}) =  \boldsymbol{x}^{\mathsf{T}}</script></span>. We may arrived the result that <span><span class=MathJax_Preview>\frac{\partial s}{\partial \boldsymbol{W}} = \boldsymbol{\delta} \ \boldsymbol{x}^{\mathsf{T}}</span><script type=math/tex>\frac{\partial s}{\partial \boldsymbol{W}}  =  \boldsymbol{\delta} \ \boldsymbol{x}^{\mathsf{T}}</script></span>. This is actually not quite wirte. The correct form should be </p> <div> <div class=MathJax_Preview>\begin{align*} \underbrace{\frac{\partial s}{\partial \boldsymbol{W}}}_{n \times m} = \underbrace{\boldsymbol{\delta}^{\mathsf{T}}}_{n \times 1} \ \underbrace{\boldsymbol{x}^{\mathsf{T}}}_{1 \times m}. \end{align*}</div> <script type="math/tex; mode=display">\begin{align*}
\underbrace{\frac{\partial s}{\partial \boldsymbol{W}}}_{n \times m}  = \underbrace{\boldsymbol{\delta}^{\mathsf{T}}}_{n \times 1} \ \underbrace{\boldsymbol{x}^{\mathsf{T}}}_{1 \times m}.
\end{align*}</script> </div> <p>You may wonder why is this form instead of the one we derived directly. The explanation from the CS224N is that we would like to follow the shape convention so as to make the chain rule implementation more efficient (matrix multiplication instead of loops). <strong>Different from the Jacobian form, shape convention states that the shape of the gradient is the same shape of parameters.</strong> The resolution here is to use Jacobian form as much as possible and reshape to follow the convention at the end. Because Jacobian form makes the chain rule easy and the shape convention makes the implementation of gradient descent easy.</p> <h2 id=computation-graphs-and-backpropagation>Computation graphs and backpropagation<a class=headerlink href=#computation-graphs-and-backpropagation title="Permanent link">&para;</a></h2> <p>We have shown how to compute the partial derivatives using chain rule. This is almost the backpropagation alrogithm. If we want to add on to that and make the algorithm complete, the only thing we need is how to reuse gradient computed for higher layers. We leverage the computation graph to explain this. From the computation graph, you'll see the intuiation of reusing the partial derivatives computed for higher layers in computing derivatives for lower layers so as to minimize computation.</p> <p>We represent our neural net equations as a computation graph as following:</p> <p><img alt="computation graph.png-31.8kB" src=http://static.zybuluo.com/iurnah/8bknov4w2x9igornns45qkty/computation%20graph.png></p> <ul> <li>source nodes represent inputs </li> <li>interior nodes represent operations</li> <li>edges pass along result of the operation</li> </ul> <p>When following computation order to carry out the computations from inputs, it is enssetially forward propagation. Backpropagation is to pass along gradients backwards. This can be illustrated in the following computation graph.</p> <p><img alt="computation graph backprop.png-50.6kB" src=http://static.zybuluo.com/iurnah/axcuqzot3sjkopc0pauxnxd1/computation%20graph%20backprop.png></p> <p>The partial derivative with respect to a parameter reflect how changing the parameter would effect the output value. The output value in the backprop is usually a loss function (or error). Intuitively, you can see backprop is push the error back to the lower layer through a bunch of operation nodes, the arrived error is a measure of the error at that particular layer, training is try to reduce this backprop-ed error by adjusting the local parameters, the effect of reducing the local error will be forward propagated to the output, the error at the output should also reduced. We use gradient descent to make this process to converge as soon as possible. </p> <p>To understand it better, let's look at a single node as following. </p> <p><img alt="local gradient chain rule.png-63.1kB" src=http://static.zybuluo.com/iurnah/b8zvwgl64h6y4y98lxj0vpxr/local%20gradient%20chain%20rule.png></p> <p>We define "local gradient" for each node as the gradient of it's output with respect to it's input. By chain rule, the downstream gradient is equal to the multiplication of the upstream gradient and the local gradient. When having multipe local gradients, they are pushed back to each input using chain rule. CS224N lecture 4 slides have step by step example of backprop. From the example, we can got some intuitions about some nodes' effects. For example, when push back gradients along outward branches from a single node, the gradients should be sumed up; "+" node distributes the upstream gradient to each summand; "max" node simply routes the upstream gradients.</p> <p>When update gradient with backprop, you should compute all the gradients at once. With this computation graph notion, following routine captures the gist of backprop in a very decent manner.</p> <blockquote> <ol> <li>Fprop: visit nodes in topological sorted order</li> <li>Compute value of node given predecessors</li> <li>Bprop:</li> <li>initialize output gradient = 1</li> <li>visit nodes in reverse order:<ul> <li>Compute gradient wrt each node using gradient wrt successors</li> <li><span><span class=MathJax_Preview>\{y_1, y_2, \cdots, y_2\} = \mathrm{successors\ of\ } x</span><script type=math/tex>\{y_1, y_2, \cdots, y_2\} = \mathrm{successors\ of\ } x</script></span> \begin{equation<em>}\frac{\partial z}{\partial \boldsymbol{x}} = \sum_{i=i}^n \frac{\partial z}{\partial y_i} \frac{\partial y_i}{\partial x} \end{equation</em>}</li> </ul> </li> </ol> </blockquote> <p>If done correctly, the big <span><span class=MathJax_Preview>O</span><script type=math/tex>O</script></span> complexity of forward propagation and backpropagation is the same.</p> <h2 id=automatic-differentiation>Automatic differentiation<a class=headerlink href=#automatic-differentiation title="Permanent link">&para;</a></h2> <p>The gradient computation can be automatically inferred from the symbolic expression of the fprop. but this is not commonly used in practice. Each node type needs to know how to compute its output and how to compute the gradient wrt its inputs given the gradient wrt its output. Modern DL frameworks (Tensorflow, PyTorch, etc.) do backpropagation for you but mainly leave layer/node writer to hand-calculate the local derivative. Following is a simple demo of how to implement forward propagation and backpropagation.</p> <div class=codehilite><pre><span></span><code><span class=k>class</span> <span class=nc>MultiplyGate</span><span class=p>(</span><span class=nb>object</span><span class=p>):</span>
    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>):</span>
        <span class=n>z</span> <span class=o>=</span> <span class=n>x</span> <span class=o>*</span> <span class=n>y</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>x</span> <span class=o>=</span> <span class=n>x</span> <span class=c1># must keep these around!</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>y</span> <span class=o>=</span> <span class=n>y</span>
        <span class=k>return</span> <span class=n>z</span>

    <span class=k>def</span> <span class=nf>backword</span><span class=p>(</span><span class=n>dz</span><span class=p>):</span>
        <span class=n>dx</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>y</span> <span class=o>*</span> <span class=n>dz</span> <span class=c1># [dz/dx * dL/dz]</span>
        <span class=n>dy</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>x</span> <span class=o>*</span> <span class=n>dz</span> <span class=c1># [dz/dy * dL/dz]</span>
        <span class=k>return</span> <span class=p>[</span><span class=n>dx</span><span class=p>,</span> <span class=n>dy</span><span class=p>]</span>
</code></pre></div> <ol> <li>nonlinear activation functions: sigmoid, tanh, hard tanh, ReLU</li> <li>Learning rates: start 0.001. power of 10. halve the learning rate every k epochs. formula: <span><span class=MathJax_Preview>lr = lr_0 e^{-k t}</span><script type=math/tex>lr = lr_0 e^{-k t}</script></span> for each epoch <span><span class=MathJax_Preview>t</span><script type=math/tex>t</script></span></li> </ol> <h2 id=regularization>Regularization<a class=headerlink href=#regularization title="Permanent link">&para;</a></h2> <p>LFD</p> <ol> <li>starting point: L2 regularization</li> <li>f</li> </ol> </article> </div> </div> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-footer-copyright> <div class=md-footer-copyright__highlight> Copyright &copy; 2020 Rui Han </div> powered by <a href=https://www.mkdocs.org target=_blank rel=noopener>MkDocs</a> and <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs</a> </div> </div> </div> </footer> </div> <script src=../../../assets/javascripts/application.c33a9706.js></script> <script>app.initialize({version:"1.1",url:{base:"../../.."}})</script> <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script> </body> </html>