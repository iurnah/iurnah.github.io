<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><link href=https://ruihan.org/courses/cs224n/write-up/ rel=canonical><link rel="shortcut icon" href=../../../assets/images/favicon.png><meta name=generator content="mkdocs-1.1.2, mkdocs-material-6.0.2"><title>CS224N Write-up - RUIHAN.ORG</title><link rel=stylesheet href=../../../assets/stylesheets/main.38780c08.min.css><link rel=stylesheet href=../../../assets/stylesheets/palette.3f72e892.min.css><meta name=theme-color content=#000000><link href=https://fonts.gstatic.com rel=preconnect crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback"><style>body,input{font-family:"Roboto",-apple-system,BlinkMacSystemFont,Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono",SFMono-Regular,Consolas,Menlo,monospace}</style></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=black data-md-color-accent=black> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#cs224n-nlp-with-deep-learning class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class=md-header data-md-component=header> <nav class="md-header-nav md-grid" aria-label=Header> <a href=https://ruihan.org title=RUIHAN.ORG class="md-header-nav__button md-logo" aria-label=RUIHAN.ORG> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 003-3 3 3 0 00-3-3 3 3 0 00-3 3 3 3 0 003 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg> </a> <label class="md-header-nav__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg> </label> <div class=md-header-nav__title data-md-component=header-title> <div class=md-header-nav__ellipsis> <span class="md-header-nav__topic md-ellipsis"> RUIHAN.ORG </span> <span class="md-header-nav__topic md-ellipsis"> CS224N Write-up </span> </div> </div> <label class="md-header-nav__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query data-md-state=active> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </label> <button type=reset class="md-search__icon md-icon" aria-label=Clear data-md-component=search-reset tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg> </button> </form> <div class=md-search__output> <div class=md-search__scrollwrap data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list></ol> </div> </div> </div> </div> </div> <div class=md-header-nav__source> <a href=https://github.com/iurnah/ruihan.org title="Go to repository" class=md-source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 00-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 01-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 000 40.81l195.61 195.6a28.86 28.86 0 0040.8 0l194.69-194.69a28.86 28.86 0 000-40.81z"/></svg> </div> <div class=md-source__repository> iurnah/ruihan.org </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class="md-tabs md-tabs--active" aria-label=Tabs data-md-component=tabs> <div class="md-tabs__inner md-grid"> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../../../books/ class=md-tabs__link> Books </a> </li> <li class=md-tabs__item> <a href=../../ class="md-tabs__link md-tabs__link--active"> Courses </a> </li> <li class=md-tabs__item> <a href=../../../leetcode/ class=md-tabs__link> Leetcode </a> </li> <li class=md-tabs__item> <a href=../../../research/ class=md-tabs__link> Research </a> </li> <li class=md-tabs__item> <a href=../../../seedlabs/ class=md-tabs__link> SEED Labs </a> </li> <li class=md-tabs__item> <a href=../../../system-design/ class=md-tabs__link> System Design </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=https://ruihan.org title=RUIHAN.ORG class="md-nav__button md-logo" aria-label=RUIHAN.ORG> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 003-3 3 3 0 00-3-3 3 3 0 00-3 3 3 3 0 003 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg> </a> RUIHAN.ORG </label> <div class=md-nav__source> <a href=https://github.com/iurnah/ruihan.org title="Go to repository" class=md-source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 00-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 01-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 000 40.81l195.61 195.6a28.86 28.86 0 0040.8 0l194.69-194.69a28.86 28.86 0 000-40.81z"/></svg> </div> <div class=md-source__repository> iurnah/ruihan.org </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1 type=checkbox id=nav-1> <label class=md-nav__link for=nav-1> Books <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Books data-md-level=1> <label class=md-nav__title for=nav-1> <span class="md-nav__icon md-icon"></span> Books </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../books/ class=md-nav__link> Index </a> </li> <li class=md-nav__item> <a href=../../../books/accelerated-cpp/notes/ class=md-nav__link> Accelerated C++ </a> </li> <li class=md-nav__item> <a href=../../../books/mining-massive-datasets/notes/ class=md-nav__link> Mining Massive Datasets </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-2 type=checkbox id=nav-2 checked> <label class=md-nav__link for=nav-2> Courses <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Courses data-md-level=1> <label class=md-nav__title for=nav-2> <span class="md-nav__icon md-icon"></span> Courses </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../ class=md-nav__link> Index </a> </li> <li class=md-nav__item> <a href=../../machine-learning-coursera/notes/ class=md-nav__link> Machine Learning (Coursera) </a> </li> <li class=md-nav__item> <a href=../lec-notes/ class=md-nav__link> CS224N Lecture Notes </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" data-md-toggle=toc type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> CS224N Write-up <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> CS224N Write-up </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=#part-1-word-embeddings-based-on-distributional-semantic class=md-nav__link> Part 1 Word Embeddings Based on Distributional Semantic </a> <nav class=md-nav aria-label="Part 1 Word Embeddings Based on Distributional Semantic"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#introduction class=md-nav__link> Introduction </a> </li> <li class=md-nav__item> <a href=#word-representations class=md-nav__link> Word representations </a> </li> <li class=md-nav__item> <a href=#word2vec class=md-nav__link> word2vec </a> </li> <li class=md-nav__item> <a href=#gradient-descent-to-optimize-log-likelihood-loss-function class=md-nav__link> Gradient descent to optimize log likelihood loss function </a> </li> <li class=md-nav__item> <a href=#gradient-descent-to-optimize-cross-entropy-loss-function class=md-nav__link> Gradient descent to optimize cross entropy loss function </a> </li> <li class=md-nav__item> <a href=#skip-gram-model class=md-nav__link> Skip-gram model </a> <nav class=md-nav aria-label="Skip-gram model"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#cross-entropy-loss-function class=md-nav__link> Cross entropy loss function </a> </li> <li class=md-nav__item> <a href=#gradient-for-center-word class=md-nav__link> Gradient for center word </a> </li> <li class=md-nav__item> <a href=#gradient-for-output-word class=md-nav__link> Gradient for output word </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#negative-sampling class=md-nav__link> Negative sampling </a> <nav class=md-nav aria-label="Negative sampling"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#gradient-for-all-of-word-vectors class=md-nav__link> Gradient for all of word vectors </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#cbow-model class=md-nav__link> CBOW model </a> </li> <li class=md-nav__item> <a href=#glove-model class=md-nav__link> GloVe model </a> </li> <li class=md-nav__item> <a href=#word-vector-evaluation class=md-nav__link> Word vector evaluation </a> </li> <li class=md-nav__item> <a href=#summary class=md-nav__link> Summary </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#part-2-neural-networks-and-backpropagation class=md-nav__link> Part 2 Neural Networks and Backpropagation </a> <nav class=md-nav aria-label="Part 2 Neural Networks and Backpropagation"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#intro-to-neural-networks class=md-nav__link> Intro to Neural Networks </a> </li> <li class=md-nav__item> <a href=#forward-propagation-in-matrix-notation class=md-nav__link> Forward Propagation in Matrix Notation </a> </li> <li class=md-nav__item> <a href=#word-window-classification-using-neural-networks class=md-nav__link> Word Window Classification Using Neural Networks </a> <nav class=md-nav aria-label="Word Window Classification Using Neural Networks"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#forward-propagation class=md-nav__link> Forward propagation </a> </li> <li class=md-nav__item> <a href=#gradients-and-jacobians-matrix class=md-nav__link> Gradients and Jacobians Matrix </a> </li> <li class=md-nav__item> <a href=#computing-gradients-with-the-chain-rule class=md-nav__link> Computing gradients with the chain rule </a> </li> <li class=md-nav__item> <a href=#shape-convention class=md-nav__link> Shape convention </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#computation-graphs-and-backpropagation class=md-nav__link> Computation graphs and backpropagation </a> </li> <li class=md-nav__item> <a href=#automatic-differentiation class=md-nav__link> Automatic differentiation </a> </li> <li class=md-nav__item> <a href=#regularization class=md-nav__link> Regularization </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#part-3-language-model-and-rnn class=md-nav__link> Part 3 Language Model and RNN </a> <nav class=md-nav aria-label="Part 3 Language Model and RNN"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#n-gram class=md-nav__link> n-gram </a> </li> <li class=md-nav__item> <a href=#neural-language-model class=md-nav__link> Neural language model </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../coursera-dl4-cnn/notes/ class=md-nav__link> Convolutional Neural Networks </a> </li> <li class=md-nav__item> <a href=../../mining-massive-datasets/notes/ class=md-nav__link> Mining Massive Data Sets </a> </li> <li class=md-nav__item> <a href=../../6.431-probability/notes/ class=md-nav__link> 6.431 Probability </a> </li> <li class=md-nav__item> <a href=../../learning-from-data/notes.md class=md-nav__link> Learning From Data </a> </li> <li class=md-nav__item> <a href=../../9chap-system-design/notes/ class=md-nav__link> Nine Chapter System Design </a> </li> <li class=md-nav__item> <a href=../../9chap-dynamic-prog/notes/ class=md-nav__link> Nine Chapter Dynamic Programming </a> </li> <li class=md-nav__item> <a href=../../func-prog-in-scala/notes/ class=md-nav__link> Functional Programming Principles in Scala </a> </li> <li class=md-nav__item> <a href=../../applied-scrum-for-agile/notes/ class=md-nav__link> Applied Scrum for Agile Project Management </a> </li> <li class=md-nav__item> <a href=../../concurrent-prog-java/notes/ class=md-nav__link> Concurrent Programming in Java </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-3 type=checkbox id=nav-3> <label class=md-nav__link for=nav-3> Leetcode <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Leetcode data-md-level=1> <label class=md-nav__title for=nav-3> <span class="md-nav__icon md-icon"></span> Leetcode </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../leetcode/ class=md-nav__link> Index </a> </li> <li class=md-nav__item> <a href=../../../leetcode/array/notes/ class=md-nav__link> Array </a> </li> <li class=md-nav__item> <a href=../../../leetcode/backtracking/notes/ class=md-nav__link> Backtracking </a> </li> <li class=md-nav__item> <a href=../../../leetcode/binary-search/notes/ class=md-nav__link> Binary Search </a> </li> <li class=md-nav__item> <a href=../../../leetcode/breadth-first-search/notes/ class=md-nav__link> Breadth-First Search (BFS) </a> </li> <li class=md-nav__item> <a href=../../../leetcode/depth-first-search/notes/ class=md-nav__link> Depth-First Search (DFS) </a> </li> <li class=md-nav__item> <a href=../../../leetcode/dynamic-programming/notes/ class=md-nav__link> Dynamic Programming </a> </li> <li class=md-nav__item> <a href=../../../leetcode/graph/notes/ class=md-nav__link> Graph </a> </li> <li class=md-nav__item> <a href=../../../leetcode/heap/notes/ class=md-nav__link> Heap </a> </li> <li class=md-nav__item> <a href=../../../leetcode/interval/notes/ class=md-nav__link> Interval </a> </li> <li class=md-nav__item> <a href=../../../leetcode/linked-list/notes/ class=md-nav__link> Linked List </a> </li> <li class=md-nav__item> <a href=../../../leetcode/math/notes/ class=md-nav__link> Math </a> </li> <li class=md-nav__item> <a href=../../../leetcode/stack/notes/ class=md-nav__link> Stack </a> </li> <li class=md-nav__item> <a href=../../../leetcode/string/notes/ class=md-nav__link> String </a> </li> <li class=md-nav__item> <a href=../../../leetcode/tree/notes/ class=md-nav__link> Tree </a> </li> <li class=md-nav__item> <a href=../../../leetcode/trie/notes/ class=md-nav__link> Trie </a> </li> <li class=md-nav__item> <a href=../../../leetcode/union-find/notes/ class=md-nav__link> Union Find </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-4 type=checkbox id=nav-4> <label class=md-nav__link for=nav-4> Research <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Research data-md-level=1> <label class=md-nav__title for=nav-4> <span class="md-nav__icon md-icon"></span> Research </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../research/ class=md-nav__link> Index </a> </li> <li class=md-nav__item> <a href=../../../research/paper-reading/notes/ class=md-nav__link> Paper Reading </a> </li> <li class=md-nav__item> <a href=../../../research/coalition-game/notes/ class=md-nav__link> Coalition Game </a> </li> <li class=md-nav__item> <a href=../../../research/contextual-bandit/notes/ class=md-nav__link> Contextual Multi-Armed Bandit </a> </li> <li class=md-nav__item> <a href=../../../research/tfidf-score/notes/ class=md-nav__link> TF-IDF for Information Retrieval </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-5 type=checkbox id=nav-5> <label class=md-nav__link for=nav-5> SEED Labs <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="SEED Labs" data-md-level=1> <label class=md-nav__title for=nav-5> <span class="md-nav__icon md-icon"></span> SEED Labs </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../seedlabs/ class=md-nav__link> Index </a> </li> <li class=md-nav__item> <a href=../../../seedlabs/applied-crypto/notes/ class=md-nav__link> Applied Cryptograph Notes </a> </li> <li class=md-nav__item> <a href=../../../seedlabs/public-key-cryptography-and-pki/notes/ class=md-nav__link> Public Key Cryptography and PKI </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-6 type=checkbox id=nav-6> <label class=md-nav__link for=nav-6> System Design <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="System Design" data-md-level=1> <label class=md-nav__title for=nav-6> <span class="md-nav__icon md-icon"></span> System Design </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../system-design/ class=md-nav__link> Index </a> </li> <li class=md-nav__item> <a href=../../../system-design/browser/notes.md class=md-nav__link> How Browser Works </a> </li> <li class=md-nav__item> <a href=../../../system-design/concurrency/notes/ class=md-nav__link> Concurrency and Synchronization </a> </li> <li class=md-nav__item> <a href=../../../system-design/concepts/notes/ class=md-nav__link> Distributed System Concepts </a> </li> <li class=md-nav__item> <a href=../../../system-design/patterns/notes/ class=md-nav__link> Design Patterns </a> </li> <li class=md-nav__item> <a href=../../../system-design/problems/tinyurl/notes/ class=md-nav__link> How to Design TinyUrl </a> </li> <li class=md-nav__item> <a href=../../../system-design/problems/twitter/notes/ class=md-nav__link> How to Design Twitter </a> </li> <li class=md-nav__item> <a href=../../../system-design/problems/ticketmaster/notes/ class=md-nav__link> How to Design Ticketmaster </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=#part-1-word-embeddings-based-on-distributional-semantic class=md-nav__link> Part 1 Word Embeddings Based on Distributional Semantic </a> <nav class=md-nav aria-label="Part 1 Word Embeddings Based on Distributional Semantic"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#introduction class=md-nav__link> Introduction </a> </li> <li class=md-nav__item> <a href=#word-representations class=md-nav__link> Word representations </a> </li> <li class=md-nav__item> <a href=#word2vec class=md-nav__link> word2vec </a> </li> <li class=md-nav__item> <a href=#gradient-descent-to-optimize-log-likelihood-loss-function class=md-nav__link> Gradient descent to optimize log likelihood loss function </a> </li> <li class=md-nav__item> <a href=#gradient-descent-to-optimize-cross-entropy-loss-function class=md-nav__link> Gradient descent to optimize cross entropy loss function </a> </li> <li class=md-nav__item> <a href=#skip-gram-model class=md-nav__link> Skip-gram model </a> <nav class=md-nav aria-label="Skip-gram model"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#cross-entropy-loss-function class=md-nav__link> Cross entropy loss function </a> </li> <li class=md-nav__item> <a href=#gradient-for-center-word class=md-nav__link> Gradient for center word </a> </li> <li class=md-nav__item> <a href=#gradient-for-output-word class=md-nav__link> Gradient for output word </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#negative-sampling class=md-nav__link> Negative sampling </a> <nav class=md-nav aria-label="Negative sampling"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#gradient-for-all-of-word-vectors class=md-nav__link> Gradient for all of word vectors </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#cbow-model class=md-nav__link> CBOW model </a> </li> <li class=md-nav__item> <a href=#glove-model class=md-nav__link> GloVe model </a> </li> <li class=md-nav__item> <a href=#word-vector-evaluation class=md-nav__link> Word vector evaluation </a> </li> <li class=md-nav__item> <a href=#summary class=md-nav__link> Summary </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#part-2-neural-networks-and-backpropagation class=md-nav__link> Part 2 Neural Networks and Backpropagation </a> <nav class=md-nav aria-label="Part 2 Neural Networks and Backpropagation"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#intro-to-neural-networks class=md-nav__link> Intro to Neural Networks </a> </li> <li class=md-nav__item> <a href=#forward-propagation-in-matrix-notation class=md-nav__link> Forward Propagation in Matrix Notation </a> </li> <li class=md-nav__item> <a href=#word-window-classification-using-neural-networks class=md-nav__link> Word Window Classification Using Neural Networks </a> <nav class=md-nav aria-label="Word Window Classification Using Neural Networks"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#forward-propagation class=md-nav__link> Forward propagation </a> </li> <li class=md-nav__item> <a href=#gradients-and-jacobians-matrix class=md-nav__link> Gradients and Jacobians Matrix </a> </li> <li class=md-nav__item> <a href=#computing-gradients-with-the-chain-rule class=md-nav__link> Computing gradients with the chain rule </a> </li> <li class=md-nav__item> <a href=#shape-convention class=md-nav__link> Shape convention </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#computation-graphs-and-backpropagation class=md-nav__link> Computation graphs and backpropagation </a> </li> <li class=md-nav__item> <a href=#automatic-differentiation class=md-nav__link> Automatic differentiation </a> </li> <li class=md-nav__item> <a href=#regularization class=md-nav__link> Regularization </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#part-3-language-model-and-rnn class=md-nav__link> Part 3 Language Model and RNN </a> <nav class=md-nav aria-label="Part 3 Language Model and RNN"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#n-gram class=md-nav__link> n-gram </a> </li> <li class=md-nav__item> <a href=#neural-language-model class=md-nav__link> Neural language model </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class=md-content> <article class="md-content__inner md-typeset"> <h1 id=cs224n-nlp-with-deep-learning>CS224N NLP with Deep Learning<a class=headerlink href=#cs224n-nlp-with-deep-learning title="Permanent link">&para;</a></h1> <h2 id=part-1-word-embeddings-based-on-distributional-semantic>Part 1 Word Embeddings Based on Distributional Semantic<a class=headerlink href=#part-1-word-embeddings-based-on-distributional-semantic title="Permanent link">&para;</a></h2> <p>Date: April 20th 2019</p> <p>If you have no experience in the Natrual Language Processing (NLP) field and asked to name a few practical NLP applications, what would you list? Google Translation or Amazon Echo voice assistant may first come into your mind. How did those products understand a sentence or human conversation? Engineers built computational models that understand our language as our brain does. To build such a computational model, the following components are necessary.</p> <ol> <li>A large corpus of text (input training data)</li> <li>A method to represent each word from the corpus (feature representation)</li> <li>A starting model that barely understands English at the beginning but can be improved by "reading" more words from the corpus (parametric function).</li> <li>An algorithm for the model to correct itself if it makes a mistake in understanding (learning algorithm/optimization method)</li> <li>A measurement that can qualify the mistake the model made (loss function)</li> </ol> <h3 id=introduction>Introduction<a class=headerlink href=#introduction title="Permanent link">&para;</a></h3> <p>Starting with this post, I will write materials from my understanding of the Stanford CS224N. The plan is to journal down all the learning notes I learned about the above 5 components. The goal is to provide a systematic understand of the gist of the components for real applications.</p> <ul> <li>Part 1 is about language models and word embeddings.</li> <li>Part 2 discusses neural networks and backpropagation algorithm.</li> <li>Part 3 revisits the language model and introduces recurrent neural networks.</li> <li>Part 4 studies advanced RNN, Long short-term memory (LSTM), and gated recurrent networks (GRN).</li> </ul> <h3 id=word-representations>Word representations<a class=headerlink href=#word-representations title="Permanent link">&para;</a></h3> <p>How do we represent word with meaning on a computer? Before 2013, wordNet and one-hot vector are most popular in word meaning representations. WordNet is a manually compiled thesaurus containing lists of synonym sets and hypernyms. Like most of the manual stuff, it is subjective, unscalable, inaccurate in computing word similarity, and impossible to maintain and keep up-to-data. One-hot vectors represent word meaning using discrete symbolic <span><span class=MathJax_Preview>1</span><script type=math/tex>1</script></span>s in a long stream of <span><span class=MathJax_Preview>0</span><script type=math/tex>0</script></span> of vector elements. It suffers from sparsity issues and many other drawbacks. We will not spend time on that outdated method. Instead, we will focus on the embedding method using the idea of word vector.</p> <p>The core idea of this embedding method is based on the remarkable insight on word meaning called <strong>distributional semantics</strong>. It conjectures that a word’s meaning is given by the words that frequently appear close-by. It is proposed by J. R. Firth. Here is the famous quote:</p> <div class="admonition quote"> <p class=admonition-title>Quote</p> <p>"You shall know a word by the company it keeps" -- J. R. Firth 1957: 11</p> </div> <p>In this method, we will build a dense vector for each word, chosen so that it is similar to vectors of words that appear in similar contexts. word vector representation also called <strong>distributed representation</strong>, <strong>or word embeddings</strong>. A work vector might look likes this,</p> <div> <div class=MathJax_Preview> \mathrm{banking} = \begin{pmatrix} 0.286\\ 0.792\\ −0.177\\ −0.107\\ 0.109\\ −0.542\\ 0.349\\ 0.271 \end{pmatrix} </div> <script type="math/tex; mode=display">
\mathrm{banking} =
\begin{pmatrix}
0.286\\
0.792\\
−0.177\\
−0.107\\
0.109\\
−0.542\\
0.349\\
0.271
\end{pmatrix}
</script> </div> <h3 id=word2vec>word2vec<a class=headerlink href=#word2vec title="Permanent link">&para;</a></h3> <p>Word2vec (Mikolov et al. 2013) is a framework for learning word vectors. Word2vec go through windows of words and calculate the probability of the context word given the center word (or vice versa) using the similarity of the word vectors. It keeps adjusting the word vectors to maximize this probability. Vividly, these two pictures show the general idea of how word2vec works.</p> <p><img alt=word2vec1.png src=../fig/word2vec1.png></p> <p><img alt=word2vec2.png src=../fig/word2vec2.png></p> <p>Formally, for a single prediction, the probability is <span><span class=MathJax_Preview>P(o|c)</span><script type=math/tex>P(o|c)</script></span>, interpreted as the probability of the outer word <span><span class=MathJax_Preview>o</span><script type=math/tex>o</script></span> given the center word <span><span class=MathJax_Preview>c</span><script type=math/tex>c</script></span>. For the large corpus including <span><span class=MathJax_Preview>T</span><script type=math/tex>T</script></span> words and each position <span><span class=MathJax_Preview>t = 1, 2, \cdots, T</span><script type=math/tex>t = 1, 2, \cdots, T</script></span>, we predict context words within a window of fixed size <span><span class=MathJax_Preview>m</span><script type=math/tex>m</script></span>, given center word <span><span class=MathJax_Preview>w_t</span><script type=math/tex>w_t</script></span>. The model likelihood can be written as the following</p> <div> <div class=MathJax_Preview>\begin{equation*} \mathrm{likelihood} = L(\theta) = \prod_{t=1}^T\prod_{\substack{-m \le j \le m\\ j \ne 0}} p(w_{t+j}|w_t; \theta) \end{equation*}</div> <script type="math/tex; mode=display">\begin{equation*}
\mathrm{likelihood} = L(\theta) = \prod_{t=1}^T\prod_{\substack{-m \le j \le m\\
j \ne 0}} p(w_{t+j}|w_t; \theta)
\end{equation*}</script> </div> <p>The objective function <span><span class=MathJax_Preview>J(\theta)</span><script type=math/tex>J(\theta)</script></span> is the average negative log likelihood:</p> <div> <div class=MathJax_Preview>\begin{equation*} J(\theta) = -\frac{1}{T}\log{L(\theta)} = -\frac{1}{T} \sum_{t=1}^T\sum_{\substack{-m \le j \le m\\ j \ne 0}} \log p(w_{t+j}|w_t; \theta) \end{equation*}</div> <script type="math/tex; mode=display">\begin{equation*}
J(\theta) = -\frac{1}{T}\log{L(\theta)} = -\frac{1}{T} \sum_{t=1}^T\sum_{\substack{-m \le j \le m\\
j \ne 0}} \log p(w_{t+j}|w_t; \theta)
\end{equation*}</script> </div> <p>We've defined the cost function by now. In order to minimize the loss, we need to know how <span><span class=MathJax_Preview>p(w_{t+j}|w_t; \theta)</span><script type=math/tex>p(w_{t+j}|w_t; \theta)</script></span> can be calculated. One function we can use to calculate the probability value is the <span><span class=MathJax_Preview>\mathrm{softmax}</span><script type=math/tex>\mathrm{softmax}</script></span> function.</p> <div> <div class=MathJax_Preview>\begin{equation*} \mathrm{softmax}(x)_i = \frac{\exp(x_i)}{\sum_{j=1}^{n}\exp(x_j)} \end{equation*}</div> <script type="math/tex; mode=display">\begin{equation*}
\mathrm{softmax}(x)_i = \frac{\exp(x_i)}{\sum_{j=1}^{n}\exp(x_j)}
\end{equation*}</script> </div> <p>Particularly we will write the probability as the following format.</p> <div> <div class=MathJax_Preview>\begin{equation*} p(o|c) = \frac{\exp(u_o^T v_c)}{\sum_{w=1}^{V}\exp(u_w^T v_c)} \end{equation*}</div> <script type="math/tex; mode=display">\begin{equation*}
p(o|c) = \frac{\exp(u_o^T v_c)}{\sum_{w=1}^{V}\exp(u_w^T v_c)}
\end{equation*}</script> </div> <p>There are several points need to be emphasized.</p> <ul> <li>Two vectors will be obtained for each individual word, one as center word <span><span class=MathJax_Preview>v_w</span><script type=math/tex>v_w</script></span>, and the other context word <span><span class=MathJax_Preview>u_w</span><script type=math/tex>u_w</script></span>. <span><span class=MathJax_Preview>d</span><script type=math/tex>d</script></span> is the dimension of the word vector. <span><span class=MathJax_Preview>V</span><script type=math/tex>V</script></span> is the vocabulary size.</li> </ul> <div> <div class=MathJax_Preview>\begin{equation*} \theta = \begin{bmatrix} v_{\mathrm{aardvark}}\\ v_{\mathrm{a}}\\ \cdots \\ v_{\mathrm{zebra}}\\ u_{\mathrm{aardvark}}\\ u_{\mathrm{a}}\\ \cdots \\ u_{\mathrm{zebra}}\\ \end{bmatrix} \in \mathbb{R}^{2dV} \end{equation*}</div> <script type="math/tex; mode=display">\begin{equation*}
\theta =
\begin{bmatrix}
v_{\mathrm{aardvark}}\\
v_{\mathrm{a}}\\
\cdots \\
v_{\mathrm{zebra}}\\
u_{\mathrm{aardvark}}\\
u_{\mathrm{a}}\\
\cdots \\
u_{\mathrm{zebra}}\\
\end{bmatrix} \in \mathbb{R}^{2dV}
\end{equation*}</script> </div> <ul> <li>The dot product in the exponet compares similarity of <span><span class=MathJax_Preview>o</span><script type=math/tex>o</script></span> and <span><span class=MathJax_Preview>c</span><script type=math/tex>c</script></span>. Larger dot product indicates larger probability.</li> <li>The denorminator sum over entire vocabulary to give normalized probability distribution.</li> <li>The <span><span class=MathJax_Preview>\mathrm{softmax}</span><script type=math/tex>\mathrm{softmax}</script></span> function maps aribitrary values <span><span class=MathJax_Preview>x_i</span><script type=math/tex>x_i</script></span> to a probability distribution <span><span class=MathJax_Preview>p_i</span><script type=math/tex>p_i</script></span>. “<span><span class=MathJax_Preview>\mathrm{max}</span><script type=math/tex>\mathrm{max}</script></span>” because it amplifies the probability of the largest <span><span class=MathJax_Preview>x_i</span><script type=math/tex>x_i</script></span>, “<span><span class=MathJax_Preview>\mathrm{soft}</span><script type=math/tex>\mathrm{soft}</script></span>” because it still assigns some probabilities to smaller <span><span class=MathJax_Preview>x_i</span><script type=math/tex>x_i</script></span>. It is very commonly used in deep learning.</li> </ul> <h3 id=gradient-descent-to-optimize-log-likelihood-loss-function>Gradient descent to optimize log likelihood loss function<a class=headerlink href=#gradient-descent-to-optimize-log-likelihood-loss-function title="Permanent link">&para;</a></h3> <p>This section I will purely focus on how to derive the gradient of the log likelihood loss function with respect to center word using the chain rule. Once we have the computed gradients, we are ready to implement it in matrix form and train the word vectors. This model is called skip-gram.</p> <ul> <li>loss function in <span><span class=MathJax_Preview>p(w_{t+j}|w_t)</span><script type=math/tex>p(w_{t+j}|w_t)</script></span></li> </ul> <div> <div class=MathJax_Preview>\begin{equation*} J(\theta) = -\frac{1}{T} \sum_{t=1}^T\sum_{\substack{-m \le j \le m\\ j \ne 0}} \log p(w_{t+j}|w_t; \theta) \end{equation*}</div> <script type="math/tex; mode=display">\begin{equation*}
J(\theta) = -\frac{1}{T} \sum_{t=1}^T\sum_{\substack{-m \le j \le m\\
j \ne 0}} \log p(w_{t+j}|w_t; \theta)
\end{equation*}</script> </div> <ul> <li><span><span class=MathJax_Preview>p(w_{t+j}|w_t)</span><script type=math/tex>p(w_{t+j}|w_t)</script></span> in softmax form</li> </ul> <div> <div class=MathJax_Preview>\begin{equation*} p(o|c) = \frac{\exp(u_o^T v_c)}{\sum_{w=1}^{V}\exp(u_w^T v_c)} \end{equation*}</div> <script type="math/tex; mode=display">\begin{equation*}
p(o|c) = \frac{\exp(u_o^T v_c)}{\sum_{w=1}^{V}\exp(u_w^T v_c)}
\end{equation*}</script> </div> <p>We would like to find the following derivatives:</p> <div> <div class=MathJax_Preview>\begin{equation*} 1. \frac{\partial}{\partial v_c} \log p(o|c) \\ 2. \frac{\partial}{\partial u_o} \log p(o|c) \end{equation*}</div> <script type="math/tex; mode=display">\begin{equation*}
1. \frac{\partial}{\partial v_c} \log p(o|c) \\
2. \frac{\partial}{\partial u_o} \log p(o|c)
\end{equation*}</script> </div> <p>Let's now start working with the first one, the derivative wrt. <span><span class=MathJax_Preview>v_c</span><script type=math/tex>v_c</script></span>.</p> <div> <div class=MathJax_Preview>\begin{align*} \frac{\partial}{\partial v_c} \log p(o|c) &amp; = \frac{\partial}{\partial v_c} \log{\frac{\exp(u_o^T v_c)}{\sum_{w=1}^{V}\exp(u_w^T v_c)}} \\ &amp;= \frac{\partial}{\partial v_c} \log{\exp(u_o^T v_c)} - \frac{\partial}{\partial v_c} \log{\sum_{w=1}^{V}\exp(u_w^T v_c)} \;\;\;\cdots (\log\frac{a}{b} = \log a - \log b) \\ &amp; = \frac{\partial}{\partial v_c} (u_o^T v_c) - \frac{1}{\sum_{w=1}^{V}\exp(u_w^T v_c)} \sum_{x=1}^{V} \frac{\partial}{\partial v_c} \exp(u_x^T v_c) \;\;\;\cdots \mathrm{chain\ rule}\\ &amp; = u_o - \frac{1}{\sum_{w=1}^{V}\exp(u_w^T v_c)} \sum_{x=1}^{V} \exp(u_x^{T} v_c) \frac{\partial}{\partial v_c} (u_x^T v_c) \;\;\;\cdots \mathrm{chain\ rule}\\ &amp; = u_o - \frac{1}{\sum_{w=1}^{V}\exp(u_w^T v_c)} \sum_{x=1}^{V} \exp(u_x^{T} v_c) \cdot u_x \\ &amp; = u_o - \sum_{x=1}^{V} \frac{\exp(u_x^{T} v_c)}{\sum_{w=1}^{V}\exp(u_w^T v_c)} \cdot u_x \\ &amp; = \underbrace{u_o}_{\mathrm{current}} - \underbrace{\sum_{x=1}^{V} p(x|c) \cdot u_x}_{\mathrm{expectation}} \end{align*}</div> <script type="math/tex; mode=display">\begin{align*}
\frac{\partial}{\partial v_c} \log p(o|c)
& = \frac{\partial}{\partial v_c} \log{\frac{\exp(u_o^T v_c)}{\sum_{w=1}^{V}\exp(u_w^T v_c)}} \\
&= \frac{\partial}{\partial v_c} \log{\exp(u_o^T v_c)} - \frac{\partial}{\partial v_c} \log{\sum_{w=1}^{V}\exp(u_w^T v_c)} \;\;\;\cdots (\log\frac{a}{b} = \log a - \log b) \\
& = \frac{\partial}{\partial v_c} (u_o^T v_c) - \frac{1}{\sum_{w=1}^{V}\exp(u_w^T v_c)} \sum_{x=1}^{V} \frac{\partial}{\partial v_c} \exp(u_x^T v_c) \;\;\;\cdots \mathrm{chain\ rule}\\
& = u_o - \frac{1}{\sum_{w=1}^{V}\exp(u_w^T v_c)} \sum_{x=1}^{V} \exp(u_x^{T} v_c) \frac{\partial}{\partial v_c} (u_x^T v_c) \;\;\;\cdots \mathrm{chain\ rule}\\
& = u_o - \frac{1}{\sum_{w=1}^{V}\exp(u_w^T v_c)} \sum_{x=1}^{V} \exp(u_x^{T} v_c) \cdot u_x \\
& = u_o - \sum_{x=1}^{V} \frac{\exp(u_x^{T} v_c)}{\sum_{w=1}^{V}\exp(u_w^T v_c)} \cdot u_x \\
& = \underbrace{u_o}_{\mathrm{current}} - \underbrace{\sum_{x=1}^{V} p(x|c) \cdot u_x}_{\mathrm{expectation}}
\end{align*}</script> </div> <p>The result is remarkable. It have great intuition in it. The gradient represent the slop in the multidimentional space that we should walk along to reach the optima. The result gradient we got tell us that the slop equals to the difference of current context vector <span><span class=MathJax_Preview>\boldsymbol{u}_o</span><script type=math/tex>\boldsymbol{u}_o</script></span> and the expected context vector (the weighted average over all context vectors). It has nothing todo with the center word <span><span class=MathJax_Preview>c</span><script type=math/tex>c</script></span>.</p> <p>To compute the gradient of <span><span class=MathJax_Preview>J(\theta)</span><script type=math/tex>J(\theta)</script></span> with respect to the center word <span><span class=MathJax_Preview>c</span><script type=math/tex>c</script></span>, you have to sum up all the gradients obtained from word windows when <span><span class=MathJax_Preview>c</span><script type=math/tex>c</script></span> is the center word. The gradient with respect to the context word will be very similar, the chain rule is also handy in that case.</p> <p>Once all the gradient with respect to center words and context words are calculated. We can use gradient descent to update the model parameters, which in this case is all the word vectors. Because we have two vectors for each word, when update the parameters we will use the average of the two vectors to update.</p> <h3 id=gradient-descent-to-optimize-cross-entropy-loss-function>Gradient descent to optimize cross entropy loss function<a class=headerlink href=#gradient-descent-to-optimize-cross-entropy-loss-function title="Permanent link">&para;</a></h3> <p>Alternatively, we could also use cross entropy loss function. CS224N 2017 assignment 1 requires to derive the gradient of cross entropy loss function. This section, we will go step by step to derive the gradient when using cross entropy loss function and <span><span class=MathJax_Preview>\mathrm{softmax}</span><script type=math/tex>\mathrm{softmax}</script></span> activation function in the out put layer. the cross entropy function is defined as follows</p> <div> <div class=MathJax_Preview>\begin{equation*} \mathrm{CE}(\boldsymbol{y}, \boldsymbol{\hat y}) = - \sum_i y_i \log (\hat y_i) \end{equation*}</div> <script type="math/tex; mode=display">\begin{equation*}
\mathrm{CE}(\boldsymbol{y}, \boldsymbol{\hat y}) = - \sum_i y_i \log (\hat y_i)
\end{equation*}</script> </div> <p>Notice the <span><span class=MathJax_Preview>\boldsymbol{y}</span><script type=math/tex>\boldsymbol{y}</script></span> is the one-hot label vector, and <span><span class=MathJax_Preview>\boldsymbol{\hat y}</span><script type=math/tex>\boldsymbol{\hat y}</script></span> is the predicted probability for all classes. The index <span><span class=MathJax_Preview>i</span><script type=math/tex>i</script></span> is the index of the one-hot element in individual label vectors. Therefore the definition of cross entropy loss function is defined as followinng if we use <span><span class=MathJax_Preview>\mathrm{softmax}</span><script type=math/tex>\mathrm{softmax}</script></span> to predict <span><span class=MathJax_Preview>\hat y_i</span><script type=math/tex>\hat y_i</script></span>,</p> <div> <div class=MathJax_Preview>\begin{equation*} J_{\mathrm{CE}}(\theta) = - \sum_i^N y_i \log (\hat y_i) = -\sum_i^N y_i \log (\mathrm{softmax}(\theta)_i) \end{equation*}</div> <script type="math/tex; mode=display">\begin{equation*}
J_{\mathrm{CE}}(\theta) = - \sum_i^N y_i \log (\hat y_i) = -\sum_i^N y_i \log (\mathrm{softmax}(\theta)_i)
\end{equation*}</script> </div> <p>The gradient of <span><span class=MathJax_Preview>\frac{\partial}{\partial \theta}J_{\mathrm{CE}}(\theta)</span><script type=math/tex>\frac{\partial}{\partial \theta}J_{\mathrm{CE}}(\theta)</script></span> can be derived using chain rule. Because we will use the gradient of the <span><span class=MathJax_Preview>\mathrm{softmax}</span><script type=math/tex>\mathrm{softmax}</script></span> for the derivation, Let's derive <span><span class=MathJax_Preview>\mathrm{softmax}</span><script type=math/tex>\mathrm{softmax}</script></span> gradient first.</p> <div> <div class=MathJax_Preview>\begin{equation*} \frac{\partial \hat y_i}{\theta_j} = \frac{\partial }{\theta_j}\mathrm{softmax}(\theta)_i = \frac{\partial }{\theta_j} \frac{e^{\theta_i}}{\sum_{k=1}^{n}e^{\theta_k}} \end{equation*}</div> <script type="math/tex; mode=display">\begin{equation*}
\frac{\partial \hat y_i}{\theta_j} =
\frac{\partial }{\theta_j}\mathrm{softmax}(\theta)_i = \frac{\partial }{\theta_j} \frac{e^{\theta_i}}{\sum_{k=1}^{n}e^{\theta_k}}
\end{equation*}</script> </div> <p>Here we should use two tricks to derive this gradient.</p> <ol> <li>Quotient rule</li> <li>separate into 2 cases: when <span><span class=MathJax_Preview>i = j</span><script type=math/tex>i = j</script></span> and <span><span class=MathJax_Preview>i \ne j</span><script type=math/tex>i \ne j</script></span>.</li> </ol> <p>If <span><span class=MathJax_Preview>i = j</span><script type=math/tex>i = j</script></span>, we have</p> <div> <div class=MathJax_Preview>\begin{align*} \frac{\partial }{\theta_j} \frac{e^{\theta_i}}{\sum_{k=1}^{n}e^{\theta_k}} &amp; = \frac{e^{\theta_i}\sum_{k=1}^{n}e^{\theta_k} - e^{\theta_i}e^{\theta_j} }{\Big(\sum_{k=1}^{n}e^{\theta_k}\Big)^2} \\ &amp; = \frac{e^{\theta_i}\Big(\sum_{k=1}^{n}e^{\theta_k} - e^{\theta_j}\Big)}{\Big(\sum_{k=1}^{n}e^{\theta_k}\Big)^2} \\ &amp; = \frac{e^{\theta_i}}{\sum_{k=1}^{n}e^{\theta_k}} \frac{\sum_{k=1}^{n}e^{\theta_k} - e^{\theta_j}}{\sum_{k=1}^{n}e^{\theta_k}} \\ &amp; = \hat y_i(1 - \hat y_j) \end{align*}</div> <script type="math/tex; mode=display">\begin{align*}
\frac{\partial }{\theta_j} \frac{e^{\theta_i}}{\sum_{k=1}^{n}e^{\theta_k}} & = \frac{e^{\theta_i}\sum_{k=1}^{n}e^{\theta_k} - e^{\theta_i}e^{\theta_j} }{\Big(\sum_{k=1}^{n}e^{\theta_k}\Big)^2} \\
& = \frac{e^{\theta_i}\Big(\sum_{k=1}^{n}e^{\theta_k} - e^{\theta_j}\Big)}{\Big(\sum_{k=1}^{n}e^{\theta_k}\Big)^2} \\
& = \frac{e^{\theta_i}}{\sum_{k=1}^{n}e^{\theta_k}} \frac{\sum_{k=1}^{n}e^{\theta_k} - e^{\theta_j}}{\sum_{k=1}^{n}e^{\theta_k}} \\
& = \hat y_i(1 - \hat y_j)
\end{align*}</script> </div> <p>if <span><span class=MathJax_Preview>i \ne j</span><script type=math/tex>i \ne j</script></span>, we have</p> <div> <div class=MathJax_Preview>\begin{align*} \frac{\partial }{\theta_j} \frac{e^{\theta_i}}{\sum_{k=1}^{n}e^{\theta_k}} &amp; = \frac{0 - e^{\theta_i}e^{\theta_j} }{\Big(\sum_{k=1}^{n}e^{\theta_k}\Big)^2} \\ &amp; = \frac{e^{\theta_i}}{\sum_{k=1}^{n}e^{\theta_k}} \frac{ - e^{\theta_j}}{\sum_{k=1}^{n}e^{\theta_k}} \\ &amp; = -\hat y_i \hat y_j \end{align*}</div> <script type="math/tex; mode=display">\begin{align*}
\frac{\partial }{\theta_j} \frac{e^{\theta_i}}{\sum_{k=1}^{n}e^{\theta_k}} & = \frac{0 - e^{\theta_i}e^{\theta_j} }{\Big(\sum_{k=1}^{n}e^{\theta_k}\Big)^2} \\
& = \frac{e^{\theta_i}}{\sum_{k=1}^{n}e^{\theta_k}} \frac{ - e^{\theta_j}}{\sum_{k=1}^{n}e^{\theta_k}} \\
& = -\hat y_i  \hat y_j
\end{align*}</script> </div> <p>Now let calculate the gradient of the cross-entropy loss function. Notice the gradient is concerning the <span><span class=MathJax_Preview>i</span><script type=math/tex>i</script></span>th parameter. This is because we can use the gradient of the <span><span class=MathJax_Preview>\mathrm{softmax}</span><script type=math/tex>\mathrm{softmax}</script></span> (the <span><span class=MathJax_Preview>\frac{\partial \hat y_k}{\partial \theta_i}</span><script type=math/tex>\frac{\partial  \hat y_k}{\partial \theta_i}</script></span> term) conveniently.</p> <div> <div class=MathJax_Preview>\begin{align*} \frac{\partial J_{\mathrm{CE}}(\theta)}{\partial \theta_i} &amp; = - \sum_k^N y_k \frac{\partial \log (\hat y_k)}{\partial \theta_i} \\ &amp; = - \sum_k^N y_k \frac{\partial \log (\hat y_k)}{\partial \hat y_k} \frac{\partial \hat y_k}{\partial \theta_i} \\ &amp; = - \sum_k^N y_k \frac{1}{\hat y_k} \frac{\partial \hat y_k}{\partial \theta_i} \\ &amp; = - \Big(\underbrace{y_k \frac{1}{\hat y_k} \hat y_k(1 - \hat y_i)}_{k = i} + \underbrace{\sum_{k\ne i}^N y_k \frac{1}{\hat y_k} (-\hat y_k \hat y_i) \Big)}_{k \ne i} \\ &amp; = - \Big(y_i (1 - \hat y_i) - \sum_{k\ne i}^N y_k \hat y_i \Big) \\ &amp; = - y_i + y_i \hat y_i + \sum_{k\ne i}^N y_k \hat y_i \\ &amp; = \hat y_i\Big(y_i + \sum_{k\ne i}^N y_k\Big) - \hat y_i \\ &amp; = \hat y_i \cdot 1 - y_i = \hat y_i - y_i \end{align*}</div> <script type="math/tex; mode=display">\begin{align*}
\frac{\partial J_{\mathrm{CE}}(\theta)}{\partial \theta_i} & = - \sum_k^N y_k \frac{\partial \log (\hat y_k)}{\partial \theta_i} \\
& = - \sum_k^N y_k \frac{\partial \log (\hat y_k)}{\partial \hat y_k} \frac{\partial  \hat y_k}{\partial \theta_i} \\
& = -  \sum_k^N y_k \frac{1}{\hat y_k} \frac{\partial \hat y_k}{\partial \theta_i} \\
& = - \Big(\underbrace{y_k \frac{1}{\hat y_k} \hat y_k(1 - \hat y_i)}_{k = i} + \underbrace{\sum_{k\ne i}^N y_k \frac{1}{\hat y_k} (-\hat y_k  \hat y_i) \Big)}_{k \ne i} \\
& = - \Big(y_i (1 - \hat y_i) - \sum_{k\ne i}^N y_k \hat y_i \Big) \\
& = - y_i + y_i \hat y_i + \sum_{k\ne i}^N y_k \hat y_i \\
& = \hat y_i\Big(y_i + \sum_{k\ne i}^N y_k\Big) - \hat y_i \\
& = \hat y_i \cdot 1 - y_i = \hat y_i - y_i
\end{align*}</script> </div> <p>Write in vector form, we will have</p> <div> <div class=MathJax_Preview> \frac{\partial J_{\mathrm{CE}}(\boldsymbol{y}, \boldsymbol{\hat y})}{\partial \boldsymbol{\theta}} = \boldsymbol{\hat y} - \boldsymbol{y}. </div> <script type="math/tex; mode=display">
\frac{\partial J_{\mathrm{CE}}(\boldsymbol{y}, \boldsymbol{\hat y})}{\partial \boldsymbol{\theta}} = \boldsymbol{\hat y} - \boldsymbol{y}.
</script> </div> <p>With this gradient, we can update the model parameters, namely the word vectors. In the next post, I will discuss neural networks, which have more layers than the word2vec. In neural networks, there are more parameters need to be trained and the gradients with respect all the parameters need to be derived.</p> <h3 id=skip-gram-model>Skip-gram model<a class=headerlink href=#skip-gram-model title="Permanent link">&para;</a></h3> <p>Skip-gram model uses the center word to predict the surrounding.</p> <p>We have derived the gradient for the skip-gram model using log-likelihood loss function and <span><span class=MathJax_Preview>\mathrm{softmax}</span><script type=math/tex>\mathrm{softmax}</script></span>. This section will derive the gradient for cross-entropy loss function. The probability output function will keep using the <span><span class=MathJax_Preview>\mathrm{softmax}</span><script type=math/tex>\mathrm{softmax}</script></span>.</p> <h4 id=cross-entropy-loss-function>Cross entropy loss function<a class=headerlink href=#cross-entropy-loss-function title="Permanent link">&para;</a></h4> <p>Since we have derived above that <span><span class=MathJax_Preview>\frac{\partial J_{\mathrm{CE}}(\boldsymbol{y}, \boldsymbol{\hat y})}{\partial \boldsymbol{\theta}} = \boldsymbol{\hat y} - \boldsymbol{y}</span><script type=math/tex>\frac{\partial J_{\mathrm{CE}}(\boldsymbol{y}, \boldsymbol{\hat y})}{\partial \boldsymbol{\theta}} = \boldsymbol{\hat y} - \boldsymbol{y}</script></span>. In this case, the vector <span><span class=MathJax_Preview>\boldsymbol{\theta} = [\boldsymbol{u_1^{\mathsf{T}}}\boldsymbol{v_c}, \boldsymbol{u_2^{\mathsf{T}}}\boldsymbol{v_c}, \cdots, \boldsymbol{u_W^{\mathsf{T}}}\boldsymbol{v_c}]</span><script type=math/tex>\boldsymbol{\theta} = [\boldsymbol{u_1^{\mathsf{T}}}\boldsymbol{v_c}, \boldsymbol{u_2^{\mathsf{T}}}\boldsymbol{v_c}, \cdots, \boldsymbol{u_W^{\mathsf{T}}}\boldsymbol{v_c}]</script></span>.</p> <h4 id=gradient-for-center-word>Gradient for center word<a class=headerlink href=#gradient-for-center-word title="Permanent link">&para;</a></h4> <p>Borrow the above steps to derive the gradient with respect to <span><span class=MathJax_Preview>\boldsymbol{\theta}</span><script type=math/tex>\boldsymbol{\theta}</script></span>, we have</p> <div> <div class=MathJax_Preview>\begin{align*} \frac{\partial }{\partial \boldsymbol{v_c}}J_{\mathrm{softmax-CE}}(\boldsymbol{o}, \boldsymbol{v_c}, \boldsymbol{U}) &amp; = \frac{\partial J_{\mathrm{CE}}(\theta)}{\partial \theta_i} \frac{\partial \theta_i}{\boldsymbol{v_c}} \\ &amp; = (\hat y_i - y_i) \boldsymbol{u_i^{\mathsf{T}}} \end{align*}</div> <script type="math/tex; mode=display">\begin{align*}
\frac{\partial }{\partial \boldsymbol{v_c}}J_{\mathrm{softmax-CE}}(\boldsymbol{o}, \boldsymbol{v_c}, \boldsymbol{U}) & = \frac{\partial J_{\mathrm{CE}}(\theta)}{\partial \theta_i} \frac{\partial \theta_i}{\boldsymbol{v_c}} \\ & = (\hat y_i - y_i) \boldsymbol{u_i^{\mathsf{T}}}
\end{align*}</script> </div> <p>Notice here the <span><span class=MathJax_Preview>\boldsymbol{u_i}</span><script type=math/tex>\boldsymbol{u_i}</script></span> is a column vector. And the derivative of <span><span class=MathJax_Preview>\boldsymbol{y} = \boldsymbol{a}^{\mathsf{T}}\boldsymbol{x}</span><script type=math/tex>\boldsymbol{y} = \boldsymbol{a}^{\mathsf{T}}\boldsymbol{x}</script></span> with respect to <span><span class=MathJax_Preview>\boldsymbol{x}</span><script type=math/tex>\boldsymbol{x}</script></span> is <span><span class=MathJax_Preview>\boldsymbol{a}^{\mathsf{T}}</span><script type=math/tex>\boldsymbol{a}^{\mathsf{T}}</script></span>. Written in vector form, we will get</p> <div> <div class=MathJax_Preview>\begin{equation*} \frac{\partial J}{\partial \boldsymbol{v_c}} = \boldsymbol{U}(\boldsymbol{\hat y} - \boldsymbol{y}) \end{equation*}</div> <script type="math/tex; mode=display">\begin{equation*}
\frac{\partial J}{\partial \boldsymbol{v_c}} = \boldsymbol{U}(\boldsymbol{\hat y} - \boldsymbol{y})
\end{equation*}</script> </div> <p>In the above gradient, <span><span class=MathJax_Preview>U = [\boldsymbol{u_1}, \boldsymbol{u_1}, \cdots, \boldsymbol{u_W}]</span><script type=math/tex>U = [\boldsymbol{u_1}, \boldsymbol{u_1}, \cdots, \boldsymbol{u_W}]</script></span> is the matrix of all the output vectors. <span><span class=MathJax_Preview>\boldsymbol{u_1}</span><script type=math/tex>\boldsymbol{u_1}</script></span> is a column word vector. The component <span><span class=MathJax_Preview>\boldsymbol{\hat y} - \boldsymbol{y}</span><script type=math/tex>\boldsymbol{\hat y} - \boldsymbol{y}</script></span> is a also a column vector with length of <span><span class=MathJax_Preview>W</span><script type=math/tex>W</script></span>. The above gradient can be viewed as scaling each output vector <span><span class=MathJax_Preview>\boldsymbol{u_i}</span><script type=math/tex>\boldsymbol{u_i}</script></span> by the scaler <span><span class=MathJax_Preview>\hat y_i - y.</span><script type=math/tex>\hat y_i - y.</script></span> Alternatively, the gradient can also be wrote as distributive form</p> <div> <div class=MathJax_Preview>\begin{equation*} \frac{\partial J}{\partial \boldsymbol{v_c}} = \boldsymbol{U}\boldsymbol{\hat y} - \boldsymbol{U}\boldsymbol{y} = -\boldsymbol{u_i} + \sum_{w=1}^{W}\hat y_w \boldsymbol{u_w} \end{equation*}</div> <script type="math/tex; mode=display">\begin{equation*}
\frac{\partial J}{\partial \boldsymbol{v_c}} = \boldsymbol{U}\boldsymbol{\hat y} - \boldsymbol{U}\boldsymbol{y} = -\boldsymbol{u_i} + \sum_{w=1}^{W}\hat y_w \boldsymbol{u_w}
\end{equation*}</script> </div> <p>The index <span><span class=MathJax_Preview>i</span><script type=math/tex>i</script></span> in the above equation is corresponding to the index of the none zero element in the one-hot vector <span><span class=MathJax_Preview>\boldsymbol{y}</span><script type=math/tex>\boldsymbol{y}</script></span>. Here we can see <span><span class=MathJax_Preview>\boldsymbol{y}</span><script type=math/tex>\boldsymbol{y}</script></span> as the true label of the output word.</p> <h4 id=gradient-for-output-word>Gradient for output word<a class=headerlink href=#gradient-for-output-word title="Permanent link">&para;</a></h4> <p>We can also calculate the gradient with respect to the output word.</p> <div> <div class=MathJax_Preview>\begin{align*} \frac{\partial }{\partial \boldsymbol{u_w}}J_{\mathrm{softmax-CE}}(\boldsymbol{o}, \boldsymbol{v_c}, \boldsymbol{U}) &amp; = \frac{\partial J_{\mathrm{CE}}(\theta)}{\partial \theta_i} \frac{\partial \theta_i}{\boldsymbol{u_w}} \\ &amp; = (\hat y_i - y_i) \boldsymbol{v_c} \end{align*}</div> <script type="math/tex; mode=display">\begin{align*}
\frac{\partial }{\partial \boldsymbol{u_w}}J_{\mathrm{softmax-CE}}(\boldsymbol{o}, \boldsymbol{v_c}, \boldsymbol{U}) & = \frac{\partial J_{\mathrm{CE}}(\theta)}{\partial \theta_i} \frac{\partial \theta_i}{\boldsymbol{u_w}} \\ & = (\hat y_i - y_i) \boldsymbol{v_c}
\end{align*}</script> </div> <p>Notice here we apply <span><span class=MathJax_Preview>\frac{\partial \boldsymbol{a}^{\mathsf{T}} \boldsymbol{x}}{\partial \boldsymbol{a}} = x</span><script type=math/tex>\frac{\partial \boldsymbol{a}^{\mathsf{T}} \boldsymbol{x}}{\partial \boldsymbol{a}} = x</script></span>. Writen the gradient in matrix format, we have</p> <div> <div class=MathJax_Preview>\begin{equation*} \frac{\partial J}{\partial \boldsymbol{U}} = \boldsymbol{v_c}(\boldsymbol{\hat y} - \boldsymbol{y})^{\mathsf{T}} \end{equation*}</div> <script type="math/tex; mode=display">\begin{equation*}
\frac{\partial J}{\partial \boldsymbol{U}} =  \boldsymbol{v_c}(\boldsymbol{\hat y} - \boldsymbol{y})^{\mathsf{T}}
\end{equation*}</script> </div> <p>Notice the shape of the gradient. It determines how the notation looks like. In the above notation notice the shape of the output word gradient <span><span class=MathJax_Preview>\frac{\partial J}{\partial \boldsymbol{U}}</span><script type=math/tex>\frac{\partial J}{\partial \boldsymbol{U}}</script></span> is <span><span class=MathJax_Preview>d \times W</span><script type=math/tex>d \times W</script></span>. As we will discussion in the next post that it is a convention to make the shape of gradient as the shape of the input vectors. in this case the shape of <span><span class=MathJax_Preview>U</span><script type=math/tex>U</script></span> and <span><span class=MathJax_Preview>\frac{\partial J}{\partial \boldsymbol{U}}</span><script type=math/tex>\frac{\partial J}{\partial \boldsymbol{U}}</script></span> are the same.</p> <h3 id=negative-sampling>Negative sampling<a class=headerlink href=#negative-sampling title="Permanent link">&para;</a></h3> <p>The cost function for a single word prediction using nagative sampling is the following</p> <div> <div class=MathJax_Preview>\begin{align*} J_{\mathrm{neg-sample}}(\boldsymbol{o},\boldsymbol{v_c},\boldsymbol{U}) &amp; = -\log(\sigma(\boldsymbol{u_o}^{\mathsf{T}} \boldsymbol{v_c})) - \sum_{k=1}^{K} \log(\sigma(-\boldsymbol{u_k}^{\mathsf{T}} \boldsymbol{v_c})) \end{align*}</div> <script type="math/tex; mode=display">\begin{align*}
J_{\mathrm{neg-sample}}(\boldsymbol{o},\boldsymbol{v_c},\boldsymbol{U}) & = -\log(\sigma(\boldsymbol{u_o}^{\mathsf{T}} \boldsymbol{v_c})) - \sum_{k=1}^{K} \log(\sigma(-\boldsymbol{u_k}^{\mathsf{T}} \boldsymbol{v_c}))
\end{align*}</script> </div> <p>It comes from the original paper by Mikolov et al. <span><span class=MathJax_Preview>\sigma</span><script type=math/tex>\sigma</script></span> is the sigmoid function <span><span class=MathJax_Preview>\sigma(x) = \frac{1}{1+e^{-x}}</span><script type=math/tex>\sigma(x) = \frac{1}{1+e^{-x}}</script></span>. The ideas is to reduce the optimization computation by only sampling a small part of the vocabulary that have a lower probability of being context words of one another. The first lecture notes from CS224N discussed briefly the origin and intuition of the negative sampling loss function. Here we will focus on deriving the gradient and implementation ideas.</p> <p>With the fact that <span><span class=MathJax_Preview>\frac{\mathrm{d}\sigma(x)}{\mathrm{d} x} = \sigma(x)(1-\sigma(x))</span><script type=math/tex>\frac{\mathrm{d}\sigma(x)}{\mathrm{d} x} = \sigma(x)(1-\sigma(x))</script></span> and the chain rule, it is not hard to derive the gradients result as following</p> <div> <div class=MathJax_Preview>\begin{align*} \frac{\partial J}{\partial \boldsymbol{v_c}} &amp; = (\sigma(\boldsymbol{u_o}^{\mathsf{T}}\boldsymbol{v_c}) - 1)\boldsymbol{u_o} - \sum_{k=1}^{K}(\sigma(-\boldsymbol{u_k}^{\mathsf{T}}\boldsymbol{v_c}) - 1)\boldsymbol{u_k} \\ \frac{\partial J}{\partial \boldsymbol{u_o}} &amp; = (\sigma(\boldsymbol{u_o}^{\mathsf{T}}\boldsymbol{v_c}) - 1)\boldsymbol{v_c} \\ \frac{\partial J}{\partial \boldsymbol{u_k}} &amp; = -\sigma(-\boldsymbol{u_k}^{\mathsf{T}}\boldsymbol{v_c}) - 1)\boldsymbol{v_c}, \mathrm{for\ all\ } k = 1, 2, \cdots, K \end{align*}</div> <script type="math/tex; mode=display">\begin{align*}
\frac{\partial J}{\partial \boldsymbol{v_c}} & = (\sigma(\boldsymbol{u_o}^{\mathsf{T}}\boldsymbol{v_c}) - 1)\boldsymbol{u_o} - \sum_{k=1}^{K}(\sigma(-\boldsymbol{u_k}^{\mathsf{T}}\boldsymbol{v_c}) - 1)\boldsymbol{u_k} \\
\frac{\partial J}{\partial \boldsymbol{u_o}} & = (\sigma(\boldsymbol{u_o}^{\mathsf{T}}\boldsymbol{v_c}) - 1)\boldsymbol{v_c} \\
\frac{\partial J}{\partial \boldsymbol{u_k}} & = -\sigma(-\boldsymbol{u_k}^{\mathsf{T}}\boldsymbol{v_c}) - 1)\boldsymbol{v_c},  \mathrm{for\ all\ } k = 1, 2, \cdots, K
\end{align*}</script> </div> <p>How to sample the <span><span class=MathJax_Preview>\boldsymbol{u_k}</span><script type=math/tex>\boldsymbol{u_k}</script></span> in practice? The best known sampling method is based on the Unigram Model raise to the power of 3/4. Unigram Model is the counts of each word in the particular corpus (not the vocabulary).</p> <h4 id=gradient-for-all-of-word-vectors>Gradient for all of word vectors<a class=headerlink href=#gradient-for-all-of-word-vectors title="Permanent link">&para;</a></h4> <p>Since skip-gram model is using one center word to predict all its context words. Given a word context size of <span><span class=MathJax_Preview>m</span><script type=math/tex>m</script></span>, we obtain a set of context words <span><span class=MathJax_Preview>[\mathrm{word}_{c-m}, \cdots, \mathrm{word}_{c-1}, \mathrm{word}_{c}, \mathrm{word}_{c+1}, \cdots, \mathrm{word}_{c+m}]</span><script type=math/tex>[\mathrm{word}_{c-m}, \cdots, \mathrm{word}_{c-1}, \mathrm{word}_{c}, \mathrm{word}_{c+1}, \cdots, \mathrm{word}_{c+m}]</script></span>. For each window, We need to predict <span><span class=MathJax_Preview>2m</span><script type=math/tex>2m</script></span> context word given the center word. Denote the "input" and "output" word vectors for <span><span class=MathJax_Preview>\mathrm{word}_k</span><script type=math/tex>\mathrm{word}_k</script></span> as <span><span class=MathJax_Preview>\boldsymbol{v}_k</span><script type=math/tex>\boldsymbol{v}_k</script></span> and <span><span class=MathJax_Preview>\boldsymbol{u}_k</span><script type=math/tex>\boldsymbol{u}_k</script></span> respectively. The cost for the entire context window with size <span><span class=MathJax_Preview>m</span><script type=math/tex>m</script></span> centered around <span><span class=MathJax_Preview>\mathrm{word}_c</span><script type=math/tex>\mathrm{word}_c</script></span>would be</p> <div> <div class=MathJax_Preview>\begin{equation*} J_{\mathrm{skip-gram}}(\mathrm{word}_{c-m \;\cdots\; c+m}) = \sum_{\substack{-m \le j \le m\\ j \ne 0}} F(\boldsymbol{u}_{c+j}, \boldsymbol{v}_c). \end{equation*}</div> <script type="math/tex; mode=display">\begin{equation*}
J_{\mathrm{skip-gram}}(\mathrm{word}_{c-m \;\cdots\; c+m}) = \sum_{\substack{-m \le j \le m\\
j \ne 0}} F(\boldsymbol{u}_{c+j}, \boldsymbol{v}_c).
\end{equation*}</script> </div> <p><span><span class=MathJax_Preview>F</span><script type=math/tex>F</script></span> is a placeholder notation to represent the cost function given the center word for different model. Therefore for skip-gram, the gradients for the cost of one context window are</p> <div> <div class=MathJax_Preview>\begin{align*} \frac{J_{\mathrm{skip-gram}}(\mathrm{word}_{c-m \;\cdots\; c+m})}{\partial \boldsymbol{U}} &amp; = \sum_{\substack{-m \le j \le m\\ j \ne 0}} \frac{\partial F(\boldsymbol{u}_{c+j}, \boldsymbol{v}_c)}{\partial \boldsymbol{v}_c},\\ \frac{J_{\mathrm{skip-gram}}(\mathrm{word}_{c-m \;\cdots\; c+m})}{\partial \boldsymbol{v}_c} &amp; = \sum_{\substack{-m \le j \le m\\ j \ne 0}} \frac{\partial F(\boldsymbol{u}_{c+j}, \boldsymbol{v}_c)}{\partial \boldsymbol{v}_c}, \\ \frac{J_{\mathrm{skip-gram}}(\mathrm{word}_{c-m \;\cdots\; c+m})}{\partial \boldsymbol{v}_j} &amp; = 0, \forall j \ne c \end{align*}</div> <script type="math/tex; mode=display">\begin{align*}
\frac{J_{\mathrm{skip-gram}}(\mathrm{word}_{c-m \;\cdots\; c+m})}{\partial \boldsymbol{U}} & = \sum_{\substack{-m \le j \le m\\ j \ne 0}} \frac{\partial F(\boldsymbol{u}_{c+j}, \boldsymbol{v}_c)}{\partial \boldsymbol{v}_c},\\
\frac{J_{\mathrm{skip-gram}}(\mathrm{word}_{c-m \;\cdots\; c+m})}{\partial \boldsymbol{v}_c} & = \sum_{\substack{-m \le j \le m\\ j \ne 0}} \frac{\partial F(\boldsymbol{u}_{c+j}, \boldsymbol{v}_c)}{\partial \boldsymbol{v}_c}, \\
\frac{J_{\mathrm{skip-gram}}(\mathrm{word}_{c-m \;\cdots\; c+m})}{\partial \boldsymbol{v}_j} & = 0, \forall j \ne c
\end{align*}</script> </div> <h3 id=cbow-model>CBOW model<a class=headerlink href=#cbow-model title="Permanent link">&para;</a></h3> <p>Continuous bag-of-words (CBOW) is using the context words to predect the center words. Different from skip-gram model, in CBOW, we will use <span><span class=MathJax_Preview>2m</span><script type=math/tex>2m</script></span> context word vectors as we predict probability of a word is the center word. For a simple variant of CBOW, we could sum up all the <span><span class=MathJax_Preview>2m</span><script type=math/tex>2m</script></span> word vectors in one context vector <span><span class=MathJax_Preview>\hat{\boldsymbol{v}}</span><script type=math/tex>\hat{\boldsymbol{v}}</script></span> and use the similar cost function with <span><span class=MathJax_Preview>\mathrm{softmax}</span><script type=math/tex>\mathrm{softmax}</script></span> as we did in skip-gram model.</p> <div> <div class=MathJax_Preview>\begin{equation*} \hat{\boldsymbol{v}} = \sum_{\substack{-m \le j \le m\\ j \ne 0}} \boldsymbol{v}_{c+j}. \end{equation*}</div> <script type="math/tex; mode=display">\begin{equation*}
\hat{\boldsymbol{v}} = \sum_{\substack{-m \le j \le m\\ 
j \ne 0}} \boldsymbol{v}_{c+j}.
\end{equation*}</script> </div> <p>Similar to skip-gram, we have</p> <div> <div class=MathJax_Preview>\begin{align*} \frac{J_{\mathrm{CBOW}}(\mathrm{word}_{c-m \;\cdots\; c+m})}{\partial \boldsymbol{U}} &amp; = \frac{\partial F(\boldsymbol{u}_{c}, \hat{\boldsymbol{v}})}{\partial \boldsymbol{U}},\\ \frac{J_{\mathrm{CBOW}}(\mathrm{word}_{c-m \;\cdots\; c+m})}{\partial \boldsymbol{v}_j} &amp; = \frac{\partial F(\boldsymbol{u}_{c}, \hat{\boldsymbol{v}})}{\partial \boldsymbol{v}_j}, \forall j \in \{c-m, \cdots, c-1, c+1, \cdots, c+m\}\\ \frac{J_{\mathrm{CBOW}}(\mathrm{word}_{c-m \;\cdots\; c+m})}{\partial \boldsymbol{v}_j} &amp; = 0, \forall j \notin \{c-m, \cdots, c-1, c+1, \cdots, c+m\}. \end{align*}</div> <script type="math/tex; mode=display">\begin{align*}
\frac{J_{\mathrm{CBOW}}(\mathrm{word}_{c-m \;\cdots\; c+m})}{\partial \boldsymbol{U}} & = \frac{\partial F(\boldsymbol{u}_{c}, \hat{\boldsymbol{v}})}{\partial \boldsymbol{U}},\\
\frac{J_{\mathrm{CBOW}}(\mathrm{word}_{c-m \;\cdots\; c+m})}{\partial \boldsymbol{v}_j} & = \frac{\partial F(\boldsymbol{u}_{c}, \hat{\boldsymbol{v}})}{\partial \boldsymbol{v}_j}, \forall j \in \{c-m, \cdots, c-1, c+1, \cdots, c+m\}\\
\frac{J_{\mathrm{CBOW}}(\mathrm{word}_{c-m \;\cdots\; c+m})}{\partial \boldsymbol{v}_j} & = 0, \forall j \notin \{c-m, \cdots, c-1, c+1, \cdots, c+m\}.
\end{align*}</script> </div> <h3 id=glove-model>GloVe model<a class=headerlink href=#glove-model title="Permanent link">&para;</a></h3> <p>TODO, the paper and lecture.</p> <h3 id=word-vector-evaluation>Word vector evaluation<a class=headerlink href=#word-vector-evaluation title="Permanent link">&para;</a></h3> <h3 id=summary>Summary<a class=headerlink href=#summary title="Permanent link">&para;</a></h3> <p>This post focused on gradient derivation for various word embedding models. The bootstrap model is based on the distributional semantic, by which to predict the probability of a word given some other words from the fixed corpus. We use the <span><span class=MathJax_Preview>\mathrm{softmax}</span><script type=math/tex>\mathrm{softmax}</script></span> function to compute the probability. To update the word vectors, we introduce the likelihood function and derived its gradient. In that derivation, the parameter <span><span class=MathJax_Preview>\boldsymbol{\theta}</span><script type=math/tex>\boldsymbol{\theta}</script></span> is the hyper parameters related to word vector that can be used to compute the probability. We continue to derive the gradient of the cross entropy loss function for a single word prediction. The result gradient of the cross entropy loss function is the key for all our later gradient derivation. After that we introduce the word2vec family of word embedding, skip-gram and CBOW models. In word2vec we use the dot product of the center word and its context words to compute the probability. We derive the gradient with respect to both center word and context word when using cross entropy loss function.</p> <p>Negative sampling is also discussed as it improves the computation cost by many factors. With the discussion of skip-gram model, CBOW model is easier to present. One thing you need to distinguish is that whether the gradient is for a particular word prediction, for the whole window of the words, or the over all objective for the corpus. For skip-gram, we first compute the gradient for each word prediction in the given context window, then sum up all the gradient to update the cost function of that window. We move the window over the corpus until finish all the word updates. This whole process is only one update. While using negative sampling, the process becomes more efficient, we don't have to go through all the window, only sampling <span><span class=MathJax_Preview>K</span><script type=math/tex>K</script></span> windows with Unigram Model raise to the <span><span class=MathJax_Preview>3/4</span><script type=math/tex>3/4</script></span> power. In the simple CBOW model discussed, we add up all the context vectors first, then only update a single gradient for that window corresponding to the center word. We repeat this process for different windows to complete one update.</p> <h2 id=part-2-neural-networks-and-backpropagation>Part 2 Neural Networks and Backpropagation<a class=headerlink href=#part-2-neural-networks-and-backpropagation title="Permanent link">&para;</a></h2> <p>Date: April 15th 2019</p> <p>We discussed the softmax classifier in Part 1 and its major drawback that the classifier only gives linear decision boundaries. In Part 2, Neural Networks will be introduced to demonstrate that it can learn much more complex functions and nonlinear decision boundaries.</p> <h3 id=intro-to-neural-networks>Intro to Neural Networks<a class=headerlink href=#intro-to-neural-networks title="Permanent link">&para;</a></h3> <table> <thead> <tr> <th align=center></th> <th align=center></th> </tr> </thead> <tbody> <tr> <td align=center>Biological Neuron</td> <td align=center><img alt="Biological Neuron" src=../fig/neuron.png></td> </tr> <tr> <td align=center>Mathematical Model</td> <td align=center><img alt="Mathematical model" src=../fig/neuron_model.jpeg></td> </tr> <tr> <td align=center>Simplified Neuron</td> <td align=center><img alt="simplified neuron" src=../fig/simplified_neuron.png></td> </tr> </tbody> </table> <p>The neuron can be modeled as a binary logistic regression unit as in the last row of the table above. It can be further simplified as following functions,</p> <div> <div class=MathJax_Preview>\begin{equation*} h_{w, b} (\boldsymbol{x})=f(\boldsymbol{w}^\mathsf{T} \boldsymbol{x} + b) \\ f(z) = \frac{1}{1 + e^{-z}}. \end{equation*}</div> <script type="math/tex; mode=display">\begin{equation*}
h_{w, b} (\boldsymbol{x})=f(\boldsymbol{w}^\mathsf{T} \boldsymbol{x} + b) \\
f(z) = \frac{1}{1 + e^{-z}}.
\end{equation*}</script> </div> <ul> <li><span><span class=MathJax_Preview>\boldsymbol{x}</span><script type=math/tex>\boldsymbol{x}</script></span> is the inputs</li> <li><span><span class=MathJax_Preview>\boldsymbol{w}</span><script type=math/tex>\boldsymbol{w}</script></span> is the weights</li> <li><span><span class=MathJax_Preview>b</span><script type=math/tex>b</script></span> is a bias term</li> <li><span><span class=MathJax_Preview>h</span><script type=math/tex>h</script></span> is a hidden layer function</li> <li><span><span class=MathJax_Preview>f</span><script type=math/tex>f</script></span> is a nonlinear activation function (sigmoid, tanh, etc.)</li> </ul> <p>If we feed a vector of inputs through a bunch of logistic regression (sigmoid) functions, then we get a vector of outputs. The CS224N lecture note 3 section 1.2 have a complete derivation on multiple sigmoid units. But we don’t have to decide ahead of time what variables these logistic regressions are trying to predict! It is the loss function that will direct what the intermediate hidden variables should be, so as to do a good job at predicting the targets for the next layer, etc.</p> <h3 id=forward-propagation-in-matrix-notation>Forward Propagation in Matrix Notation<a class=headerlink href=#forward-propagation-in-matrix-notation title="Permanent link">&para;</a></h3> <p>In a multilayer neural network, not only did we have multiple sigmoid units, but we also have more than one layer. Let's explicitly write down the signal transformation (aka forward propagation) from one layer to another referring to this network from Andrew's ML course.</p> <p><img alt=Neural_network.png src=../fig/Neural_network.png></p> <p>We will use the following notation convention</p> <ul> <li><span><span class=MathJax_Preview>a^{(j)}_i</span><script type=math/tex>a^{(j)}_i</script></span> to represent the "activation" of unit <span><span class=MathJax_Preview>i</span><script type=math/tex>i</script></span> in layer <span><span class=MathJax_Preview>j</span><script type=math/tex>j</script></span>.</li> <li><span><span class=MathJax_Preview>W^{(j)}</span><script type=math/tex>W^{(j)}</script></span> to represent the matrix of weights controlling function mapping from layer <span><span class=MathJax_Preview>j</span><script type=math/tex>j</script></span> to layer <span><span class=MathJax_Preview>j + 1</span><script type=math/tex>j + 1</script></span>.</li> </ul> <p>The value at each node can be calculated as</p> <div> <div class=MathJax_Preview>\begin{equation*} a^{(2)}_1 = f(w^{(1)}_{10} x_0 + w^{(1)}_{11} x_1 + w^{(1)}_{12} x_2 + w^{(1)}_{13} x_3) \\ a^{(2)}_2 = f(w^{(1)}_{20} x_0 + w^{(1)}_{21} x_1 + w^{(1)}_{22} x_2 + w^{(1)}_{23} x_3) \\ a^{(2)}_3 = f(w^{(1)}_{30} x_0 + w^{(1)}_{31} x_1 + w^{(1)}_{32} x_2 + w^{(1)}_{33} x_3) \\ \text{and} \\ h_{w} (\boldsymbol{x}) = a^{(3)}_1 = f(w^{(2)}_{10} a^{(2)}_0 + w^{(2)}_{11} a^{(2)}_1 + w^{(2)}_{12} a^{(2)}_2 + w^{(2)}_{13} a^{(2)}_3) \end{equation*}</div> <script type="math/tex; mode=display">\begin{equation*}
a^{(2)}_1 = f(w^{(1)}_{10} x_0 + w^{(1)}_{11} x_1 + w^{(1)}_{12} x_2 + w^{(1)}_{13} x_3) \\
a^{(2)}_2 = f(w^{(1)}_{20} x_0 + w^{(1)}_{21} x_1 + w^{(1)}_{22} x_2 + w^{(1)}_{23} x_3) \\
a^{(2)}_3 = f(w^{(1)}_{30} x_0 + w^{(1)}_{31} x_1 + w^{(1)}_{32} x_2 + w^{(1)}_{33} x_3) \\
\text{and} \\
h_{w} (\boldsymbol{x}) = a^{(3)}_1 = f(w^{(2)}_{10} a^{(2)}_0 + w^{(2)}_{11} a^{(2)}_1 + w^{(2)}_{12} a^{(2)}_2 + w^{(2)}_{13} a^{(2)}_3)
\end{equation*}</script> </div> <p>write the matrix <span><span class=MathJax_Preview>W^{(j)}</span><script type=math/tex>W^{(j)}</script></span> explicity,</p> <div> <div class=MathJax_Preview>\begin{equation*} W^{(1)} = \begin{bmatrix} w^{(1)}_{10} &amp; w^{(1)}_{11} &amp; w^{(1)}_{12} &amp; w^{(1)}_{13} \\ w^{(1)}_{20} &amp; w^{(1)}_{21} &amp; w^{(1)}_{22} &amp; w^{(1)}_{23} \\ w^{(1)}_{30} &amp; w^{(1)}_{31} &amp; w^{(1)}_{32} &amp; w^{(1)}_{33} \end{bmatrix} \end{equation*}</div> <script type="math/tex; mode=display">\begin{equation*}
W^{(1)} =
\begin{bmatrix}
w^{(1)}_{10} & w^{(1)}_{11} & w^{(1)}_{12} & w^{(1)}_{13} \\
w^{(1)}_{20} & w^{(1)}_{21} & w^{(1)}_{22} & w^{(1)}_{23} \\
w^{(1)}_{30} & w^{(1)}_{31} & w^{(1)}_{32} & w^{(1)}_{33}
\end{bmatrix}
\end{equation*}</script> </div> <div> <div class=MathJax_Preview>\begin{equation*} W^{(2)} = \begin{bmatrix} w^{(2)}_{10} &amp; w^{(2)}_{11} &amp; w^{(2)}_{12} &amp; w^{(2)}_{13} \end{bmatrix} \end{equation*}</div> <script type="math/tex; mode=display">\begin{equation*}
W^{(2)} =
\begin{bmatrix}
w^{(2)}_{10} & w^{(2)}_{11} & w^{(2)}_{12} & w^{(2)}_{13}
\end{bmatrix}
\end{equation*}</script> </div> <p>With the above form, we can use matrix notation as</p> <div> <div class=MathJax_Preview>\begin{align*} &amp; \boldsymbol{a}^{(2)} = f(\boldsymbol{W}^{(1)} \boldsymbol{x}) \\ h_{w} (\boldsymbol{x}) = &amp; \boldsymbol{a}^{(3)} = f(\boldsymbol{W}^{(2)} \boldsymbol{a}^{(2)}) \end{align*}</div> <script type="math/tex; mode=display">\begin{align*}
& \boldsymbol{a}^{(2)} = f(\boldsymbol{W}^{(1)} \boldsymbol{x}) \\
h_{w} (\boldsymbol{x})  = & \boldsymbol{a}^{(3)} = f(\boldsymbol{W}^{(2)} \boldsymbol{a}^{(2)})
\end{align*}</script> </div> <p>We can see from above notations that if the network has <span><span class=MathJax_Preview>s_j</span><script type=math/tex>s_j</script></span> units in layer <span><span class=MathJax_Preview>j</span><script type=math/tex>j</script></span> and <span><span class=MathJax_Preview>s_{j+1}</span><script type=math/tex>s_{j+1}</script></span> in layer <span><span class=MathJax_Preview>j + 1</span><script type=math/tex>j + 1</script></span>, the matrix <span><span class=MathJax_Preview>W^{(j)}</span><script type=math/tex>W^{(j)}</script></span> will be of dimention <span><span class=MathJax_Preview>s_{j+1} \times s_{j}+1</span><script type=math/tex>s_{j+1} \times s_{j}+1</script></span>. It could be interpreted as "the dimention of <span><span class=MathJax_Preview>W^{(j)}</span><script type=math/tex>W^{(j)}</script></span> is the number of nodes in the next layer (layer <span><span class=MathJax_Preview>j + 1</span><script type=math/tex>j + 1</script></span>) <span><span class=MathJax_Preview>\times</span><script type=math/tex>\times</script></span> the number of nodes in the current layer <span><span class=MathJax_Preview>+</span><script type=math/tex>+</script></span> 1.</p> <div class="admonition note"> <p class=admonition-title>Note that in cs224n the matrix notation is slightly different</p> <div> <div class=MathJax_Preview>\begin{align*} &amp; \boldsymbol{a}^{(2)} = f(\boldsymbol{W}^{(1)} \boldsymbol{x} + \boldsymbol{b}^{(1)}) \\ h_{w} (\boldsymbol{x}) = &amp; \boldsymbol{a}^{(3)} = f(\boldsymbol{W}^{(2)} \boldsymbol{a}^{(2)} + \boldsymbol{b}^{(2)}) \end{align*}</div> <script type="math/tex; mode=display">\begin{align*}
& \boldsymbol{a}^{(2)} = f(\boldsymbol{W}^{(1)} \boldsymbol{x} + \boldsymbol{b}^{(1)}) \\
h_{w} (\boldsymbol{x}) = & \boldsymbol{a}^{(3)} = f(\boldsymbol{W}^{(2)} \boldsymbol{a}^{(2)} + \boldsymbol{b}^{(2)})
\end{align*}</script> </div> </div> <p>the different is from how we denote the bias. The two are enssentially the same, but be cautions that the matrix dimentions are different.</p> <h3 id=word-window-classification-using-neural-networks>Word Window Classification Using Neural Networks<a class=headerlink href=#word-window-classification-using-neural-networks title="Permanent link">&para;</a></h3> <p>From now on, let's switch the notation to cs224n so that we can derive the backpropagation algorithm for word window classification and get more intuition about the backprop. The drawing of the neural nets in cs224n for word window classification is less dynamic and slitly different from the drawing from Andrew's ML class. The figure from cs224n may be slightly confusing at first, but it is good to understand it from this word window classification application.</p> <h4 id=forward-propagation>Forward propagation<a class=headerlink href=#forward-propagation title="Permanent link">&para;</a></h4> <p>Firstly, the goal of this classification task is to classify whether the center word is a location. Similar to word2vec, we will go over all positions in a corpus. But this time, it will be supervised and only some positions should get a high score.</p> <p><img alt="Forward Propagation" src=../fig/forward-prop.png></p> <p>The figure above illustrate the feed-forward process. We use the method by Collobert &amp; Weston (2008, 2011). An unnormalized score will be calculated from the activation <span><span class=MathJax_Preview>\boldsymbol{a} = [a_1, a_2, a_3, \cdots]^\mathsf{T}</span><script type=math/tex>\boldsymbol{a} = [a_1, a_2, a_3, \cdots]^\mathsf{T}</script></span>.</p> <div> <div class=MathJax_Preview>\begin{equation*} \text{scroe}(\boldsymbol{x}) = U^\mathsf{T} \boldsymbol{a} \in \mathbb{R} \end{equation*}</div> <script type="math/tex; mode=display">\begin{equation*}
\text{scroe}(\boldsymbol{x}) = U^\mathsf{T} \boldsymbol{a} \in \mathbb{R}
\end{equation*}</script> </div> <p>We will use max-margin loss as our loss function. The training is essentially to find the optimal weights <span><span class=MathJax_Preview>W</span><script type=math/tex>W</script></span> by minimize the max-margin loss</p> <div> <div class=MathJax_Preview>\begin{equation*} J = \max(0, 1 - s + s_c) \end{equation*}</div> <script type="math/tex; mode=display">\begin{equation*}
J = \max(0, 1 - s + s_c)
\end{equation*}</script> </div> <p><span><span class=MathJax_Preview>s</span><script type=math/tex>s</script></span> is the score of a window that have a location in the center. <span><span class=MathJax_Preview>s_c</span><script type=math/tex>s_c</script></span> is the score of a window that doesn't have a location in the center. For full objective function: Sample several corrupt windows per true one. Sum over all training windows. It is similar to negative sampling in word2vec.</p> <p>We will use gradient descent to update the parameter so as to minimize the loss function. The key is how to calculate the gradient with respect to the model parameters, namely <span><span class=MathJax_Preview>\nabla_{\theta} J(\theta)</span><script type=math/tex>\nabla_{\theta} J(\theta)</script></span>. Here we use <span><span class=MathJax_Preview>\theta</span><script type=math/tex>\theta</script></span> to represent the hyperthetic parameters, it can include the <span><span class=MathJax_Preview>W</span><script type=math/tex>W</script></span> and other parameters of the model.</p> <div> <div class=MathJax_Preview>\begin{equation*} \theta^{\text{new}} = \theta^{\text{old}} - \alpha\nabla_{\theta} J(\theta) \end{equation*}</div> <script type="math/tex; mode=display">\begin{equation*}
\theta^{\text{new}} = \theta^{\text{old}} - \alpha\nabla_{\theta} J(\theta)
\end{equation*}</script> </div> <h4 id=gradients-and-jacobians-matrix>Gradients and Jacobians Matrix<a class=headerlink href=#gradients-and-jacobians-matrix title="Permanent link">&para;</a></h4> <p>At first, let us layout the input and all the equations in this simple neural network.</p> <table> <thead> <tr> <th>Input Layer</th> <th>Hidden Layer</th> <th>Output Layer</th> </tr> </thead> <tbody> <tr> <td><span><span class=MathJax_Preview>\boldsymbol{x}</span><script type=math/tex>\boldsymbol{x}</script></span></td> <td><span><span class=MathJax_Preview>\boldsymbol{z} = \boldsymbol{W} \boldsymbol{x} + \boldsymbol{b}, \\ \boldsymbol{h} = f(\boldsymbol{z})</span><script type=math/tex>\boldsymbol{z} = \boldsymbol{W} \boldsymbol{x} + \boldsymbol{b}, \\ \boldsymbol{h} = f(\boldsymbol{z})</script></span></td> <td><span><span class=MathJax_Preview>s = \boldsymbol{u}^{\mathsf{T}}\boldsymbol{h}</span><script type=math/tex>s = \boldsymbol{u}^{\mathsf{T}}\boldsymbol{h}</script></span></td> </tr> </tbody> </table> <p>To update the parameters in this model, namely <span><span class=MathJax_Preview>\boldsymbol{x}, \boldsymbol{W}, \boldsymbol{b}</span><script type=math/tex>\boldsymbol{x}, \boldsymbol{W}, \boldsymbol{b}</script></span>, we would like to compute the derivitavies of <span><span class=MathJax_Preview>s</span><script type=math/tex>s</script></span> with respect to all these parameters. We have to use the chain rule to compute it. What chain rule says is that we can compute the partial derivatives of each individual functions and then multiply them together to get the derivative with respect the specific variable. For example, <span><span class=MathJax_Preview>\frac{\partial s}{\partial \boldsymbol{b}}</span><script type=math/tex>\frac{\partial s}{\partial \boldsymbol{b}}</script></span> is computed as <span><span class=MathJax_Preview>\frac{\partial s}{\partial \boldsymbol{b}} = \frac{\partial s}{\partial \boldsymbol{h}} \cdot \frac{\partial \boldsymbol{h}}{\partial \boldsymbol{z}} \cdot \frac{\partial \boldsymbol{z}}{\partial \boldsymbol{b}}</span><script type=math/tex>\frac{\partial s}{\partial \boldsymbol{b}} = \frac{\partial s}{\partial \boldsymbol{h}} \cdot \frac{\partial \boldsymbol{h}}{\partial \boldsymbol{z}} \cdot \frac{\partial \boldsymbol{z}}{\partial \boldsymbol{b}}</script></span>. This seems very easy to understand, but when it comes to implemenation in vectorized format, it become confusing for those who doesn't work on matrix calculus for quite a while like me. I want to get the points straight here. What exactly is <span><span class=MathJax_Preview>\frac{\partial s}{\partial \boldsymbol{b}}</span><script type=math/tex>\frac{\partial s}{\partial \boldsymbol{b}}</script></span> and <span><span class=MathJax_Preview>\frac{\partial \boldsymbol{h}}{\partial \boldsymbol{z}}</span><script type=math/tex>\frac{\partial \boldsymbol{h}}{\partial \boldsymbol{z}}</script></span>. Note both <span><span class=MathJax_Preview>\boldsymbol{h}</span><script type=math/tex>\boldsymbol{h}</script></span> and <span><span class=MathJax_Preview>\boldsymbol{z}</span><script type=math/tex>\boldsymbol{z}</script></span> are vectors. To calculate these two gradient, simply remember the following two rules:</p> <ol> <li> <p>Given a function with 1 output and <span><span class=MathJax_Preview>n</span><script type=math/tex>n</script></span> inputs <span><span class=MathJax_Preview>f(\boldsymbol{x}) = f(x_1, x_2, \cdots, x_n)</span><script type=math/tex>f(\boldsymbol{x}) = f(x_1, x_2, \cdots, x_n)</script></span>, it's gradient is a vector of partial derivatives with respect to each input (take the gradient element wise). $$ \frac{\partial f}{\partial \boldsymbol{x}} = \Bigg [ \frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \cdots, \frac{\partial f}{\partial x_n}, \Bigg ] $$</p> </li> <li> <p>Given a function with <span><span class=MathJax_Preview>m</span><script type=math/tex>m</script></span> output and <span><span class=MathJax_Preview>n</span><script type=math/tex>n</script></span> inputs $$ \boldsymbol{f}(\boldsymbol{x}) = \big [f_1(x_1, x_2, \cdots, x_n), \cdots, f_m(x_1, x_2, \cdots, x_n) \big ], $$ it's gradient is an <span><span class=MathJax_Preview>m \times n</span><script type=math/tex>m \times n</script></span> matrix of partial derivatives <span><span class=MathJax_Preview>(\frac{\partial \boldsymbol{f}}{\partial \boldsymbol{x}})_{ij} = \dfrac{\partial f_i}{\partial x_j}</span><script type=math/tex>(\frac{\partial \boldsymbol{f}}{\partial \boldsymbol{x}})_{ij} = \dfrac{\partial f_i}{\partial x_j}</script></span>. This matrix is also called Jacobian matrix.</p> </li> </ol> <div> <div class=MathJax_Preview>\begin{equation*} \frac{\partial \boldsymbol{f}}{\partial \boldsymbol{x}} = \begin{bmatrix} \dfrac{\partial \boldsymbol{f}}{\partial x_1} &amp; \cdots &amp; \dfrac{\partial \boldsymbol{f}}{\partial x_n} \end{bmatrix} = \begin{bmatrix} \dfrac{\partial f_1}{\partial x_1} &amp; \cdots &amp; \dfrac{\partial f_1}{\partial x_n}\\ \vdots &amp; \ddots &amp; \vdots\\ \dfrac{\partial f_m}{\partial x_1} &amp; \cdots &amp; \dfrac{\partial f_m}{\partial x_n} \end{bmatrix} \end{equation*}</div> <script type="math/tex; mode=display">\begin{equation*}
\frac{\partial \boldsymbol{f}}{\partial \boldsymbol{x}} 
= \begin{bmatrix}
\dfrac{\partial \boldsymbol{f}}{\partial x_1} & \cdots & \dfrac{\partial \boldsymbol{f}}{\partial x_n} 
\end{bmatrix}
= \begin{bmatrix}
\dfrac{\partial f_1}{\partial x_1} & \cdots & \dfrac{\partial f_1}{\partial x_n}\\
\vdots & \ddots & \vdots\\
\dfrac{\partial f_m}{\partial x_1} & \cdots & \dfrac{\partial f_m}{\partial x_n} \end{bmatrix}
\end{equation*}</script> </div> <h4 id=computing-gradients-with-the-chain-rule>Computing gradients with the chain rule<a class=headerlink href=#computing-gradients-with-the-chain-rule title="Permanent link">&para;</a></h4> <p>With these two rules we can calculate the partials. We will use <span><span class=MathJax_Preview>\frac{\partial s}{\partial \boldsymbol{b}}</span><script type=math/tex>\frac{\partial s}{\partial \boldsymbol{b}}</script></span> as an example.</p> <div> <div class=MathJax_Preview>\begin{equation*} \frac{\partial s}{\partial \boldsymbol{b}} = \frac{\partial s}{\partial \boldsymbol{h}} \frac{\partial \boldsymbol{h}}{\partial \boldsymbol{z}} \frac{\partial \boldsymbol{z}}{\partial \boldsymbol{b}} \end{equation*}</div> <script type="math/tex; mode=display">\begin{equation*}
\frac{\partial s}{\partial \boldsymbol{b}} = \frac{\partial s}{\partial \boldsymbol{h}} \frac{\partial \boldsymbol{h}}{\partial \boldsymbol{z}} \frac{\partial \boldsymbol{z}}{\partial \boldsymbol{b}}
\end{equation*}</script> </div> <ul> <li> <p><span><span class=MathJax_Preview>\frac{\partial s}{\partial \boldsymbol{h}} = \frac{\partial}{\partial \boldsymbol{h}}(\boldsymbol{u}^{\mathsf{T}}\boldsymbol{h}) = \boldsymbol{u}^{\mathsf{T}}</span><script type=math/tex>\frac{\partial s}{\partial \boldsymbol{h}} = \frac{\partial}{\partial \boldsymbol{h}}(\boldsymbol{u}^{\mathsf{T}}\boldsymbol{h}) = \boldsymbol{u}^{\mathsf{T}}</script></span></p> </li> <li> <p><span><span class=MathJax_Preview>\frac{\partial \boldsymbol{h}}{\partial \boldsymbol{z}} = \frac{\partial}{\partial \boldsymbol{z}}(f(\boldsymbol{z})) = \begin{pmatrix} f'(z_1) &amp; &amp; 0 \\ &amp; \ddots &amp; \\ 0 &amp; &amp; f'(z_n) \end{pmatrix} = \mathrm{diag} (f' (\boldsymbol{z}))</span><script type=math/tex>\frac{\partial \boldsymbol{h}}{\partial \boldsymbol{z}} = \frac{\partial}{\partial \boldsymbol{z}}(f(\boldsymbol{z})) = \begin{pmatrix}
f'(z_1) & & 0 \\
& \ddots & \\
0 & & f'(z_n)
\end{pmatrix} = \mathrm{diag} (f' (\boldsymbol{z}))</script></span></p> </li> <li> <p><span><span class=MathJax_Preview>\frac{\partial \boldsymbol{z}}{\partial \boldsymbol{b}} = \frac{\partial}{\partial \boldsymbol{b}}(\boldsymbol{W}\boldsymbol{x} + \boldsymbol{b}) = \boldsymbol{I}</span><script type=math/tex>\frac{\partial \boldsymbol{z}}{\partial \boldsymbol{b}} = \frac{\partial}{\partial \boldsymbol{b}}(\boldsymbol{W}\boldsymbol{x} + \boldsymbol{b}) = \boldsymbol{I}</script></span></p> </li> </ul> <p>Therefore,</p> <div> <div class=MathJax_Preview>\begin{equation*} \frac{\partial s}{\partial \boldsymbol{b}} = \boldsymbol{u}^{\mathsf{T}} \mathrm{diag}(f'(\boldsymbol{z})) \boldsymbol{I} = \boldsymbol{u}^{\mathsf{T}} \circ f'(\boldsymbol{z}) \end{equation*}</div> <script type="math/tex; mode=display">\begin{equation*}
\frac{\partial s}{\partial \boldsymbol{b}} = \boldsymbol{u}^{\mathsf{T}} \mathrm{diag}(f'(\boldsymbol{z})) \boldsymbol{I} = \boldsymbol{u}^{\mathsf{T}} \circ f'(\boldsymbol{z})
\end{equation*}</script> </div> <p>Similarly we can calculate the partial with respect to <span><span class=MathJax_Preview>\boldsymbol{W}</span><script type=math/tex>\boldsymbol{W}</script></span> and <span><span class=MathJax_Preview>\boldsymbol{x}</span><script type=math/tex>\boldsymbol{x}</script></span>. Since the top layer partials are already calculated, we can reuese the results. We denote those reusable partials as <span><span class=MathJax_Preview>\delta</span><script type=math/tex>\delta</script></span> meaning local error signal.</p> <div> <div class=MathJax_Preview>\begin{align*} \frac{\partial s}{\partial \boldsymbol{W}} &amp; = \frac{\partial s}{\partial \boldsymbol{h}} \frac{\partial \boldsymbol{h}}{\partial \boldsymbol{z}} \frac{\partial \boldsymbol{z}}{\partial \boldsymbol{W}} \\ \\ \frac{\partial s}{\partial \boldsymbol{x}} &amp; = \frac{\partial s}{\partial \boldsymbol{h}} \frac{\partial \boldsymbol{h}}{\partial \boldsymbol{z}} \frac{\partial \boldsymbol{z}}{\partial \boldsymbol{x}} \\ \\ \frac{\partial s}{\partial \boldsymbol{b}} &amp; = \underbrace{\frac{\partial s}{\partial \boldsymbol{h}} \frac{\partial \boldsymbol{h}}{\partial \boldsymbol{z}}}_{\mathrm{reuse}} \frac{\partial \boldsymbol{z}}{\partial \boldsymbol{b}} = \boldsymbol{\delta} \frac{\partial \boldsymbol{z}}{\partial \boldsymbol{b}} = \boldsymbol{\delta} = \boldsymbol{u}^{\mathsf{T}} \circ f'(\boldsymbol{z}) \end{align*}</div> <script type="math/tex; mode=display">\begin{align*}
\frac{\partial s}{\partial \boldsymbol{W}} & =  \frac{\partial s}{\partial \boldsymbol{h}} \frac{\partial \boldsymbol{h}}{\partial \boldsymbol{z}} \frac{\partial \boldsymbol{z}}{\partial \boldsymbol{W}} \\ \\
\frac{\partial s}{\partial \boldsymbol{x}} & = \frac{\partial s}{\partial \boldsymbol{h}} \frac{\partial \boldsymbol{h}}{\partial \boldsymbol{z}} \frac{\partial \boldsymbol{z}}{\partial \boldsymbol{x}} \\ \\
\frac{\partial s}{\partial \boldsymbol{b}} & =  \underbrace{\frac{\partial s}{\partial \boldsymbol{h}}
\frac{\partial \boldsymbol{h}}{\partial \boldsymbol{z}}}_{\mathrm{reuse}} \frac{\partial \boldsymbol{z}}{\partial \boldsymbol{b}} = \boldsymbol{\delta} \frac{\partial \boldsymbol{z}}{\partial \boldsymbol{b}} = \boldsymbol{\delta} = \boldsymbol{u}^{\mathsf{T}} \circ f'(\boldsymbol{z})
\end{align*}</script> </div> <h4 id=shape-convention>Shape convention<a class=headerlink href=#shape-convention title="Permanent link">&para;</a></h4> <p>What does the shape of the derivatives looks like in practice? How can we make the chain rule computation efficient? According to the aforementioned gradient calculation rules, <span><span class=MathJax_Preview>\frac{\partial s}{\partial \boldsymbol{W}}</span><script type=math/tex>\frac{\partial s}{\partial \boldsymbol{W}}</script></span> is a row vector. The chain rule gave</p> <div> <div class=MathJax_Preview>\begin{equation*} \frac{\partial s}{\partial \boldsymbol{W}} = \boldsymbol{\delta} \frac{\partial \boldsymbol{z}}{\partial \boldsymbol{W}}. \end{equation*}</div> <script type="math/tex; mode=display">\begin{equation*}
\frac{\partial s}{\partial \boldsymbol{W}}  =  \boldsymbol{\delta} \frac{\partial \boldsymbol{z}}{\partial \boldsymbol{W}}.
\end{equation*}</script> </div> <p>We know from Jacobians that <span><span class=MathJax_Preview>\frac{\partial\boldsymbol{z}}{\partial \boldsymbol{W}} = \frac{\partial}{\partial \boldsymbol{W}}(\boldsymbol{W}\boldsymbol{x} + \boldsymbol{b}) = \boldsymbol{x}^{\mathsf{T}}</span><script type=math/tex>\frac{\partial\boldsymbol{z}}{\partial \boldsymbol{W}} = \frac{\partial}{\partial \boldsymbol{W}}(\boldsymbol{W}\boldsymbol{x} + \boldsymbol{b}) = \boldsymbol{x}^{\mathsf{T}}</script></span>. We may arrived the result that <span><span class=MathJax_Preview>\frac{\partial s}{\partial \boldsymbol{W}} = \boldsymbol{\delta} \ \boldsymbol{x}^{\mathsf{T}}</span><script type=math/tex>\frac{\partial s}{\partial \boldsymbol{W}}  =  \boldsymbol{\delta} \ \boldsymbol{x}^{\mathsf{T}}</script></span>. This is actually not quite wirte. The correct form should be</p> <div> <div class=MathJax_Preview>\begin{align*} \underbrace{\frac{\partial s}{\partial \boldsymbol{W}}}_{n \times m} = \underbrace{\boldsymbol{\delta}^{\mathsf{T}}}_{n \times 1} \ \underbrace{\boldsymbol{x}^{\mathsf{T}}}_{1 \times m}. \end{align*}</div> <script type="math/tex; mode=display">\begin{align*}
\underbrace{\frac{\partial s}{\partial \boldsymbol{W}}}_{n \times m}  = \underbrace{\boldsymbol{\delta}^{\mathsf{T}}}_{n \times 1} \ \underbrace{\boldsymbol{x}^{\mathsf{T}}}_{1 \times m}.
\end{align*}</script> </div> <div class="admonition note"> <p class=admonition-title>Note</p> <p>You may wonder why is this form instead of the one we derived directly. The explanation from the CS224N is that we would like to follow the shape convention so as to make the chain rule implementation more efficient (matrix multiplication instead of loops). Different from the Jacobian form, shape convention states that the shape of the gradient is the same shape of parameters. The resolution here is to use Jacobian form as much as possible and reshape to follow the convention at the end. Because Jacobian form makes the chain rule easy and the shape convention makes the implementation of gradient descent easy.</p> </div> <h3 id=computation-graphs-and-backpropagation>Computation graphs and backpropagation<a class=headerlink href=#computation-graphs-and-backpropagation title="Permanent link">&para;</a></h3> <p>We have shown how to compute the partial derivatives using the chain rule. This is almost the backpropagation algorithm. If we want to add on to that and make the algorithm complete, the only thing we need is how to reuse gradient computed for higher layers. We leverage the computation graph to explain this. From the computation graph, you'll get the intuition of reusing the partial derivatives computed for higher layers in computing derivatives for lower layers so as to minimize computation.</p> <p>We represent our neural net equations as a computation graph as following:</p> <p><img alt="Computation Graph" src=../fig/computation-graph.png></p> <ul> <li>source nodes represent inputs</li> <li>interior nodes represent operations</li> <li>edges pass along result of the operation</li> </ul> <p>When following computation order to carry out the computations from inputs, it is forward propagation. Backpropagation is to pass along gradients backwards. This can be illustrated in the following computation graph.</p> <p><img alt="Computation Graph Backprop" src=../fig/computation-graph-backprop.png></p> <p>The partial derivative with respect to a parameter reflect how changing the parameter would effect the output value. The output value in the backprop is usually a loss function (or error). Intuitively, you can see backprop is push the error back to the lower layer through a bunch of operation nodes, the arrived error is a measure of the error at that particular layer, training is try to reduce this backprop-ed error by adjusting the local parameters, the effect of reducing the local error will be forward propagated to the output, the error at the output should also reduced. We use gradient descent to make this process to converge as soon as possible.</p> <p>To understand it better, let's look at a single node.</p> <p><img alt="Local Gradient Chain Rule" src=../fig/local-gradient-chain-rule.png></p> <p>We define "local gradient" for each node as the gradient of it's output with respect to it's input. By chain rule, the downstream gradient is equal to the multiplication of the upstream gradient and the local gradient. When having multipe local gradients, they are pushed back to each input using chain rule. CS224N lecture 4 slides have step by step example of backprop. From the example, we can got some intuitions about some nodes' effects. For example, when push back gradients along outward branches from a single node, the gradients should be sumed up; "+" node distributes the upstream gradient to each summand; "max" node simply routes the upstream gradients.</p> <p>When update gradient with backprop, you should compute all the gradients at once. With this computation graph notion, following routine captures the gist of backprop in a very decent manner.</p> <blockquote> <ol> <li>Fprop: visit nodes in topological sorted order</li> <li>Compute value of node given predecessors</li> <li>Bprop:</li> <li>initialize output gradient = 1</li> <li>visit nodes in reverse order:<ul> <li>Compute gradient wrt each node using gradient wrt successors</li> <li><span><span class=MathJax_Preview>\{y_1, y_2, \cdots, y_2\} = \mathrm{successors\ of\ } x</span><script type=math/tex>\{y_1, y_2, \cdots, y_2\} = \mathrm{successors\ of\ } x</script></span></li> </ul> </li> </ol> <div> <div class=MathJax_Preview>\begin{equation*}\frac{\partial z}{\partial \boldsymbol{x}} = \sum_{i=i}^n \frac{\partial z}{\partial y_i} \frac{\partial y_i}{\partial x} \end{equation*}</div> <script type="math/tex; mode=display">\begin{equation*}\frac{\partial z}{\partial \boldsymbol{x}} = \sum_{i=i}^n \frac{\partial z}{\partial y_i} \frac{\partial y_i}{\partial x} \end{equation*}</script> </div> </blockquote> <p>If done correctly, the big <span><span class=MathJax_Preview>O</span><script type=math/tex>O</script></span> complexity of forward propagation and backpropagation is the same.</p> <h3 id=automatic-differentiation>Automatic differentiation<a class=headerlink href=#automatic-differentiation title="Permanent link">&para;</a></h3> <p>The gradient computation can be automatically inferred from the symbolic expression of the fprop. but this is not commonly used in practice. Each node type needs to know how to compute its output and how to compute the gradient wrt its inputs given the gradient wrt its output. Modern DL frameworks (Tensorflow, PyTorch, etc.) do backpropagation for you but mainly leave layer/node writer to hand-calculate the local derivative. Following is a simple demo of how to implement forward propagation and backpropagation.</p> <div class=highlight><pre><span></span><code><span class=k>class</span> <span class=nc>MultiplyGate</span><span class=p>(</span><span class=nb>object</span><span class=p>):</span>
    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>):</span>
        <span class=n>z</span> <span class=o>=</span> <span class=n>x</span> <span class=o>*</span> <span class=n>y</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>x</span> <span class=o>=</span> <span class=n>x</span> <span class=c1># must keep these around!</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>y</span> <span class=o>=</span> <span class=n>y</span>
        <span class=k>return</span> <span class=n>z</span>

    <span class=k>def</span> <span class=nf>backword</span><span class=p>(</span><span class=n>dz</span><span class=p>):</span>
        <span class=n>dx</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>y</span> <span class=o>*</span> <span class=n>dz</span> <span class=c1># [dz/dx * dL/dz]</span>
        <span class=n>dy</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>x</span> <span class=o>*</span> <span class=n>dz</span> <span class=c1># [dz/dy * dL/dz]</span>
        <span class=k>return</span> <span class=p>[</span><span class=n>dx</span><span class=p>,</span> <span class=n>dy</span><span class=p>]</span>
</code></pre></div> <ol> <li>nonlinear activation functions: sigmoid, tanh, hard tanh, ReLU</li> <li>Learning rates: start 0.001. power of 10. halve the learning rate every k epochs. formula: <span><span class=MathJax_Preview>lr = lr_0 e^{-k t}</span><script type=math/tex>lr = lr_0 e^{-k t}</script></span> for each epoch <span><span class=MathJax_Preview>t</span><script type=math/tex>t</script></span></li> </ol> <h3 id=regularization>Regularization<a class=headerlink href=#regularization title="Permanent link">&para;</a></h3> <p>LFD</p> <ol> <li>starting point: L2 regularization</li> </ol> <h2 id=part-3-language-model-and-rnn>Part 3 Language Model and RNN<a class=headerlink href=#part-3-language-model-and-rnn title="Permanent link">&para;</a></h2> <h3 id=n-gram>n-gram<a class=headerlink href=#n-gram title="Permanent link">&para;</a></h3> <ul> <li>assumption: <span><span class=MathJax_Preview>\boldsymbol{x}^{(t+1)}</span><script type=math/tex>\boldsymbol{x}^{(t+1)}</script></span> depends only on previous <span><span class=MathJax_Preview>n-1</span><script type=math/tex>n-1</script></span> words.</li> <li>Sparsity problem, --&gt; backoff (n-1)-gram</li> <li>practically, n has to be less than 5.</li> </ul> <h3 id=neural-language-model>Neural language model<a class=headerlink href=#neural-language-model title="Permanent link">&para;</a></h3> <ul> <li>fixed-window neural language model</li> <li>Recurrent Neural Network</li> </ul> </article> </div> </div> </main> <footer class=md-footer> <div class=md-footer-nav> <nav class="md-footer-nav__inner md-grid" aria-label=Footer> <a href=../lec-notes/ class="md-footer-nav__link md-footer-nav__link--prev" rel=prev> <div class="md-footer-nav__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </div> <div class=md-footer-nav__title> <div class=md-ellipsis> <span class=md-footer-nav__direction> Previous </span> CS224N Lecture Notes </div> </div> </a> <a href=../../coursera-dl4-cnn/notes/ class="md-footer-nav__link md-footer-nav__link--next" rel=next> <div class=md-footer-nav__title> <div class=md-ellipsis> <span class=md-footer-nav__direction> Next </span> Convolutional Neural Networks </div> </div> <div class="md-footer-nav__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg> </div> </a> </nav> </div> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-footer-copyright> <div class=md-footer-copyright__highlight> Copyright &copy; 2020 Rui Han </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> </div> </div> </footer> </div> <script src=../../../assets/javascripts/vendor.77e55a48.min.js></script> <script src=../../../assets/javascripts/bundle.9554a270.min.js></script><script id=__lang type=application/json>{"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing"}</script> <script>
        app = initialize({
          base: "../../..",
          features: ['navigation.tabs'],
          search: Object.assign({
            worker: "../../../assets/javascripts/worker/search.4ac00218.min.js"
          }, typeof search !== "undefined" && search)
        })
      </script> <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script> </body> </html>