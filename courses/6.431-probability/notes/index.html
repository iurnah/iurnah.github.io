<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><link href=https://ruihan.org/courses/6.431-probability/notes/ rel=canonical><meta name=lang:clipboard.copy content="Copy to clipboard"><meta name=lang:clipboard.copied content="Copied to clipboard"><meta name=lang:search.language content=en><meta name=lang:search.pipeline.stopwords content=True><meta name=lang:search.pipeline.trimmer content=True><meta name=lang:search.result.none content="No matching documents"><meta name=lang:search.result.one content="1 matching document"><meta name=lang:search.result.other content="# matching documents"><meta name=lang:search.tokenizer content=[\s\-]+><link rel="shortcut icon" href=../../../assets/images/favicon.png><meta name=generator content="mkdocs-1.1, mkdocs-material-4.6.3"><title>6.431 Probability - RUIHAN.ORG</title><link rel=stylesheet href=../../../assets/stylesheets/application.adb8469c.css><link rel=stylesheet href=../../../assets/stylesheets/application-palette.a8b3c06d.css><meta name=theme-color content><script src=../../../assets/javascripts/modernizr.86422ebf.js></script><link href=https://fonts.gstatic.com rel=preconnect crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback"><style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style><link rel=stylesheet href=../../../assets/fonts/material-icons.css></head> <body dir=ltr data-md-color-primary=black data-md-color-accent=black> <svg class=md-svg> <defs> <svg xmlns=http://www.w3.org/2000/svg width=416 height=448 viewbox="0 0 416 448" id=__github><path fill=currentColor d="M160 304q0 10-3.125 20.5t-10.75 19T128 352t-18.125-8.5-10.75-19T96 304t3.125-20.5 10.75-19T128 256t18.125 8.5 10.75 19T160 304zm160 0q0 10-3.125 20.5t-10.75 19T288 352t-18.125-8.5-10.75-19T256 304t3.125-20.5 10.75-19T288 256t18.125 8.5 10.75 19T320 304zm40 0q0-30-17.25-51T296 232q-10.25 0-48.75 5.25Q229.5 240 208 240t-39.25-2.75Q130.75 232 120 232q-29.5 0-46.75 21T56 304q0 22 8 38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0 37.25-1.75t35-7.375 30.5-15 20.25-25.75T360 304zm56-44q0 51.75-15.25 82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5T212 416q-19.5 0-35.5-.75t-36.875-3.125-38.125-7.5-34.25-12.875T37 371.5t-21.5-28.75Q0 312 0 260q0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25 30.875Q171.5 96 212 96q37 0 70 8 26.25-20.5 46.75-30.25T376 64q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34 99.5z"/></svg> </defs> </svg> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay data-md-component=overlay for=__drawer></label> <a href=#60416431-probability-the-science-of-uncertainty-and-data tabindex=0 class=md-skip> Skip to content </a> <header class=md-header data-md-component=header> <nav class="md-header-nav md-grid"> <div class=md-flex> <div class="md-flex__cell md-flex__cell--shrink"> <a href=https://ruihan.org title=RUIHAN.ORG aria-label=RUIHAN.ORG class="md-header-nav__button md-logo"> <i class=md-icon></i> </a> </div> <div class="md-flex__cell md-flex__cell--shrink"> <label class="md-icon md-icon--menu md-header-nav__button" for=__drawer></label> </div> <div class="md-flex__cell md-flex__cell--stretch"> <div class="md-flex__ellipsis md-header-nav__title" data-md-component=title> <span class=md-header-nav__topic> RUIHAN.ORG </span> <span class=md-header-nav__topic> 6.431 Probability </span> </div> </div> <div class="md-flex__cell md-flex__cell--shrink"> <label class="md-icon md-icon--search md-header-nav__button" for=__search></label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input aria-label=search name=query placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=query data-md-state=active> <label class="md-icon md-search__icon" for=__search></label> <button type=reset class="md-icon md-search__icon" data-md-component=reset tabindex=-1> &#xE5CD; </button> </form> <div class=md-search__output> <div class=md-search__scrollwrap data-md-scrollfix> <div class=md-search-result data-md-component=result> <div class=md-search-result__meta> Type to start searching </div> <ol class=md-search-result__list></ol> </div> </div> </div> </div> </div> </div> <div class="md-flex__cell md-flex__cell--shrink"> <div class=md-header-nav__source> <a href=https://github.com/iurnah/ruihan.org title="Go to repository" class=md-source data-md-source=github> <div class=md-source__icon> <svg viewbox="0 0 24 24" width=24 height=24> <use xlink:href=#__github width=24 height=24></use> </svg> </div> <div class=md-source__repository> iurnah/ruihan.org </div> </a> </div> </div> </div> </nav> </header> <div class=md-container> <nav class="md-tabs md-tabs--active" data-md-component=tabs> <div class="md-tabs__inner md-grid"> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../../../leetcode/binary-search/notes/ class=md-tabs__link> Leetcode </a> </li> <li class=md-tabs__item> <a href=../../cs224n/notes/ class="md-tabs__link md-tabs__link--active"> Course Notes </a> </li> <li class=md-tabs__item> <a href=../../../seedlabs/public-key-cryptography-and-pki/notes/ class=md-tabs__link> SEED Labs </a> </li> </ul> </div> </nav> <main class=md-main role=main> <div class="md-main__inner md-grid" data-md-component=container> <div class="md-sidebar md-sidebar--primary" data-md-component=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary" data-md-level=0> <label class="md-nav__title md-nav__title--site" for=__drawer> <a href=https://ruihan.org title=RUIHAN.ORG class="md-nav__button md-logo"> <i class=md-icon></i> </a> RUIHAN.ORG </label> <div class=md-nav__source> <a href=https://github.com/iurnah/ruihan.org title="Go to repository" class=md-source data-md-source=github> <div class=md-source__icon> <svg viewbox="0 0 24 24" width=24 height=24> <use xlink:href=#__github width=24 height=24></use> </svg> </div> <div class=md-source__repository> iurnah/ruihan.org </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-toggle md-nav__toggle" data-md-toggle=nav-1 type=checkbox id=nav-1> <label class=md-nav__link for=nav-1> Leetcode </label> <nav class=md-nav data-md-component=collapsible data-md-level=1> <label class=md-nav__title for=nav-1> Leetcode </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../leetcode/binary-search/notes/ title="Binary Search" class=md-nav__link> Binary Search </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-toggle md-nav__toggle" data-md-toggle=nav-2 type=checkbox id=nav-2 checked> <label class=md-nav__link for=nav-2> Course Notes </label> <nav class=md-nav data-md-component=collapsible data-md-level=1> <label class=md-nav__title for=nav-2> Course Notes </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../cs224n/notes/ title="CS224N NLP with Deep Learning" class=md-nav__link> CS224N NLP with Deep Learning </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-toggle md-nav__toggle" data-md-toggle=toc type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> 6.431 Probability </label> <a href=./ title="6.431 Probability" class="md-nav__link md-nav__link--active"> 6.431 Probability </a> <nav class="md-nav md-nav--secondary"> <label class=md-nav__title for=__toc>Table of contents</label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=#lecture-1-september-10-2018 class=md-nav__link> Lecture 1 (September 10, 2018) </a> <nav class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a href=#probability-models class=md-nav__link> Probability Models </a> </li> <li class=md-nav__item> <a href=#sample-space class=md-nav__link> Sample space </a> </li> <li class=md-nav__item> <a href=#probability-laws class=md-nav__link> Probability laws </a> <nav class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a href=#probability-axioms class=md-nav__link> Probability axioms </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#consequences-of-axioms-properties-of-probability-axioms class=md-nav__link> Consequences of axioms (properties of probability axioms) </a> </li> <li class=md-nav__item> <a href=#probability-calculation-steps class=md-nav__link> Probability calculation steps </a> </li> <li class=md-nav__item> <a href=#countable-additivity class=md-nav__link> Countable additivity </a> </li> <li class=md-nav__item> <a href=#countable-additivity-axiom-refined-probability-axiom class=md-nav__link> Countable Additivity Axiom (refined probability axiom) </a> </li> <li class=md-nav__item> <a href=#interpretations-of-probability-theory class=md-nav__link> Interpretations of probability theory </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#lecture-2-conditioning-and-bayes-rule class=md-nav__link> Lecture 2 Conditioning and Baye's rule </a> <nav class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a href=#conditional-probability class=md-nav__link> Conditional probability </a> </li> <li class=md-nav__item> <a href=#multiplication-rule class=md-nav__link> Multiplication rule </a> </li> <li class=md-nav__item> <a href=#total-probability-theorem-divide-and-conquer class=md-nav__link> Total probability theorem (divide and conquer) </a> </li> <li class=md-nav__item> <a href=#bayes-rule class=md-nav__link> Baye's rule </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#lecture-3-independence class=md-nav__link> Lecture 3 Independence </a> <nav class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a href=#independence-of-two-events class=md-nav__link> Independence of two events </a> </li> <li class=md-nav__item> <a href=#definition-of-independence class=md-nav__link> Definition of independence </a> </li> <li class=md-nav__item> <a href=#independence-of-event-complements class=md-nav__link> Independence of event complements </a> </li> <li class=md-nav__item> <a href=#conditioning-independence class=md-nav__link> Conditioning independence </a> </li> <li class=md-nav__item> <a href=#independence-of-a-collection-of-events class=md-nav__link> Independence of a collection of events </a> </li> <li class=md-nav__item> <a href=#pairwise-independence class=md-nav__link> pairwise independence </a> <nav class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a href=#independence-vs-pairwise-independence class=md-nav__link> independence V.S. pairwise independence </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#reliability class=md-nav__link> Reliability </a> </li> <li class=md-nav__item> <a href=#monty-hall-problem class=md-nav__link> Monty Hall problem </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#lecture-4 class=md-nav__link> Lecture 4 </a> </li> <li class=md-nav__item> <a href=#lecture-5 class=md-nav__link> Lecture 5 </a> <nav class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a href=#definition-of-random-variables class=md-nav__link> Definition of random variables </a> </li> <li class=md-nav__item> <a href=#bernoulli-and-indicator-random-variables class=md-nav__link> Bernoulli and indicator random variables </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#lecture-6 class=md-nav__link> Lecture 6 </a> </li> <li class=md-nav__item> <a href=#lecture-7 class=md-nav__link> Lecture 7 </a> <nav class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a href=#independence-variances-and-binomial-variance class=md-nav__link> Independence, variances, and binomial variance </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#lecture-8 class=md-nav__link> Lecture 8 </a> <nav class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a href=#exponential-random-variables class=md-nav__link> Exponential random variables </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#lecture-9 class=md-nav__link> Lecture 9 </a> <nav class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a href=#conditioning-a-continuous-random-variable-on-an-event class=md-nav__link> Conditioning a continuous random variable on an event </a> </li> <li class=md-nav__item> <a href=#memorylessness-of-the-exponential-pdf class=md-nav__link> Memorylessness of the exponential PDF </a> </li> <li class=md-nav__item> <a href=#total-probability-and-expectation-theorems class=md-nav__link> Total probability and expectation theorems </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#lecture-10 class=md-nav__link> Lecture 10 </a> <nav class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a href=#total-probability-and-total-expectation-theorems class=md-nav__link> Total probability and total expectation theorems </a> </li> <li class=md-nav__item> <a href=#solved-problems-lecture-8-10 class=md-nav__link> Solved problems (Lecture 8 - 10) </a> <nav class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a href=#10-buffons-needle-and-monte-carlo-simulation class=md-nav__link> 10 Buffon's needle and Monte Carlo Simulation </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#lecture-11 class=md-nav__link> Lecture 11 </a> </li> <li class=md-nav__item> <a href=#lecture-12 class=md-nav__link> Lecture 12 </a> <nav class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a href=#covariance class=md-nav__link> Covariance </a> </li> <li class=md-nav__item> <a href=#covariance-properties class=md-nav__link> Covariance properties </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#lecture-13 class=md-nav__link> Lecture 13 </a> <nav class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a href=#the-conditional-expectation-as-a-random-variable-bf-e-bigx-mid-ybigbf-e-bigx-mid-ybig class=md-nav__link> The conditional expectation as a random variable {\bf E} \big[X \mid Y\big]{\bf E} \big[X \mid Y\big] </a> </li> <li class=md-nav__item> <a href=#the-law-of-iterated-expectations class=md-nav__link> The law of iterated expectations </a> </li> <li class=md-nav__item> <a href=#the-conditional-variance-as-a-random-variable-textbfvarx-mid-y-ytextbfvarx-mid-y-y class=md-nav__link> The conditional variance as a random variable \textbf{Var}(X \mid Y = y)\textbf{Var}(X \mid Y = y) </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#lecture-14-introduction-to-bayesian-inference class=md-nav__link> Lecture 14 Introduction to Bayesian Inference </a> <nav class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a href=#the-bayesian-inference-framework class=md-nav__link> The Bayesian inference framework </a> </li> <li class=md-nav__item> <a href=#conditional-probability-of-error-and-total-probability-of-error class=md-nav__link> Conditional probability of error and total probability of error </a> </li> <li class=md-nav__item> <a href=#discrete-unknowns-discrete-observation-example-8-exercise-discrete-unknowns class=md-nav__link> Discrete unknowns, discrete observation example (8. Exercise: Discrete unknowns) </a> </li> <li class=md-nav__item> <a href=#discrete-parameter-continuous-observation class=md-nav__link> Discrete parameter, continuous observation </a> </li> <li class=md-nav__item> <a href=#continous-parameter-continuous-observation class=md-nav__link> continous parameter, continuous observation </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#lecture-15-linear-models-with-normal-noise class=md-nav__link> Lecture 15 Linear models with normal noise </a> <nav class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a href=#recognizing-normal-pdfs class=md-nav__link> recognizing normal PDFs </a> </li> <li class=md-nav__item> <a href=#the-mean-squared-error class=md-nav__link> The mean squared error </a> </li> <li class=md-nav__item> <a href=#measurement-estimate-and-learning class=md-nav__link> Measurement, estimate, and learning </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#lecture-16-least-mean-square-lms-estimation class=md-nav__link> Lecture 16 Least mean square (LMS) estimation </a> <nav class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a href=#lms-estimation-without-any-observations class=md-nav__link> LMS estimation without any observations </a> </li> <li class=md-nav__item> <a href=#lms-estimation-single-unknown-and-observation class=md-nav__link> LMS estimation; single unknown and observation </a> </li> <li class=md-nav__item> <a href=#lms-performance-evaluation class=md-nav__link> LMS performance evaluation </a> </li> <li class=md-nav__item> <a href=#the-multidimensional-case class=md-nav__link> The multidimensional case </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#lecture-17-linear-least-mean-squares-llms-estimation class=md-nav__link> Lecture 17 Linear least mean squares (LLMS) estimation </a> </li> <li class=md-nav__item> <a href=#lecture-18-inequalities-convergence-and-the-weak-law-of-large-numbers class=md-nav__link> Lecture 18 Inequalities, convergence, and the Weak Law of Large Numbers </a> <nav class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a href=#the-weak-law-of-large-numbers class=md-nav__link> The Weak Law of Large Numbers </a> </li> <li class=md-nav__item> <a href=#interpreting-the-wlln class=md-nav__link> Interpreting the WLLN </a> </li> <li class=md-nav__item> <a href=#application-of-wlln-polling class=md-nav__link> Application of WLLN - polling </a> </li> <li class=md-nav__item> <a href=#convergence-in-probability class=md-nav__link> Convergence in probability </a> </li> <li class=md-nav__item> <a href=#convergence-in-probability-examples class=md-nav__link> Convergence in probability examples </a> </li> <li class=md-nav__item> <a href=#related-topics class=md-nav__link> Related topics </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#lecture-19-the-central-limit-theorem-clt class=md-nav__link> Lecture 19 The Central Limit Theorem (CLT) </a> <nav class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a href=#what-exactly-does-the-clt-say-practice class=md-nav__link> What exactly does the CLT say? - Practice </a> </li> <li class=md-nav__item> <a href=#central-limit-theory-examples class=md-nav__link> Central Limit Theory examples </a> <nav class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a href=#airline-booking class=md-nav__link> Airline booking </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#normal-approximation-to-the-binomial class=md-nav__link> Normal approximation to the binomial </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#lecture-20-introduction-to-classical-statistics class=md-nav__link> Lecture 20 Introduction to classical statistics </a> <nav class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a href=#overview-of-the-classical-statistical-framework class=md-nav__link> Overview of the classical statistical framework </a> </li> <li class=md-nav__item> <a href=#confidence-intervals-interpretation class=md-nav__link> Confidence intervals interpretation </a> <nav class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a href=#classical-statsistic-interpretation class=md-nav__link> Classical statsistic interpretation </a> </li> <li class=md-nav__item> <a href=#bayes-interpretation-bayesians-confidence-interval class=md-nav__link> Bayes' interpretation (Bayesian's Confidence Interval) </a> </li> <li class=md-nav__item> <a href=#confidence-intervals-for-the-estimation-of-the-mean class=md-nav__link> Confidence intervals for the estimation of the mean </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#lecture-21 class=md-nav__link> Lecture 21 </a> </li> <li class=md-nav__item> <a href=#lecture-22 class=md-nav__link> Lecture 22 </a> </li> <li class=md-nav__item> <a href=#lecture-23 class=md-nav__link> Lecture 23 </a> </li> <li class=md-nav__item> <a href=#lecture-24 class=md-nav__link> Lecture 24 </a> </li> <li class=md-nav__item> <a href=#lecture-25 class=md-nav__link> Lecture 25 </a> </li> <li class=md-nav__item> <a href=#lecture-26 class=md-nav__link> Lecture 26 </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-toggle md-nav__toggle" data-md-toggle=nav-3 type=checkbox id=nav-3> <label class=md-nav__link for=nav-3> SEED Labs </label> <nav class=md-nav data-md-component=collapsible data-md-level=1> <label class=md-nav__title for=nav-3> SEED Labs </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../seedlabs/public-key-cryptography-and-pki/notes/ title="Public Key Cryptography and PKI" class=md-nav__link> Public Key Cryptography and PKI </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary"> <label class=md-nav__title for=__toc>Table of contents</label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=#lecture-1-september-10-2018 class=md-nav__link> Lecture 1 (September 10, 2018) </a> <nav class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a href=#probability-models class=md-nav__link> Probability Models </a> </li> <li class=md-nav__item> <a href=#sample-space class=md-nav__link> Sample space </a> </li> <li class=md-nav__item> <a href=#probability-laws class=md-nav__link> Probability laws </a> <nav class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a href=#probability-axioms class=md-nav__link> Probability axioms </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#consequences-of-axioms-properties-of-probability-axioms class=md-nav__link> Consequences of axioms (properties of probability axioms) </a> </li> <li class=md-nav__item> <a href=#probability-calculation-steps class=md-nav__link> Probability calculation steps </a> </li> <li class=md-nav__item> <a href=#countable-additivity class=md-nav__link> Countable additivity </a> </li> <li class=md-nav__item> <a href=#countable-additivity-axiom-refined-probability-axiom class=md-nav__link> Countable Additivity Axiom (refined probability axiom) </a> </li> <li class=md-nav__item> <a href=#interpretations-of-probability-theory class=md-nav__link> Interpretations of probability theory </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#lecture-2-conditioning-and-bayes-rule class=md-nav__link> Lecture 2 Conditioning and Baye's rule </a> <nav class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a href=#conditional-probability class=md-nav__link> Conditional probability </a> </li> <li class=md-nav__item> <a href=#multiplication-rule class=md-nav__link> Multiplication rule </a> </li> <li class=md-nav__item> <a href=#total-probability-theorem-divide-and-conquer class=md-nav__link> Total probability theorem (divide and conquer) </a> </li> <li class=md-nav__item> <a href=#bayes-rule class=md-nav__link> Baye's rule </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#lecture-3-independence class=md-nav__link> Lecture 3 Independence </a> <nav class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a href=#independence-of-two-events class=md-nav__link> Independence of two events </a> </li> <li class=md-nav__item> <a href=#definition-of-independence class=md-nav__link> Definition of independence </a> </li> <li class=md-nav__item> <a href=#independence-of-event-complements class=md-nav__link> Independence of event complements </a> </li> <li class=md-nav__item> <a href=#conditioning-independence class=md-nav__link> Conditioning independence </a> </li> <li class=md-nav__item> <a href=#independence-of-a-collection-of-events class=md-nav__link> Independence of a collection of events </a> </li> <li class=md-nav__item> <a href=#pairwise-independence class=md-nav__link> pairwise independence </a> <nav class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a href=#independence-vs-pairwise-independence class=md-nav__link> independence V.S. pairwise independence </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#reliability class=md-nav__link> Reliability </a> </li> <li class=md-nav__item> <a href=#monty-hall-problem class=md-nav__link> Monty Hall problem </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#lecture-4 class=md-nav__link> Lecture 4 </a> </li> <li class=md-nav__item> <a href=#lecture-5 class=md-nav__link> Lecture 5 </a> <nav class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a href=#definition-of-random-variables class=md-nav__link> Definition of random variables </a> </li> <li class=md-nav__item> <a href=#bernoulli-and-indicator-random-variables class=md-nav__link> Bernoulli and indicator random variables </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#lecture-6 class=md-nav__link> Lecture 6 </a> </li> <li class=md-nav__item> <a href=#lecture-7 class=md-nav__link> Lecture 7 </a> <nav class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a href=#independence-variances-and-binomial-variance class=md-nav__link> Independence, variances, and binomial variance </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#lecture-8 class=md-nav__link> Lecture 8 </a> <nav class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a href=#exponential-random-variables class=md-nav__link> Exponential random variables </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#lecture-9 class=md-nav__link> Lecture 9 </a> <nav class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a href=#conditioning-a-continuous-random-variable-on-an-event class=md-nav__link> Conditioning a continuous random variable on an event </a> </li> <li class=md-nav__item> <a href=#memorylessness-of-the-exponential-pdf class=md-nav__link> Memorylessness of the exponential PDF </a> </li> <li class=md-nav__item> <a href=#total-probability-and-expectation-theorems class=md-nav__link> Total probability and expectation theorems </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#lecture-10 class=md-nav__link> Lecture 10 </a> <nav class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a href=#total-probability-and-total-expectation-theorems class=md-nav__link> Total probability and total expectation theorems </a> </li> <li class=md-nav__item> <a href=#solved-problems-lecture-8-10 class=md-nav__link> Solved problems (Lecture 8 - 10) </a> <nav class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a href=#10-buffons-needle-and-monte-carlo-simulation class=md-nav__link> 10 Buffon's needle and Monte Carlo Simulation </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#lecture-11 class=md-nav__link> Lecture 11 </a> </li> <li class=md-nav__item> <a href=#lecture-12 class=md-nav__link> Lecture 12 </a> <nav class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a href=#covariance class=md-nav__link> Covariance </a> </li> <li class=md-nav__item> <a href=#covariance-properties class=md-nav__link> Covariance properties </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#lecture-13 class=md-nav__link> Lecture 13 </a> <nav class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a href=#the-conditional-expectation-as-a-random-variable-bf-e-bigx-mid-ybigbf-e-bigx-mid-ybig class=md-nav__link> The conditional expectation as a random variable {\bf E} \big[X \mid Y\big]{\bf E} \big[X \mid Y\big] </a> </li> <li class=md-nav__item> <a href=#the-law-of-iterated-expectations class=md-nav__link> The law of iterated expectations </a> </li> <li class=md-nav__item> <a href=#the-conditional-variance-as-a-random-variable-textbfvarx-mid-y-ytextbfvarx-mid-y-y class=md-nav__link> The conditional variance as a random variable \textbf{Var}(X \mid Y = y)\textbf{Var}(X \mid Y = y) </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#lecture-14-introduction-to-bayesian-inference class=md-nav__link> Lecture 14 Introduction to Bayesian Inference </a> <nav class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a href=#the-bayesian-inference-framework class=md-nav__link> The Bayesian inference framework </a> </li> <li class=md-nav__item> <a href=#conditional-probability-of-error-and-total-probability-of-error class=md-nav__link> Conditional probability of error and total probability of error </a> </li> <li class=md-nav__item> <a href=#discrete-unknowns-discrete-observation-example-8-exercise-discrete-unknowns class=md-nav__link> Discrete unknowns, discrete observation example (8. Exercise: Discrete unknowns) </a> </li> <li class=md-nav__item> <a href=#discrete-parameter-continuous-observation class=md-nav__link> Discrete parameter, continuous observation </a> </li> <li class=md-nav__item> <a href=#continous-parameter-continuous-observation class=md-nav__link> continous parameter, continuous observation </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#lecture-15-linear-models-with-normal-noise class=md-nav__link> Lecture 15 Linear models with normal noise </a> <nav class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a href=#recognizing-normal-pdfs class=md-nav__link> recognizing normal PDFs </a> </li> <li class=md-nav__item> <a href=#the-mean-squared-error class=md-nav__link> The mean squared error </a> </li> <li class=md-nav__item> <a href=#measurement-estimate-and-learning class=md-nav__link> Measurement, estimate, and learning </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#lecture-16-least-mean-square-lms-estimation class=md-nav__link> Lecture 16 Least mean square (LMS) estimation </a> <nav class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a href=#lms-estimation-without-any-observations class=md-nav__link> LMS estimation without any observations </a> </li> <li class=md-nav__item> <a href=#lms-estimation-single-unknown-and-observation class=md-nav__link> LMS estimation; single unknown and observation </a> </li> <li class=md-nav__item> <a href=#lms-performance-evaluation class=md-nav__link> LMS performance evaluation </a> </li> <li class=md-nav__item> <a href=#the-multidimensional-case class=md-nav__link> The multidimensional case </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#lecture-17-linear-least-mean-squares-llms-estimation class=md-nav__link> Lecture 17 Linear least mean squares (LLMS) estimation </a> </li> <li class=md-nav__item> <a href=#lecture-18-inequalities-convergence-and-the-weak-law-of-large-numbers class=md-nav__link> Lecture 18 Inequalities, convergence, and the Weak Law of Large Numbers </a> <nav class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a href=#the-weak-law-of-large-numbers class=md-nav__link> The Weak Law of Large Numbers </a> </li> <li class=md-nav__item> <a href=#interpreting-the-wlln class=md-nav__link> Interpreting the WLLN </a> </li> <li class=md-nav__item> <a href=#application-of-wlln-polling class=md-nav__link> Application of WLLN - polling </a> </li> <li class=md-nav__item> <a href=#convergence-in-probability class=md-nav__link> Convergence in probability </a> </li> <li class=md-nav__item> <a href=#convergence-in-probability-examples class=md-nav__link> Convergence in probability examples </a> </li> <li class=md-nav__item> <a href=#related-topics class=md-nav__link> Related topics </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#lecture-19-the-central-limit-theorem-clt class=md-nav__link> Lecture 19 The Central Limit Theorem (CLT) </a> <nav class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a href=#what-exactly-does-the-clt-say-practice class=md-nav__link> What exactly does the CLT say? - Practice </a> </li> <li class=md-nav__item> <a href=#central-limit-theory-examples class=md-nav__link> Central Limit Theory examples </a> <nav class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a href=#airline-booking class=md-nav__link> Airline booking </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#normal-approximation-to-the-binomial class=md-nav__link> Normal approximation to the binomial </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#lecture-20-introduction-to-classical-statistics class=md-nav__link> Lecture 20 Introduction to classical statistics </a> <nav class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a href=#overview-of-the-classical-statistical-framework class=md-nav__link> Overview of the classical statistical framework </a> </li> <li class=md-nav__item> <a href=#confidence-intervals-interpretation class=md-nav__link> Confidence intervals interpretation </a> <nav class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a href=#classical-statsistic-interpretation class=md-nav__link> Classical statsistic interpretation </a> </li> <li class=md-nav__item> <a href=#bayes-interpretation-bayesians-confidence-interval class=md-nav__link> Bayes' interpretation (Bayesian's Confidence Interval) </a> </li> <li class=md-nav__item> <a href=#confidence-intervals-for-the-estimation-of-the-mean class=md-nav__link> Confidence intervals for the estimation of the mean </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#lecture-21 class=md-nav__link> Lecture 21 </a> </li> <li class=md-nav__item> <a href=#lecture-22 class=md-nav__link> Lecture 22 </a> </li> <li class=md-nav__item> <a href=#lecture-23 class=md-nav__link> Lecture 23 </a> </li> <li class=md-nav__item> <a href=#lecture-24 class=md-nav__link> Lecture 24 </a> </li> <li class=md-nav__item> <a href=#lecture-25 class=md-nav__link> Lecture 25 </a> </li> <li class=md-nav__item> <a href=#lecture-26 class=md-nav__link> Lecture 26 </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content> <article class="md-content__inner md-typeset"> <h1 id=60416431-probability-the-science-of-uncertainty-and-data>6.041/6.431 Probability - The Science of Uncertainty and Data<a class=headerlink href=#60416431-probability-the-science-of-uncertainty-and-data title="Permanent link">&para;</a></h1> <h2 id=lecture-1-september-10-2018>Lecture 1 (September 10, 2018)<a class=headerlink href=#lecture-1-september-10-2018 title="Permanent link">&para;</a></h2> <h3 id=probability-models>Probability Models<a class=headerlink href=#probability-models title="Permanent link">&para;</a></h3> <p>To specify such a model, it includes 2 steps:</p> <ol> <li>Sample space: describe possible outcomes of an experiment</li> <li>Probability laws: describe beliefs about the likelihood of outcomes (or assign a probability to outcomes)</li> </ol> <h3 id=sample-space>Sample space<a class=headerlink href=#sample-space title="Permanent link">&para;</a></h3> <p>list of possible outcomes must be mutually exclusive, collectively exhaustive, and at the right granularity.</p> <h3 id=probability-laws>Probability laws<a class=headerlink href=#probability-laws title="Permanent link">&para;</a></h3> <h4 id=probability-axioms>Probability axioms<a class=headerlink href=#probability-axioms title="Permanent link">&para;</a></h4> <blockquote> <p>Motivation: Assign individual probability to a single dot is a challenge (in the dart example, each dot has zero probability), so we assign a probability to the subset of the sample space.</p> </blockquote> <ol> <li>Nonnegativity: <span><span class=MathJax_Preview>{\bf P}(A) \geq 0</span><script type=math/tex>{\bf P}(A) \geq 0</script></span></li> <li>Normalization: <span><span class=MathJax_Preview>{\bf P}(\Omega) = 1</span><script type=math/tex>{\bf P}(\Omega) = 1</script></span></li> <li>(Finite) Additivity: if <span><span class=MathJax_Preview>A \cap B = \emptyset</span><script type=math/tex>A \cap B = \emptyset</script></span> then <span><span class=MathJax_Preview>{\bf P}(A \cup B) = {\bf P}(A) + {\bf P}(B)</span><script type=math/tex>{\bf P}(A \cup B) = {\bf P}(A) + {\bf P}(B)</script></span> (need to be refined in future)</li> </ol> <h3 id=consequences-of-axioms-properties-of-probability-axioms>Consequences of axioms (properties of probability axioms)<a class=headerlink href=#consequences-of-axioms-properties-of-probability-axioms title="Permanent link">&para;</a></h3> <ul> <li><span><span class=MathJax_Preview>{\bf P}(A) \leq 1</span><script type=math/tex>{\bf P}(A) \leq 1</script></span></li> <li><span><span class=MathJax_Preview>{\bf P}(\emptyset) = 0</span><script type=math/tex>{\bf P}(\emptyset) = 0</script></span></li> <li><span><span class=MathJax_Preview>{\bf P}(A) + {\bf P}(A^c) = 1</span><script type=math/tex>{\bf P}(A) + {\bf P}(A^c) = 1</script></span></li> <li><span><span class=MathJax_Preview>A, B, \text{and } C</span><script type=math/tex>A, B, \text{and } C</script></span> are disjoint: <span><span class=MathJax_Preview>{\bf P}(A \cup B \cup C) = {\bf P}(A) + {\bf P}(B) + {\bf P}(C)</span><script type=math/tex>{\bf P}(A \cup B \cup C) = {\bf P}(A) + {\bf P}(B) + {\bf P}(C)</script></span></li> <li><span><span class=MathJax_Preview>A_1, \cdots A_k,</span><script type=math/tex>A_1, \cdots A_k,</script></span> are disjoint: <span><span class=MathJax_Preview>{\bf P}(A_1 \cup \cdots, \cup A_k) = \sum_{i - 1}^k{\bf P}(A_i)</span><script type=math/tex>{\bf P}(A_1 \cup \cdots, \cup A_k) = \sum_{i - 1}^k{\bf P}(A_i)</script></span></li> <li>If <span><span class=MathJax_Preview>A \subset B</span><script type=math/tex>A \subset B</script></span>, then <span><span class=MathJax_Preview>{\bf P}(A) \leq {\bf P}(B)</span><script type=math/tex>{\bf P}(A) \leq {\bf P}(B)</script></span></li> <li><span><span class=MathJax_Preview>{\bf P}(A \cup B) = {\bf P}(A) + {\bf P}(B) - {\bf P}(A \cap B)</span><script type=math/tex>{\bf P}(A \cup B) = {\bf P}(A) + {\bf P}(B) - {\bf P}(A \cap B)</script></span></li> <li><span><span class=MathJax_Preview>{\bf P}(A \cup B \cup C) = {\bf P}(A) + {\bf P}(A^c \cap B) + {\bf P}(A^c \cap B^c \cap C)</span><script type=math/tex>{\bf P}(A \cup B \cup C) = {\bf P}(A) + {\bf P}(A^c \cap B) + {\bf P}(A^c \cap B^c \cap C)</script></span></li> <li>Union Bound: <span><span class=MathJax_Preview>{\bf P}(A \cup B) \leq {\bf P}(A) + {\bf P}(B)</span><script type=math/tex>{\bf P}(A \cup B) \leq {\bf P}(A) + {\bf P}(B)</script></span></li> </ul> <h3 id=probability-calculation-steps>Probability calculation steps<a class=headerlink href=#probability-calculation-steps title="Permanent link">&para;</a></h3> <ol> <li>Specify the sample space</li> <li>Specify a probability law</li> <li>Identify an event of interest</li> <li>Calculate</li> </ol> <h3 id=countable-additivity>Countable additivity<a class=headerlink href=#countable-additivity title="Permanent link">&para;</a></h3> <p>The 3rd probability axiom previously mentioned can be extended to countable set (all integers), which means that the element in the set can be arranged in a sequence. This is contrary to the concept of uncountable set (i.e. a dart board, 2D plane). So you need to distinguish the following concepts:</p> <ul> <li>discrete set (countable set)</li> <li>continuous set (uncountable set, impossible to arrange in a sequence)</li> <li>finite set</li> <li>infinite set (possible to be arranged in a sequence)</li> <li>discrete (countable) finite set (i.e. {1, 2, 3})</li> <li>discrete (countable) infinite set (i.e. all integers, all even integers, all old integers)</li> <li>continuous (uncountable) infinite set (i.e. <span><span class=MathJax_Preview>\{x | 0 \leq x \leq 1\}</span><script type=math/tex>\{x | 0 \leq x \leq 1\}</script></span>)</li> </ul> <p>Example: Sample space <span><span class=MathJax_Preview>\{1, 2, 3, \cdots\}</span><script type=math/tex>\{1, 2, 3, \cdots\}</script></span>, given <span><span class=MathJax_Preview>{\bf P}(n) = \frac{1}{2^n}, n = 1, 2, \cdots</span><script type=math/tex>{\bf P}(n) = \frac{1}{2^n}, n = 1, 2, \cdots</script></span>. Check the against the probability axioms that <span><span class=MathJax_Preview>\sum_{n = 1}^{\infty}{\bf P}(n) = 1</span><script type=math/tex>\sum_{n = 1}^{\infty}{\bf P}(n) = 1</script></span>. What about the probability <span><span class=MathJax_Preview>{\bf P}(\text{outcome is even})</span><script type=math/tex>{\bf P}(\text{outcome is even})</script></span>? Using countable additivity axiom.</p> <h3 id=countable-additivity-axiom-refined-probability-axiom>Countable Additivity Axiom (refined probability axiom)<a class=headerlink href=#countable-additivity-axiom-refined-probability-axiom title="Permanent link">&para;</a></h3> <div class="admonition note"> <p class=admonition-title>Note</p> <p>if <span><span class=MathJax_Preview>A_1, A_2, A_3, \cdot</span><script type=math/tex>A_1, A_2, A_3, \cdot</script></span> is an infinite <strong>sequence</strong> of disjoint events, then <span><span class=MathJax_Preview>{\bf P}(A_1 \cup A_2 \cup A_3 \cdots) = {\bf P}(A_1) + {\bf P}(A_2) + {\bf P}(A_3) + \cdots</span><script type=math/tex>{\bf P}(A_1 \cup A_2 \cup A_3 \cdots) = {\bf P}(A_1) + {\bf P}(A_2) + {\bf P}(A_3) + \cdots</script></span></p> </div> <p>To refine the 3rd probability axiom, additivity holds only for "countable" sequences of events. That means the additivity axiom must be a <strong>sequence</strong>, with finite or infinite elements.</p> <p>Note the following contradiction, the additivity axiom can not be applied to continuous sample space.</p> <p><span><span class=MathJax_Preview>A_1, A_2, A_3, \cdots</span><script type=math/tex>A_1, A_2, A_3, \cdots</script></span> are real coordinates, if you apply the additivity axiom, you will get:</p> <div> <div class=MathJax_Preview>\begin{align*} 1={\bf P}(\Omega ) &amp;={\bf P}\big (\{ A_1\} \cup \{ A_2\} \cup \{ A_3\} \cdots \big )\\ &amp;={\bf P}(\{ A_1\} )+{\bf P}(\{ A_2\} )+{\bf P}(\{ A_3\} )+\cdots \\ &amp;= 0+0+0+\cdots =0, \end{align*}</div> <script type="math/tex; mode=display">\begin{align*}
1={\bf P}(\Omega ) &={\bf P}\big (\{ A_1\} \cup \{ A_2\} \cup \{ A_3\} \cdots \big )\\
&={\bf P}(\{ A_1\} )+{\bf P}(\{ A_2\} )+{\bf P}(\{ A_3\} )+\cdots \\
&= 0+0+0+\cdots =0,
\end{align*}</script> </div> <p>which is contradicts. This is because "unit square" is not a countable set.</p> <h3 id=interpretations-of-probability-theory>Interpretations of probability theory<a class=headerlink href=#interpretations-of-probability-theory title="Permanent link">&para;</a></h3> <ul> <li> <p>frequency of events A?</p> <ul> <li>What is P(the president will be reelected)?</li> </ul> </li> <li> <p>Probability is often interpreted as:</p> <ul> <li>Description of beliefs</li> <li>Betting preference</li> </ul> </li> </ul> <p><img alt="use of probability" src=../fig/Interpretations-and-uses-of-probabilities.png></p> <h2 id=lecture-2-conditioning-and-bayes-rule>Lecture 2 Conditioning and Baye's rule<a class=headerlink href=#lecture-2-conditioning-and-bayes-rule title="Permanent link">&para;</a></h2> <h3 id=conditional-probability>Conditional probability<a class=headerlink href=#conditional-probability title="Permanent link">&para;</a></h3> <p>use new information to review a model</p> <div class="admonition notes"> <p class=admonition-title>Definition</p> <p><span><span class=MathJax_Preview>{\bf P}(A|B) =</span><script type=math/tex>{\bf P}(A|B) =</script></span> "probability of A given that B occurred"</p> <p><span><span class=MathJax_Preview>{\bf P}(A|B) = \frac{{\bf P}(A \cap B)}{{\bf P}(B)}</span><script type=math/tex>{\bf P}(A|B) = \frac{{\bf P}(A \cap B)}{{\bf P}(B)}</script></span>, defined only when <span><span class=MathJax_Preview>{\bf P}(B) &gt; 0</span><script type=math/tex>{\bf P}(B) > 0</script></span></p> </div> <p><img alt=conditional-probability src=../fig/conditional-probability.png></p> <p>Conditional probability share properties of ordinary probabilities (finite and infinite countable additivity)</p> <p>If <span><span class=MathJax_Preview>A \cap C = \emptyset</span><script type=math/tex>A \cap C = \emptyset</script></span>, then <span><span class=MathJax_Preview>{\bf P}(A \cup C|B) = P(A|B) + {\bf P}(C|B)</span><script type=math/tex>{\bf P}(A \cup C|B) = P(A|B) + {\bf P}(C|B)</script></span></p> <p><img alt=conditional-shares-ordinary src=../fig/conditional-shares-ordinary.png></p> <h3 id=multiplication-rule>Multiplication rule<a class=headerlink href=#multiplication-rule title="Permanent link">&para;</a></h3> <p><span><span class=MathJax_Preview>{\bf P}(A \cap B) = {\bf P}(A){\bf P}(B|A) = {\bf P}(B){\bf P}(A|B)</span><script type=math/tex>{\bf P}(A \cap B) = {\bf P}(A){\bf P}(B|A) = {\bf P}(B){\bf P}(A|B)</script></span></p> <p><span><span class=MathJax_Preview>{\bf P}(A \cap B \cap C) = {\bf P}(A){\bf P}(B|A){\bf P}(C|A \cap B)</span><script type=math/tex>{\bf P}(A \cap B \cap C) = {\bf P}(A){\bf P}(B|A){\bf P}(C|A \cap B)</script></span></p> <p>It also applies to n events.</p> <h3 id=total-probability-theorem-divide-and-conquer>Total probability theorem (divide and conquer)<a class=headerlink href=#total-probability-theorem-divide-and-conquer title="Permanent link">&para;</a></h3> <p>The settings:</p> <ul> <li>Partition of sample space into <span><span class=MathJax_Preview>A_1, A_2, A_3</span><script type=math/tex>A_1, A_2, A_3</script></span>,</li> <li>have <span><span class=MathJax_Preview>{\bf P}(A_i)</span><script type=math/tex>{\bf P}(A_i)</script></span> for every <span><span class=MathJax_Preview>i</span><script type=math/tex>i</script></span></li> <li>have<span><span class=MathJax_Preview>{\bf P}(B|A_i)</span><script type=math/tex>{\bf P}(B|A_i)</script></span> for every <span><span class=MathJax_Preview>i</span><script type=math/tex>i</script></span></li> </ul> <p><span><span class=MathJax_Preview>{\bf P}(B) = \sum_{i} {\bf P}(A_i){\bf P}(B|A_i)</span><script type=math/tex>{\bf P}(B) = \sum_{i} {\bf P}(A_i){\bf P}(B|A_i)</script></span></p> <p>It also applies to infinite countable sets according to the countable additivity axiom.</p> <h3 id=bayes-rule>Baye's rule<a class=headerlink href=#bayes-rule title="Permanent link">&para;</a></h3> <p>The setting is the same as the total probability theorem.</p> <ul> <li>Partition of sample space into <span><span class=MathJax_Preview>A_1, A_2, A_3</span><script type=math/tex>A_1, A_2, A_3</script></span>,</li> <li>have <span><span class=MathJax_Preview>{\bf P}(A_i)</span><script type=math/tex>{\bf P}(A_i)</script></span> for every <span><span class=MathJax_Preview>i</span><script type=math/tex>i</script></span> (initial believes)</li> <li>have<span><span class=MathJax_Preview>{\bf P}(B|A_i)</span><script type=math/tex>{\bf P}(B|A_i)</script></span> for every <span><span class=MathJax_Preview>i</span><script type=math/tex>i</script></span></li> </ul> <p>revise "believe" given that B occurred:</p> <p><span><span class=MathJax_Preview>{\bf P}(A_i|B) = \frac{{\bf P}(A_i \cap B)}{{\bf P}(B)}</span><script type=math/tex>{\bf P}(A_i|B) = \frac{{\bf P}(A_i \cap B)}{{\bf P}(B)}</script></span></p> <p>It turns out that we can use multiplication rule to calculate the nominator and use the total probability rule to calculate the denominator, thusly</p> <p><span><span class=MathJax_Preview>{\bf P}(A_i|B) = \frac{{\bf P}(A_i){\bf P}(B|A_i)}{\sum_j{\bf P}(A_j){\bf P}(B|A_i)}</span><script type=math/tex>{\bf P}(A_i|B) = \frac{{\bf P}(A_i){\bf P}(B|A_i)}{\sum_j{\bf P}(A_j){\bf P}(B|A_i)}</script></span></p> <p><img alt=bayes-rule src=../fig/bayes-rule.png></p> <h2 id=lecture-3-independence>Lecture 3 Independence<a class=headerlink href=#lecture-3-independence title="Permanent link">&para;</a></h2> <h3 id=independence-of-two-events>Independence of two events<a class=headerlink href=#independence-of-two-events title="Permanent link">&para;</a></h3> <div class="admonition note"> <p class=admonition-title>Motivation</p> <p>coin toss example: First toss is head or tail doesn't affect the probability of second toss is head.</p> </div> <p>Occurance of <span><span class=MathJax_Preview>A</span><script type=math/tex>A</script></span> privides no new information about <span><span class=MathJax_Preview>B</span><script type=math/tex>B</script></span>: <span><span class=MathJax_Preview>{\bf P}(B|A) = {\bf P}(B)</span><script type=math/tex>{\bf P}(B|A) = {\bf P}(B)</script></span>.</p> <h3 id=definition-of-independence>Definition of independence<a class=headerlink href=#definition-of-independence title="Permanent link">&para;</a></h3> <div class="admonition note"> <p class=admonition-title>Formal definition</p> <p><span><span class=MathJax_Preview>{\bf P}(A \cap B) = {\bf P}(A) \cdot {\bf P}(B)</span><script type=math/tex>{\bf P}(A \cap B) = {\bf P}(A) \cdot {\bf P}(B)</script></span></p> </div> <ul> <li>symmetric with respect to events.</li> <li>implies <span><span class=MathJax_Preview>{\bf P}(A|B) = {\bf P}(A)</span><script type=math/tex>{\bf P}(A|B) = {\bf P}(A)</script></span>.</li> <li>Applies even if <span><span class=MathJax_Preview>{\bf P}(A) = 0</span><script type=math/tex>{\bf P}(A) = 0</script></span>.</li> </ul> <div class="admonition warning"> <p class=admonition-title>Distinction between disjoin and independence</p> <p>Disjoin usually means dependent, because if one event happens we know the other event will not happen. <img alt=disjoin-and-independence src=../fig/disjoin-and-independence.png></p> </div> <h3 id=independence-of-event-complements>Independence of event complements<a class=headerlink href=#independence-of-event-complements title="Permanent link">&para;</a></h3> <p>If <span><span class=MathJax_Preview>A</span><script type=math/tex>A</script></span> and <span><span class=MathJax_Preview>B</span><script type=math/tex>B</script></span> are independent, the <span><span class=MathJax_Preview>A</span><script type=math/tex>A</script></span> and <span><span class=MathJax_Preview>B^c</span><script type=math/tex>B^c</script></span> are independent.</p> <p>If <span><span class=MathJax_Preview>A</span><script type=math/tex>A</script></span> and <span><span class=MathJax_Preview>B</span><script type=math/tex>B</script></span> are independent, the <span><span class=MathJax_Preview>A^c</span><script type=math/tex>A^c</script></span> and <span><span class=MathJax_Preview>B^c</span><script type=math/tex>B^c</script></span> are independent.</p> <h3 id=conditioning-independence>Conditioning independence<a class=headerlink href=#conditioning-independence title="Permanent link">&para;</a></h3> <p>Ordinary independence properties also apply to conditional independence. But independence doesn't imply conditional independence.</p> <p><img alt=conditional-independence src=../fig/conditional-independence.png></p> <h3 id=independence-of-a-collection-of-events>Independence of a collection of events<a class=headerlink href=#independence-of-a-collection-of-events title="Permanent link">&para;</a></h3> <p>Ituition "definition": Information on some of the events doesn't change probability related to the remaining events.</p> <div class="admonition note"> <p class=admonition-title>Formal definition</p> <p><img alt=independent-of-events src=../fig/independent-of-events.png></p> </div> <h3 id=pairwise-independence>pairwise independence<a class=headerlink href=#pairwise-independence title="Permanent link">&para;</a></h3> <h4 id=independence-vs-pairwise-independence>independence V.S. pairwise independence<a class=headerlink href=#independence-vs-pairwise-independence title="Permanent link">&para;</a></h4> <p>Used two independence fair coin tosses as an example to show that pariwise independence isn't enough to show independence of collection of events.</p> <p><img alt=pairwise-but-no src=../fig/pairwise-but-no.png></p> <h3 id=reliability>Reliability<a class=headerlink href=#reliability title="Permanent link">&para;</a></h3> <p>The king's sibling puzzle</p> <h3 id=monty-hall-problem>Monty Hall problem<a class=headerlink href=#monty-hall-problem title="Permanent link">&para;</a></h3> <h2 id=lecture-4>Lecture 4<a class=headerlink href=#lecture-4 title="Permanent link">&para;</a></h2> <h2 id=lecture-5>Lecture 5<a class=headerlink href=#lecture-5 title="Permanent link">&para;</a></h2> <h3 id=definition-of-random-variables>Definition of random variables<a class=headerlink href=#definition-of-random-variables title="Permanent link">&para;</a></h3> <ul> <li>A random variable ("r.v.") associates a value (a number) to every possible outcome.</li> <li>Mathematically, A function from sample space <span><span class=MathJax_Preview>\Omega</span><script type=math/tex>\Omega</script></span> to the real numbers (discrete or continuous).</li> <li>We can have several random variables defined on the same sample space. (In Lecture 18, <span><span class=MathJax_Preview>X_1, X_2, \cdots</span><script type=math/tex>X_1, X_2, \cdots</script></span> are independent random variables from the same distribution)</li> <li>A function of one or several random variables is also another random variable.</li> <li>A concrete example to understand the concept of a random variable is take a class of students, selecting students and measuring their body mass or height will give us the random variable <span><span class=MathJax_Preview>W</span><script type=math/tex>W</script></span> and <span><span class=MathJax_Preview>H</span><script type=math/tex>H</script></span>. The "Body mass index" is another random variable that is a function of the random variable <span><span class=MathJax_Preview>W</span><script type=math/tex>W</script></span> and <span><span class=MathJax_Preview>H</span><script type=math/tex>H</script></span>.</li> </ul> <h3 id=bernoulli-and-indicator-random-variables>Bernoulli and indicator random variables<a class=headerlink href=#bernoulli-and-indicator-random-variables title="Permanent link">&para;</a></h3> <ul> <li>Bernoulli random variable <span><span class=MathJax_Preview>X</span><script type=math/tex>X</script></span>:</li> <li>models a trial that results in success/failure, heads/tails, etc.</li> <li>indicator random variables of event <span><span class=MathJax_Preview>A</span><script type=math/tex>A</script></span>: <span><span class=MathJax_Preview>I_A = 1</span><script type=math/tex>I_A = 1</script></span> if <span><span class=MathJax_Preview>A</span><script type=math/tex>A</script></span> occurs.</li> <li>connection between events and random variables. <span><span class=MathJax_Preview>P(A) = P(I_A = 1) = P_{I_A}(1)</span><script type=math/tex>P(A) = P(I_A = 1) = P_{I_A}(1)</script></span>,</li> <li><span><span class=MathJax_Preview>P(A)</span><script type=math/tex>P(A)</script></span>: probability of event A happens</li> <li><span><span class=MathJax_Preview>P(I_A = 1)</span><script type=math/tex>P(I_A = 1)</script></span>: probablistic notition of indication R.V. when <span><span class=MathJax_Preview>I_A</span><script type=math/tex>I_A</script></span> equal to <span><span class=MathJax_Preview>1</span><script type=math/tex>1</script></span></li> <li><span><span class=MathJax_Preview>P_{I_A}(1)</span><script type=math/tex>P_{I_A}(1)</script></span>: PMF notation.</li> </ul> <h2 id=lecture-6>Lecture 6<a class=headerlink href=#lecture-6 title="Permanent link">&para;</a></h2> <h2 id=lecture-7>Lecture 7<a class=headerlink href=#lecture-7 title="Permanent link">&para;</a></h2> <h3 id=independence-variances-and-binomial-variance>Independence, variances, and binomial variance<a class=headerlink href=#independence-variances-and-binomial-variance title="Permanent link">&para;</a></h3> <h2 id=lecture-8>Lecture 8<a class=headerlink href=#lecture-8 title="Permanent link">&para;</a></h2> <h3 id=exponential-random-variables>Exponential random variables<a class=headerlink href=#exponential-random-variables title="Permanent link">&para;</a></h3> <ul> <li> <p>PDF: <span><span class=MathJax_Preview>f_{X}(x) = \begin{cases} \lambda e^{-\lambda x}, &amp; x \ge 0 \\ 0, &amp; x &lt; 0 \end{cases}</span><script type=math/tex>f_{X}(x) =
\begin{cases}
  \lambda e^{-\lambda x}, & x \ge 0 \\
  0,  &  x < 0
\end{cases}</script></span></p> </li> <li> <p>Probability of greater than <span><span class=MathJax_Preview>a</span><script type=math/tex>a</script></span>,</p> </li> </ul> <div> <div class=MathJax_Preview>\begin{align*} P(X \ge a) &amp;= \displaystyle \int_a^{+\infty} \lambda e^{-\lambda x}dx \\ &amp;= - \displaystyle \int_a^{+\infty} de^{-\lambda x} \\ &amp;= -e^{-\lambda x}\Big|_a^{+\infty} = -e^{-\lambda \cdot +\infty} + e^{-\lambda a} = e^{-\lambda a} \end{align*}</div> <script type="math/tex; mode=display">\begin{align*}
P(X \ge a) &= \displaystyle \int_a^{+\infty} \lambda e^{-\lambda x}dx \\
&= - \displaystyle \int_a^{+\infty} de^{-\lambda x} \\
&= -e^{-\lambda x}\Big|_a^{+\infty} = -e^{-\lambda \cdot +\infty} + e^{-\lambda a} = e^{-\lambda a}
\end{align*}</script> </div> <ul> <li>Expectation of exponential random variable</li> </ul> <p><span><span class=MathJax_Preview>{\bf E}\big[X\big] = \displaystyle \int_0^{\infty} x \cdot \lambda e^{-\lambda x} dx = 1/\lambda</span><script type=math/tex>{\bf E}\big[X\big] = \displaystyle \int_0^{\infty} x \cdot \lambda e^{-\lambda x} dx = 1/\lambda</script></span></p> <ul> <li>Second moment of an exponential random variable</li> </ul> <p><span><span class=MathJax_Preview>{\bf E}\big[X^2 \big] = \displaystyle \int_0^{\infty} x^2 \cdot \lambda e^{-\lambda x} dx = 2/\lambda^2</span><script type=math/tex>{\bf E}\big[X^2 \big] = \displaystyle \int_0^{\infty} x^2 \cdot \lambda e^{-\lambda x} dx = 2/\lambda^2</script></span></p> <ul> <li>Variance of an exponential random variable</li> </ul> <p><span><span class=MathJax_Preview>\textbf{Var}(X) = {\bf E}\big[X^2 \big] - \Big({\bf E}\big[X\big]\Big)^2 = 1/\lambda^2</span><script type=math/tex>\textbf{Var}(X) = {\bf E}\big[X^2 \big] - \Big({\bf E}\big[X\big]\Big)^2 = 1/\lambda^2</script></span></p> <h2 id=lecture-9>Lecture 9<a class=headerlink href=#lecture-9 title="Permanent link">&para;</a></h2> <h3 id=conditioning-a-continuous-random-variable-on-an-event>Conditioning a continuous random variable on an event<a class=headerlink href=#conditioning-a-continuous-random-variable-on-an-event title="Permanent link">&para;</a></h3> <h3 id=memorylessness-of-the-exponential-pdf>Memorylessness of the exponential PDF<a class=headerlink href=#memorylessness-of-the-exponential-pdf title="Permanent link">&para;</a></h3> <h3 id=total-probability-and-expectation-theorems>Total probability and expectation theorems<a class=headerlink href=#total-probability-and-expectation-theorems title="Permanent link">&para;</a></h3> <h2 id=lecture-10>Lecture 10<a class=headerlink href=#lecture-10 title="Permanent link">&para;</a></h2> <h3 id=total-probability-and-total-expectation-theorems>Total probability and total expectation theorems<a class=headerlink href=#total-probability-and-total-expectation-theorems title="Permanent link">&para;</a></h3> <p>Follows from the discrete cases:</p> <table> <thead> <tr> <th align=left>Rules</th> <th>Discrete</th> <th>continous</th> </tr> </thead> <tbody> <tr> <td align=left>Total probability</td> <td><span><span class=MathJax_Preview>p_X(x) = \displaystyle \sum\limits_{y} p_Y(y)p_{X \mid Y}(x \mid y)</span><script type=math/tex>p_X(x) = \displaystyle \sum\limits_{y} p_Y(y)p_{X \mid Y}(x \mid y)</script></span></td> <td><span><span class=MathJax_Preview>f_X(x) = \displaystyle \int_{-\infty}^{+\infty} f_Y(y) f_{X \mid Y}(x \mid y) dy</span><script type=math/tex>f_X(x) = \displaystyle \int_{-\infty}^{+\infty} f_Y(y) f_{X \mid Y}(x \mid y) dy</script></span></td> </tr> <tr> <td align=left>Conditional expectation</td> <td><span><span class=MathJax_Preview>{\bf E}\big[X \mid Y = y \big] = \displaystyle \sum\limits_{x} x p_{X \mid Y}(x \mid y)</span><script type=math/tex>{\bf E}\big[X \mid Y = y \big] = \displaystyle \sum\limits_{x} x p_{X \mid Y}(x \mid y)</script></span></td> <td><span><span class=MathJax_Preview>{\bf E}\big[X \mid Y = y \big] = \displaystyle {\int_{-\infty}^{+\infty}} x f_{X \mid Y}(x \mid y) dx</span><script type=math/tex>{\bf E}\big[X \mid Y = y \big] = \displaystyle {\int_{-\infty}^{+\infty}} x f_{X \mid Y}(x \mid y) dx</script></span></td> </tr> <tr> <td align=left>Total expectation</td> <td><span><span class=MathJax_Preview>{\bf E}\big[X \big] = \displaystyle \sum\limits_{y} p_{Y}(y){\bf E} \big[X \mid Y = y \big]</span><script type=math/tex>{\bf E}\big[X \big] = \displaystyle \sum\limits_{y} p_{Y}(y){\bf E} \big[X \mid Y = y \big]</script></span> derivation hint: replace with the conditional expectatioin</td> <td><span><span class=MathJax_Preview>{\bf E}\big[X \big] = \displaystyle {\int_{-\infty}^{+\infty}} f_{Y}(y){\bf E}\big[X \mid Y = y \big] dy</span><script type=math/tex>{\bf E}\big[X \big] = \displaystyle {\int_{-\infty}^{+\infty}} f_{Y}(y){\bf E}\big[X \mid Y = y \big] dy</script></span> derivation hint: replace with the conditional expectatioin</td> </tr> <tr> <td align=left>Expected value rule</td> <td><span><span class=MathJax_Preview>{\bf E}\big[ g(X) \mid Y = y \big] = \displaystyle {\sum\limits_{x}} g(x) p_{X \mid Y}(x \mid y)</span><script type=math/tex>{\bf E}\big[ g(X) \mid Y = y \big] = \displaystyle {\sum\limits_{x}} g(x) p_{X \mid Y}(x \mid y)</script></span></td> <td><span><span class=MathJax_Preview>{\bf E}\big[ g(X) \mid Y = y \big] = \displaystyle {\int_{-\infty}^{+\infty}} g(x) f_{X \mid Y}(x \mid y) dx</span><script type=math/tex>{\bf E}\big[ g(X) \mid Y = y \big] = \displaystyle {\int_{-\infty}^{+\infty}} g(x) f_{X \mid Y}(x \mid y) dx</script></span></td> </tr> </tbody> </table> <h3 id=solved-problems-lecture-8-10>Solved problems (Lecture 8 - 10)<a class=headerlink href=#solved-problems-lecture-8-10 title="Permanent link">&para;</a></h3> <h4 id=10-buffons-needle-and-monte-carlo-simulation>10 Buffon's needle and Monte Carlo Simulation<a class=headerlink href=#10-buffons-needle-and-monte-carlo-simulation title="Permanent link">&para;</a></h4> <p>This problem is discussed in the text page 161, Example 3.11.</p> <h2 id=lecture-11>Lecture 11<a class=headerlink href=#lecture-11 title="Permanent link">&para;</a></h2> <h2 id=lecture-12>Lecture 12<a class=headerlink href=#lecture-12 title="Permanent link">&para;</a></h2> <h3 id=covariance>Covariance<a class=headerlink href=#covariance title="Permanent link">&para;</a></h3> <ul> <li>Definition: <span><span class=MathJax_Preview>\textbf{cov}(X, Y) = {\bf E}\big[(X - {\bf E}[X]) (Y - {\bf E}[Y])\big]</span><script type=math/tex>\textbf{cov}(X, Y) = {\bf E}\big[(X - {\bf E}[X]) (Y - {\bf E}[Y])\big]</script></span></li> <li>Covariance tell use whether two r.v.s tend to move together, both to low or both to high</li> <li>Two random variables <span><span class=MathJax_Preview>X</span><script type=math/tex>X</script></span> and <span><span class=MathJax_Preview>Y</span><script type=math/tex>Y</script></span> are independent indicates: <span><span class=MathJax_Preview>\textbf{cov}(X, Y) = 0</span><script type=math/tex>\textbf{cov}(X, Y) = 0</script></span>, but the converse it not true.</li> </ul> <h3 id=covariance-properties>Covariance properties<a class=headerlink href=#covariance-properties title="Permanent link">&para;</a></h3> <h2 id=lecture-13>Lecture 13<a class=headerlink href=#lecture-13 title="Permanent link">&para;</a></h2> <h3 id=the-conditional-expectation-as-a-random-variable-bf-e-bigx-mid-ybigbf-e-bigx-mid-ybig>The conditional expectation as a random variable <span><span class=MathJax_Preview>{\bf E} \big[X \mid Y\big]</span><script type=math/tex>{\bf E} \big[X \mid Y\big]</script></span><a class=headerlink href=#the-conditional-expectation-as-a-random-variable-bf-e-bigx-mid-ybigbf-e-bigx-mid-ybig title="Permanent link">&para;</a></h3> <ul> <li>Given a function <span><span class=MathJax_Preview>h(x) = x^2</span><script type=math/tex>h(x) = x^2</script></span> for all <span><span class=MathJax_Preview>x</span><script type=math/tex>x</script></span>, and a random variable <span><span class=MathJax_Preview>X</span><script type=math/tex>X</script></span>. what is <span><span class=MathJax_Preview>h(X)</span><script type=math/tex>h(X)</script></span>?</li> <li><span><span class=MathJax_Preview>h(X)</span><script type=math/tex>h(X)</script></span> is a function of a random variable <span><span class=MathJax_Preview>X</span><script type=math/tex>X</script></span>. <span><span class=MathJax_Preview>h(X)</span><script type=math/tex>h(X)</script></span> itself is a random variable that take value <span><span class=MathJax_Preview>x^2</span><script type=math/tex>x^2</script></span> if <span><span class=MathJax_Preview>X</span><script type=math/tex>X</script></span> happens to take the value <span><span class=MathJax_Preview>x</span><script type=math/tex>x</script></span>. (you should really understand this statement, it is crucial to understand the concept of conditional expectatioin)</li> <li>For discrete case: <span><span class=MathJax_Preview>{\bf E}\big[X \mid Y = y \big] = \displaystyle \sum\limits_{x} x p_{X \mid Y}(x \mid y)</span><script type=math/tex>{\bf E}\big[X \mid Y = y \big] = \displaystyle \sum\limits_{x} x p_{X \mid Y}(x \mid y)</script></span>, in which different vaule of <span><span class=MathJax_Preview>y</span><script type=math/tex>y</script></span> will give us the different value of conditional expectation of <span><span class=MathJax_Preview>X</span><script type=math/tex>X</script></span>. Then we can really treat <span><span class=MathJax_Preview>{\bf E}\big[X \mid Y = y \big]</span><script type=math/tex>{\bf E}\big[X \mid Y = y \big]</script></span> as a function of <span><span class=MathJax_Preview>y</span><script type=math/tex>y</script></span>, noted as <span><span class=MathJax_Preview>g(y)</span><script type=math/tex>g(y)</script></span>.</li> <li>From the above reasoning, because <span><span class=MathJax_Preview>Y</span><script type=math/tex>Y</script></span> is a random variable, <span><span class=MathJax_Preview>g(Y)</span><script type=math/tex>g(Y)</script></span> is also a reandom variable, that takes the value of <span><span class=MathJax_Preview>g(y) = {\bf E}\big[X \mid Y = y \big]</span><script type=math/tex>g(y) = {\bf E}\big[X \mid Y = y \big]</script></span> if <span><span class=MathJax_Preview>Y</span><script type=math/tex>Y</script></span> happens to take the value <span><span class=MathJax_Preview>y</span><script type=math/tex>y</script></span>.</li> <li><span><span class=MathJax_Preview>g(Y) = {\bf E}\big[X \mid Y\big]</span><script type=math/tex>g(Y) = {\bf E}\big[X \mid Y\big]</script></span>, is</li> <li>a function of <span><span class=MathJax_Preview>Y</span><script type=math/tex>Y</script></span></li> <li>a random variable</li> <li>has a distribution, mean, variance, etc.</li> </ul> <h3 id=the-law-of-iterated-expectations>The law of iterated expectations<a class=headerlink href=#the-law-of-iterated-expectations title="Permanent link">&para;</a></h3> <p>By calculate <span><span class=MathJax_Preview>{\bf E}\Big[{\bf E}\big[X \mid Y\big]\Big]</span><script type=math/tex>{\bf E}\Big[{\bf E}\big[X \mid Y\big]\Big]</script></span>, we could abtain that it is equal to <span><span class=MathJax_Preview>{\bf E}\big[X\big]</span><script type=math/tex>{\bf E}\big[X\big]</script></span></p> <h3 id=the-conditional-variance-as-a-random-variable-textbfvarx-mid-y-ytextbfvarx-mid-y-y>The conditional variance as a random variable <span><span class=MathJax_Preview>\textbf{Var}(X \mid Y = y)</span><script type=math/tex>\textbf{Var}(X \mid Y = y)</script></span><a class=headerlink href=#the-conditional-variance-as-a-random-variable-textbfvarx-mid-y-ytextbfvarx-mid-y-y title="Permanent link">&para;</a></h3> <ul> <li>definition: <span><span class=MathJax_Preview>\textbf{Var}(X \mid Y = y) = {\bf E}\Big[\big(X - {\bf E}\big[X \mid Y = y \big]\big)^2\mid Y = y\Big]</span><script type=math/tex>\textbf{Var}(X \mid Y = y) = {\bf E}\Big[\big(X - {\bf E}\big[X \mid Y = y \big]\big)^2\mid Y = y\Big]</script></span></li> <li><span><span class=MathJax_Preview>\textbf{Var}(X\mid Y)</span><script type=math/tex>\textbf{Var}(X\mid Y)</script></span> is also a random variable, it takes the value <span><span class=MathJax_Preview>\textbf{Var}(X\mid y)</span><script type=math/tex>\textbf{Var}(X\mid y)</script></span> when <span><span class=MathJax_Preview>Y</span><script type=math/tex>Y</script></span> happens to take the value of <span><span class=MathJax_Preview>y</span><script type=math/tex>y</script></span>.</li> <li>Total variance rule: <span><span class=MathJax_Preview>\textbf{Var}(X) = {\bf E}\Big[\textbf{Var}(X \mid Y)\Big] + \textbf{Var}\Big({\bf E}\big[X\mid Y\big]\Big)</span><script type=math/tex>\textbf{Var}(X) = {\bf E}\Big[\textbf{Var}(X \mid Y)\Big] + \textbf{Var}\Big({\bf E}\big[X\mid Y\big]\Big)</script></span></li> </ul> <h2 id=lecture-14-introduction-to-bayesian-inference>Lecture 14 Introduction to Bayesian Inference<a class=headerlink href=#lecture-14-introduction-to-bayesian-inference title="Permanent link">&para;</a></h2> <h3 id=the-bayesian-inference-framework>The Bayesian inference framework<a class=headerlink href=#the-bayesian-inference-framework title="Permanent link">&para;</a></h3> <p>Treat the unknown <span><span class=MathJax_Preview>\Theta</span><script type=math/tex>\Theta</script></span> as a random variable. with prior <span><span class=MathJax_Preview>p_{\Theta}</span><script type=math/tex>p_{\Theta}</script></span> or <span><span class=MathJax_Preview>f_{\Theta}</span><script type=math/tex>f_{\Theta}</script></span>. With the observations <span><span class=MathJax_Preview>X</span><script type=math/tex>X</script></span>, using Bayes rule to obtain the posterior probability of <span><span class=MathJax_Preview>\Theta</span><script type=math/tex>\Theta</script></span>, <span><span class=MathJax_Preview>P_{\Theta|X}(\cdot|X=x)</span><script type=math/tex>P_{\Theta|X}(\cdot|X=x)</script></span>. Once the posterior is available, the estimator can be obtained via maximize a posterioral probability (MAP) rule or leaset mean squares (LMS). The performance can be measured by "probability of error" or "mean squared error".</p> <p><img alt="Screen Shot 2018-11-12 at 8.28.07 PM.png-74.4kB" src=http://static.zybuluo.com/iurnah/2ivtzqonj3ubmoa2hntahbmy/Screen%20Shot%202018-11-12%20at%208.28.07%20PM.png></p> <h3 id=conditional-probability-of-error-and-total-probability-of-error>Conditional probability of error and total probability of error<a class=headerlink href=#conditional-probability-of-error-and-total-probability-of-error title="Permanent link">&para;</a></h3> <ul> <li>The conditional probability of other estimators that made a mistake. One of the properties of MAP rule is the it guarantee smallest probability of error.</li> <li>The taotal probability of error is an abstract notion of error for MAP rule to estimator. It can be caculated using total probability theory.</li> </ul> <h3 id=discrete-unknowns-discrete-observation-example-8-exercise-discrete-unknowns>Discrete unknowns, discrete observation example (8. Exercise: Discrete unknowns)<a class=headerlink href=#discrete-unknowns-discrete-observation-example-8-exercise-discrete-unknowns title="Permanent link">&para;</a></h3> <p>Let <span><span class=MathJax_Preview>\Theta_1</span><script type=math/tex>\Theta_1</script></span> and <span><span class=MathJax_Preview>\Theta_2</span><script type=math/tex>\Theta_2</script></span> be some unobserved Bernoulli random variables and let be an observation. Conditional on <span><span class=MathJax_Preview>X = x</span><script type=math/tex>X = x</script></span>, the posterior joint PMF of <span><span class=MathJax_Preview>\Theta_1</span><script type=math/tex>\Theta_1</script></span> and <span><span class=MathJax_Preview>\Theta_2</span><script type=math/tex>\Theta_2</script></span> is given by</p> <p><span><span class=MathJax_Preview>p_{\Theta _1,\Theta _2\mid X}(\theta _1,\theta _2\mid x) = \begin{cases} 0.26, &amp; \mbox{if } \theta _1=0, \theta _2=0, \\ 0.26, &amp; \mbox{if } \theta _1=0, \theta _2=1, \\ 0.21, &amp; \mbox{if } \theta _1=1, \theta _2=0, \\ 0.27, &amp; \mbox{if } \theta _1=1, \theta _2=1, \\ 0, &amp; \mbox{otherwise.} \end{cases}</span><script type=math/tex>p_{\Theta _1,\Theta _2\mid X}(\theta _1,\theta _2\mid x) =
\begin{cases} 0.26, &  \mbox{if } \theta _1=0, \theta _2=0,
    \\ 0.26, &  \mbox{if } \theta _1=0, \theta _2=1,
    \\ 0.21, &  \mbox{if } \theta _1=1, \theta _2=0,
    \\ 0.27, &  \mbox{if } \theta _1=1, \theta _2=1,
    \\ 0, &  \mbox{otherwise.}
\end{cases}</script></span></p> <ul> <li>What is the estimate of <span><span class=MathJax_Preview>(\Theta_1, \Theta_2)</span><script type=math/tex>(\Theta_1, \Theta_2)</script></span> provided by MAP rule?</li> <li>(1, 1), because the <span><span class=MathJax_Preview>p_{\Theta _1,\Theta _2\mid X}(1, 1\mid x)</span><script type=math/tex>p_{\Theta _1,\Theta _2\mid X}(1, 1\mid x)</script></span> is the maximum of all.</li> <li>What is the MAP estimate of <span><span class=MathJax_Preview>\Theta_1</span><script type=math/tex>\Theta_1</script></span> based on <span><span class=MathJax_Preview>X</span><script type=math/tex>X</script></span>, that is, the one that maximizes <span><span class=MathJax_Preview>p_{\Theta_1 \mid X}(\theta_1 \mid x)</span><script type=math/tex>p_{\Theta_1 \mid X}(\theta_1 \mid x)</script></span>?</li> <li>0, from the marginal PMF, you can see that <span><span class=MathJax_Preview>p_{\Theta_1 \mid X}(0 \mid x)</span><script type=math/tex>p_{\Theta_1 \mid X}(0 \mid x)</script></span> is maximized</li> <li>The moral of this problem is that an estimate of $ \Theta_1 $ obtained by identifying the maximum of the join PMF of all unknown random variables can be different from the MAP estimate of <span><span class=MathJax_Preview>\Theta_1</span><script type=math/tex>\Theta_1</script></span> from the marginal PMF.</li> </ul> <h3 id=discrete-parameter-continuous-observation>Discrete parameter, continuous observation<a class=headerlink href=#discrete-parameter-continuous-observation title="Permanent link">&para;</a></h3> <ul> <li>Digital signal transmission</li> </ul> <h3 id=continous-parameter-continuous-observation>continous parameter, continuous observation<a class=headerlink href=#continous-parameter-continuous-observation title="Permanent link">&para;</a></h3> <ul> <li>Analog signal transmission</li> </ul> <h2 id=lecture-15-linear-models-with-normal-noise>Lecture 15 Linear models with normal noise<a class=headerlink href=#lecture-15-linear-models-with-normal-noise title="Permanent link">&para;</a></h2> <h3 id=recognizing-normal-pdfs>recognizing normal PDFs<a class=headerlink href=#recognizing-normal-pdfs title="Permanent link">&para;</a></h3> <p><span><span class=MathJax_Preview>f_X(x) = c \cdot e^{-(\alpha x^2 + \beta x + \gamma)}</span><script type=math/tex>f_X(x) = c \cdot e^{-(\alpha x^2 + \beta x + \gamma)}</script></span> is a normal random variable with <span><span class=MathJax_Preview>\mu = -\frac{\beta}{2\alpha}</span><script type=math/tex>\mu = -\frac{\beta}{2\alpha}</script></span> and variance <span><span class=MathJax_Preview>\frac{1}{2\alpha}</span><script type=math/tex>\frac{1}{2\alpha}</script></span></p> <h3 id=the-mean-squared-error>The mean squared error<a class=headerlink href=#the-mean-squared-error title="Permanent link">&para;</a></h3> <p>The most important take away from this section is that for normal unknown and normal noise signal, <span><span class=MathJax_Preview>X = \Theta + W</span><script type=math/tex>X = \Theta + W</script></span>, no matter which <span><span class=MathJax_Preview>x_i</span><script type=math/tex>x_i</script></span> we observed, the mean squared error estimates <span><span class=MathJax_Preview>{\hat \theta}</span><script type=math/tex>{\hat \theta}</script></span> are the same (the variance of <span><span class=MathJax_Preview>\Theta</span><script type=math/tex>\Theta</script></span>). In other words, the remaining uncertainty about <span><span class=MathJax_Preview>\theta</span><script type=math/tex>\theta</script></span> after an observation is the same no matter what the observation is. The observation only determined the estimated mean of the random variable <span><span class=MathJax_Preview>\theta</span><script type=math/tex>\theta</script></span>. (See the drawing at the end of the lecture)</p> <h3 id=measurement-estimate-and-learning>Measurement, estimate, and learning<a class=headerlink href=#measurement-estimate-and-learning title="Permanent link">&para;</a></h3> <ul> <li>How to measure the gravitational attraction constant</li> </ul> <h2 id=lecture-16-least-mean-square-lms-estimation>Lecture 16 Least mean square (LMS) estimation<a class=headerlink href=#lecture-16-least-mean-square-lms-estimation title="Permanent link">&para;</a></h2> <h3 id=lms-estimation-without-any-observations>LMS estimation without any observations<a class=headerlink href=#lms-estimation-without-any-observations title="Permanent link">&para;</a></h3> <ul> <li>Given the prior <span><span class=MathJax_Preview>p_{\Theta}(\theta)</span><script type=math/tex>p_{\Theta}(\theta)</script></span> unknow random variable <span><span class=MathJax_Preview>\Theta</span><script type=math/tex>\Theta</script></span>, what's you realy want to estimate?</li> <li>You may interested in a point estimate.</li> <li>You may interested in find the estimator.</li> <li>With no observation, to minimize the mean squared error (MSE), <span><span class=MathJax_Preview>{\bf E}\Big[\big(\Theta - \hat \theta \big)^2\Big]</span><script type=math/tex>{\bf E}\Big[\big(\Theta - \hat \theta \big)^2\Big]</script></span>, the value <span><span class=MathJax_Preview>\hat \theta = {\bf E}\big[X\big]</span><script type=math/tex>\hat \theta = {\bf E}\big[X\big]</script></span> minimize the error. and the minimum error equal to <span><span class=MathJax_Preview>\textbf{Var}(\Theta)</span><script type=math/tex>\textbf{Var}(\Theta)</script></span></li> <li>Optimal mean squared error: <span><span class=MathJax_Preview>{\bf E}\Big[\big(\Theta - {\bf E}\big[\Theta \big] \big)^2\Big] = \textbf{Var}(\Theta)</span><script type=math/tex>{\bf E}\Big[\big(\Theta - {\bf E}\big[\Theta \big] \big)^2\Big] = \textbf{Var}(\Theta)</script></span>. Because <span><span class=MathJax_Preview>{\bf E}\big[\Theta \big]</span><script type=math/tex>{\bf E}\big[\Theta \big]</script></span> is the optimal value that minimize the MSE, if we replace <span><span class=MathJax_Preview>\hat \theta</span><script type=math/tex>\hat \theta</script></span> with <span><span class=MathJax_Preview>{\bf E}\big[\Theta \big]</span><script type=math/tex>{\bf E}\big[\Theta \big]</script></span> in the expression MSE, it coincidently match the definition of variance. You can alse use the <span><span class=MathJax_Preview>{\bf E}\Big[\big(\Theta - \hat \theta \big)^2\Big] = \textbf{Var}(\Theta - \hat \theta) + \Big({\bf E}\big[\Theta - \hat \theta \big]\Big)^2</span><script type=math/tex>{\bf E}\Big[\big(\Theta - \hat \theta \big)^2\Big] = \textbf{Var}(\Theta - \hat \theta) + \Big({\bf E}\big[\Theta - \hat \theta \big]\Big)^2</script></span> to derive <span><span class=MathJax_Preview>\hat \theta = {\bf E}\big[X\big]</span><script type=math/tex>\hat \theta = {\bf E}\big[X\big]</script></span> is the optimal value.</li> </ul> <h3 id=lms-estimation-single-unknown-and-observation>LMS estimation; single unknown and observation<a class=headerlink href=#lms-estimation-single-unknown-and-observation title="Permanent link">&para;</a></h3> <ul> <li>Goal: interested in a point estimate <span><span class=MathJax_Preview>\hat \theta</span><script type=math/tex>\hat \theta</script></span> of unknow random variable <span><span class=MathJax_Preview>\Theta</span><script type=math/tex>\Theta</script></span>, with prior <span><span class=MathJax_Preview>p_{\Theta}(\theta)</span><script type=math/tex>p_{\Theta}(\theta)</script></span>. (Given observation <span><span class=MathJax_Preview>X</span><script type=math/tex>X</script></span>; model <span><span class=MathJax_Preview>p_{X\mid \Theta}(x\mid \theta)</span><script type=math/tex>p_{X\mid \Theta}(x\mid \theta)</script></span>)</li> <li>We want to minimize the MSE, because this time we have the particular observation <span><span class=MathJax_Preview>X = x</span><script type=math/tex>X = x</script></span>, we now live in a conditional universe, we need to minimize the conditional MSE, <span><span class=MathJax_Preview>{\bf E}\Big[\big(\Theta - {\bf E}\big[\Theta \big] \big)^2 \mid X = x \Big]</span><script type=math/tex>{\bf E}\Big[\big(\Theta - {\bf E}\big[\Theta \big] \big)^2 \mid X = x \Big]</script></span>, the optimal value is <span><span class=MathJax_Preview>\hat \theta = {\bf E}\big[\Theta \mid X = x\big]</span><script type=math/tex>\hat \theta = {\bf E}\big[\Theta \mid X = x\big]</script></span></li> <li>By the "happens to take" reasoning and the iterated expection rule, we can achieve the conclusion: that <span><span class=MathJax_Preview>\hat \Theta_{LMS} = {\bf E}\big[\Theta \mid X\big]</span><script type=math/tex>\hat \Theta_{LMS} = {\bf E}\big[\Theta \mid X\big]</script></span> minimize the MSE <span><span class=MathJax_Preview>{\bf E}\Big[\big(\Theta - g(X) \big)^2 \Big]</span><script type=math/tex>{\bf E}\Big[\big(\Theta - g(X) \big)^2 \Big]</script></span> over all estimators <span><span class=MathJax_Preview>\hat \Theta = g(X)</span><script type=math/tex>\hat \Theta = g(X)</script></span>.</li> </ul> <h3 id=lms-performance-evaluation>LMS performance evaluation<a class=headerlink href=#lms-performance-evaluation title="Permanent link">&para;</a></h3> <ul> <li>MSE: <span><span class=MathJax_Preview>{\bf E}\Big[\big(\Theta - {\bf E}\big[\Theta \mid X = x\big] \big)^2 \mid X = x \Big] = \textbf{Var}(\Theta \mid X = x)</span><script type=math/tex>{\bf E}\Big[\big(\Theta - {\bf E}\big[\Theta \mid X = x\big] \big)^2 \mid X = x \Big] = \textbf{Var}(\Theta \mid X = x)</script></span></li> <li>Expected performance of the design: <span><span class=MathJax_Preview>{\bf E}\Big[\big(\Theta - {\bf E}\big[\Theta \mid X\big] \big)^2 \Big] = {\bf E}\Big[\textbf{Var}(\Theta \mid X)\Big]</span><script type=math/tex>{\bf E}\Big[\big(\Theta - {\bf E}\big[\Theta \mid X\big] \big)^2 \Big] = {\bf E}\Big[\textbf{Var}(\Theta \mid X)\Big]</script></span></li> <li>LMS relavant to estimation (not hypothesis testing)</li> <li>Same as MAP if the posterior is unimodal and symmetric around the mean.</li> <li>e.g. when the posterior is normal (the case in "linear-normal" models)</li> </ul> <h3 id=the-multidimensional-case>The multidimensional case<a class=headerlink href=#the-multidimensional-case title="Permanent link">&para;</a></h3> <h2 id=lecture-17-linear-least-mean-squares-llms-estimation>Lecture 17 Linear least mean squares (LLMS) estimation<a class=headerlink href=#lecture-17-linear-least-mean-squares-llms-estimation title="Permanent link">&para;</a></h2> <h2 id=lecture-18-inequalities-convergence-and-the-weak-law-of-large-numbers>Lecture 18 Inequalities, convergence, and the Weak Law of Large Numbers<a class=headerlink href=#lecture-18-inequalities-convergence-and-the-weak-law-of-large-numbers title="Permanent link">&para;</a></h2> <h3 id=the-weak-law-of-large-numbers>The Weak Law of Large Numbers<a class=headerlink href=#the-weak-law-of-large-numbers title="Permanent link">&para;</a></h3> <ul> <li> <p><span><span class=MathJax_Preview>X_1, X_2, \cdots, X_n</span><script type=math/tex>X_1, X_2, \cdots, X_n</script></span>, i.i.d.; finite mean <span><span class=MathJax_Preview>\mu</span><script type=math/tex>\mu</script></span> and variance <span><span class=MathJax_Preview>\sigma^2</span><script type=math/tex>\sigma^2</script></span></p> </li> <li> <p>Sample mean <span><span class=MathJax_Preview>M_n</span><script type=math/tex>M_n</script></span>: it is not a constant, but a function of multiple random variables. You can understand it this way. In a experiment which you draw the exam score of student one by one, once you finish the first draw <span><span class=MathJax_Preview>X_1</span><script type=math/tex>X_1</script></span>, the first sample is a fixed real number. But before your first draw, <span><span class=MathJax_Preview>X_1</span><script type=math/tex>X_1</script></span> can be any score and it is a random variable. After you draw a total of <span><span class=MathJax_Preview>n</span><script type=math/tex>n</script></span> scores, you have all the fixed value of <span><span class=MathJax_Preview>X_1, X_2, \cdots, X_n</span><script type=math/tex>X_1, X_2, \cdots, X_n</script></span>, the sample mean is determined for this draw. However, abstractly, sample mean is a random variable, if you draw another <span><span class=MathJax_Preview>n</span><script type=math/tex>n</script></span> example scores to get the sample mean, you get a different set of <span><span class=MathJax_Preview>X_1, X_2, \cdots, X_n</span><script type=math/tex>X_1, X_2, \cdots, X_n</script></span>. The randomness comes from the randomness of different experiments, in each of the experiment, the sample <span><span class=MathJax_Preview>X_i</span><script type=math/tex>X_i</script></span> is not random.</p> </li> <li> <p><span><span class=MathJax_Preview>{\bf E}\big[M_n\big] =\mu</span><script type=math/tex>{\bf E}\big[M_n\big] =\mu</script></span>. Two level of averaging. <span><span class=MathJax_Preview>M_n</span><script type=math/tex>M_n</script></span> itself is averaging over n samples. Since these n samples are drawn randomly, taking the expectation of <span><span class=MathJax_Preview>M_n</span><script type=math/tex>M_n</script></span> is averaging all possible sample means obtained through many (infinity) experiments.</p> </li> <li> <p><span><span class=MathJax_Preview>\textbf{Var}(M_n) = \frac{1}{n^2} \cdot n \cdot \textbf{Var}(X_1) = \frac{\sigma^2}{n}</span><script type=math/tex>\textbf{Var}(M_n) = \frac{1}{n^2} \cdot n \cdot \textbf{Var}(X_1) = \frac{\sigma^2}{n}</script></span>.</p> </li> <li> <p>Apply Chebyshev inequality: <span><span class=MathJax_Preview>\textbf{P}(|M_n - \mu| \ge \epsilon) \le \frac{\textbf{var}(M_n)}{\epsilon^2} = \frac{\sigma^2}{n\epsilon^2} \rightarrow 0</span><script type=math/tex>\textbf{P}(|M_n - \mu| \ge \epsilon) \le \frac{\textbf{var}(M_n)}{\epsilon^2} = \frac{\sigma^2}{n\epsilon^2} \rightarrow 0</script></span>, as <span><span class=MathJax_Preview>n \rightarrow \infty</span><script type=math/tex>n \rightarrow \infty</script></span></p> </li> </ul> <div class="admonition note"> <p class=admonition-title>Weak Law of Large Numbers</p> <p>For <span><span class=MathJax_Preview>\epsilon &gt; 0</span><script type=math/tex>\epsilon > 0</script></span>, <span><span class=MathJax_Preview>\textbf{P}(|M_n - \mu| \ge \epsilon) = \textbf{P}\Big(\Big|\frac{X_1 + X_2 + \cdots + X_n}{n} - \mu\Big| \ge \epsilon \Big) \rightarrow 0</span><script type=math/tex>\textbf{P}(|M_n - \mu| \ge \epsilon) = \textbf{P}\Big(\Big|\frac{X_1 + X_2 + \cdots + X_n}{n} - \mu\Big| \ge \epsilon \Big) \rightarrow 0</script></span>, as <span><span class=MathJax_Preview>n \rightarrow \infty</span><script type=math/tex>n \rightarrow \infty</script></span>.</p> </div> <h3 id=interpreting-the-wlln>Interpreting the WLLN<a class=headerlink href=#interpreting-the-wlln title="Permanent link">&para;</a></h3> <ul> <li>One experiment</li> <li>many measurements <span><span class=MathJax_Preview>X_i = \mu + W_i</span><script type=math/tex>X_i = \mu + W_i</script></span>, where <span><span class=MathJax_Preview>W_i</span><script type=math/tex>W_i</script></span> is noise, <span><span class=MathJax_Preview>\textbf{E}[W_i] = 0</span><script type=math/tex>\textbf{E}[W_i] = 0</script></span>, independent <span><span class=MathJax_Preview>W_i</span><script type=math/tex>W_i</script></span>.</li> <li>Sample mean <span><span class=MathJax_Preview>M_n</span><script type=math/tex>M_n</script></span> is unlikely to be far off from true mean <span><span class=MathJax_Preview>\mu</span><script type=math/tex>\mu</script></span></li> <li>Many independent repetitions of the same experiments</li> <li>event <span><span class=MathJax_Preview>A</span><script type=math/tex>A</script></span>, with <span><span class=MathJax_Preview>p = \textbf{P}(A)</span><script type=math/tex>p = \textbf{P}(A)</script></span></li> <li><span><span class=MathJax_Preview>X_i</span><script type=math/tex>X_i</script></span>: indicator of event <span><span class=MathJax_Preview>A</span><script type=math/tex>A</script></span></li> <li>The sampel mean <span><span class=MathJax_Preview>M_n</span><script type=math/tex>M_n</script></span> is the empirical frequency of event <span><span class=MathJax_Preview>A</span><script type=math/tex>A</script></span></li> </ul> <h3 id=application-of-wlln-polling>Application of WLLN - polling<a class=headerlink href=#application-of-wlln-polling title="Permanent link">&para;</a></h3> <ul> <li>The probability of error greater than <span><span class=MathJax_Preview>\epsilon</span><script type=math/tex>\epsilon</script></span> is smaller than a certain probability.</li> <li>You can use Chebyshev inequality and WLLN to estimate how many samples you need to fulfill a specific error probability requirement.</li> </ul> <h3 id=convergence-in-probability>Convergence in probability<a class=headerlink href=#convergence-in-probability title="Permanent link">&para;</a></h3> <ul> <li>Definition:</li> </ul> <blockquote> <p>A sequence <span><span class=MathJax_Preview>Y_n</span><script type=math/tex>Y_n</script></span> converges in probability to a number <span><span class=MathJax_Preview>a</span><script type=math/tex>a</script></span> if For any <span><span class=MathJax_Preview>\epsilon &gt; 0</span><script type=math/tex>\epsilon > 0</script></span>, <span><span class=MathJax_Preview>\displaystyle \lim_{n\rightarrow\infty}\textbf{P}(|Y_n - a| \ge \epsilon) = 0</span><script type=math/tex>\displaystyle \lim_{n\rightarrow\infty}\textbf{P}(|Y_n - a| \ge \epsilon) = 0</script></span> * Comparison between ordinary convergence and convergence in probability:</p> </blockquote> <p><img alt="understanding convergence in probability" src=http://static.zybuluo.com/iurnah/5uynu5cc4lzzjrnwn4401ccx/Screen%20Shot%202018-11-22%20at%2011.36.38%20AM.png></p> <h3 id=convergence-in-probability-examples>Convergence in probability examples<a class=headerlink href=#convergence-in-probability-examples title="Permanent link">&para;</a></h3> <ul> <li>convergence in probability doesn't imply convergence of the expectations.</li> <li>How to find what value it converges to? Make an educated conjecture about the limit <span><span class=MathJax_Preview>\tau</span><script type=math/tex>\tau</script></span>, write <span><span class=MathJax_Preview>{\bf P}(|Y_n - \tau| \ge \epsilon)</span><script type=math/tex>{\bf P}(|Y_n - \tau| \ge \epsilon)</script></span>, and derive the value of it to where you can observe from the expression that the probability <span><span class=MathJax_Preview>\to 0</span><script type=math/tex>\to 0</script></span>, when <span><span class=MathJax_Preview>n \to \infty</span><script type=math/tex>n \to \infty</script></span>.</li> </ul> <h3 id=related-topics>Related topics<a class=headerlink href=#related-topics title="Permanent link">&para;</a></h3> <ul> <li>Better bounds/approximations on tail probabilities</li> <li>Markov and Chebyshev inequalities</li> <li>Chernoff bound</li> <li>Central limit theorem</li> <li>Different types of convergence</li> <li>Convergence in probability</li> <li>Convergence "with probability 1"</li> <li>Strong law of large numbers</li> <li>Convergence of a sequence of distributions (CDFs) to a limiting CDF.</li> </ul> <h2 id=lecture-19-the-central-limit-theorem-clt>Lecture 19 The Central Limit Theorem (CLT)<a class=headerlink href=#lecture-19-the-central-limit-theorem-clt title="Permanent link">&para;</a></h2> <ul> <li>Considering the sum of random variable <span><span class=MathJax_Preview>S_n = X_1 + X_2, + \cdots + X_n</span><script type=math/tex>S_n = X_1 + X_2, + \cdots + X_n</script></span>, (<span><span class=MathJax_Preview>X_1, X_2, \cdots, X_n</span><script type=math/tex>X_1, X_2, \cdots, X_n</script></span> are i.i.d. with finite mean <span><span class=MathJax_Preview>\mu</span><script type=math/tex>\mu</script></span> and variance <span><span class=MathJax_Preview>\sigma^2</span><script type=math/tex>\sigma^2</script></span>) <span><span class=MathJax_Preview>Z_n = \frac{S_n - n\mu}{\sqrt{n}\sigma}</span><script type=math/tex>Z_n = \frac{S_n - n\mu}{\sqrt{n}\sigma}</script></span>, then we have the central limit Theorem,<blockquote> <p>Central Limit Theorem: For every <span><span class=MathJax_Preview>z</span><script type=math/tex>z</script></span>: <span><span class=MathJax_Preview>\displaystyle \lim_{n \to \infty}\textbf{P}(Z_n \le z) = \textbf{P}(Z \le z)</span><script type=math/tex>\displaystyle \lim_{n \to \infty}\textbf{P}(Z_n \le z) = \textbf{P}(Z \le z)</script></span></p> </blockquote> </li> </ul> <h3 id=what-exactly-does-the-clt-say-practice>What exactly does the CLT say? - Practice<a class=headerlink href=#what-exactly-does-the-clt-say-practice title="Permanent link">&para;</a></h3> <ul> <li>The practice of normal approximations:<blockquote> <p>We have linear expression between <span><span class=MathJax_Preview>S_n</span><script type=math/tex>S_n</script></span> and <span><span class=MathJax_Preview>Z_n</span><script type=math/tex>Z_n</script></span>: <span><span class=MathJax_Preview>S_n = \sqrt n \sigma Z_n + n\mu</span><script type=math/tex>S_n = \sqrt n \sigma Z_n + n\mu</script></span>, since <span><span class=MathJax_Preview>Z_n</span><script type=math/tex>Z_n</script></span> can be treated as if it were normal, <span><span class=MathJax_Preview>S_n</span><script type=math/tex>S_n</script></span> can be treated as if normal: <span><span class=MathJax_Preview>N(n\mu, n\sigma^2)</span><script type=math/tex>N(n\mu, n\sigma^2)</script></span></p> </blockquote> </li> <li>Can we use the CLT when n is "moderate"? i.e. n = 30?</li> <li>Usually, yes. When the distribution of <span><span class=MathJax_Preview>X</span><script type=math/tex>X</script></span> has common features with a normal distribution.</li> <li>symmetry and unimodality help</li> </ul> <h3 id=central-limit-theory-examples>Central Limit Theory examples<a class=headerlink href=#central-limit-theory-examples title="Permanent link">&para;</a></h3> <ul> <li><span><span class=MathJax_Preview>\textbf{P}(S_{\color{red}n} \le {\color{red}a}) \approx {\color{red}b}</span><script type=math/tex>\textbf{P}(S_{\color{red}n} \le {\color{red}a}) \approx {\color{red}b}</script></span> Given two parameters, find the third.</li> <li>Package weight <span><span class=MathJax_Preview>X_i</span><script type=math/tex>X_i</script></span>, i.i.d. exponential, <span><span class=MathJax_Preview>\lambda = 1/2</span><script type=math/tex>\lambda = 1/2</script></span>, <span><span class=MathJax_Preview>\mu = \sigma = 2</span><script type=math/tex>\mu = \sigma = 2</script></span></li> <li>Load container with <span><span class=MathJax_Preview>n = 100</span><script type=math/tex>n = 100</script></span> packages, what's the probability that the overall weight is heavier than 210? <span><span class=MathJax_Preview>\textbf{P}(S_n \ge 210) = ?</span><script type=math/tex>\textbf{P}(S_n \ge 210) = ?</script></span></li> <li>Load container with <span><span class=MathJax_Preview>n = 100</span><script type=math/tex>n = 100</script></span> packages. Choose the "capacity" a so that <span><span class=MathJax_Preview>\textbf{P}(S_n \ge a) \approx 0.05</span><script type=math/tex>\textbf{P}(S_n \ge a) \approx 0.05</script></span></li> <li>Fix the capacity at 210, how large can <span><span class=MathJax_Preview>n</span><script type=math/tex>n</script></span> be, so that <span><span class=MathJax_Preview>\textbf{P}(S_n \ge 210) \approx 0.05</span><script type=math/tex>\textbf{P}(S_n \ge 210) \approx 0.05</script></span></li> <li>Load container until wight exceeds 210, <span><span class=MathJax_Preview>N</span><script type=math/tex>N</script></span> is the number of packages loaded, find <span><span class=MathJax_Preview>\textbf{P}(N &gt; 100)</span><script type=math/tex>\textbf{P}(N > 100)</script></span></li> </ul> <h4 id=airline-booking>Airline booking<a class=headerlink href=#airline-booking title="Permanent link">&para;</a></h4> <blockquote> <p>For any given flight, an airline tries to sell as many tickets as possible. Suppose that on average, of ticket holders fail to show up, all independent of one another. Knowing this, an airline will sell more tickets than there are seats available (i.e., overbook the flight) and hope that there is a sufficient number of ticket holders who do not show up, to compensate for its overbooking. Using the Central Limit Theorem, determine, the maximum number of tickets an airline can sell on a flight with 400 seats so that it can be approximately confident that all ticket holders who do show up will be able to board the plane. Use the de Moivre-Laplace -correction in your calculations. Hint: You may have to solve numerically a quadratic equation.</p> </blockquote> <p>Solution: Each ticket can either be used or not used by the the passager. It can be modeled with the random variable: <span><span class=MathJax_Preview>X \sim \operatorname{Bern} \left({p}\right)</span><script type=math/tex>X \sim \operatorname{Bern} \left({p}\right)</script></span>, <span><span class=MathJax_Preview>p=0.8</span><script type=math/tex>p=0.8</script></span>, then the total passergers presented at airport is <span><span class=MathJax_Preview>S_n = X_1 + X_2 + \cdots + X_n</span><script type=math/tex>S_n = X_1 + X_2 + \cdots + X_n</script></span>, which is a Binomial: <span><span class=MathJax_Preview>S_n \sim \operatorname{B} \left({n, p}\right)</span><script type=math/tex>S_n \sim \operatorname{B} \left({n, p}\right)</script></span>. For binomial random variable, <span><span class=MathJax_Preview>\mu = np = 0.8n, \sigma^2 = np(1-p) = 0.16n</span><script type=math/tex>\mu = np = 0.8n, \sigma^2 = np(1-p) = 0.16n</script></span>. Our requirements is <span><span class=MathJax_Preview>\textbf{P}(S_n \le 400) \approx 0.99</span><script type=math/tex>\textbf{P}(S_n \le 400) \approx 0.99</script></span>. Normalize <span><span class=MathJax_Preview>S_n</span><script type=math/tex>S_n</script></span> in the probability and treat the normalized random variable <span><span class=MathJax_Preview>Z_n</span><script type=math/tex>Z_n</script></span> as a normal distribution, refer to the normal table, and solve the number <span><span class=MathJax_Preview>n</span><script type=math/tex>n</script></span>.</p> <div> <div class=MathJax_Preview>\begin{align*} \textbf{P}(S_n \le 400) &amp;\approx 0.99 \\ \textbf{P}\Big(\frac{S_n - 0.8n}{0.4\sqrt{n}} \le \frac{400.5 - 0.8n}{0.4\sqrt{n}}\Big) &amp;\approx 0.99 \\ \textbf{P}\Big(Z_n \le \frac{400.5 - 0.8n}{0.4\sqrt{n}}\Big) &amp;\approx 0.99 \\ \Phi\Big(\frac{400.5 - 0.8n}{0.4\sqrt{n}}\Big) &amp;\approx 0.99 \\ \frac{400.5 - 0.8n}{0.4\sqrt{n}} &amp;= 2.33 \end{align*}</div> <script type="math/tex; mode=display">\begin{align*}
\textbf{P}(S_n \le 400) &\approx 0.99 \\
\textbf{P}\Big(\frac{S_n - 0.8n}{0.4\sqrt{n}} \le \frac{400.5 - 0.8n}{0.4\sqrt{n}}\Big) &\approx 0.99 \\
\textbf{P}\Big(Z_n \le \frac{400.5 - 0.8n}{0.4\sqrt{n}}\Big) &\approx 0.99 \\
\Phi\Big(\frac{400.5 - 0.8n}{0.4\sqrt{n}}\Big) &\approx 0.99 \\
\frac{400.5 - 0.8n}{0.4\sqrt{n}} &= 2.33
\end{align*}</script> </div> <p>Solve <span><span class=MathJax_Preview>n</span><script type=math/tex>n</script></span> to obtain <span><span class=MathJax_Preview>n = 475</span><script type=math/tex>n = 475</script></span></p> <h3 id=normal-approximation-to-the-binomial>Normal approximation to the binomial<a class=headerlink href=#normal-approximation-to-the-binomial title="Permanent link">&para;</a></h3> <ul> <li>Take <span><span class=MathJax_Preview>X_i</span><script type=math/tex>X_i</script></span> as independent Bernuolli random variables: <span><span class=MathJax_Preview>X \sim \operatorname{Bern} \left({p}\right)</span><script type=math/tex>X \sim \operatorname{Bern} \left({p}\right)</script></span>, <span><span class=MathJax_Preview>0 &lt; p &lt; 1</span><script type=math/tex>0 < p < 1</script></span>, then <span><span class=MathJax_Preview>S_n = X_1 + X_2 + \cdots + X_n</span><script type=math/tex>S_n = X_1 + X_2 + \cdots + X_n</script></span> is Binomial: <span><span class=MathJax_Preview>X \sim \operatorname{B} \left({n, p}\right)</span><script type=math/tex>X \sim \operatorname{B} \left({n, p}\right)</script></span>. Binomial random variable <span><span class=MathJax_Preview>S_n</span><script type=math/tex>S_n</script></span> have <span><span class=MathJax_Preview>\mu = np</span><script type=math/tex>\mu = np</script></span>, <span><span class=MathJax_Preview>\sigma^2=np(1-p)</span><script type=math/tex>\sigma^2=np(1-p)</script></span>. According CLT, The normalized random variable <span><span class=MathJax_Preview>\frac{S_n-np}{\sqrt{np(1-p)}}</span><script type=math/tex>\frac{S_n-np}{\sqrt{np(1-p)}}</script></span> is a standard normal. In order to find the <span><span class=MathJax_Preview>\textbf{P}(S_n \le 21)</span><script type=math/tex>\textbf{P}(S_n \le 21)</script></span> (given <span><span class=MathJax_Preview>n = 36</span><script type=math/tex>n = 36</script></span>, <span><span class=MathJax_Preview>p = 0.5</span><script type=math/tex>p = 0.5</script></span>), we use the equvilent of events to transform the probability <span><span class=MathJax_Preview>\textbf{P}(S_n \le 21)</span><script type=math/tex>\textbf{P}(S_n \le 21)</script></span> into another probability about a normal random vairable <span><span class=MathJax_Preview>\textbf{P}\Big(\frac{S_n - 18}{3} \le \frac{21-28}{3}\Big) = \textbf{P}(Z_n \le 1) = \Phi(1) = 0.8413</span><script type=math/tex>\textbf{P}\Big(\frac{S_n - 18}{3} \le \frac{21-28}{3}\Big) = \textbf{P}(Z_n \le 1) = \Phi(1) = 0.8413</script></span>.</li> <li>The 1/2 correction for integer random variables. To get a improved estimate of binomial from normal, we can take the middle point of two integers. Instead using <span><span class=MathJax_Preview>\textbf{P}(S_n \le 21)</span><script type=math/tex>\textbf{P}(S_n \le 21)</script></span> or <span><span class=MathJax_Preview>\textbf{P}(S_n \lt 22)</span><script type=math/tex>\textbf{P}(S_n \lt 22)</script></span>, we use <span><span class=MathJax_Preview>\textbf{P}(S_n \lt 21.5)</span><script type=math/tex>\textbf{P}(S_n \lt 21.5)</script></span> which is very accurate</li> <li>De Moivre-Laplace CLT to the binomial. To estimate <span><span class=MathJax_Preview>\textbf{P}(S_n=19)</span><script type=math/tex>\textbf{P}(S_n=19)</script></span>, we take <span><span class=MathJax_Preview>\textbf{P}(18.5 \le S_n \le 19.5)</span><script type=math/tex>\textbf{P}(18.5 \le S_n \le 19.5)</script></span> and get an accurate estimate.</li> </ul> <h2 id=lecture-20-introduction-to-classical-statistics>Lecture 20 Introduction to classical statistics<a class=headerlink href=#lecture-20-introduction-to-classical-statistics title="Permanent link">&para;</a></h2> <h3 id=overview-of-the-classical-statistical-framework>Overview of the classical statistical framework<a class=headerlink href=#overview-of-the-classical-statistical-framework title="Permanent link">&para;</a></h3> <table> <thead> <tr> <th align=left>attributes</th> <th>Bayesian</th> <th>classical</th> </tr> </thead> <tbody> <tr> <td align=left>unkonwn</td> <td><span><span class=MathJax_Preview>\Theta</span><script type=math/tex>\Theta</script></span> is r.v.</td> <td><span><span class=MathJax_Preview>\theta</span><script type=math/tex>\theta</script></span> is a fixed value</td> </tr> <tr> <td align=left>known</td> <td><span><span class=MathJax_Preview>p_\Theta</span><script type=math/tex>p_\Theta</script></span> prior distribution and samples <span><span class=MathJax_Preview>X_i</span><script type=math/tex>X_i</script></span></td> <td>only sample <span><span class=MathJax_Preview>X_i</span><script type=math/tex>X_i</script></span></td> </tr> <tr> <td align=left>model</td> <td><span><span class=MathJax_Preview>p_{X \mid \Theta}</span><script type=math/tex>p_{X \mid \Theta}</script></span>, where observation is generated</td> <td><span><span class=MathJax_Preview>p_X(x; \theta)</span><script type=math/tex>p_X(x; \theta)</script></span>,<span><span class=MathJax_Preview>\theta</span><script type=math/tex>\theta</script></span> is a real-valued parameter of the model,</td> </tr> <tr> <td align=left>method</td> <td>use bayes rule to obtain <span><span class=MathJax_Preview>p_{\Theta \mid X}</span><script type=math/tex>p_{\Theta \mid X}</script></span>, <span><span class=MathJax_Preview>\big(p_X = {\int_{-\infty}^{+\infty}} p_\Theta \cdot p_{X \mid \Theta}d\theta\big)</span><script type=math/tex>\big(p_X = {\int_{-\infty}^{+\infty}} p_\Theta \cdot p_{X \mid \Theta}d\theta\big)</script></span>, then determind the estimate</td> <td>Design a estimator <span><span class=MathJax_Preview>\hat\Theta</span><script type=math/tex>\hat\Theta</script></span>, to keep estimate error <span><span class=MathJax_Preview>\hat\Theta - \theta</span><script type=math/tex>\hat\Theta - \theta</script></span> small</td> </tr> <tr> <td align=left>estimator</td> <td>MAP, LMS</td> <td>ML</td> </tr> </tbody> </table> <p><img alt="Screen Shot 2018-11-27 at 11.53.51 AM.png-268.5kB" src=http://static.zybuluo.com/iurnah/9g342so1e6z2xlagd1eqgaf6/Screen%20Shot%202018-11-27%20at%2011.53.51%20AM.png></p> <h3 id=confidence-intervals-interpretation>Confidence intervals interpretation<a class=headerlink href=#confidence-intervals-interpretation title="Permanent link">&para;</a></h3> <blockquote> <p>Every day, I try to estimate an unknown parameter using a fresh data set. I look at the data and then I use some formulas to calculate a 70% confidence interval, <span><span class=MathJax_Preview>[\hat\Theta^-, \hat\Theta^+]</span><script type=math/tex>[\hat\Theta^-, \hat\Theta^+]</script></span>, based on the day's data.</p> </blockquote> <h4 id=classical-statsistic-interpretation>Classical statsistic interpretation<a class=headerlink href=#classical-statsistic-interpretation title="Permanent link">&para;</a></h4> <p>If today I got the confidence interval <span><span class=MathJax_Preview>[0.41, 0.47]</span><script type=math/tex>[0.41, 0.47]</script></span>, that doesn't mean there is <span><span class=MathJax_Preview>70\%</span><script type=math/tex>70\%</script></span> probability that the true value will be inside <span><span class=MathJax_Preview>[0.41, 0.47]</span><script type=math/tex>[0.41, 0.47]</script></span>. What the confidence interval is that 70% of those confidence intervals you obtained in all those days will include the true value. The "confidence" (or probability) can be think of <span><span class=MathJax_Preview>\frac{\text{All CIs that include the true value}}{\text{All CIs that include the true value + CIs that exclude the true value}}</span><script type=math/tex>\frac{\text{All CIs that include the true value}}{\text{All CIs that include the true value + CIs that exclude the true value}}</script></span>. You cannot speak about a particular interval, and once you do that, there is no randomness anymore, the true value is either inside the interval or not, there are no other probabilities so to speak.</p> <h4 id=bayes-interpretation-bayesians-confidence-interval>Bayes' interpretation (Bayesian's Confidence Interval)<a class=headerlink href=#bayes-interpretation-bayesians-confidence-interval title="Permanent link">&para;</a></h4> <p>Alternatively, I decided to use a Bayesian approach, by viewing the unknown parameter, denoted by <span><span class=MathJax_Preview>\Theta</span><script type=math/tex>\Theta</script></span>, as a continuous random variable and assuming a prior PDF for <span><span class=MathJax_Preview>\Theta</span><script type=math/tex>\Theta</script></span>. I observe a specific value <span><span class=MathJax_Preview>x</span><script type=math/tex>x</script></span>, calculate the posterior <span><span class=MathJax_Preview>f_{\Theta}{X}(\theta|x)</span><script type=math/tex>f_{\Theta}{X}(\theta|x)</script></span>, and find out that</p> <div> <div class=MathJax_Preview>\begin{align*} \int _{0.41}^{0.47} f_{\Theta |X}(\theta \, |\, x)\, d\theta =0.70. \end{align*}</div> <script type="math/tex; mode=display">\begin{align*}
\int _{0.41}^{0.47} f_{\Theta |X}(\theta \, |\, x)\, d\theta =0.70.
\end{align*}</script> </div> <p>This time, I can say that there is a probability <span><span class=MathJax_Preview>70\%</span><script type=math/tex>70\%</script></span> that the unknown parameter is inside the (Bayesian) confidence interval <span><span class=MathJax_Preview>[0.41, 0.47]</span><script type=math/tex>[0.41, 0.47]</script></span>.</p> <h4 id=confidence-intervals-for-the-estimation-of-the-mean>Confidence intervals for the estimation of the mean<a class=headerlink href=#confidence-intervals-for-the-estimation-of-the-mean title="Permanent link">&para;</a></h4> <blockquote> <p>I asked you to estimate the mean of i.i.d variables <span><span class=MathJax_Preview>X_1, X_2, \cdots, X_n</span><script type=math/tex>X_1, X_2, \cdots, X_n</script></span> with true mean <span><span class=MathJax_Preview>\theta</span><script type=math/tex>\theta</script></span>, and variance <span><span class=MathJax_Preview>\sigma^2</span><script type=math/tex>\sigma^2</script></span>, and the estimate should achieve <span><span class=MathJax_Preview>95\%</span><script type=math/tex>95\%</script></span> confidence interval. How you proceed with it?</p> </blockquote> <ol> <li>By Central Limit Theory, your estimate values <span><span class=MathJax_Preview>\hat\Theta</span><script type=math/tex>\hat\Theta</script></span> is an normal distribution. By standardizing it, you get the standard normal <span><span class=MathJax_Preview>Z_n = \frac{\hat\Theta - \theta}{\sigma/\sqrt{n}}</span><script type=math/tex>Z_n = \frac{\hat\Theta - \theta}{\sigma/\sqrt{n}}</script></span>. <span><span class=MathJax_Preview>95\%</span><script type=math/tex>95\%</script></span> confidence interverl means standard normal is between symetric <span><span class=MathJax_Preview>95\%</span><script type=math/tex>95\%</script></span> intervals <span><span class=MathJax_Preview>[-b, b]</span><script type=math/tex>[-b, b]</script></span>. By looking up the normal table, <span><span class=MathJax_Preview>b = 1.69</span><script type=math/tex>b = 1.69</script></span> corresponds to the probability <span><span class=MathJax_Preview>97.5\%</span><script type=math/tex>97.5\%</script></span>, so <span><span class=MathJax_Preview>\textbf{P}(-1.69 \le Z_n \le 1.69) = 95\%</span><script type=math/tex>\textbf{P}(-1.69 \le Z_n \le 1.69) = 95\%</script></span> thus <span><span class=MathJax_Preview>\textbf{P}\big(\frac{|\hat\Theta - \theta|}{\sigma/\sqrt{n}} \le 1.69\big) = 95\%</span><script type=math/tex>\textbf{P}\big(\frac{|\hat\Theta - \theta|}{\sigma/\sqrt{n}} \le 1.69\big) = 95\%</script></span>, which can be rewrite as</li> </ol> <div> <div class=MathJax_Preview>\begin{align*} \textbf{P}\Big(\hat\Theta -\frac{1.96\sigma}{\sqrt n} \le \theta \le \hat\Theta +\frac{1.96\sigma}{\sqrt n}\Big) = 95\%. \end{align*}</div> <script type="math/tex; mode=display">\begin{align*}
\textbf{P}\Big(\hat\Theta -\frac{1.96\sigma}{\sqrt n} \le \theta \le \hat\Theta +\frac{1.96\sigma}{\sqrt n}\Big) = 95\%.
\end{align*}</script> </div> <p>This is the way to construct the confidence interval. If you have the <span><span class=MathJax_Preview>\sigma</span><script type=math/tex>\sigma</script></span> and <span><span class=MathJax_Preview>n</span><script type=math/tex>n</script></span>, you have a concrete <span><span class=MathJax_Preview>95\%</span><script type=math/tex>95\%</script></span> confidence interval to report.</p> <p>However, the <span><span class=MathJax_Preview>\sigma</span><script type=math/tex>\sigma</script></span> is usually unknown, what you can do is to use the estimated <span><span class=MathJax_Preview>\hat\Theta</span><script type=math/tex>\hat\Theta</script></span> to estimate the <span><span class=MathJax_Preview>\sigma</span><script type=math/tex>\sigma</script></span>.</p> <h2 id=lecture-21>Lecture 21<a class=headerlink href=#lecture-21 title="Permanent link">&para;</a></h2> <h2 id=lecture-22>Lecture 22<a class=headerlink href=#lecture-22 title="Permanent link">&para;</a></h2> <h2 id=lecture-23>Lecture 23<a class=headerlink href=#lecture-23 title="Permanent link">&para;</a></h2> <h2 id=lecture-24>Lecture 24<a class=headerlink href=#lecture-24 title="Permanent link">&para;</a></h2> <h2 id=lecture-25>Lecture 25<a class=headerlink href=#lecture-25 title="Permanent link">&para;</a></h2> <h2 id=lecture-26>Lecture 26<a class=headerlink href=#lecture-26 title="Permanent link">&para;</a></h2> </article> </div> </div> </main> <footer class=md-footer> <div class=md-footer-nav> <nav class="md-footer-nav__inner md-grid"> <a href=../../cs224n/notes/ title="CS224N NLP with Deep Learning" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel=prev> <div class="md-flex__cell md-flex__cell--shrink"> <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i> </div> <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title"> <span class=md-flex__ellipsis> <span class=md-footer-nav__direction> Previous </span> CS224N NLP with Deep Learning </span> </div> </a> <a href=../../../seedlabs/public-key-cryptography-and-pki/notes/ title="Public Key Cryptography and PKI" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel=next> <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title"> <span class=md-flex__ellipsis> <span class=md-footer-nav__direction> Next </span> Public Key Cryptography and PKI </span> </div> <div class="md-flex__cell md-flex__cell--shrink"> <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i> </div> </a> </nav> </div> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-footer-copyright> <div class=md-footer-copyright__highlight> Copyright &copy; 2020 Rui Han </div> powered by <a href=https://www.mkdocs.org target=_blank rel=noopener>MkDocs</a> and <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs</a> </div> </div> </div> </footer> </div> <script src=../../../assets/javascripts/application.c33a9706.js></script> <script>app.initialize({version:"1.1",url:{base:"../../.."}})</script> <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script> </body> </html>