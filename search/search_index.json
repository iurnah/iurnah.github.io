{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to RUIHAN.ORG \u00b6 Books \u00b6 Accelerated C++ Course Notes \u00b6 Machine Learning CS224N Lecture Notes CS224N Write-up 6.431 Probability Learning From Data 6.828 Operating System Engineering Functional Programming Principles in Scala Leetcode \u00b6 Array Binary Search Binary Search Tree Binary Indexed Tree Backtracking Bread First Search Bit Manipulation Depth First Search Design Divide And Conquer Dynamic Programming Geometry Graph Greedy Hash Heap Interval List Math Queue Recursion Reservoir Sampling Segment Tree Sort Stack String Topological Sort Tree Trie Two Pointers Union Find Research \u00b6 Coalition Game Contextual Multi-Armed Bandit SEED Labs \u00b6 Public-Key Cryptography and PKI","title":"Welcome to RUIHAN.ORG"},{"location":"#welcome-to-ruihanorg","text":"","title":"Welcome to RUIHAN.ORG"},{"location":"#books","text":"Accelerated C++","title":"Books"},{"location":"#course-notes","text":"Machine Learning CS224N Lecture Notes CS224N Write-up 6.431 Probability Learning From Data 6.828 Operating System Engineering Functional Programming Principles in Scala","title":"Course Notes"},{"location":"#leetcode","text":"Array Binary Search Binary Search Tree Binary Indexed Tree Backtracking Bread First Search Bit Manipulation Depth First Search Design Divide And Conquer Dynamic Programming Geometry Graph Greedy Hash Heap Interval List Math Queue Recursion Reservoir Sampling Segment Tree Sort Stack String Topological Sort Tree Trie Two Pointers Union Find","title":"Leetcode"},{"location":"#research","text":"Coalition Game Contextual Multi-Armed Bandit","title":"Research"},{"location":"#seed-labs","text":"Public-Key Cryptography and PKI","title":"SEED Labs"},{"location":"books/accelerated-cpp/notes/","text":"Accelerated C++ Reading Notes \u00b6 Chapter 0 \u00b6 Expression -> results + side effects Every operand has a type std::out has type std::ostream << is left-associative, mean: (std::cout << \"Hello, world!\") << std::endl int main(){} return 0 if return sccessful, otherwise return failure. exercise 0-7 tell us comments like this wouldn't work /*comments1 /*comment2*/comment3*/ is illigle. exercise 0-8 tell us comments like this would work fine // This is a comment that extends over several lines // by using // at the beginning of each line instead of using /* // or */ to delimit comments. Chapter 1 \u00b6 >> begins by discarding whitespace chars (space, tab, backspace, or the end of line) from the input, then reads chars into a virable until it encounters another whitespace character or end-of-file. input-output library saves its output in an internal data structure called a buffer, which it uses to optimize output operations. it use the buffer to accumulate the characters to be written, and flushes the buffer, only when necessary. 3 events cause the flush: when it is full; when it is asked to read from the std input; when we explicitly say to do so; How to print a framed string? std::string constructor: const std::string greeting(greeting.size(), ' ') will generate a whitespace string with length of greeting.size(). character literal should be enclosed by ' ' , string literal should be enclosed by \" \" . Built-in type char and wchar_t , which is big enough for holding characters for languages such as Chinese. Chapter 2 \u00b6 while statement, loop invariant : which is a property that we assert will be true about a while each time it is about to test its condition. i.e. write r line so far // invariant: we have written r rows so far int r = 0 ; // setting r to 0 makes the invariant true while ( r != rows ) { // we can assume that the invariant is true here // writing a row of output makes the invariant false std :: cout << std :: endl ; // incrementing r makes the invariant true again ++ r ; } // we can conclude that the invariant is true here The std::string::size_type type for strings length or . This is to protect the int from being overflow if we have an arbitrary long input. Asymmetric range [n, m) is better then a symmetric range [n, m]. always start the range with 0 such as [0, n) [n, m) have m-n elements, [n, m] have m-n+1 elements. empty range i.e. [n,n) . modulo operation equivalent : x % y <==> x - ((x/y) * y) pay attention to the problem 2-4, adding code to ensure the invariant for outer while. Chapter 3 \u00b6 double is even faster than float, double has at least 15 significant digits. float has at least 6 digits. while(cin >> x) is equal to cin >> x; while(cin) , because >> operator return its left operand streamsize prec = cout.pecision(); and manipulator setprecision(3) ==> 56.5 or 5.65 std::vector<double>::size_type is analogous to the one in string::size_type sort function in <algorithm> header prototype: sort(homework.begin(), homework.end()) , sort in place vector class provide two member functions: begin() and end() calculate the median of a sorted vector homework: vec_sz mid = size / 2 ; double median ; median = size % 2 == 0 ? ( homework [ mid ] + homework [ mid - 1 ]) / 2 : homework [ mid ]; vec.end() return a value that denotes one past the last elelment in v. streamsize The type of the value expected by setprecision() and returned by precision() . Defined in <ios> . The return value of the vector<int>::type_size is unsigned integral number. Doing operation with it could not possibly generate negative value, i.e. if vec.size() is 5, vec.size() - 6 is not negative, it will be positive. Chapter 4 (organize program .h and .cc files, iostream as a argument, exception handling basics) \u00b6 When a program throws an exception, the program stop at the part of the program in which the throw appears. appears, and passes to another part of the program, along with an exception object , which contains information that the caller can use to act on the exception. Domain error: throw domain_error(\"median of an empty vector\") ; Defined in <stdexcept> , it is used in reporting that a function's argument is outside the set of values that the function can accept. domain error is one of logic_error in , there are other type of exception: runtime_error, which includes overflow_error , underflow_error , etc. ref:runtime_error . const means we will promise not modify the variable. & means a reference or an alias. i.e. vector < doubel > homework ; const vector < doubel >& chw = homework ; //chw is a synonym for homework reference to reference is the same as reference to the original variable. when define non const reference, we have to make sure the original variable or reference isn't a const. otherwise, it will be illigel. i.e. with above definition, we cannot do this vector<doubel>& hw2=chw; a const argument could take a const parameter. iostream as a parameter to a function: (alwasy keep in mind that a iostream is a type, it has other properties such as vector or int have.) istream & read_hw ( istream & in , vector < double >& hw ){} __lvalue: __We must pass an lvalue argument to a reference parameter. An lvalue is a value that denotes a nontemporary object. For example, a variable is an lvalue, as is a reference, or the result of calling a function that returns a reference. An expression that generates an arithmetic value, such as sum / count, is not an lvalue. clear() member functions for istream. This is to ensure the eof or non-valid input data will not effect reading the next data. __Alwasy run cin.clear(), before we try to read again. pass by value: vector<double> vec : this will copy the argument, the original will not be modified. pass by reference: vector<double>& vec : this will not copy the argument, but will modify the original argument. This is good convention for object as a parameter, because copying will introduce overhead. pass by const reference: const vector<double>& vec : this will not copy the argument as well as promise not to modify the passed argument. try {} catch {} clause, we normally break down to multiple statements in the try clause, because we want to avoid multiple side effect. For example, try { double final_grade = grade ( midterm , final , homework ); streamsize prec = cout . precision (); cout << \"Your final grade is \" << setprecision ( 3 ) << final_grade << setprecision ( prec ) << endl ; } catch ( domain_error ) { cout << endl << \"You must enter your grades. \" \"Please try again.\" << endl ; return 1 ; } is better then written as try { streamsize prec = cout . precision (); cout << \"Your final grade is \" << setprecision ( 3 ) << grade ( midterm , final , homework ) << setprecision ( prec ); } because the later can generate ambigious error message that not easy to debug. sort object Student_info , we have to use another form of sort(student.begin(), students.end()) with a extra parameter, which is a predicate to compare the two object. i.e. bool compare ( const Student_info & x , cosnt Student_info & y ) { return x . name < y . name ; } sort ( students . begin (), students . end (), compare ); Formatting the output, if we want to do the following Bob 88 Christopher 90 we could do this: maxlen = max ( maxlen , record . name . size ()); cout << students [ i ]. name << string ( maxlen + 1 - students [ i ]. name . size (), ' ' ); Notice the string(num, ' '); instantiate a string has num of spaces. The example program from this chapter is worth of keeping here for references. There is a lot information included in it. //Calculate the grade for many students, including reading the data in, //how to sort according there name and how to format the out put in a nice printing, ect. #include <algorithm> #include <iomanip> #include <iostream> #include <stdexcept> #include <string> #include <vector> using std :: cin ; using std :: cout ; using std :: domain_error ; using std :: endl ; using std :: istream ; using std :: ostream ; using std :: setprecision ; using std :: sort ; using std :: streamsize ; using std :: string ; using std :: vector ; // compute the median of a `vector<double>' // note that calling this function copies the entire argument `vector' double median ( vector < double > vec ) { #ifdef _MSC_VER typedef std :: vector < double >:: size_type vec_sz ; #else typedef vector < double >:: size_type vec_sz ; #endif vec_sz size = vec . size (); if ( size == 0 ) throw domain_error ( \"median of an empty vector\" ); sort ( vec . begin (), vec . end ()); vec_sz mid = size / 2 ; return size % 2 == 0 ? ( vec [ mid ] + vec [ mid - 1 ]) / 2 : vec [ mid ]; } // compute a student's overall grade from midterm and final exam grades and homework grade double grade ( double midterm , double final , double homework ) { return 0.2 * midterm + 0.4 * final + 0.4 * homework ; } // compute a student's overall grade from midterm and final exam grades // and vector of homework grades. // this function does not copy its argument, because `median' does so for us. double grade ( double midterm , double final , const vector < double >& hw ) { if ( hw . size () == 0 ) throw domain_error ( \"student has done no homework\" ); return grade ( midterm , final , median ( hw )); } // read homework grades from an input stream into a `vector<double>' istream & read_hw ( istream & in , vector < double >& hw ) { if ( in ) { // get rid of previous contents hw . clear (); // read homework grades double x ; while ( in >> x ) hw . push_back ( x ); // clear the stream so that input will work for the next student in . clear (); } return in ; } int main () { // ask for and read the student's name cout << \"Please enter your first name: \" ; string name ; cin >> name ; cout << \"Hello, \" << name << \"!\" << endl ; // ask for and read the midterm and final grades cout << \"Please enter your midterm and final exam grades: \" ; double midterm , final ; cin >> midterm >> final ; // ask for the homework grades cout << \"Enter all your homework grades, \" \"followed by end-of-file: \" ; vector < double > homework ; // read the homework grades read_hw ( cin , homework ); // compute and generate the final grade, if possible try { double final_grade = grade ( midterm , final , homework ); streamsize prec = cout . precision (); cout << \"Your final grade is \" << setprecision ( 3 ) << final_grade << setprecision ( prec ) << endl ; } catch ( domain_error ) { cout << endl << \"You must enter your grades. \" \"Please try again.\" << endl ; return 1 ; } return 0 ; } hearder file should declare only the names that are necessary. Header files should use fully qualified names rather than using-declarations. (Avoid using namespace std ;) #include <vector> double median ( std :: vector < double > ); Avoid multiple inclusion #ifndef __THIS_HEADER_H__ #define __THIS_HEADER_H__ //your program #endif type of exceptions logic_error domain_error length_error out_of_range invalid_argument runtime_error range_error overflow_error underflow_error exceptional handling try { // code Initiates a block that might throw an exception. } catch ( t ) { // code } //real use case. try { double final_grade = grade ( students [ i ]); streamsize prec = cout . precision (); cout << setprecision ( 3 ) << final_grade << setprecision ( prec ); } catch ( domain_error e ) { cout << e . what (); } Concludes the try block and handles exceptions that match the type t . The code following the catch performs whatever action is appropriate to handle the exception reported in t . throw e ; Terminates the current function; throws the value e back to the caller. e.what() : return a value that report on what happened to cause the error. str.width([n]) and std::setw(n) both used to set the output width. Chapter 5 (sequential containers (vector, list) and analyzing strings) \u00b6 5.1 \u00b6 write a function extract_fails() to seperate the students that failed the course. The ideas is to use two seperate vector to hold the ones that passed and the ones that failed. To make it better, we only use one vector fails to hold the failed students, and erase them from the original vector. however, be cautions in using the erase memeber function for vectors. 1. the indexes of element after the removed element will change. 2. the size will change. 3. efficiency problem. Introduced the necessity of using iterator . Briefly, it is for efficiency optimization. The indexing is random access, which is more expensive to maintain the data structure properties, such as for vector. iterator allow us to separate the data access manner (sequential v.s. random) in a container, so as to implement different container to cope with a different need. All this work is because of efficiency concerns. 5.2 \u00b6 Beside providing access or modify operations, a iterator is able to restrict the available operations in ways that correspond to what the iterator can handle efficiently. Generally, two type of iterators: const_iterator and iterator . when we do vector<student>::const_iterator iter = S.begin() , there is an explicit type conversion happened because S.begin() is a type of iterator . The usage of iterator vector<int>::iterator iter; , either iter->name or (*iter).name . 5.5 \u00b6 vector and list differ in that if you call erase(iter) member function of the container. vector will invalidate all the iterators following iter in the vector. Even this is the case, we can use erase to delete a element from the container. i.e. iter = students.erase(iter); , iter will point to the next element of the removed element. Similarly, call push_back member function will invalidate all the iters of the vector. while for list container, call erase(iter) only invalidate the iter erased, not others. and call push_back will not invalidate other iterators. Because list doesn't support random access. We cannot use the sort() function from <algorithm> . Instead we have to use the member function that optimized for list container. 5.8 \u00b6 Use inset function to do vertical concatenation ret.insert(ret.end(), bottom.begin(), bottom.end()) Notes \u00b6 students.erase(students.begin()+i) , remove the ith object in the container students . iterator properties: Identifies a container and an element in the container Lets us examine the value stored in that element Provides operations for moving between elements in the container Restricts the available operations in ways that correspond to what the container can handle efficiently iterator types: container-type::iterator : to change the container value container-type::const_iterator : to only read the container value container-type::reverse_iterator container-type::const_reverse_iterator *iter return a lvalue. we can replace (*iter).name with iter->name students.erase(iter) will invalidate all the iterators following the elements that has been removed. After iter = students.erase(iter) , iter will point to the first element following the removed element. From vector to list . vector\u007f list optimized for fast random access optimized for fast insertion and deletion Using push_back to append an element to a vector invalidates all iterators referring to that vector. the erase and push_back operations do not invalidate iterators to other elements. list : doesn't support random access, so the STL <algorithm> library function sort() doens't apply to list string example, split a sentence into words. vector < string > split ( const string & s ) { vector < string > ret ; typedef string :: size_type string_size ; string_size i = 0 ; // invariant: we have processed characters [original value of i, i) while ( i != s . size ()) { // ignore leading blanks // invariant: characters in range [original i, current i) are all spaces while ( i != s . size () && isspace ( s [ i ])) ++ i ; // find end of next word string_size string_size j = i ; // invariant: none of the characters in range [original j, current j)is a space while ( j != s . size () && ! isspace ( s [ j ])) j ++ ; // if we found some nonwhitespace characters if ( i != j ){ // copy from s starting at i and taking j - i chars ret . push_back ( s . substr ( i , j - i )); i = j ; } } return ret ; } isspace is in the header file <cctype> isspace(c) true if c is a whitespace character. isalpha(c) true if c is an alphabetic character. isdigit(c) true if c is a digit character. isalnum(c) true if c is a letter or a digit. ispunct(c) true if c is a punctuation character. isupper(c) true if c is an uppercase letter. islower(c) true if c is a lowercase letter. toupper(c) Yields the uppercase equivalent to c tolower(c) Yields the lowercase equivalent to c In the ret.push_back(s.str(i, j-i)) , the j-i indicate a open range [s[i], s[j]) while (cin >> s) is read one work at a time, because the std::cin seperated by white spaces. It terminate until a invalid input is entered or a EOF. while (getline(cin, s)) is reading one line at a time, it return false when EOF entered or invalid chars. How to framing a word characters. How to cancatenate two vector?, we can do insert(ret.end(), bottom.begin(), bottom.end()) , note the first argument provide the iterator before which the element will be inserted. How to concatenate two pictures horizontally like the bellow pictures in case 1: this is an ************** example * this is an * to * example * illustrate * to * framing * illustrate * * framing * ************** pictures in case 2: ************** this is an * this is an * example * example * to * to * illustrate * illustrate * framing * framing * ************** vector < string > hcat ( const vector < string >& left , const vector < string >& right ) { vector < string > ret ; // add 1 to leave a space between pictures string :: size_type width1 = width ( left ) + 1 ; // indices to look at elements from left and right respectively vector < string >:: size_type i = 0 , j = 0 ; // continue until we've seen all rows from both pictures while ( i != left . size () || j != right . size ()) { // construct new string to hold characters from both pictures string s ; // copy a row from the left-hand side, if there is one if ( i != left . size ()) s = left [ i ++ ]; // pad to full width s += string ( width1 - s . size (), ' ' ); // copy a row from the right-hand side, if there is one if ( j != right . size ()) s += right [ j ++ ]; // add s to the picture we're creating ret . push_back ( s ); } return ret ; } vec.reserve(n) : Reserves space to hold n elements, but does not initialize them. This operation does not change the size of the container. It affects only the frequency with which vector may have to allocate memory in response to repeated calls to insert or push_back. c.rbegin() and c.rend() are iterator refering to the last and (one beyond) the first element in the container that grant access to the container's elements in reverse order. Chapter 6 \u00b6 Chapter 7 \u00b6 Chapter 8 \u00b6 The language feature that implements generic functions is called template functions. template header template<class T> \"instantiation\" Keyword typename , i.e. typedef typename vector<T>::size_type vec_sz; \"you must precede the entire name by typename to let the implementation know to treat the name as a type.\" The C++ standard says nothing about how implementations should manage template instantiation, so every implementation handles instantiation in its own particular way. While we cannot say exactly how your compiler will handle instantiation, there are two important points to keep in mind: The first is that for C++ implementations that follow the traditional edit-compile-link model, instantiation often happens not at compile time, but at link time. It is not until the templates are instantiated that the implementation can verify that the template code can be used with the types that were specified. Hence, it is possible to get what seem like compile-time errors at link time. The second point matters if you write your own templates: Most current implementations require that in order to instantiate a template, the definition of the template, not just the declaration, has to be accessible to the implementation. Generally, this requirement implies access to the source files that define the template, as well as the header file. How the implementation locates the source file differs from one implementation to another. Many implementations expect the header file for the template to include the source file, either directly or via a #include. The most certain way to know what your implementation expects is to check its documentation. parameter type to a generic function should keep consistent. For example, We cannot pass int and double to the following function: tmeplate < class T > T max ( const T & left , const T & right ) { return left > right ? left : right ; } Data structure indepnedence: why we write the find function as find(c.begin(), c.end(), val) ? (it is the only way to write generic functions that works on more than 1 element types) Why not write as the form c.find(val) or find(c, val) ? iterator categories: 1.Sequential read-only access (input iterator) 2.Sequential write-only access (output iterator) 3.Sequentila read-wirte access (input-output iterator) 4.Reverseible access 5.Random access \"input iterator\" - interator support \"++, ==, !=, unary *, and it->first\". We say we give find two input iterators as parameters. template < class In , class X > In find ( In begin , In end , constX & x ) { if ( begin == end || * begin == x ) return ; begin ++ ; return find ( begin , end , x ); } \"output iterator\" -interator support *dest = _value_, dest++, and ++dest template < class In , class Out > Out copy ( In begin , In end , Out dest ) { while ( begin != end ) * dest ++ = * begin ++ ; return dest ; } \"input-output iterator\" - iterator support *it, ++it, it++, (but not --it or it--), it == j, it != j, it->member template < class For , class X > void replace ( For beg , For end , const X & x , const X & y ) { while ( beg != end ){ if ( * beg == x ) * beg = y ; ++ beg ; } } \"reverse interator\" - also support --it and it-- template < class Bi > void reverse ( Bi begin , Bi end ) { while ( begin != end ) { -- end ; if ( begin != end ) swap ( * begin ++ , * end ); } } Random access - support p + n, p - n, n + p, p-q, p[n], (equivalent to *(p + n)) p < q, p > q, p <= q, and p >= q template < class Ran , class X > bool binary_search ( Ran begin , Ran end , const X & x ) { while ( begin < end ) { // find the midpoint of the range Ran mid = begin + ( end - begin ) / 2 ; // see which part of the range contains x; keep looking only in that part if ( x < * mid ) end = mid ; else if ( * mid < x ) begin = mid + 1 ; // if we got here, then *mid == x so we're done else return true ; } return false ; } off-the-end values, it always ensure the range is [begin, end). The advantage? (see section 8.2.7) Input and output iterators input iterator for copy vector < int > v ; // read ints from the standard input and append them to v copy ( istream_iterator < int > ( cin ), istream_iterator < int > (), back_inserter ( v )); ouput iterator for copy // write the elements of v each separated from the other by a space copy ( v . begin (), v . end (), ostream_iterator < int > ( cout , \" \" )); Chapter 9 \u00b6 Using the :: before the function name of a non-member function called by a member function. doubel Student_info :: grad () const { return :: grade ( midterm , final , homework ); } const for member function means this member function will not change the member variable. Only const member functions may be called for const objects. We cannot call non-const functions on const objects. such as read memeber on const Student_info . When we pass a non-const object to a function that take const reference. The function will treat the object as if it were const, and compiler will only permit it to call const memeber functions. When we pass a nonconst object to a function that takes a const reference, then the function treats that object as if it were const, and the compiler will permit it to call only const members of such objects. difference of class and struct : default protection. class --> private between { and first label. explicitly define a accessor read function, string name() const { return n; } will return a copy of member variable n instead return a reference, because we don't want the user to modifiy it. The \"Synthesized constructor\" will initialized the data memebers to a value based on how the object is created. if the object is local variable, will be default-initialized (undefined). If the object is used to init a container element, the members will be value-initialized(zero). Initialization rules: If an object is of a class type that defines one or more constructors, then the appropriate constructor completely controls initialization of the objects of that class. If an object is of built-in type, then value-initializing it sets it to zero, and default-initializing it gives it an undefined value. Otherwise, the object can be only of a class type that does not define any constructors. In that case, value- or default-initializing the object value- or default-initializes each of its data members. This initialization process will be recursive if any of the data members is of a class type with its own constructor. constructor initializers Student_info::Student_info() : final(0), midterm(0){} when we create a object: the implementation allocate memory for the new object. it initializes the object, as directed by the constructor's initializer list. it executes the constructor body. The implementation initializes every data member of every object, regardless of whether the constructor initializer list mentions those members. The constructor body may change these initial values subsequently, but the initialization happens before the constructor body begins execution. It is usually better to give a member an initial value explicitly, rather than assigning to it in the body of the constructor. By initializing rather than assigning a value, we avoid doing the same work twice. Constructors with Arguments: Student_info::Student_info(istream& is) { read(is); } Chapter 10 \u00b6 All you can do with a function is to take its address or call it. Any use of function that is not a call is assumed to be taking its address. function pointer declarition: int (*fp)(int) in which fp is a function pointer, if we have another function definition: int next(int){ return n+1; } we can use it like this fp = &next or fp = next . With & or without it is essentially same. define a function pointer point to a function: vector<string>(*sp)(const string &) = split; we can call the next function such as i = (*fp)(i); or i = fp(i); calling function pointer automatically calling the function itself. function with a return value as a function pointer, can use typedef . For example: //define analysis_fp as the name of the type of an appropriate pointer typedef double ( * analysis_fp )( const vector < Student_info >& ); //get_analysis_ptr returns a pointer to an analysis function analysis_fp get_analysis_ptr (); //the alternative and most important trick that has been played in the S2E tcg components. doubel ( * get_analysis_ptr ())( const vector < Student_info >& ); function pointer as parameter to find_if , Notice the Pred can be any type as long as f(*begin) has meaningful value. bool is_negative ( int n ) { return n < 0 ; } template < class In , class Pred > In find_if ( In begin , In end , Pred f ) { while ( begin != end && ! f ( * begin )) ++ begin ; return begin ; } // call it vector < int >:: iterator i = find_if ( v . begin (), v . end (), is_negative ); <cstddef> header: size_t : unsigned type large enough to hold the size of any object. ptrdiff_t : the type of p - q , p, q are both pointer. static means only initialize once, not everytime the function calle or the object is initialized. sizeof() operator reports the results in bytes . ifstream and ofstream object doesn't like string for file path. It almost always require the name of the file to be a pointer to the initial element of a null-terminated character array. simplicity. What if the string facilities doesn't exist. historical. fstream is earlier than string facilities in c++ compatibility. easier to interface with OS file I/O, which typically use such pointers to communicate. using c_str member function for string literal. `ifstream infile(filepath.c_str()); example that read every file supplied in the commandline. int main ( int argc , char ** argv ) { int fail_count = 0 ; for ( int i = 1 ; i < argc ; i ++ ){ ifstream in ( argv [ i ]); if ( in ){ string s ; while ( getline ( in , s )) cout << s << endl ; } else { cerr << \"cannot open file \" << argv [ i ] << endl ; ++ fail_count ; } } return fail_count ; // very neat trick played here. } Three kinds of memory management automatica memory management (local variable) statically allocated memory ( static int x ) it allocate once and only once before the function contain the statement is ever called. every call to pointer_to_static will return a pointer to the same object. the pointer will be valid as long as the program runs, and invalid afterward. dynamic allocation Allocate object of type T. new T(args) i.e. int* p = new int(32); allocate a object int with initial value is 32. Chapter 11 (Implement a vector class) \u00b6 template function V.S. template class template <typename T> T Vec (T a) { // function body } and template <class T> class Vec { public: // interface private: // implementation } What it does when use new to allocate memory. (i.e. new T[n] ) allocate memory initialize the element by running the default constructor. the class T should have a default constructor. A template class type should have the control over how a object created, copied, assigned, or destroyed. explicit Vec(size_type n) { create(n); } mean using the constructor should be explicitly declared, such as Vec(5) , not vec = 5 Type names for the members. Using typedef such as typedef T* iterator . Define a overloaded operator: like define a function, the type of the operator(uniary or binary) defines how many parameters the function will have. If the operator is a function that is not a member, then the function has as many arguments as the operator has operands. The first argument is bound to the left operand; the second is bound to the right operand. If the operator is defined as a member function, then its left operand is implicitly bound to the object on which the operator is invoked. Member operator functions, therefore, take one less argument than the operator indicates. Index operator MUST be a member function. T& operator[](size_type i) { return data[i]; } , User might also want to only read the element through the index operator, so we can also define another overlaoded version const T& operator[](size_type i) const { return data[i]; } . Notice the index operator will return a reference instead of a value. implicitly copying passing by value in function parameter passing. vector<int> i; double d; d = median(i); return value from a function. ( string line; vector<string> words = split(line); ) explicitly copying assignment: vector<Student_info> vec = vs; Copy constructor: is a member function with the same name as the name of the class template < class T > class Vec { public : Vec ( const Vec & v ); { create ( v . begin (), v . end ()); } //copy constructor }; using reference because we are defining what it means by copy, so we go deep to the granuality of call by reference to avoid copying. copying object shouldn't change the original vector, so we use const. Because the copy of vector object is actually copy the pointer, the new copy of the original object contain the same data, point to the same data area. We should make sure they are not contain the same underlying storage when making copies of objects. We should do this: (note the create function hasn't been implemented yet) template < class T > class Vec { public : Vec ( const Vec & v ) { create ( v . begin (), v . end ()); } //copy constructor made a copy. } assignment operator : it must be defined as a member function.(may have multiple overloaded versions.) Assignment differs from the copy constructor in that assignment always involves obliterating an existing value (the left-hand side) and replacing it with a new value (the right-hand side). template < class T > class Vec { public : Vec & operator = ( const Vec & ); //assignment operator }; template < class T > Vec < T >& Vec < T >:: operator = ( const Vec & rhs ) { if ( & rhs != this ){ uncreate (); create ( rhs . begin (), rhs . end ()); } return * this ; //why we need '*this' here instead of 'this' } return reference uncreate and create return variable scope How to define a tempalte member function outside of the class? When we should have the T in Vec<T>& Vec<T>::operator=(const Vec& rhs) ? the oprator= have two different meanings in C++ Initialization. Such as we do vector<int> vec = v(10); or int a = 10; we are invoking the copy constructor. Initialization involves creating a new object and giving it a value at the same time. Initialization happens: In variable declarations (explicitly) For function parameters on entry to a function (implicitly) For the return value of a function on return from the function (implicitly) In constructor initializers (explicitly) Assignment, we are calling operator= . Assignment (operator=) always obliterates a previous value; initialization never does so. examples: string url_ch = \"~;/?:@=&$-_.+!*'(),\" // initialization,(constructor + copy constructor) string spaces ( url_ch . size (), ' ' ) ; // initialization string y ; // initialization y = url_ch ; // assignment, call the operator= and obliterate a previous value. //more complex ones vector < string > split ( const string & ); // function declaration vector < string > v ; // initialization v = split ( line ); // on entry, initialization of split's parameter from line; // on exit, both initialization of the return value // and assignment to v The declaration of split above is interesting because it defines a return type that is a class type. Assigning a class type return value from a function is a two-step process: First, the copy constructor is run to copy the return value into a temporary at the call site. Then the assignment operator is run to assign the value of that temporary to the left-hand operand. Constructors always control initialization. The operator= member function always controls assignment. Defalut action regarding the copy constructor, assignment operator, and destructor: rule of three: copy constructor, destructor, and assignment operator. if you defind a class, you probably need the following for copy control and assignment operators. T :: T () one or more constructors , perhaps with arguments T ::~ T () the destructor T :: T ( const T & ) the copy constructor T :: operator = ( const T & ) the assignment operator the compiler will invoke them whenever an object of our type is created, copied, assigned, or destroyed. Remember that objects may be created, copied, or destroyed implicitly. Whether implicitly or explicitly, the compiler will invoke the appropriate operation. consideration in design a vector class: constructor type definition index and size (overload operators) copy control destructor Flexible Memory Management, those functions that used to implement the create and uncreate functions. new always initialized every object by using constructor T::T() . If we want to initialized by ourselves, we have to do it twice. allocator<T> class in <memory> library. Members and non member function: T * allocate ( size_t ); void deallocate ( T * , size_t ); void construct ( T * , const T & ) ; void destroy ( T * ); template < class Out , class T > void uninitialized_fill ( Out , Out , const T & ); template < class In , class Out > Out uninitialized_copy ( In , In , Out ); Chapter 12 (Making class objects working like values) \u00b6 Chapter 13 (Inheritance) \u00b6 the derived class will not inherit the following: constuctor, assignment operator, and destructor. Keyword protected allows the derived class to access the private member of the base. derived class is constructed by the following steps: allocate memory for the entire object.(base member and derived class member.) call base constructor to initialize the base part. initialize the member of the derived class by initializer list. call constructor of the derived class. NOTE: However, it doesn't select which base constructor to run, we have to explicitly involke it. \"The derived-class constructor initializer names its base class followed by a (possibly empty) list of arguments. These arguments are the initial values to use in constructing the base- class part; they serve to select the base-class constructor to run in order to initialize the base.\" If we pass Grad* to function that take Core* , Compiler convert grad* to Core* and bind the parameter to a Core* type. Static binding V.S. Dynamic binding. \"The phrase dynamic binding captures the notion that functions may be bound at run time, as opposed to static bindings that happen at compile time.\" Virtual Function: (mainly for pointer and references, not for explicit object, because the later is bind to the function in compile time.) It come into being in the following accasion: bool compare_grade ( const Core & c1 , const Core & c2 ) { return c1 . grade () < c2 . grade (); } which function to call, it has to be decide in run time. The reason is that the parameter type const Core& can also accept a type Grade* . More examples. Core c ; Grad g ; Core * p ; Core & r = g ; c . grade (); // statically bound to Core::grade() g . grade (); // statically bound to Core::grade() p -> grade (); // dynamically bound, depending on the type of the object to which p points r . grade (); // dynamically bound, depending on the type of the object to which p points if we defind the bool compare_grade(const Core C1, const Core C2) , if we pass Grad to it, it cut down to its core part. The two grade() would be identically from Core . If we define pointer parameters, the compiler will convert Grad* to a Core* , and would bind the pointer to the Core part of the Grad object. polymorphism: one type (base type) stand for many types (by reference and poitners). \"C++ supports polymorphism through the dynamic-binding properties of virtual functions. When we call a virtual through a pointer or reference, we make a polymorphic call. The type of the reference (or pointer) is fixed, but the type of the object to which it refers (or points) can be the type of the reference (or pointer) or any type derived from it. Thus, we can potentially call one of many functions through a single type.\" virtual function must be defined, regardless of whether the program calles them. virtual destructor: usually in base not in derived class. it usually empty if not other special thing need todo. virtual properties are inherented, such as virtual function or virtual destructor, the keyword \"virtual\" only need to be defined in the base class, and no need to redeclared in derived class. virtual destructor: when you delete the heap memory using the command delete, the pointer operand for delete might be more than one class types. you have give the compiler right indication what object space to release, we use the virutal destructor to do this, for example: class Core (){ public : virutal ~ Core (){} //empty destructor is enough } In this case the delete will automatically select the synthesized approperiate destructor for base class. A virtual destructor is needed any time it is possible that an object of derived type is destroyed through a pointer to base. A virtual destructor is inherited and we don't need to add the virtual destructor to the derived class such as Grad . Programming technique: handle class. hide the pointer manipulations and encapsulate the pointer to Core . static member function. Static member functions differ from ordinary member functions in that they do not operate on an object of the class type. Unlike other member functions, they are associated with the class, not with a particular object. How to implement copy constructor? give the handle class a virtual function clone() to implement the copy constructor. another wrapper!!! class Core { friend class Student_info ; protected : virtual Core * clone () const { return new Core ( * this );} //as before. }; Notice that the copy constructor didn't defined explicitly. It is synthesized by the implementation. (default copy constructor) Ordinarily, when a derived class redefines a function from the base class, it does so exactly: the parameter list and the return type are identical. However, if the base-class function returns a pointer (or reference) to a base class, then the derived-class function can return a pointer (or reference) to a corresponding derived class. \"Finally, the objects that were allocated inside the read for the Student_info function will be automatically freed when we exit main. On exiting main, the vector will be destroyed. The destructor for vector will destroy each element in students, which will cause the destructor for Student_info to be run. When that destructor runs, it will delete each of the objects allocated in read.\" look at the following piece of code: what will happen, if you mistake on the type of the class. vector < Core > students ; Grad g ( cin ); // read a Grad students . push_back ( g ); // Store only the core part of the object. What will happen is that push_back will expect that it was given a Core object, and will construct a Core element, copying only the Core parts of the object, ignoring whatever is specific to the Grad class. We can control which function to call by specify the scope operator, such as when r is a reference to Grad, we can call the regrade function of Core. r.Core::regrade(100); keep in mind that base function is always hiden if you call the derived class function member when the two are have same form (see 13.6.2 in page 347)","title":"Accelerated C++"},{"location":"books/accelerated-cpp/notes/#accelerated-c-reading-notes","text":"","title":"Accelerated C++ Reading Notes"},{"location":"books/accelerated-cpp/notes/#chapter-0","text":"Expression -> results + side effects Every operand has a type std::out has type std::ostream << is left-associative, mean: (std::cout << \"Hello, world!\") << std::endl int main(){} return 0 if return sccessful, otherwise return failure. exercise 0-7 tell us comments like this wouldn't work /*comments1 /*comment2*/comment3*/ is illigle. exercise 0-8 tell us comments like this would work fine // This is a comment that extends over several lines // by using // at the beginning of each line instead of using /* // or */ to delimit comments.","title":"Chapter 0"},{"location":"books/accelerated-cpp/notes/#chapter-1","text":">> begins by discarding whitespace chars (space, tab, backspace, or the end of line) from the input, then reads chars into a virable until it encounters another whitespace character or end-of-file. input-output library saves its output in an internal data structure called a buffer, which it uses to optimize output operations. it use the buffer to accumulate the characters to be written, and flushes the buffer, only when necessary. 3 events cause the flush: when it is full; when it is asked to read from the std input; when we explicitly say to do so; How to print a framed string? std::string constructor: const std::string greeting(greeting.size(), ' ') will generate a whitespace string with length of greeting.size(). character literal should be enclosed by ' ' , string literal should be enclosed by \" \" . Built-in type char and wchar_t , which is big enough for holding characters for languages such as Chinese.","title":"Chapter 1"},{"location":"books/accelerated-cpp/notes/#chapter-2","text":"while statement, loop invariant : which is a property that we assert will be true about a while each time it is about to test its condition. i.e. write r line so far // invariant: we have written r rows so far int r = 0 ; // setting r to 0 makes the invariant true while ( r != rows ) { // we can assume that the invariant is true here // writing a row of output makes the invariant false std :: cout << std :: endl ; // incrementing r makes the invariant true again ++ r ; } // we can conclude that the invariant is true here The std::string::size_type type for strings length or . This is to protect the int from being overflow if we have an arbitrary long input. Asymmetric range [n, m) is better then a symmetric range [n, m]. always start the range with 0 such as [0, n) [n, m) have m-n elements, [n, m] have m-n+1 elements. empty range i.e. [n,n) . modulo operation equivalent : x % y <==> x - ((x/y) * y) pay attention to the problem 2-4, adding code to ensure the invariant for outer while.","title":"Chapter 2"},{"location":"books/accelerated-cpp/notes/#chapter-3","text":"double is even faster than float, double has at least 15 significant digits. float has at least 6 digits. while(cin >> x) is equal to cin >> x; while(cin) , because >> operator return its left operand streamsize prec = cout.pecision(); and manipulator setprecision(3) ==> 56.5 or 5.65 std::vector<double>::size_type is analogous to the one in string::size_type sort function in <algorithm> header prototype: sort(homework.begin(), homework.end()) , sort in place vector class provide two member functions: begin() and end() calculate the median of a sorted vector homework: vec_sz mid = size / 2 ; double median ; median = size % 2 == 0 ? ( homework [ mid ] + homework [ mid - 1 ]) / 2 : homework [ mid ]; vec.end() return a value that denotes one past the last elelment in v. streamsize The type of the value expected by setprecision() and returned by precision() . Defined in <ios> . The return value of the vector<int>::type_size is unsigned integral number. Doing operation with it could not possibly generate negative value, i.e. if vec.size() is 5, vec.size() - 6 is not negative, it will be positive.","title":"Chapter 3"},{"location":"books/accelerated-cpp/notes/#chapter-4-organize-program-h-and-cc-files-iostream-as-a-argument-exception-handling-basics","text":"When a program throws an exception, the program stop at the part of the program in which the throw appears. appears, and passes to another part of the program, along with an exception object , which contains information that the caller can use to act on the exception. Domain error: throw domain_error(\"median of an empty vector\") ; Defined in <stdexcept> , it is used in reporting that a function's argument is outside the set of values that the function can accept. domain error is one of logic_error in , there are other type of exception: runtime_error, which includes overflow_error , underflow_error , etc. ref:runtime_error . const means we will promise not modify the variable. & means a reference or an alias. i.e. vector < doubel > homework ; const vector < doubel >& chw = homework ; //chw is a synonym for homework reference to reference is the same as reference to the original variable. when define non const reference, we have to make sure the original variable or reference isn't a const. otherwise, it will be illigel. i.e. with above definition, we cannot do this vector<doubel>& hw2=chw; a const argument could take a const parameter. iostream as a parameter to a function: (alwasy keep in mind that a iostream is a type, it has other properties such as vector or int have.) istream & read_hw ( istream & in , vector < double >& hw ){} __lvalue: __We must pass an lvalue argument to a reference parameter. An lvalue is a value that denotes a nontemporary object. For example, a variable is an lvalue, as is a reference, or the result of calling a function that returns a reference. An expression that generates an arithmetic value, such as sum / count, is not an lvalue. clear() member functions for istream. This is to ensure the eof or non-valid input data will not effect reading the next data. __Alwasy run cin.clear(), before we try to read again. pass by value: vector<double> vec : this will copy the argument, the original will not be modified. pass by reference: vector<double>& vec : this will not copy the argument, but will modify the original argument. This is good convention for object as a parameter, because copying will introduce overhead. pass by const reference: const vector<double>& vec : this will not copy the argument as well as promise not to modify the passed argument. try {} catch {} clause, we normally break down to multiple statements in the try clause, because we want to avoid multiple side effect. For example, try { double final_grade = grade ( midterm , final , homework ); streamsize prec = cout . precision (); cout << \"Your final grade is \" << setprecision ( 3 ) << final_grade << setprecision ( prec ) << endl ; } catch ( domain_error ) { cout << endl << \"You must enter your grades. \" \"Please try again.\" << endl ; return 1 ; } is better then written as try { streamsize prec = cout . precision (); cout << \"Your final grade is \" << setprecision ( 3 ) << grade ( midterm , final , homework ) << setprecision ( prec ); } because the later can generate ambigious error message that not easy to debug. sort object Student_info , we have to use another form of sort(student.begin(), students.end()) with a extra parameter, which is a predicate to compare the two object. i.e. bool compare ( const Student_info & x , cosnt Student_info & y ) { return x . name < y . name ; } sort ( students . begin (), students . end (), compare ); Formatting the output, if we want to do the following Bob 88 Christopher 90 we could do this: maxlen = max ( maxlen , record . name . size ()); cout << students [ i ]. name << string ( maxlen + 1 - students [ i ]. name . size (), ' ' ); Notice the string(num, ' '); instantiate a string has num of spaces. The example program from this chapter is worth of keeping here for references. There is a lot information included in it. //Calculate the grade for many students, including reading the data in, //how to sort according there name and how to format the out put in a nice printing, ect. #include <algorithm> #include <iomanip> #include <iostream> #include <stdexcept> #include <string> #include <vector> using std :: cin ; using std :: cout ; using std :: domain_error ; using std :: endl ; using std :: istream ; using std :: ostream ; using std :: setprecision ; using std :: sort ; using std :: streamsize ; using std :: string ; using std :: vector ; // compute the median of a `vector<double>' // note that calling this function copies the entire argument `vector' double median ( vector < double > vec ) { #ifdef _MSC_VER typedef std :: vector < double >:: size_type vec_sz ; #else typedef vector < double >:: size_type vec_sz ; #endif vec_sz size = vec . size (); if ( size == 0 ) throw domain_error ( \"median of an empty vector\" ); sort ( vec . begin (), vec . end ()); vec_sz mid = size / 2 ; return size % 2 == 0 ? ( vec [ mid ] + vec [ mid - 1 ]) / 2 : vec [ mid ]; } // compute a student's overall grade from midterm and final exam grades and homework grade double grade ( double midterm , double final , double homework ) { return 0.2 * midterm + 0.4 * final + 0.4 * homework ; } // compute a student's overall grade from midterm and final exam grades // and vector of homework grades. // this function does not copy its argument, because `median' does so for us. double grade ( double midterm , double final , const vector < double >& hw ) { if ( hw . size () == 0 ) throw domain_error ( \"student has done no homework\" ); return grade ( midterm , final , median ( hw )); } // read homework grades from an input stream into a `vector<double>' istream & read_hw ( istream & in , vector < double >& hw ) { if ( in ) { // get rid of previous contents hw . clear (); // read homework grades double x ; while ( in >> x ) hw . push_back ( x ); // clear the stream so that input will work for the next student in . clear (); } return in ; } int main () { // ask for and read the student's name cout << \"Please enter your first name: \" ; string name ; cin >> name ; cout << \"Hello, \" << name << \"!\" << endl ; // ask for and read the midterm and final grades cout << \"Please enter your midterm and final exam grades: \" ; double midterm , final ; cin >> midterm >> final ; // ask for the homework grades cout << \"Enter all your homework grades, \" \"followed by end-of-file: \" ; vector < double > homework ; // read the homework grades read_hw ( cin , homework ); // compute and generate the final grade, if possible try { double final_grade = grade ( midterm , final , homework ); streamsize prec = cout . precision (); cout << \"Your final grade is \" << setprecision ( 3 ) << final_grade << setprecision ( prec ) << endl ; } catch ( domain_error ) { cout << endl << \"You must enter your grades. \" \"Please try again.\" << endl ; return 1 ; } return 0 ; } hearder file should declare only the names that are necessary. Header files should use fully qualified names rather than using-declarations. (Avoid using namespace std ;) #include <vector> double median ( std :: vector < double > ); Avoid multiple inclusion #ifndef __THIS_HEADER_H__ #define __THIS_HEADER_H__ //your program #endif type of exceptions logic_error domain_error length_error out_of_range invalid_argument runtime_error range_error overflow_error underflow_error exceptional handling try { // code Initiates a block that might throw an exception. } catch ( t ) { // code } //real use case. try { double final_grade = grade ( students [ i ]); streamsize prec = cout . precision (); cout << setprecision ( 3 ) << final_grade << setprecision ( prec ); } catch ( domain_error e ) { cout << e . what (); } Concludes the try block and handles exceptions that match the type t . The code following the catch performs whatever action is appropriate to handle the exception reported in t . throw e ; Terminates the current function; throws the value e back to the caller. e.what() : return a value that report on what happened to cause the error. str.width([n]) and std::setw(n) both used to set the output width.","title":"Chapter 4 (organize program .h and .cc files, iostream as a argument, exception handling basics)"},{"location":"books/accelerated-cpp/notes/#chapter-5-sequential-containers-vector-list-and-analyzing-strings","text":"","title":"Chapter 5 (sequential containers (vector, list) and analyzing strings)"},{"location":"books/accelerated-cpp/notes/#51","text":"write a function extract_fails() to seperate the students that failed the course. The ideas is to use two seperate vector to hold the ones that passed and the ones that failed. To make it better, we only use one vector fails to hold the failed students, and erase them from the original vector. however, be cautions in using the erase memeber function for vectors. 1. the indexes of element after the removed element will change. 2. the size will change. 3. efficiency problem. Introduced the necessity of using iterator . Briefly, it is for efficiency optimization. The indexing is random access, which is more expensive to maintain the data structure properties, such as for vector. iterator allow us to separate the data access manner (sequential v.s. random) in a container, so as to implement different container to cope with a different need. All this work is because of efficiency concerns.","title":"5.1"},{"location":"books/accelerated-cpp/notes/#52","text":"Beside providing access or modify operations, a iterator is able to restrict the available operations in ways that correspond to what the iterator can handle efficiently. Generally, two type of iterators: const_iterator and iterator . when we do vector<student>::const_iterator iter = S.begin() , there is an explicit type conversion happened because S.begin() is a type of iterator . The usage of iterator vector<int>::iterator iter; , either iter->name or (*iter).name .","title":"5.2"},{"location":"books/accelerated-cpp/notes/#55","text":"vector and list differ in that if you call erase(iter) member function of the container. vector will invalidate all the iterators following iter in the vector. Even this is the case, we can use erase to delete a element from the container. i.e. iter = students.erase(iter); , iter will point to the next element of the removed element. Similarly, call push_back member function will invalidate all the iters of the vector. while for list container, call erase(iter) only invalidate the iter erased, not others. and call push_back will not invalidate other iterators. Because list doesn't support random access. We cannot use the sort() function from <algorithm> . Instead we have to use the member function that optimized for list container.","title":"5.5"},{"location":"books/accelerated-cpp/notes/#58","text":"Use inset function to do vertical concatenation ret.insert(ret.end(), bottom.begin(), bottom.end())","title":"5.8"},{"location":"books/accelerated-cpp/notes/#notes","text":"students.erase(students.begin()+i) , remove the ith object in the container students . iterator properties: Identifies a container and an element in the container Lets us examine the value stored in that element Provides operations for moving between elements in the container Restricts the available operations in ways that correspond to what the container can handle efficiently iterator types: container-type::iterator : to change the container value container-type::const_iterator : to only read the container value container-type::reverse_iterator container-type::const_reverse_iterator *iter return a lvalue. we can replace (*iter).name with iter->name students.erase(iter) will invalidate all the iterators following the elements that has been removed. After iter = students.erase(iter) , iter will point to the first element following the removed element. From vector to list . vector\u007f list optimized for fast random access optimized for fast insertion and deletion Using push_back to append an element to a vector invalidates all iterators referring to that vector. the erase and push_back operations do not invalidate iterators to other elements. list : doesn't support random access, so the STL <algorithm> library function sort() doens't apply to list string example, split a sentence into words. vector < string > split ( const string & s ) { vector < string > ret ; typedef string :: size_type string_size ; string_size i = 0 ; // invariant: we have processed characters [original value of i, i) while ( i != s . size ()) { // ignore leading blanks // invariant: characters in range [original i, current i) are all spaces while ( i != s . size () && isspace ( s [ i ])) ++ i ; // find end of next word string_size string_size j = i ; // invariant: none of the characters in range [original j, current j)is a space while ( j != s . size () && ! isspace ( s [ j ])) j ++ ; // if we found some nonwhitespace characters if ( i != j ){ // copy from s starting at i and taking j - i chars ret . push_back ( s . substr ( i , j - i )); i = j ; } } return ret ; } isspace is in the header file <cctype> isspace(c) true if c is a whitespace character. isalpha(c) true if c is an alphabetic character. isdigit(c) true if c is a digit character. isalnum(c) true if c is a letter or a digit. ispunct(c) true if c is a punctuation character. isupper(c) true if c is an uppercase letter. islower(c) true if c is a lowercase letter. toupper(c) Yields the uppercase equivalent to c tolower(c) Yields the lowercase equivalent to c In the ret.push_back(s.str(i, j-i)) , the j-i indicate a open range [s[i], s[j]) while (cin >> s) is read one work at a time, because the std::cin seperated by white spaces. It terminate until a invalid input is entered or a EOF. while (getline(cin, s)) is reading one line at a time, it return false when EOF entered or invalid chars. How to framing a word characters. How to cancatenate two vector?, we can do insert(ret.end(), bottom.begin(), bottom.end()) , note the first argument provide the iterator before which the element will be inserted. How to concatenate two pictures horizontally like the bellow pictures in case 1: this is an ************** example * this is an * to * example * illustrate * to * framing * illustrate * * framing * ************** pictures in case 2: ************** this is an * this is an * example * example * to * to * illustrate * illustrate * framing * framing * ************** vector < string > hcat ( const vector < string >& left , const vector < string >& right ) { vector < string > ret ; // add 1 to leave a space between pictures string :: size_type width1 = width ( left ) + 1 ; // indices to look at elements from left and right respectively vector < string >:: size_type i = 0 , j = 0 ; // continue until we've seen all rows from both pictures while ( i != left . size () || j != right . size ()) { // construct new string to hold characters from both pictures string s ; // copy a row from the left-hand side, if there is one if ( i != left . size ()) s = left [ i ++ ]; // pad to full width s += string ( width1 - s . size (), ' ' ); // copy a row from the right-hand side, if there is one if ( j != right . size ()) s += right [ j ++ ]; // add s to the picture we're creating ret . push_back ( s ); } return ret ; } vec.reserve(n) : Reserves space to hold n elements, but does not initialize them. This operation does not change the size of the container. It affects only the frequency with which vector may have to allocate memory in response to repeated calls to insert or push_back. c.rbegin() and c.rend() are iterator refering to the last and (one beyond) the first element in the container that grant access to the container's elements in reverse order.","title":"Notes"},{"location":"books/accelerated-cpp/notes/#chapter-6","text":"","title":"Chapter 6"},{"location":"books/accelerated-cpp/notes/#chapter-7","text":"","title":"Chapter 7"},{"location":"books/accelerated-cpp/notes/#chapter-8","text":"The language feature that implements generic functions is called template functions. template header template<class T> \"instantiation\" Keyword typename , i.e. typedef typename vector<T>::size_type vec_sz; \"you must precede the entire name by typename to let the implementation know to treat the name as a type.\" The C++ standard says nothing about how implementations should manage template instantiation, so every implementation handles instantiation in its own particular way. While we cannot say exactly how your compiler will handle instantiation, there are two important points to keep in mind: The first is that for C++ implementations that follow the traditional edit-compile-link model, instantiation often happens not at compile time, but at link time. It is not until the templates are instantiated that the implementation can verify that the template code can be used with the types that were specified. Hence, it is possible to get what seem like compile-time errors at link time. The second point matters if you write your own templates: Most current implementations require that in order to instantiate a template, the definition of the template, not just the declaration, has to be accessible to the implementation. Generally, this requirement implies access to the source files that define the template, as well as the header file. How the implementation locates the source file differs from one implementation to another. Many implementations expect the header file for the template to include the source file, either directly or via a #include. The most certain way to know what your implementation expects is to check its documentation. parameter type to a generic function should keep consistent. For example, We cannot pass int and double to the following function: tmeplate < class T > T max ( const T & left , const T & right ) { return left > right ? left : right ; } Data structure indepnedence: why we write the find function as find(c.begin(), c.end(), val) ? (it is the only way to write generic functions that works on more than 1 element types) Why not write as the form c.find(val) or find(c, val) ? iterator categories: 1.Sequential read-only access (input iterator) 2.Sequential write-only access (output iterator) 3.Sequentila read-wirte access (input-output iterator) 4.Reverseible access 5.Random access \"input iterator\" - interator support \"++, ==, !=, unary *, and it->first\". We say we give find two input iterators as parameters. template < class In , class X > In find ( In begin , In end , constX & x ) { if ( begin == end || * begin == x ) return ; begin ++ ; return find ( begin , end , x ); } \"output iterator\" -interator support *dest = _value_, dest++, and ++dest template < class In , class Out > Out copy ( In begin , In end , Out dest ) { while ( begin != end ) * dest ++ = * begin ++ ; return dest ; } \"input-output iterator\" - iterator support *it, ++it, it++, (but not --it or it--), it == j, it != j, it->member template < class For , class X > void replace ( For beg , For end , const X & x , const X & y ) { while ( beg != end ){ if ( * beg == x ) * beg = y ; ++ beg ; } } \"reverse interator\" - also support --it and it-- template < class Bi > void reverse ( Bi begin , Bi end ) { while ( begin != end ) { -- end ; if ( begin != end ) swap ( * begin ++ , * end ); } } Random access - support p + n, p - n, n + p, p-q, p[n], (equivalent to *(p + n)) p < q, p > q, p <= q, and p >= q template < class Ran , class X > bool binary_search ( Ran begin , Ran end , const X & x ) { while ( begin < end ) { // find the midpoint of the range Ran mid = begin + ( end - begin ) / 2 ; // see which part of the range contains x; keep looking only in that part if ( x < * mid ) end = mid ; else if ( * mid < x ) begin = mid + 1 ; // if we got here, then *mid == x so we're done else return true ; } return false ; } off-the-end values, it always ensure the range is [begin, end). The advantage? (see section 8.2.7) Input and output iterators input iterator for copy vector < int > v ; // read ints from the standard input and append them to v copy ( istream_iterator < int > ( cin ), istream_iterator < int > (), back_inserter ( v )); ouput iterator for copy // write the elements of v each separated from the other by a space copy ( v . begin (), v . end (), ostream_iterator < int > ( cout , \" \" ));","title":"Chapter 8"},{"location":"books/accelerated-cpp/notes/#chapter-9","text":"Using the :: before the function name of a non-member function called by a member function. doubel Student_info :: grad () const { return :: grade ( midterm , final , homework ); } const for member function means this member function will not change the member variable. Only const member functions may be called for const objects. We cannot call non-const functions on const objects. such as read memeber on const Student_info . When we pass a non-const object to a function that take const reference. The function will treat the object as if it were const, and compiler will only permit it to call const memeber functions. When we pass a nonconst object to a function that takes a const reference, then the function treats that object as if it were const, and the compiler will permit it to call only const members of such objects. difference of class and struct : default protection. class --> private between { and first label. explicitly define a accessor read function, string name() const { return n; } will return a copy of member variable n instead return a reference, because we don't want the user to modifiy it. The \"Synthesized constructor\" will initialized the data memebers to a value based on how the object is created. if the object is local variable, will be default-initialized (undefined). If the object is used to init a container element, the members will be value-initialized(zero). Initialization rules: If an object is of a class type that defines one or more constructors, then the appropriate constructor completely controls initialization of the objects of that class. If an object is of built-in type, then value-initializing it sets it to zero, and default-initializing it gives it an undefined value. Otherwise, the object can be only of a class type that does not define any constructors. In that case, value- or default-initializing the object value- or default-initializes each of its data members. This initialization process will be recursive if any of the data members is of a class type with its own constructor. constructor initializers Student_info::Student_info() : final(0), midterm(0){} when we create a object: the implementation allocate memory for the new object. it initializes the object, as directed by the constructor's initializer list. it executes the constructor body. The implementation initializes every data member of every object, regardless of whether the constructor initializer list mentions those members. The constructor body may change these initial values subsequently, but the initialization happens before the constructor body begins execution. It is usually better to give a member an initial value explicitly, rather than assigning to it in the body of the constructor. By initializing rather than assigning a value, we avoid doing the same work twice. Constructors with Arguments: Student_info::Student_info(istream& is) { read(is); }","title":"Chapter 9"},{"location":"books/accelerated-cpp/notes/#chapter-10","text":"All you can do with a function is to take its address or call it. Any use of function that is not a call is assumed to be taking its address. function pointer declarition: int (*fp)(int) in which fp is a function pointer, if we have another function definition: int next(int){ return n+1; } we can use it like this fp = &next or fp = next . With & or without it is essentially same. define a function pointer point to a function: vector<string>(*sp)(const string &) = split; we can call the next function such as i = (*fp)(i); or i = fp(i); calling function pointer automatically calling the function itself. function with a return value as a function pointer, can use typedef . For example: //define analysis_fp as the name of the type of an appropriate pointer typedef double ( * analysis_fp )( const vector < Student_info >& ); //get_analysis_ptr returns a pointer to an analysis function analysis_fp get_analysis_ptr (); //the alternative and most important trick that has been played in the S2E tcg components. doubel ( * get_analysis_ptr ())( const vector < Student_info >& ); function pointer as parameter to find_if , Notice the Pred can be any type as long as f(*begin) has meaningful value. bool is_negative ( int n ) { return n < 0 ; } template < class In , class Pred > In find_if ( In begin , In end , Pred f ) { while ( begin != end && ! f ( * begin )) ++ begin ; return begin ; } // call it vector < int >:: iterator i = find_if ( v . begin (), v . end (), is_negative ); <cstddef> header: size_t : unsigned type large enough to hold the size of any object. ptrdiff_t : the type of p - q , p, q are both pointer. static means only initialize once, not everytime the function calle or the object is initialized. sizeof() operator reports the results in bytes . ifstream and ofstream object doesn't like string for file path. It almost always require the name of the file to be a pointer to the initial element of a null-terminated character array. simplicity. What if the string facilities doesn't exist. historical. fstream is earlier than string facilities in c++ compatibility. easier to interface with OS file I/O, which typically use such pointers to communicate. using c_str member function for string literal. `ifstream infile(filepath.c_str()); example that read every file supplied in the commandline. int main ( int argc , char ** argv ) { int fail_count = 0 ; for ( int i = 1 ; i < argc ; i ++ ){ ifstream in ( argv [ i ]); if ( in ){ string s ; while ( getline ( in , s )) cout << s << endl ; } else { cerr << \"cannot open file \" << argv [ i ] << endl ; ++ fail_count ; } } return fail_count ; // very neat trick played here. } Three kinds of memory management automatica memory management (local variable) statically allocated memory ( static int x ) it allocate once and only once before the function contain the statement is ever called. every call to pointer_to_static will return a pointer to the same object. the pointer will be valid as long as the program runs, and invalid afterward. dynamic allocation Allocate object of type T. new T(args) i.e. int* p = new int(32); allocate a object int with initial value is 32.","title":"Chapter 10"},{"location":"books/accelerated-cpp/notes/#chapter-11-implement-a-vector-class","text":"template function V.S. template class template <typename T> T Vec (T a) { // function body } and template <class T> class Vec { public: // interface private: // implementation } What it does when use new to allocate memory. (i.e. new T[n] ) allocate memory initialize the element by running the default constructor. the class T should have a default constructor. A template class type should have the control over how a object created, copied, assigned, or destroyed. explicit Vec(size_type n) { create(n); } mean using the constructor should be explicitly declared, such as Vec(5) , not vec = 5 Type names for the members. Using typedef such as typedef T* iterator . Define a overloaded operator: like define a function, the type of the operator(uniary or binary) defines how many parameters the function will have. If the operator is a function that is not a member, then the function has as many arguments as the operator has operands. The first argument is bound to the left operand; the second is bound to the right operand. If the operator is defined as a member function, then its left operand is implicitly bound to the object on which the operator is invoked. Member operator functions, therefore, take one less argument than the operator indicates. Index operator MUST be a member function. T& operator[](size_type i) { return data[i]; } , User might also want to only read the element through the index operator, so we can also define another overlaoded version const T& operator[](size_type i) const { return data[i]; } . Notice the index operator will return a reference instead of a value. implicitly copying passing by value in function parameter passing. vector<int> i; double d; d = median(i); return value from a function. ( string line; vector<string> words = split(line); ) explicitly copying assignment: vector<Student_info> vec = vs; Copy constructor: is a member function with the same name as the name of the class template < class T > class Vec { public : Vec ( const Vec & v ); { create ( v . begin (), v . end ()); } //copy constructor }; using reference because we are defining what it means by copy, so we go deep to the granuality of call by reference to avoid copying. copying object shouldn't change the original vector, so we use const. Because the copy of vector object is actually copy the pointer, the new copy of the original object contain the same data, point to the same data area. We should make sure they are not contain the same underlying storage when making copies of objects. We should do this: (note the create function hasn't been implemented yet) template < class T > class Vec { public : Vec ( const Vec & v ) { create ( v . begin (), v . end ()); } //copy constructor made a copy. } assignment operator : it must be defined as a member function.(may have multiple overloaded versions.) Assignment differs from the copy constructor in that assignment always involves obliterating an existing value (the left-hand side) and replacing it with a new value (the right-hand side). template < class T > class Vec { public : Vec & operator = ( const Vec & ); //assignment operator }; template < class T > Vec < T >& Vec < T >:: operator = ( const Vec & rhs ) { if ( & rhs != this ){ uncreate (); create ( rhs . begin (), rhs . end ()); } return * this ; //why we need '*this' here instead of 'this' } return reference uncreate and create return variable scope How to define a tempalte member function outside of the class? When we should have the T in Vec<T>& Vec<T>::operator=(const Vec& rhs) ? the oprator= have two different meanings in C++ Initialization. Such as we do vector<int> vec = v(10); or int a = 10; we are invoking the copy constructor. Initialization involves creating a new object and giving it a value at the same time. Initialization happens: In variable declarations (explicitly) For function parameters on entry to a function (implicitly) For the return value of a function on return from the function (implicitly) In constructor initializers (explicitly) Assignment, we are calling operator= . Assignment (operator=) always obliterates a previous value; initialization never does so. examples: string url_ch = \"~;/?:@=&$-_.+!*'(),\" // initialization,(constructor + copy constructor) string spaces ( url_ch . size (), ' ' ) ; // initialization string y ; // initialization y = url_ch ; // assignment, call the operator= and obliterate a previous value. //more complex ones vector < string > split ( const string & ); // function declaration vector < string > v ; // initialization v = split ( line ); // on entry, initialization of split's parameter from line; // on exit, both initialization of the return value // and assignment to v The declaration of split above is interesting because it defines a return type that is a class type. Assigning a class type return value from a function is a two-step process: First, the copy constructor is run to copy the return value into a temporary at the call site. Then the assignment operator is run to assign the value of that temporary to the left-hand operand. Constructors always control initialization. The operator= member function always controls assignment. Defalut action regarding the copy constructor, assignment operator, and destructor: rule of three: copy constructor, destructor, and assignment operator. if you defind a class, you probably need the following for copy control and assignment operators. T :: T () one or more constructors , perhaps with arguments T ::~ T () the destructor T :: T ( const T & ) the copy constructor T :: operator = ( const T & ) the assignment operator the compiler will invoke them whenever an object of our type is created, copied, assigned, or destroyed. Remember that objects may be created, copied, or destroyed implicitly. Whether implicitly or explicitly, the compiler will invoke the appropriate operation. consideration in design a vector class: constructor type definition index and size (overload operators) copy control destructor Flexible Memory Management, those functions that used to implement the create and uncreate functions. new always initialized every object by using constructor T::T() . If we want to initialized by ourselves, we have to do it twice. allocator<T> class in <memory> library. Members and non member function: T * allocate ( size_t ); void deallocate ( T * , size_t ); void construct ( T * , const T & ) ; void destroy ( T * ); template < class Out , class T > void uninitialized_fill ( Out , Out , const T & ); template < class In , class Out > Out uninitialized_copy ( In , In , Out );","title":"Chapter 11 (Implement a vector class)"},{"location":"books/accelerated-cpp/notes/#chapter-12-making-class-objects-working-like-values","text":"","title":"Chapter 12 (Making class objects working like values)"},{"location":"books/accelerated-cpp/notes/#chapter-13-inheritance","text":"the derived class will not inherit the following: constuctor, assignment operator, and destructor. Keyword protected allows the derived class to access the private member of the base. derived class is constructed by the following steps: allocate memory for the entire object.(base member and derived class member.) call base constructor to initialize the base part. initialize the member of the derived class by initializer list. call constructor of the derived class. NOTE: However, it doesn't select which base constructor to run, we have to explicitly involke it. \"The derived-class constructor initializer names its base class followed by a (possibly empty) list of arguments. These arguments are the initial values to use in constructing the base- class part; they serve to select the base-class constructor to run in order to initialize the base.\" If we pass Grad* to function that take Core* , Compiler convert grad* to Core* and bind the parameter to a Core* type. Static binding V.S. Dynamic binding. \"The phrase dynamic binding captures the notion that functions may be bound at run time, as opposed to static bindings that happen at compile time.\" Virtual Function: (mainly for pointer and references, not for explicit object, because the later is bind to the function in compile time.) It come into being in the following accasion: bool compare_grade ( const Core & c1 , const Core & c2 ) { return c1 . grade () < c2 . grade (); } which function to call, it has to be decide in run time. The reason is that the parameter type const Core& can also accept a type Grade* . More examples. Core c ; Grad g ; Core * p ; Core & r = g ; c . grade (); // statically bound to Core::grade() g . grade (); // statically bound to Core::grade() p -> grade (); // dynamically bound, depending on the type of the object to which p points r . grade (); // dynamically bound, depending on the type of the object to which p points if we defind the bool compare_grade(const Core C1, const Core C2) , if we pass Grad to it, it cut down to its core part. The two grade() would be identically from Core . If we define pointer parameters, the compiler will convert Grad* to a Core* , and would bind the pointer to the Core part of the Grad object. polymorphism: one type (base type) stand for many types (by reference and poitners). \"C++ supports polymorphism through the dynamic-binding properties of virtual functions. When we call a virtual through a pointer or reference, we make a polymorphic call. The type of the reference (or pointer) is fixed, but the type of the object to which it refers (or points) can be the type of the reference (or pointer) or any type derived from it. Thus, we can potentially call one of many functions through a single type.\" virtual function must be defined, regardless of whether the program calles them. virtual destructor: usually in base not in derived class. it usually empty if not other special thing need todo. virtual properties are inherented, such as virtual function or virtual destructor, the keyword \"virtual\" only need to be defined in the base class, and no need to redeclared in derived class. virtual destructor: when you delete the heap memory using the command delete, the pointer operand for delete might be more than one class types. you have give the compiler right indication what object space to release, we use the virutal destructor to do this, for example: class Core (){ public : virutal ~ Core (){} //empty destructor is enough } In this case the delete will automatically select the synthesized approperiate destructor for base class. A virtual destructor is needed any time it is possible that an object of derived type is destroyed through a pointer to base. A virtual destructor is inherited and we don't need to add the virtual destructor to the derived class such as Grad . Programming technique: handle class. hide the pointer manipulations and encapsulate the pointer to Core . static member function. Static member functions differ from ordinary member functions in that they do not operate on an object of the class type. Unlike other member functions, they are associated with the class, not with a particular object. How to implement copy constructor? give the handle class a virtual function clone() to implement the copy constructor. another wrapper!!! class Core { friend class Student_info ; protected : virtual Core * clone () const { return new Core ( * this );} //as before. }; Notice that the copy constructor didn't defined explicitly. It is synthesized by the implementation. (default copy constructor) Ordinarily, when a derived class redefines a function from the base class, it does so exactly: the parameter list and the return type are identical. However, if the base-class function returns a pointer (or reference) to a base class, then the derived-class function can return a pointer (or reference) to a corresponding derived class. \"Finally, the objects that were allocated inside the read for the Student_info function will be automatically freed when we exit main. On exiting main, the vector will be destroyed. The destructor for vector will destroy each element in students, which will cause the destructor for Student_info to be run. When that destructor runs, it will delete each of the objects allocated in read.\" look at the following piece of code: what will happen, if you mistake on the type of the class. vector < Core > students ; Grad g ( cin ); // read a Grad students . push_back ( g ); // Store only the core part of the object. What will happen is that push_back will expect that it was given a Core object, and will construct a Core element, copying only the Core parts of the object, ignoring whatever is specific to the Grad class. We can control which function to call by specify the scope operator, such as when r is a reference to Grad, we can call the regrade function of Core. r.Core::regrade(100); keep in mind that base function is always hiden if you call the derived class function member when the two are have same form (see 13.6.2 in page 347)","title":"Chapter 13 (Inheritance)"},{"location":"courses/6.431-probability/notes/","text":"6.041/6.431 Probability - The Science of Uncertainty and Data \u00b6 Lecture 1 (September 10, 2018) \u00b6 Probability Models \u00b6 To specify such a model, it includes 2 steps: Sample space: describe possible outcomes of an experiment Probability laws: describe beliefs about the likelihood of outcomes (or assign a probability to outcomes) Sample space \u00b6 list of possible outcomes must be mutually exclusive, collectively exhaustive, and at the right granularity. Probability laws \u00b6 Probability axioms \u00b6 Motivation: Assign individual probability to a single dot is a challenge (in the dart example, each dot has zero probability), so we assign a probability to the subset of the sample space. Nonnegativity: {\\bf P}(A) \\geq 0 {\\bf P}(A) \\geq 0 Normalization: {\\bf P}(\\Omega) = 1 {\\bf P}(\\Omega) = 1 (Finite) Additivity: if A \\cap B = \\emptyset A \\cap B = \\emptyset then {\\bf P}(A \\cup B) = {\\bf P}(A) + {\\bf P}(B) {\\bf P}(A \\cup B) = {\\bf P}(A) + {\\bf P}(B) (need to be refined in future) Consequences of axioms (properties of probability axioms) \u00b6 {\\bf P}(A) \\leq 1 {\\bf P}(A) \\leq 1 {\\bf P}(\\emptyset) = 0 {\\bf P}(\\emptyset) = 0 {\\bf P}(A) + {\\bf P}(A^c) = 1 {\\bf P}(A) + {\\bf P}(A^c) = 1 A, B, \\text{and } C A, B, \\text{and } C are disjoint: {\\bf P}(A \\cup B \\cup C) = {\\bf P}(A) + {\\bf P}(B) + {\\bf P}(C) {\\bf P}(A \\cup B \\cup C) = {\\bf P}(A) + {\\bf P}(B) + {\\bf P}(C) A_1, \\cdots A_k, A_1, \\cdots A_k, are disjoint: {\\bf P}(A_1 \\cup \\cdots, \\cup A_k) = \\sum_{i - 1}^k{\\bf P}(A_i) {\\bf P}(A_1 \\cup \\cdots, \\cup A_k) = \\sum_{i - 1}^k{\\bf P}(A_i) If A \\subset B A \\subset B , then {\\bf P}(A) \\leq {\\bf P}(B) {\\bf P}(A) \\leq {\\bf P}(B) {\\bf P}(A \\cup B) = {\\bf P}(A) + {\\bf P}(B) - {\\bf P}(A \\cap B) {\\bf P}(A \\cup B) = {\\bf P}(A) + {\\bf P}(B) - {\\bf P}(A \\cap B) {\\bf P}(A \\cup B \\cup C) = {\\bf P}(A) + {\\bf P}(A^c \\cap B) + {\\bf P}(A^c \\cap B^c \\cap C) {\\bf P}(A \\cup B \\cup C) = {\\bf P}(A) + {\\bf P}(A^c \\cap B) + {\\bf P}(A^c \\cap B^c \\cap C) Union Bound: {\\bf P}(A \\cup B) \\leq {\\bf P}(A) + {\\bf P}(B) {\\bf P}(A \\cup B) \\leq {\\bf P}(A) + {\\bf P}(B) Probability calculation steps \u00b6 Specify the sample space Specify a probability law Identify an event of interest Calculate Countable additivity \u00b6 The 3rd probability axiom previously mentioned can be extended to countable set (all integers), which means that the element in the set can be arranged in a sequence. This is contrary to the concept of uncountable set (i.e. a dart board, 2D plane). So you need to distinguish the following concepts: discrete set (countable set) continuous set (uncountable set, impossible to arrange in a sequence) finite set infinite set (possible to be arranged in a sequence) discrete (countable) finite set (i.e. {1, 2, 3}) discrete (countable) infinite set (i.e. all integers, all even integers, all old integers) continuous (uncountable) infinite set (i.e. \\{x | 0 \\leq x \\leq 1\\} \\{x | 0 \\leq x \\leq 1\\} ) Example: Sample space \\{1, 2, 3, \\cdots\\} \\{1, 2, 3, \\cdots\\} , given {\\bf P}(n) = \\frac{1}{2^n}, n = 1, 2, \\cdots {\\bf P}(n) = \\frac{1}{2^n}, n = 1, 2, \\cdots . Check the against the probability axioms that \\sum_{n = 1}^{\\infty}{\\bf P}(n) = 1 \\sum_{n = 1}^{\\infty}{\\bf P}(n) = 1 . What about the probability {\\bf P}(\\text{outcome is even}) {\\bf P}(\\text{outcome is even}) ? Using countable additivity axiom. Countable Additivity Axiom (refined probability axiom) \u00b6 Note if A_1, A_2, A_3, \\cdot A_1, A_2, A_3, \\cdot is an infinite sequence of disjoint events, then {\\bf P}(A_1 \\cup A_2 \\cup A_3 \\cdots) = {\\bf P}(A_1) + {\\bf P}(A_2) + {\\bf P}(A_3) + \\cdots {\\bf P}(A_1 \\cup A_2 \\cup A_3 \\cdots) = {\\bf P}(A_1) + {\\bf P}(A_2) + {\\bf P}(A_3) + \\cdots To refine the 3rd probability axiom, additivity holds only for \"countable\" sequences of events. That means the additivity axiom must be a sequence , with finite or infinite elements. Note the following contradiction, the additivity axiom can not be applied to continuous sample space. A_1, A_2, A_3, \\cdots A_1, A_2, A_3, \\cdots are real coordinates, if you apply the additivity axiom, you will get: \\begin{align*} 1={\\bf P}(\\Omega ) &={\\bf P}\\big (\\{ A_1\\} \\cup \\{ A_2\\} \\cup \\{ A_3\\} \\cdots \\big )\\\\ &={\\bf P}(\\{ A_1\\} )+{\\bf P}(\\{ A_2\\} )+{\\bf P}(\\{ A_3\\} )+\\cdots \\\\ &= 0+0+0+\\cdots =0, \\end{align*} \\begin{align*} 1={\\bf P}(\\Omega ) &={\\bf P}\\big (\\{ A_1\\} \\cup \\{ A_2\\} \\cup \\{ A_3\\} \\cdots \\big )\\\\ &={\\bf P}(\\{ A_1\\} )+{\\bf P}(\\{ A_2\\} )+{\\bf P}(\\{ A_3\\} )+\\cdots \\\\ &= 0+0+0+\\cdots =0, \\end{align*} which is contradicts. This is because \"unit square\" is not a countable set. Interpretations of probability theory \u00b6 frequency of events A? What is P(the president will be reelected)? Probability is often interpreted as: Description of beliefs Betting preference Lecture 2 Conditioning and Baye's rule \u00b6 Conditional probability \u00b6 use new information to review a model Definition {\\bf P}(A|B) = {\\bf P}(A|B) = \"probability of A given that B occurred\" {\\bf P}(A|B) = \\frac{{\\bf P}(A \\cap B)}{{\\bf P}(B)} {\\bf P}(A|B) = \\frac{{\\bf P}(A \\cap B)}{{\\bf P}(B)} , defined only when {\\bf P}(B) > 0 {\\bf P}(B) > 0 Conditional probability share properties of ordinary probabilities (finite and infinite countable additivity) If A \\cap C = \\emptyset A \\cap C = \\emptyset , then {\\bf P}(A \\cup C|B) = P(A|B) + {\\bf P}(C|B) {\\bf P}(A \\cup C|B) = P(A|B) + {\\bf P}(C|B) Multiplication rule \u00b6 {\\bf P}(A \\cap B) = {\\bf P}(A){\\bf P}(B|A) = {\\bf P}(B){\\bf P}(A|B) {\\bf P}(A \\cap B) = {\\bf P}(A){\\bf P}(B|A) = {\\bf P}(B){\\bf P}(A|B) {\\bf P}(A \\cap B \\cap C) = {\\bf P}(A){\\bf P}(B|A){\\bf P}(C|A \\cap B) {\\bf P}(A \\cap B \\cap C) = {\\bf P}(A){\\bf P}(B|A){\\bf P}(C|A \\cap B) It also applies to n events. Total probability theorem (divide and conquer) \u00b6 The settings: Partition of sample space into A_1, A_2, A_3 A_1, A_2, A_3 , have {\\bf P}(A_i) {\\bf P}(A_i) for every i i have {\\bf P}(B|A_i) {\\bf P}(B|A_i) for every i i {\\bf P}(B) = \\sum_{i} {\\bf P}(A_i){\\bf P}(B|A_i) {\\bf P}(B) = \\sum_{i} {\\bf P}(A_i){\\bf P}(B|A_i) It also applies to infinite countable sets according to the countable additivity axiom. Baye's rule \u00b6 The setting is the same as the total probability theorem. Partition of sample space into A_1, A_2, A_3 A_1, A_2, A_3 , have {\\bf P}(A_i) {\\bf P}(A_i) for every i i (initial believes) have {\\bf P}(B|A_i) {\\bf P}(B|A_i) for every i i revise \"believe\" given that B occurred: {\\bf P}(A_i|B) = \\frac{{\\bf P}(A_i \\cap B)}{{\\bf P}(B)} {\\bf P}(A_i|B) = \\frac{{\\bf P}(A_i \\cap B)}{{\\bf P}(B)} It turns out that we can use multiplication rule to calculate the nominator and use the total probability rule to calculate the denominator, thusly {\\bf P}(A_i|B) = \\frac{{\\bf P}(A_i){\\bf P}(B|A_i)}{\\sum_j{\\bf P}(A_j){\\bf P}(B|A_i)} {\\bf P}(A_i|B) = \\frac{{\\bf P}(A_i){\\bf P}(B|A_i)}{\\sum_j{\\bf P}(A_j){\\bf P}(B|A_i)} Lecture 3 Independence \u00b6 Independence of two events \u00b6 Motivation coin toss example: First toss is head or tail doesn't affect the probability of second toss is head. Occurance of A A privides no new information about B B : {\\bf P}(B|A) = {\\bf P}(B) {\\bf P}(B|A) = {\\bf P}(B) . Definition of independence \u00b6 Formal definition {\\bf P}(A \\cap B) = {\\bf P}(A) \\cdot {\\bf P}(B) {\\bf P}(A \\cap B) = {\\bf P}(A) \\cdot {\\bf P}(B) symmetric with respect to events. implies {\\bf P}(A|B) = {\\bf P}(A) {\\bf P}(A|B) = {\\bf P}(A) . Applies even if {\\bf P}(A) = 0 {\\bf P}(A) = 0 . Distinction between disjoin and independence Disjoin usually means dependent, because if one event happens we know the other event will not happen. Independence of event complements \u00b6 If A A and B B are independent, the A A and B^c B^c are independent. If A A and B B are independent, the A^c A^c and B^c B^c are independent. Conditioning independence \u00b6 Ordinary independence properties also apply to conditional independence. But independence doesn't imply conditional independence. Independence of a collection of events \u00b6 Intuition \"definition\": Information on some of the events doesn't change probability related to the remaining events. Formal definition pairwise independence \u00b6 independence V.S. pairwise independence \u00b6 Used two independence fair coin tosses as an example to show that pariwise independence isn't enough to show independence of collection of events. Reliability \u00b6 The king's sibling puzzle Monty Hall problem \u00b6 Lecture 4 \u00b6 Lecture 5 \u00b6 Definition of random variables \u00b6 A random variable (\"r.v.\") associates a value (a number) to every possible outcome. Mathematically, A function from sample space \\Omega \\Omega to the real numbers (discrete or continuous). We can have several random variables defined on the same sample space. (In Lecture 18, X_1, X_2, \\cdots X_1, X_2, \\cdots are independent random variables from the same distribution) A function of one or several random variables is also another random variable. A concrete example to understand the concept of a random variable is take a class of students, selecting students and measuring their body mass or height will give us the random variable W W and H H . The \"Body mass index\" is another random variable that is a function of the random variable W W and H H . meaning of X + Y X + Y random variable X+Y X+Y takes the value x+y x+y , when X X takes the value x x , and Y Y takes the value y y . Probability mass function \u00b6 Bernoulli and indicator random variables \u00b6 Bernoulli random variable X X : X = \\begin{cases} 1, & \\textrm{w.p. } p \\\\ 0, & \\textrm{w.p. } 1-p \\end{cases} X = \\begin{cases} 1, & \\textrm{w.p. } p \\\\ 0, & \\textrm{w.p. } 1-p \\end{cases} models a trial that results in success/failure, heads/tails, etc. indicator random variables of event A A : I_A = 1 I_A = 1 if A A occurs. connection between events and random variables. P(A) = P(I_A = 1) = P_{I_A}(1) P(A) = P(I_A = 1) = P_{I_A}(1) , P(A) P(A) : probability of event A happens P(I_A = 1) P(I_A = 1) : probablistic notition of indication R.V. when I_A I_A equal to 1 1 P_{I_A}(1) P_{I_A}(1) : PMF notation. Discrete uniform random variables \u00b6 Binomial random variables \u00b6 Experiment: toss a biased coin ( P(\\textrm{Head})=p P(\\textrm{Head})=p ) n n times and observe the result (number of heads) Geometric random variables \u00b6 Expectation \u00b6 The expected value rule \u00b6 Linearity of expectations \u00b6 Lecture 6 \u00b6 Lecture 7 \u00b6 Independence, variances, and binomial variance \u00b6 Lecture 8 \u00b6 Exponential random variables \u00b6 PDF: f_{X}(x) = \\begin{cases} \\lambda e^{-\\lambda x}, & x \\ge 0 \\\\ 0, & x < 0 \\end{cases} f_{X}(x) = \\begin{cases} \\lambda e^{-\\lambda x}, & x \\ge 0 \\\\ 0, & x < 0 \\end{cases} Probability of greater than a a , \\begin{align*} P(X \\ge a) &= \\displaystyle \\int_a^{+\\infty} \\lambda e^{-\\lambda x}dx \\\\ &= - \\displaystyle \\int_a^{+\\infty} de^{-\\lambda x} \\\\ &= -e^{-\\lambda x}\\Big|_a^{+\\infty} = -e^{-\\lambda \\cdot +\\infty} + e^{-\\lambda a} = e^{-\\lambda a} \\end{align*} \\begin{align*} P(X \\ge a) &= \\displaystyle \\int_a^{+\\infty} \\lambda e^{-\\lambda x}dx \\\\ &= - \\displaystyle \\int_a^{+\\infty} de^{-\\lambda x} \\\\ &= -e^{-\\lambda x}\\Big|_a^{+\\infty} = -e^{-\\lambda \\cdot +\\infty} + e^{-\\lambda a} = e^{-\\lambda a} \\end{align*} Expectation of exponential random variable {\\bf E}\\big[X\\big] = \\displaystyle \\int_0^{\\infty} x \\cdot \\lambda e^{-\\lambda x} dx = 1/\\lambda {\\bf E}\\big[X\\big] = \\displaystyle \\int_0^{\\infty} x \\cdot \\lambda e^{-\\lambda x} dx = 1/\\lambda Second moment of an exponential random variable {\\bf E}\\big[X^2 \\big] = \\displaystyle \\int_0^{\\infty} x^2 \\cdot \\lambda e^{-\\lambda x} dx = 2/\\lambda^2 {\\bf E}\\big[X^2 \\big] = \\displaystyle \\int_0^{\\infty} x^2 \\cdot \\lambda e^{-\\lambda x} dx = 2/\\lambda^2 Variance of an exponential random variable \\textbf{Var}(X) = {\\bf E}\\big[X^2 \\big] - \\Big({\\bf E}\\big[X\\big]\\Big)^2 = 1/\\lambda^2 \\textbf{Var}(X) = {\\bf E}\\big[X^2 \\big] - \\Big({\\bf E}\\big[X\\big]\\Big)^2 = 1/\\lambda^2 Lecture 9 \u00b6 Conditioning a continuous random variable on an event \u00b6 Memorylessness of the exponential PDF \u00b6 Total probability and expectation theorems \u00b6 Lecture 10 \u00b6 Total probability and total expectation theorems \u00b6 Follows from the discrete cases: Rules Discrete continous Total probability p_X(x) = \\displaystyle \\sum\\limits_{y} p_Y(y)p_{X \\mid Y}(x \\mid y) p_X(x) = \\displaystyle \\sum\\limits_{y} p_Y(y)p_{X \\mid Y}(x \\mid y) f_X(x) = \\displaystyle \\int_{-\\infty}^{+\\infty} f_Y(y) f_{X \\mid Y}(x \\mid y) dy f_X(x) = \\displaystyle \\int_{-\\infty}^{+\\infty} f_Y(y) f_{X \\mid Y}(x \\mid y) dy Conditional expectation {\\bf E}\\big[X \\mid Y = y \\big] = \\displaystyle \\sum\\limits_{x} x p_{X \\mid Y}(x \\mid y) {\\bf E}\\big[X \\mid Y = y \\big] = \\displaystyle \\sum\\limits_{x} x p_{X \\mid Y}(x \\mid y) {\\bf E}\\big[X \\mid Y = y \\big] = \\displaystyle {\\int_{-\\infty}^{+\\infty}} x f_{X \\mid Y}(x \\mid y) dx {\\bf E}\\big[X \\mid Y = y \\big] = \\displaystyle {\\int_{-\\infty}^{+\\infty}} x f_{X \\mid Y}(x \\mid y) dx Total expectation {\\bf E}\\big[X \\big] = \\displaystyle \\sum\\limits_{y} p_{Y}(y){\\bf E} \\big[X \\mid Y = y \\big] {\\bf E}\\big[X \\big] = \\displaystyle \\sum\\limits_{y} p_{Y}(y){\\bf E} \\big[X \\mid Y = y \\big] derivation hint: replace with the conditional expectatioin {\\bf E}\\big[X \\big] = \\displaystyle {\\int_{-\\infty}^{+\\infty}} f_{Y}(y){\\bf E}\\big[X \\mid Y = y \\big] dy {\\bf E}\\big[X \\big] = \\displaystyle {\\int_{-\\infty}^{+\\infty}} f_{Y}(y){\\bf E}\\big[X \\mid Y = y \\big] dy derivation hint: replace with the conditional expectatioin Expected value rule {\\bf E}\\big[ g(X) \\mid Y = y \\big] = \\displaystyle {\\sum\\limits_{x}} g(x) p_{X \\mid Y}(x \\mid y) {\\bf E}\\big[ g(X) \\mid Y = y \\big] = \\displaystyle {\\sum\\limits_{x}} g(x) p_{X \\mid Y}(x \\mid y) {\\bf E}\\big[ g(X) \\mid Y = y \\big] = \\displaystyle {\\int_{-\\infty}^{+\\infty}} g(x) f_{X \\mid Y}(x \\mid y) dx {\\bf E}\\big[ g(X) \\mid Y = y \\big] = \\displaystyle {\\int_{-\\infty}^{+\\infty}} g(x) f_{X \\mid Y}(x \\mid y) dx Solved problems (Lecture 8 - 10) \u00b6 10 Buffon's needle and Monte Carlo Simulation \u00b6 This problem is discussed in the text page 161, Example 3.11. Lecture 11 \u00b6 Lecture 12 \u00b6 Covariance \u00b6 Definition: \\textbf{cov}(X, Y) = {\\bf E}\\big[(X - {\\bf E}[X]) (Y - {\\bf E}[Y])\\big] \\textbf{cov}(X, Y) = {\\bf E}\\big[(X - {\\bf E}[X]) (Y - {\\bf E}[Y])\\big] Covariance tell use whether two r.v.s tend to move together, both to low or both to high Two random variables X X and Y Y are independent indicates: \\textbf{cov}(X, Y) = 0 \\textbf{cov}(X, Y) = 0 , but the converse it not true. Covariance properties \u00b6 Lecture 13 \u00b6 The conditional expectation as a random variable {\\bf E} \\big[X \\mid Y\\big] {\\bf E} \\big[X \\mid Y\\big] \u00b6 Given a function h(x) = x^2 h(x) = x^2 for all x x , and a random variable X X . what is h(X) h(X) ? h(X) h(X) is a function of a random variable X X . h(X) h(X) itself is a random variable that take value x^2 x^2 if X X happens to take the value x x . (you should really understand this statement, it is crucial to understand the concept of conditional expectatioin) For discrete case: {\\bf E}\\big[X \\mid Y = y \\big] = \\displaystyle \\sum\\limits_{x} x p_{X \\mid Y}(x \\mid y) {\\bf E}\\big[X \\mid Y = y \\big] = \\displaystyle \\sum\\limits_{x} x p_{X \\mid Y}(x \\mid y) , in which different vaule of y y will give us the different value of conditional expectation of X X . Then we can really treat {\\bf E}\\big[X \\mid Y = y \\big] {\\bf E}\\big[X \\mid Y = y \\big] as a function of y y , noted as g(y) g(y) . From the above reasoning, because Y Y is a random variable, g(Y) g(Y) is also a reandom variable, that takes the value of g(y) = {\\bf E}\\big[X \\mid Y = y \\big] g(y) = {\\bf E}\\big[X \\mid Y = y \\big] if Y Y happens to take the value y y . g(Y) = {\\bf E}\\big[X \\mid Y\\big] g(Y) = {\\bf E}\\big[X \\mid Y\\big] , is a function of Y Y a random variable has a distribution, mean, variance, etc. The law of iterated expectations \u00b6 By calculate {\\bf E}\\Big[{\\bf E}\\big[X \\mid Y\\big]\\Big] {\\bf E}\\Big[{\\bf E}\\big[X \\mid Y\\big]\\Big] , we could abtain that it is equal to {\\bf E}\\big[X\\big] {\\bf E}\\big[X\\big] The conditional variance as a random variable \\textbf{Var}(X \\mid Y = y) \\textbf{Var}(X \\mid Y = y) \u00b6 definition: \\textbf{Var}(X \\mid Y = y) = {\\bf E}\\Big[\\big(X - {\\bf E}\\big[X \\mid Y = y \\big]\\big)^2\\mid Y = y\\Big] \\textbf{Var}(X \\mid Y = y) = {\\bf E}\\Big[\\big(X - {\\bf E}\\big[X \\mid Y = y \\big]\\big)^2\\mid Y = y\\Big] \\textbf{Var}(X\\mid Y) \\textbf{Var}(X\\mid Y) is also a random variable, it takes the value \\textbf{Var}(X\\mid y) \\textbf{Var}(X\\mid y) when Y Y happens to take the value of y y . Total variance rule: \\textbf{Var}(X) = {\\bf E}\\Big[\\textbf{Var}(X \\mid Y)\\Big] + \\textbf{Var}\\Big({\\bf E}\\big[X\\mid Y\\big]\\Big) \\textbf{Var}(X) = {\\bf E}\\Big[\\textbf{Var}(X \\mid Y)\\Big] + \\textbf{Var}\\Big({\\bf E}\\big[X\\mid Y\\big]\\Big) Lecture 14 Introduction to Bayesian Inference \u00b6 The Bayesian inference framework \u00b6 Treat the unknown \\Theta \\Theta as a random variable. with prior p_{\\Theta} p_{\\Theta} or f_{\\Theta} f_{\\Theta} . With the observations X X , using Bayes rule to obtain the posterior probability of \\Theta \\Theta , P_{\\Theta|X}(\\cdot|X=x) P_{\\Theta|X}(\\cdot|X=x) . Once the posterior is available, the estimator can be obtained via maximize a posterioral probability (MAP) rule or leaset mean squares (LMS). The performance can be measured by \"probability of error\" or \"mean squared error\". Conditional probability of error and total probability of error \u00b6 The conditional probability of other estimators that made a mistake. One of the properties of MAP rule is the it guarantee smallest probability of error. The taotal probability of error is an abstract notion of error for MAP rule to estimator. It can be caculated using total probability theory. Discrete unknowns, discrete observation example (8. Exercise: Discrete unknowns) \u00b6 Let \\Theta_1 \\Theta_1 and \\Theta_2 \\Theta_2 be some unobserved Bernoulli random variables and let be an observation. Conditional on X = x X = x , the posterior joint PMF of \\Theta_1 \\Theta_1 and \\Theta_2 \\Theta_2 is given by p_{\\Theta _1,\\Theta _2\\mid X}(\\theta _1,\\theta _2\\mid x) = \\begin{cases} 0.26, & \\mbox{if } \\theta _1=0, \\theta _2=0, \\\\ 0.26, & \\mbox{if } \\theta _1=0, \\theta _2=1, \\\\ 0.21, & \\mbox{if } \\theta _1=1, \\theta _2=0, \\\\ 0.27, & \\mbox{if } \\theta _1=1, \\theta _2=1, \\\\ 0, & \\mbox{otherwise.} \\end{cases} p_{\\Theta _1,\\Theta _2\\mid X}(\\theta _1,\\theta _2\\mid x) = \\begin{cases} 0.26, & \\mbox{if } \\theta _1=0, \\theta _2=0, \\\\ 0.26, & \\mbox{if } \\theta _1=0, \\theta _2=1, \\\\ 0.21, & \\mbox{if } \\theta _1=1, \\theta _2=0, \\\\ 0.27, & \\mbox{if } \\theta _1=1, \\theta _2=1, \\\\ 0, & \\mbox{otherwise.} \\end{cases} What is the estimate of (\\Theta_1, \\Theta_2) (\\Theta_1, \\Theta_2) provided by MAP rule? (1, 1), because the p_{\\Theta _1,\\Theta _2\\mid X}(1, 1\\mid x) p_{\\Theta _1,\\Theta _2\\mid X}(1, 1\\mid x) is the maximum of all. What is the MAP estimate of \\Theta_1 \\Theta_1 based on X X , that is, the one that maximizes p_{\\Theta_1 \\mid X}(\\theta_1 \\mid x) p_{\\Theta_1 \\mid X}(\\theta_1 \\mid x) ? 0, from the marginal PMF, you can see that p_{\\Theta_1 \\mid X}(0 \\mid x) p_{\\Theta_1 \\mid X}(0 \\mid x) is maximized The moral of this problem is that an estimate of $ \\Theta_1 $ obtained by identifying the maximum of the join PMF of all unknown random variables can be different from the MAP estimate of \\Theta_1 \\Theta_1 from the marginal PMF. Discrete parameter, continuous observation \u00b6 Digital signal transmission continous parameter, continuous observation \u00b6 Analog signal transmission Lecture 15 Linear models with normal noise \u00b6 recognizing normal PDFs \u00b6 f_X(x) = c \\cdot e^{-(\\alpha x^2 + \\beta x + \\gamma)} f_X(x) = c \\cdot e^{-(\\alpha x^2 + \\beta x + \\gamma)} is a normal random variable with \\mu = -\\frac{\\beta}{2\\alpha} \\mu = -\\frac{\\beta}{2\\alpha} and variance \\frac{1}{2\\alpha} \\frac{1}{2\\alpha} The mean squared error \u00b6 The most important take away from this section is that for normal unknown and normal noise signal, X = \\Theta + W X = \\Theta + W , no matter which x_i x_i we observed, the mean squared error estimates {\\hat \\theta} {\\hat \\theta} are the same (the variance of \\Theta \\Theta ). In other words, the remaining uncertainty about \\theta \\theta after an observation is the same no matter what the observation is. The observation only determined the estimated mean of the random variable \\theta \\theta . (See the drawing at the end of the lecture) Measurement, estimate, and learning \u00b6 How to measure the gravitational attraction constant Lecture 16 Least mean square (LMS) estimation \u00b6 LMS estimation without any observations \u00b6 Given the prior p_{\\Theta}(\\theta) p_{\\Theta}(\\theta) unknow random variable \\Theta \\Theta , what's you realy want to estimate? You may interested in a point estimate. You may interested in find the estimator. With no observation, to minimize the mean squared error (MSE), {\\bf E}\\Big[\\big(\\Theta - \\hat \\theta \\big)^2\\Big] {\\bf E}\\Big[\\big(\\Theta - \\hat \\theta \\big)^2\\Big] , the value \\hat \\theta = {\\bf E}\\big[X\\big] \\hat \\theta = {\\bf E}\\big[X\\big] minimize the error. and the minimum error equal to \\textbf{Var}(\\Theta) \\textbf{Var}(\\Theta) Optimal mean squared error: {\\bf E}\\Big[\\big(\\Theta - {\\bf E}\\big[\\Theta \\big] \\big)^2\\Big] = \\textbf{Var}(\\Theta) {\\bf E}\\Big[\\big(\\Theta - {\\bf E}\\big[\\Theta \\big] \\big)^2\\Big] = \\textbf{Var}(\\Theta) . Because {\\bf E}\\big[\\Theta \\big] {\\bf E}\\big[\\Theta \\big] is the optimal value that minimize the MSE, if we replace \\hat \\theta \\hat \\theta with {\\bf E}\\big[\\Theta \\big] {\\bf E}\\big[\\Theta \\big] in the expression MSE, it coincidently match the definition of variance. You can alse use the {\\bf E}\\Big[\\big(\\Theta - \\hat \\theta \\big)^2\\Big] = \\textbf{Var}(\\Theta - \\hat \\theta) + \\Big({\\bf E}\\big[\\Theta - \\hat \\theta \\big]\\Big)^2 {\\bf E}\\Big[\\big(\\Theta - \\hat \\theta \\big)^2\\Big] = \\textbf{Var}(\\Theta - \\hat \\theta) + \\Big({\\bf E}\\big[\\Theta - \\hat \\theta \\big]\\Big)^2 to derive \\hat \\theta = {\\bf E}\\big[X\\big] \\hat \\theta = {\\bf E}\\big[X\\big] is the optimal value. LMS estimation; single unknown and observation \u00b6 Goal: interested in a point estimate \\hat \\theta \\hat \\theta of unknow random variable \\Theta \\Theta , with prior p_{\\Theta}(\\theta) p_{\\Theta}(\\theta) . (Given observation X X ; model p_{X\\mid \\Theta}(x\\mid \\theta) p_{X\\mid \\Theta}(x\\mid \\theta) ) We want to minimize the MSE, because this time we have the particular observation X = x X = x , we now live in a conditional universe, we need to minimize the conditional MSE, {\\bf E}\\Big[\\big(\\Theta - {\\bf E}\\big[\\Theta \\big] \\big)^2 \\mid X = x \\Big] {\\bf E}\\Big[\\big(\\Theta - {\\bf E}\\big[\\Theta \\big] \\big)^2 \\mid X = x \\Big] , the optimal value is \\hat \\theta = {\\bf E}\\big[\\Theta \\mid X = x\\big] \\hat \\theta = {\\bf E}\\big[\\Theta \\mid X = x\\big] By the \"happens to take\" reasoning and the iterated expection rule, we can achieve the conclusion: that \\hat \\Theta_{LMS} = {\\bf E}\\big[\\Theta \\mid X\\big] \\hat \\Theta_{LMS} = {\\bf E}\\big[\\Theta \\mid X\\big] minimize the MSE {\\bf E}\\Big[\\big(\\Theta - g(X) \\big)^2 \\Big] {\\bf E}\\Big[\\big(\\Theta - g(X) \\big)^2 \\Big] over all estimators \\hat \\Theta = g(X) \\hat \\Theta = g(X) . LMS performance evaluation \u00b6 MSE: {\\bf E}\\Big[\\big(\\Theta - {\\bf E}\\big[\\Theta \\mid X = x\\big] \\big)^2 \\mid X = x \\Big] = \\textbf{Var}(\\Theta \\mid X = x) {\\bf E}\\Big[\\big(\\Theta - {\\bf E}\\big[\\Theta \\mid X = x\\big] \\big)^2 \\mid X = x \\Big] = \\textbf{Var}(\\Theta \\mid X = x) Expected performance of the design: {\\bf E}\\Big[\\big(\\Theta - {\\bf E}\\big[\\Theta \\mid X\\big] \\big)^2 \\Big] = {\\bf E}\\Big[\\textbf{Var}(\\Theta \\mid X)\\Big] {\\bf E}\\Big[\\big(\\Theta - {\\bf E}\\big[\\Theta \\mid X\\big] \\big)^2 \\Big] = {\\bf E}\\Big[\\textbf{Var}(\\Theta \\mid X)\\Big] LMS relavant to estimation (not hypothesis testing) Same as MAP if the posterior is unimodal and symmetric around the mean. e.g. when the posterior is normal (the case in \"linear-normal\" models) The multidimensional case \u00b6 Lecture 17 Linear least mean squares (LLMS) estimation \u00b6 Lecture 18 Inequalities, convergence, and the Weak Law of Large Numbers \u00b6 The Weak Law of Large Numbers \u00b6 X_1, X_2, \\cdots, X_n X_1, X_2, \\cdots, X_n , i.i.d.; finite mean \\mu \\mu and variance \\sigma^2 \\sigma^2 Sample mean M_n M_n : it is not a constant, but a function of multiple random variables. You can understand it this way. In an experiment which you draw the exam score of student one by one, once you finish the first draw X_1 X_1 , the first sample is a fixed real number. But before your first draw, X_1 X_1 can be any score and it is a random variable. After you draw a total of n n scores, you have all the fixed value of X_1, X_2, \\cdots, X_n X_1, X_2, \\cdots, X_n , the sample mean is determined for this draw. However, abstractly, sample mean is a random variable, if you draw another n n example scores to get the sample mean, you get a different set of X_1, X_2, \\cdots, X_n X_1, X_2, \\cdots, X_n . The randomness comes from the randomness of different experiments, in each of the experiment, the sample X_i X_i is not random. {\\bf E}\\big[M_n\\big] =\\mu {\\bf E}\\big[M_n\\big] =\\mu . Two level of averaging. M_n M_n itself is averaging over n samples. Since these n samples are drawn randomly, taking the expectation of M_n M_n is averaging all possible sample means obtained through many (infinity) experiments. \\textbf{Var}(M_n) = \\frac{1}{n^2} \\cdot n \\cdot \\textbf{Var}(X_1) = \\frac{\\sigma^2}{n} \\textbf{Var}(M_n) = \\frac{1}{n^2} \\cdot n \\cdot \\textbf{Var}(X_1) = \\frac{\\sigma^2}{n} . Apply Chebyshev inequality: \\textbf{P}(|M_n - \\mu| \\ge \\epsilon) \\le \\frac{\\textbf{var}(M_n)}{\\epsilon^2} = \\frac{\\sigma^2}{n\\epsilon^2} \\rightarrow 0 \\textbf{P}(|M_n - \\mu| \\ge \\epsilon) \\le \\frac{\\textbf{var}(M_n)}{\\epsilon^2} = \\frac{\\sigma^2}{n\\epsilon^2} \\rightarrow 0 , as n \\rightarrow \\infty n \\rightarrow \\infty Weak Law of Large Numbers For \\epsilon > 0 \\epsilon > 0 , \\textbf{P}(|M_n - \\mu| \\ge \\epsilon) = \\textbf{P}\\Big(\\Big|\\frac{X_1 + X_2 + \\cdots + X_n}{n} - \\mu\\Big| \\ge \\epsilon \\Big) \\rightarrow 0 \\textbf{P}(|M_n - \\mu| \\ge \\epsilon) = \\textbf{P}\\Big(\\Big|\\frac{X_1 + X_2 + \\cdots + X_n}{n} - \\mu\\Big| \\ge \\epsilon \\Big) \\rightarrow 0 , as n \\rightarrow \\infty n \\rightarrow \\infty . Interpreting the WLLN \u00b6 One experiment many measurements X_i = \\mu + W_i X_i = \\mu + W_i , where W_i W_i is noise, \\textbf{E}[W_i] = 0 \\textbf{E}[W_i] = 0 , independent W_i W_i . Sample mean M_n M_n is unlikely to be far off from true mean \\mu \\mu Many independent repetitions of the same experiments event A A , with p = \\textbf{P}(A) p = \\textbf{P}(A) X_i X_i : indicator of event A A The sampel mean M_n M_n is the empirical frequency of event A A Application of WLLN - polling \u00b6 The probability of error greater than \\epsilon \\epsilon is smaller than a certain probability. You can use Chebyshev inequality and WLLN to estimate how many samples you need to fulfill a specific error probability requirement. Convergence in probability \u00b6 Definition: A sequence Y_n Y_n converges in probability to a number a a if For any \\epsilon > 0 \\epsilon > 0 , \\displaystyle \\lim_{n\\rightarrow\\infty}\\textbf{P}(|Y_n - a| \\ge \\epsilon) = 0 \\displaystyle \\lim_{n\\rightarrow\\infty}\\textbf{P}(|Y_n - a| \\ge \\epsilon) = 0 * Comparison between ordinary convergence and convergence in probability: Convergence in probability examples \u00b6 convergence in probability doesn't imply convergence of the expectations. How to find what value it converges to? Make an educated conjecture about the limit \\tau \\tau , write {\\bf P}(|Y_n - \\tau| \\ge \\epsilon) {\\bf P}(|Y_n - \\tau| \\ge \\epsilon) , and derive the value of it to where you can observe from the expression that the probability \\to 0 \\to 0 , when n \\to \\infty n \\to \\infty . Related topics \u00b6 Better bounds/approximations on tail probabilities Markov and Chebyshev inequalities Chernoff bound Central limit theorem Different types of convergence Convergence in probability Convergence \"with probability 1\" Strong law of large numbers Convergence of a sequence of distributions (CDFs) to a limiting CDF. Lecture 19 The Central Limit Theorem (CLT) \u00b6 Considering the sum of random variable S_n = X_1 + X_2, + \\cdots + X_n S_n = X_1 + X_2, + \\cdots + X_n , ( X_1, X_2, \\cdots, X_n X_1, X_2, \\cdots, X_n are i.i.d. with finite mean \\mu \\mu and variance \\sigma^2 \\sigma^2 ) Z_n = \\frac{S_n - n\\mu}{\\sqrt{n}\\sigma} Z_n = \\frac{S_n - n\\mu}{\\sqrt{n}\\sigma} , then we have the central limit Theorem, Central Limit Theorem: For every z z : \\displaystyle \\lim_{n \\to \\infty}\\textbf{P}(Z_n \\le z) = \\textbf{P}(Z \\le z) \\displaystyle \\lim_{n \\to \\infty}\\textbf{P}(Z_n \\le z) = \\textbf{P}(Z \\le z) What exactly does the CLT say? - Practice \u00b6 The practice of normal approximations: We have linear expression between S_n S_n and Z_n Z_n : S_n = \\sqrt n \\sigma Z_n + n\\mu S_n = \\sqrt n \\sigma Z_n + n\\mu , since Z_n Z_n can be treated as if it were normal, S_n S_n can be treated as if normal: N(n\\mu, n\\sigma^2) N(n\\mu, n\\sigma^2) Can we use the CLT when n is \"moderate\"? i.e. n = 30? Usually, yes. When the distribution of X X has common features with a normal distribution. symmetry and unimodality help Central Limit Theory examples \u00b6 \\textbf{P}(S_{\\color{red}n} \\le {\\color{red}a}) \\approx {\\color{red}b} \\textbf{P}(S_{\\color{red}n} \\le {\\color{red}a}) \\approx {\\color{red}b} Given two parameters, find the third. Package weight X_i X_i , i.i.d. exponential, \\lambda = 1/2 \\lambda = 1/2 , \\mu = \\sigma = 2 \\mu = \\sigma = 2 Load container with n = 100 n = 100 packages, what's the probability that the overall weight is heavier than 210? \\textbf{P}(S_n \\ge 210) = ? \\textbf{P}(S_n \\ge 210) = ? Load container with n = 100 n = 100 packages. Choose the \"capacity\" a so that \\textbf{P}(S_n \\ge a) \\approx 0.05 \\textbf{P}(S_n \\ge a) \\approx 0.05 Fix the capacity at 210, how large can n n be, so that \\textbf{P}(S_n \\ge 210) \\approx 0.05 \\textbf{P}(S_n \\ge 210) \\approx 0.05 Load container until wight exceeds 210, N N is the number of packages loaded, find \\textbf{P}(N > 100) \\textbf{P}(N > 100) Airline booking \u00b6 For any given flight, an airline tries to sell as many tickets as possible. Suppose that on average, of ticket holders fail to show up, all independent of one another. Knowing this, an airline will sell more tickets than there are seats available (i.e., overbook the flight) and hope that there is a sufficient number of ticket holders who do not show up, to compensate for its overbooking. Using the Central Limit Theorem, determine, the maximum number of tickets an airline can sell on a flight with 400 seats so that it can be approximately confident that all ticket holders who do show up will be able to board the plane. Use the de Moivre-Laplace -correction in your calculations. Hint: You may have to solve numerically a quadratic equation. Solution: Each ticket can either be used or not used by the the passager. It can be modeled with the random variable: X \\sim \\operatorname{Bern} \\left({p}\\right) X \\sim \\operatorname{Bern} \\left({p}\\right) , p=0.8 p=0.8 , then the total passergers presented at airport is S_n = X_1 + X_2 + \\cdots + X_n S_n = X_1 + X_2 + \\cdots + X_n , which is a Binomial: S_n \\sim \\operatorname{B} \\left({n, p}\\right) S_n \\sim \\operatorname{B} \\left({n, p}\\right) . For binomial random variable, \\mu = np = 0.8n, \\sigma^2 = np(1-p) = 0.16n \\mu = np = 0.8n, \\sigma^2 = np(1-p) = 0.16n . Our requirements is \\textbf{P}(S_n \\le 400) \\approx 0.99 \\textbf{P}(S_n \\le 400) \\approx 0.99 . Normalize S_n S_n in the probability and treat the normalized random variable Z_n Z_n as a normal distribution, refer to the normal table, and solve the number n n . \\begin{align*} \\textbf{P}(S_n \\le 400) &\\approx 0.99 \\\\ \\textbf{P}\\Big(\\frac{S_n - 0.8n}{0.4\\sqrt{n}} \\le \\frac{400.5 - 0.8n}{0.4\\sqrt{n}}\\Big) &\\approx 0.99 \\\\ \\textbf{P}\\Big(Z_n \\le \\frac{400.5 - 0.8n}{0.4\\sqrt{n}}\\Big) &\\approx 0.99 \\\\ \\Phi\\Big(\\frac{400.5 - 0.8n}{0.4\\sqrt{n}}\\Big) &\\approx 0.99 \\\\ \\frac{400.5 - 0.8n}{0.4\\sqrt{n}} &= 2.33 \\end{align*} \\begin{align*} \\textbf{P}(S_n \\le 400) &\\approx 0.99 \\\\ \\textbf{P}\\Big(\\frac{S_n - 0.8n}{0.4\\sqrt{n}} \\le \\frac{400.5 - 0.8n}{0.4\\sqrt{n}}\\Big) &\\approx 0.99 \\\\ \\textbf{P}\\Big(Z_n \\le \\frac{400.5 - 0.8n}{0.4\\sqrt{n}}\\Big) &\\approx 0.99 \\\\ \\Phi\\Big(\\frac{400.5 - 0.8n}{0.4\\sqrt{n}}\\Big) &\\approx 0.99 \\\\ \\frac{400.5 - 0.8n}{0.4\\sqrt{n}} &= 2.33 \\end{align*} Solve n n to obtain n = 475 n = 475 Normal approximation to the binomial \u00b6 Take X_i X_i as independent Bernuolli random variables: X \\sim \\operatorname{Bern} \\left({p}\\right) X \\sim \\operatorname{Bern} \\left({p}\\right) , 0 < p < 1 0 < p < 1 , then S_n = X_1 + X_2 + \\cdots + X_n S_n = X_1 + X_2 + \\cdots + X_n is Binomial: X \\sim \\operatorname{B} \\left({n, p}\\right) X \\sim \\operatorname{B} \\left({n, p}\\right) . Binomial random variable S_n S_n have \\mu = np \\mu = np , \\sigma^2=np(1-p) \\sigma^2=np(1-p) . According CLT, The normalized random variable \\frac{S_n-np}{\\sqrt{np(1-p)}} \\frac{S_n-np}{\\sqrt{np(1-p)}} is a standard normal. In order to find the \\textbf{P}(S_n \\le 21) \\textbf{P}(S_n \\le 21) (given n = 36 n = 36 , p = 0.5 p = 0.5 ), we use the equvilent of events to transform the probability \\textbf{P}(S_n \\le 21) \\textbf{P}(S_n \\le 21) into another probability about a normal random vairable \\textbf{P}\\Big(\\frac{S_n - 18}{3} \\le \\frac{21-28}{3}\\Big) = \\textbf{P}(Z_n \\le 1) = \\Phi(1) = 0.8413 \\textbf{P}\\Big(\\frac{S_n - 18}{3} \\le \\frac{21-28}{3}\\Big) = \\textbf{P}(Z_n \\le 1) = \\Phi(1) = 0.8413 . The 1/2 correction for integer random variables. To get a improved estimate of binomial from normal, we can take the middle point of two integers. Instead using \\textbf{P}(S_n \\le 21) \\textbf{P}(S_n \\le 21) or \\textbf{P}(S_n \\lt 22) \\textbf{P}(S_n \\lt 22) , we use \\textbf{P}(S_n \\lt 21.5) \\textbf{P}(S_n \\lt 21.5) which is very accurate De Moivre-Laplace CLT to the binomial. To estimate \\textbf{P}(S_n=19) \\textbf{P}(S_n=19) , we take \\textbf{P}(18.5 \\le S_n \\le 19.5) \\textbf{P}(18.5 \\le S_n \\le 19.5) and get an accurate estimate. Lecture 20 Introduction to classical statistics \u00b6 Overview of the classical statistical framework \u00b6 attributes Bayesian classical unkonwn \\Theta \\Theta is r.v. \\theta \\theta is a fixed value known p_\\Theta p_\\Theta prior distribution and samples X_i X_i only sample X_i X_i model p_{X \\mid \\Theta} p_{X \\mid \\Theta} , where observation is generated p_X(x; \\theta) p_X(x; \\theta) , \\theta \\theta is a real-valued parameter of the model, method use bayes rule to obtain p_{\\Theta \\mid X} p_{\\Theta \\mid X} , \\big(p_X = {\\int_{-\\infty}^{+\\infty}} p_\\Theta \\cdot p_{X \\mid \\Theta}d\\theta\\big) \\big(p_X = {\\int_{-\\infty}^{+\\infty}} p_\\Theta \\cdot p_{X \\mid \\Theta}d\\theta\\big) , then determind the estimate Design a estimator \\hat\\Theta \\hat\\Theta , to keep estimate error \\hat\\Theta - \\theta \\hat\\Theta - \\theta small estimator MAP, LMS ML Confidence intervals interpretation \u00b6 Every day, I try to estimate an unknown parameter using a fresh data set. I look at the data and then I use some formulas to calculate a 70% confidence interval, [\\hat\\Theta^-, \\hat\\Theta^+] [\\hat\\Theta^-, \\hat\\Theta^+] , based on the day's data. Classical statsistic interpretation \u00b6 If today I got the confidence interval [0.41, 0.47] [0.41, 0.47] , that doesn't mean there is 70\\% 70\\% probability that the true value will be inside [0.41, 0.47] [0.41, 0.47] . What the confidence interval is that 70% of those confidence intervals you obtained in all those days will include the true value. The \"confidence\" (or probability) can be think of \\frac{\\text{All CIs that include the true value}}{\\text{All CIs that include the true value + CIs that exclude the true value}} \\frac{\\text{All CIs that include the true value}}{\\text{All CIs that include the true value + CIs that exclude the true value}} . You cannot speak about a particular interval, and once you do that, there is no randomness anymore, the true value is either inside the interval or not, there are no other probabilities so to speak. Bayes' interpretation (Bayesian's Confidence Interval) \u00b6 Alternatively, I decided to use a Bayesian approach, by viewing the unknown parameter, denoted by \\Theta \\Theta , as a continuous random variable and assuming a prior PDF for \\Theta \\Theta . I observe a specific value x x , calculate the posterior f_{\\Theta}{X}(\\theta|x) f_{\\Theta}{X}(\\theta|x) , and find out that \\begin{align*} \\int _{0.41}^{0.47} f_{\\Theta |X}(\\theta \\, |\\, x)\\, d\\theta =0.70. \\end{align*} \\begin{align*} \\int _{0.41}^{0.47} f_{\\Theta |X}(\\theta \\, |\\, x)\\, d\\theta =0.70. \\end{align*} This time, I can say that there is a probability 70\\% 70\\% that the unknown parameter is inside the (Bayesian) confidence interval [0.41, 0.47] [0.41, 0.47] . Confidence intervals for the estimation of the mean \u00b6 I asked you to estimate the mean of i.i.d variables X_1, X_2, \\cdots, X_n X_1, X_2, \\cdots, X_n with true mean \\theta \\theta , and variance \\sigma^2 \\sigma^2 , and the estimate should achieve 95\\% 95\\% confidence interval. How you proceed with it? By Central Limit Theory, your estimate values \\hat\\Theta \\hat\\Theta is an normal distribution. By standardizing it, you get the standard normal Z_n = \\frac{\\hat\\Theta - \\theta}{\\sigma/\\sqrt{n}} Z_n = \\frac{\\hat\\Theta - \\theta}{\\sigma/\\sqrt{n}} . 95\\% 95\\% confidence interverl means standard normal is between symetric 95\\% 95\\% intervals [-b, b] [-b, b] . By looking up the normal table, b = 1.69 b = 1.69 corresponds to the probability 97.5\\% 97.5\\% , so \\textbf{P}(-1.69 \\le Z_n \\le 1.69) = 95\\% \\textbf{P}(-1.69 \\le Z_n \\le 1.69) = 95\\% thus \\textbf{P}\\big(\\frac{|\\hat\\Theta - \\theta|}{\\sigma/\\sqrt{n}} \\le 1.69\\big) = 95\\% \\textbf{P}\\big(\\frac{|\\hat\\Theta - \\theta|}{\\sigma/\\sqrt{n}} \\le 1.69\\big) = 95\\% , which can be rewrite as \\begin{align*} \\textbf{P}\\Big(\\hat\\Theta -\\frac{1.96\\sigma}{\\sqrt n} \\le \\theta \\le \\hat\\Theta +\\frac{1.96\\sigma}{\\sqrt n}\\Big) = 95\\%. \\end{align*} \\begin{align*} \\textbf{P}\\Big(\\hat\\Theta -\\frac{1.96\\sigma}{\\sqrt n} \\le \\theta \\le \\hat\\Theta +\\frac{1.96\\sigma}{\\sqrt n}\\Big) = 95\\%. \\end{align*} This is the way to construct the confidence interval. If you have the \\sigma \\sigma and n n , you have a concrete 95\\% 95\\% confidence interval to report. However, the \\sigma \\sigma is usually unknown, what you can do is to use the estimated \\hat\\Theta \\hat\\Theta to estimate the \\sigma \\sigma . Lecture 21 \u00b6 Lecture 22 \u00b6 Lecture 23 \u00b6 Lecture 24 \u00b6 Lecture 25 \u00b6 Lecture 26 \u00b6","title":"6.431 Probability"},{"location":"courses/6.431-probability/notes/#60416431-probability-the-science-of-uncertainty-and-data","text":"","title":"6.041/6.431 Probability - The Science of Uncertainty and Data"},{"location":"courses/6.431-probability/notes/#lecture-1-september-10-2018","text":"","title":"Lecture 1 (September 10, 2018)"},{"location":"courses/6.431-probability/notes/#probability-models","text":"To specify such a model, it includes 2 steps: Sample space: describe possible outcomes of an experiment Probability laws: describe beliefs about the likelihood of outcomes (or assign a probability to outcomes)","title":"Probability Models"},{"location":"courses/6.431-probability/notes/#sample-space","text":"list of possible outcomes must be mutually exclusive, collectively exhaustive, and at the right granularity.","title":"Sample space"},{"location":"courses/6.431-probability/notes/#probability-laws","text":"","title":"Probability laws"},{"location":"courses/6.431-probability/notes/#probability-axioms","text":"Motivation: Assign individual probability to a single dot is a challenge (in the dart example, each dot has zero probability), so we assign a probability to the subset of the sample space. Nonnegativity: {\\bf P}(A) \\geq 0 {\\bf P}(A) \\geq 0 Normalization: {\\bf P}(\\Omega) = 1 {\\bf P}(\\Omega) = 1 (Finite) Additivity: if A \\cap B = \\emptyset A \\cap B = \\emptyset then {\\bf P}(A \\cup B) = {\\bf P}(A) + {\\bf P}(B) {\\bf P}(A \\cup B) = {\\bf P}(A) + {\\bf P}(B) (need to be refined in future)","title":"Probability axioms"},{"location":"courses/6.431-probability/notes/#consequences-of-axioms-properties-of-probability-axioms","text":"{\\bf P}(A) \\leq 1 {\\bf P}(A) \\leq 1 {\\bf P}(\\emptyset) = 0 {\\bf P}(\\emptyset) = 0 {\\bf P}(A) + {\\bf P}(A^c) = 1 {\\bf P}(A) + {\\bf P}(A^c) = 1 A, B, \\text{and } C A, B, \\text{and } C are disjoint: {\\bf P}(A \\cup B \\cup C) = {\\bf P}(A) + {\\bf P}(B) + {\\bf P}(C) {\\bf P}(A \\cup B \\cup C) = {\\bf P}(A) + {\\bf P}(B) + {\\bf P}(C) A_1, \\cdots A_k, A_1, \\cdots A_k, are disjoint: {\\bf P}(A_1 \\cup \\cdots, \\cup A_k) = \\sum_{i - 1}^k{\\bf P}(A_i) {\\bf P}(A_1 \\cup \\cdots, \\cup A_k) = \\sum_{i - 1}^k{\\bf P}(A_i) If A \\subset B A \\subset B , then {\\bf P}(A) \\leq {\\bf P}(B) {\\bf P}(A) \\leq {\\bf P}(B) {\\bf P}(A \\cup B) = {\\bf P}(A) + {\\bf P}(B) - {\\bf P}(A \\cap B) {\\bf P}(A \\cup B) = {\\bf P}(A) + {\\bf P}(B) - {\\bf P}(A \\cap B) {\\bf P}(A \\cup B \\cup C) = {\\bf P}(A) + {\\bf P}(A^c \\cap B) + {\\bf P}(A^c \\cap B^c \\cap C) {\\bf P}(A \\cup B \\cup C) = {\\bf P}(A) + {\\bf P}(A^c \\cap B) + {\\bf P}(A^c \\cap B^c \\cap C) Union Bound: {\\bf P}(A \\cup B) \\leq {\\bf P}(A) + {\\bf P}(B) {\\bf P}(A \\cup B) \\leq {\\bf P}(A) + {\\bf P}(B)","title":"Consequences of axioms (properties of probability axioms)"},{"location":"courses/6.431-probability/notes/#probability-calculation-steps","text":"Specify the sample space Specify a probability law Identify an event of interest Calculate","title":"Probability calculation steps"},{"location":"courses/6.431-probability/notes/#countable-additivity","text":"The 3rd probability axiom previously mentioned can be extended to countable set (all integers), which means that the element in the set can be arranged in a sequence. This is contrary to the concept of uncountable set (i.e. a dart board, 2D plane). So you need to distinguish the following concepts: discrete set (countable set) continuous set (uncountable set, impossible to arrange in a sequence) finite set infinite set (possible to be arranged in a sequence) discrete (countable) finite set (i.e. {1, 2, 3}) discrete (countable) infinite set (i.e. all integers, all even integers, all old integers) continuous (uncountable) infinite set (i.e. \\{x | 0 \\leq x \\leq 1\\} \\{x | 0 \\leq x \\leq 1\\} ) Example: Sample space \\{1, 2, 3, \\cdots\\} \\{1, 2, 3, \\cdots\\} , given {\\bf P}(n) = \\frac{1}{2^n}, n = 1, 2, \\cdots {\\bf P}(n) = \\frac{1}{2^n}, n = 1, 2, \\cdots . Check the against the probability axioms that \\sum_{n = 1}^{\\infty}{\\bf P}(n) = 1 \\sum_{n = 1}^{\\infty}{\\bf P}(n) = 1 . What about the probability {\\bf P}(\\text{outcome is even}) {\\bf P}(\\text{outcome is even}) ? Using countable additivity axiom.","title":"Countable additivity"},{"location":"courses/6.431-probability/notes/#countable-additivity-axiom-refined-probability-axiom","text":"Note if A_1, A_2, A_3, \\cdot A_1, A_2, A_3, \\cdot is an infinite sequence of disjoint events, then {\\bf P}(A_1 \\cup A_2 \\cup A_3 \\cdots) = {\\bf P}(A_1) + {\\bf P}(A_2) + {\\bf P}(A_3) + \\cdots {\\bf P}(A_1 \\cup A_2 \\cup A_3 \\cdots) = {\\bf P}(A_1) + {\\bf P}(A_2) + {\\bf P}(A_3) + \\cdots To refine the 3rd probability axiom, additivity holds only for \"countable\" sequences of events. That means the additivity axiom must be a sequence , with finite or infinite elements. Note the following contradiction, the additivity axiom can not be applied to continuous sample space. A_1, A_2, A_3, \\cdots A_1, A_2, A_3, \\cdots are real coordinates, if you apply the additivity axiom, you will get: \\begin{align*} 1={\\bf P}(\\Omega ) &={\\bf P}\\big (\\{ A_1\\} \\cup \\{ A_2\\} \\cup \\{ A_3\\} \\cdots \\big )\\\\ &={\\bf P}(\\{ A_1\\} )+{\\bf P}(\\{ A_2\\} )+{\\bf P}(\\{ A_3\\} )+\\cdots \\\\ &= 0+0+0+\\cdots =0, \\end{align*} \\begin{align*} 1={\\bf P}(\\Omega ) &={\\bf P}\\big (\\{ A_1\\} \\cup \\{ A_2\\} \\cup \\{ A_3\\} \\cdots \\big )\\\\ &={\\bf P}(\\{ A_1\\} )+{\\bf P}(\\{ A_2\\} )+{\\bf P}(\\{ A_3\\} )+\\cdots \\\\ &= 0+0+0+\\cdots =0, \\end{align*} which is contradicts. This is because \"unit square\" is not a countable set.","title":"Countable Additivity Axiom (refined probability axiom)"},{"location":"courses/6.431-probability/notes/#interpretations-of-probability-theory","text":"frequency of events A? What is P(the president will be reelected)? Probability is often interpreted as: Description of beliefs Betting preference","title":"Interpretations of probability theory"},{"location":"courses/6.431-probability/notes/#lecture-2-conditioning-and-bayes-rule","text":"","title":"Lecture 2 Conditioning and Baye's rule"},{"location":"courses/6.431-probability/notes/#conditional-probability","text":"use new information to review a model Definition {\\bf P}(A|B) = {\\bf P}(A|B) = \"probability of A given that B occurred\" {\\bf P}(A|B) = \\frac{{\\bf P}(A \\cap B)}{{\\bf P}(B)} {\\bf P}(A|B) = \\frac{{\\bf P}(A \\cap B)}{{\\bf P}(B)} , defined only when {\\bf P}(B) > 0 {\\bf P}(B) > 0 Conditional probability share properties of ordinary probabilities (finite and infinite countable additivity) If A \\cap C = \\emptyset A \\cap C = \\emptyset , then {\\bf P}(A \\cup C|B) = P(A|B) + {\\bf P}(C|B) {\\bf P}(A \\cup C|B) = P(A|B) + {\\bf P}(C|B)","title":"Conditional probability"},{"location":"courses/6.431-probability/notes/#multiplication-rule","text":"{\\bf P}(A \\cap B) = {\\bf P}(A){\\bf P}(B|A) = {\\bf P}(B){\\bf P}(A|B) {\\bf P}(A \\cap B) = {\\bf P}(A){\\bf P}(B|A) = {\\bf P}(B){\\bf P}(A|B) {\\bf P}(A \\cap B \\cap C) = {\\bf P}(A){\\bf P}(B|A){\\bf P}(C|A \\cap B) {\\bf P}(A \\cap B \\cap C) = {\\bf P}(A){\\bf P}(B|A){\\bf P}(C|A \\cap B) It also applies to n events.","title":"Multiplication rule"},{"location":"courses/6.431-probability/notes/#total-probability-theorem-divide-and-conquer","text":"The settings: Partition of sample space into A_1, A_2, A_3 A_1, A_2, A_3 , have {\\bf P}(A_i) {\\bf P}(A_i) for every i i have {\\bf P}(B|A_i) {\\bf P}(B|A_i) for every i i {\\bf P}(B) = \\sum_{i} {\\bf P}(A_i){\\bf P}(B|A_i) {\\bf P}(B) = \\sum_{i} {\\bf P}(A_i){\\bf P}(B|A_i) It also applies to infinite countable sets according to the countable additivity axiom.","title":"Total probability theorem (divide and conquer)"},{"location":"courses/6.431-probability/notes/#bayes-rule","text":"The setting is the same as the total probability theorem. Partition of sample space into A_1, A_2, A_3 A_1, A_2, A_3 , have {\\bf P}(A_i) {\\bf P}(A_i) for every i i (initial believes) have {\\bf P}(B|A_i) {\\bf P}(B|A_i) for every i i revise \"believe\" given that B occurred: {\\bf P}(A_i|B) = \\frac{{\\bf P}(A_i \\cap B)}{{\\bf P}(B)} {\\bf P}(A_i|B) = \\frac{{\\bf P}(A_i \\cap B)}{{\\bf P}(B)} It turns out that we can use multiplication rule to calculate the nominator and use the total probability rule to calculate the denominator, thusly {\\bf P}(A_i|B) = \\frac{{\\bf P}(A_i){\\bf P}(B|A_i)}{\\sum_j{\\bf P}(A_j){\\bf P}(B|A_i)} {\\bf P}(A_i|B) = \\frac{{\\bf P}(A_i){\\bf P}(B|A_i)}{\\sum_j{\\bf P}(A_j){\\bf P}(B|A_i)}","title":"Baye's rule"},{"location":"courses/6.431-probability/notes/#lecture-3-independence","text":"","title":"Lecture 3 Independence"},{"location":"courses/6.431-probability/notes/#independence-of-two-events","text":"Motivation coin toss example: First toss is head or tail doesn't affect the probability of second toss is head. Occurance of A A privides no new information about B B : {\\bf P}(B|A) = {\\bf P}(B) {\\bf P}(B|A) = {\\bf P}(B) .","title":"Independence of two events"},{"location":"courses/6.431-probability/notes/#definition-of-independence","text":"Formal definition {\\bf P}(A \\cap B) = {\\bf P}(A) \\cdot {\\bf P}(B) {\\bf P}(A \\cap B) = {\\bf P}(A) \\cdot {\\bf P}(B) symmetric with respect to events. implies {\\bf P}(A|B) = {\\bf P}(A) {\\bf P}(A|B) = {\\bf P}(A) . Applies even if {\\bf P}(A) = 0 {\\bf P}(A) = 0 . Distinction between disjoin and independence Disjoin usually means dependent, because if one event happens we know the other event will not happen.","title":"Definition of independence"},{"location":"courses/6.431-probability/notes/#independence-of-event-complements","text":"If A A and B B are independent, the A A and B^c B^c are independent. If A A and B B are independent, the A^c A^c and B^c B^c are independent.","title":"Independence of event complements"},{"location":"courses/6.431-probability/notes/#conditioning-independence","text":"Ordinary independence properties also apply to conditional independence. But independence doesn't imply conditional independence.","title":"Conditioning independence"},{"location":"courses/6.431-probability/notes/#independence-of-a-collection-of-events","text":"Intuition \"definition\": Information on some of the events doesn't change probability related to the remaining events. Formal definition","title":"Independence of a collection of events"},{"location":"courses/6.431-probability/notes/#pairwise-independence","text":"","title":"pairwise independence"},{"location":"courses/6.431-probability/notes/#independence-vs-pairwise-independence","text":"Used two independence fair coin tosses as an example to show that pariwise independence isn't enough to show independence of collection of events.","title":"independence V.S. pairwise independence"},{"location":"courses/6.431-probability/notes/#reliability","text":"The king's sibling puzzle","title":"Reliability"},{"location":"courses/6.431-probability/notes/#monty-hall-problem","text":"","title":"Monty Hall problem"},{"location":"courses/6.431-probability/notes/#lecture-4","text":"","title":"Lecture 4"},{"location":"courses/6.431-probability/notes/#lecture-5","text":"","title":"Lecture 5"},{"location":"courses/6.431-probability/notes/#definition-of-random-variables","text":"A random variable (\"r.v.\") associates a value (a number) to every possible outcome. Mathematically, A function from sample space \\Omega \\Omega to the real numbers (discrete or continuous). We can have several random variables defined on the same sample space. (In Lecture 18, X_1, X_2, \\cdots X_1, X_2, \\cdots are independent random variables from the same distribution) A function of one or several random variables is also another random variable. A concrete example to understand the concept of a random variable is take a class of students, selecting students and measuring their body mass or height will give us the random variable W W and H H . The \"Body mass index\" is another random variable that is a function of the random variable W W and H H . meaning of X + Y X + Y random variable X+Y X+Y takes the value x+y x+y , when X X takes the value x x , and Y Y takes the value y y .","title":"Definition of random variables"},{"location":"courses/6.431-probability/notes/#probability-mass-function","text":"","title":"Probability mass function"},{"location":"courses/6.431-probability/notes/#bernoulli-and-indicator-random-variables","text":"Bernoulli random variable X X : X = \\begin{cases} 1, & \\textrm{w.p. } p \\\\ 0, & \\textrm{w.p. } 1-p \\end{cases} X = \\begin{cases} 1, & \\textrm{w.p. } p \\\\ 0, & \\textrm{w.p. } 1-p \\end{cases} models a trial that results in success/failure, heads/tails, etc. indicator random variables of event A A : I_A = 1 I_A = 1 if A A occurs. connection between events and random variables. P(A) = P(I_A = 1) = P_{I_A}(1) P(A) = P(I_A = 1) = P_{I_A}(1) , P(A) P(A) : probability of event A happens P(I_A = 1) P(I_A = 1) : probablistic notition of indication R.V. when I_A I_A equal to 1 1 P_{I_A}(1) P_{I_A}(1) : PMF notation.","title":"Bernoulli and indicator random variables"},{"location":"courses/6.431-probability/notes/#discrete-uniform-random-variables","text":"","title":"Discrete uniform random variables"},{"location":"courses/6.431-probability/notes/#binomial-random-variables","text":"Experiment: toss a biased coin ( P(\\textrm{Head})=p P(\\textrm{Head})=p ) n n times and observe the result (number of heads)","title":"Binomial random variables"},{"location":"courses/6.431-probability/notes/#geometric-random-variables","text":"","title":"Geometric random variables"},{"location":"courses/6.431-probability/notes/#expectation","text":"","title":"Expectation"},{"location":"courses/6.431-probability/notes/#the-expected-value-rule","text":"","title":"The expected value rule"},{"location":"courses/6.431-probability/notes/#linearity-of-expectations","text":"","title":"Linearity of expectations"},{"location":"courses/6.431-probability/notes/#lecture-6","text":"","title":"Lecture 6"},{"location":"courses/6.431-probability/notes/#lecture-7","text":"","title":"Lecture 7"},{"location":"courses/6.431-probability/notes/#independence-variances-and-binomial-variance","text":"","title":"Independence, variances, and binomial variance"},{"location":"courses/6.431-probability/notes/#lecture-8","text":"","title":"Lecture 8"},{"location":"courses/6.431-probability/notes/#exponential-random-variables","text":"PDF: f_{X}(x) = \\begin{cases} \\lambda e^{-\\lambda x}, & x \\ge 0 \\\\ 0, & x < 0 \\end{cases} f_{X}(x) = \\begin{cases} \\lambda e^{-\\lambda x}, & x \\ge 0 \\\\ 0, & x < 0 \\end{cases} Probability of greater than a a , \\begin{align*} P(X \\ge a) &= \\displaystyle \\int_a^{+\\infty} \\lambda e^{-\\lambda x}dx \\\\ &= - \\displaystyle \\int_a^{+\\infty} de^{-\\lambda x} \\\\ &= -e^{-\\lambda x}\\Big|_a^{+\\infty} = -e^{-\\lambda \\cdot +\\infty} + e^{-\\lambda a} = e^{-\\lambda a} \\end{align*} \\begin{align*} P(X \\ge a) &= \\displaystyle \\int_a^{+\\infty} \\lambda e^{-\\lambda x}dx \\\\ &= - \\displaystyle \\int_a^{+\\infty} de^{-\\lambda x} \\\\ &= -e^{-\\lambda x}\\Big|_a^{+\\infty} = -e^{-\\lambda \\cdot +\\infty} + e^{-\\lambda a} = e^{-\\lambda a} \\end{align*} Expectation of exponential random variable {\\bf E}\\big[X\\big] = \\displaystyle \\int_0^{\\infty} x \\cdot \\lambda e^{-\\lambda x} dx = 1/\\lambda {\\bf E}\\big[X\\big] = \\displaystyle \\int_0^{\\infty} x \\cdot \\lambda e^{-\\lambda x} dx = 1/\\lambda Second moment of an exponential random variable {\\bf E}\\big[X^2 \\big] = \\displaystyle \\int_0^{\\infty} x^2 \\cdot \\lambda e^{-\\lambda x} dx = 2/\\lambda^2 {\\bf E}\\big[X^2 \\big] = \\displaystyle \\int_0^{\\infty} x^2 \\cdot \\lambda e^{-\\lambda x} dx = 2/\\lambda^2 Variance of an exponential random variable \\textbf{Var}(X) = {\\bf E}\\big[X^2 \\big] - \\Big({\\bf E}\\big[X\\big]\\Big)^2 = 1/\\lambda^2 \\textbf{Var}(X) = {\\bf E}\\big[X^2 \\big] - \\Big({\\bf E}\\big[X\\big]\\Big)^2 = 1/\\lambda^2","title":"Exponential random variables"},{"location":"courses/6.431-probability/notes/#lecture-9","text":"","title":"Lecture 9"},{"location":"courses/6.431-probability/notes/#conditioning-a-continuous-random-variable-on-an-event","text":"","title":"Conditioning a continuous random variable on an event"},{"location":"courses/6.431-probability/notes/#memorylessness-of-the-exponential-pdf","text":"","title":"Memorylessness of the exponential PDF"},{"location":"courses/6.431-probability/notes/#total-probability-and-expectation-theorems","text":"","title":"Total probability and expectation theorems"},{"location":"courses/6.431-probability/notes/#lecture-10","text":"","title":"Lecture 10"},{"location":"courses/6.431-probability/notes/#total-probability-and-total-expectation-theorems","text":"Follows from the discrete cases: Rules Discrete continous Total probability p_X(x) = \\displaystyle \\sum\\limits_{y} p_Y(y)p_{X \\mid Y}(x \\mid y) p_X(x) = \\displaystyle \\sum\\limits_{y} p_Y(y)p_{X \\mid Y}(x \\mid y) f_X(x) = \\displaystyle \\int_{-\\infty}^{+\\infty} f_Y(y) f_{X \\mid Y}(x \\mid y) dy f_X(x) = \\displaystyle \\int_{-\\infty}^{+\\infty} f_Y(y) f_{X \\mid Y}(x \\mid y) dy Conditional expectation {\\bf E}\\big[X \\mid Y = y \\big] = \\displaystyle \\sum\\limits_{x} x p_{X \\mid Y}(x \\mid y) {\\bf E}\\big[X \\mid Y = y \\big] = \\displaystyle \\sum\\limits_{x} x p_{X \\mid Y}(x \\mid y) {\\bf E}\\big[X \\mid Y = y \\big] = \\displaystyle {\\int_{-\\infty}^{+\\infty}} x f_{X \\mid Y}(x \\mid y) dx {\\bf E}\\big[X \\mid Y = y \\big] = \\displaystyle {\\int_{-\\infty}^{+\\infty}} x f_{X \\mid Y}(x \\mid y) dx Total expectation {\\bf E}\\big[X \\big] = \\displaystyle \\sum\\limits_{y} p_{Y}(y){\\bf E} \\big[X \\mid Y = y \\big] {\\bf E}\\big[X \\big] = \\displaystyle \\sum\\limits_{y} p_{Y}(y){\\bf E} \\big[X \\mid Y = y \\big] derivation hint: replace with the conditional expectatioin {\\bf E}\\big[X \\big] = \\displaystyle {\\int_{-\\infty}^{+\\infty}} f_{Y}(y){\\bf E}\\big[X \\mid Y = y \\big] dy {\\bf E}\\big[X \\big] = \\displaystyle {\\int_{-\\infty}^{+\\infty}} f_{Y}(y){\\bf E}\\big[X \\mid Y = y \\big] dy derivation hint: replace with the conditional expectatioin Expected value rule {\\bf E}\\big[ g(X) \\mid Y = y \\big] = \\displaystyle {\\sum\\limits_{x}} g(x) p_{X \\mid Y}(x \\mid y) {\\bf E}\\big[ g(X) \\mid Y = y \\big] = \\displaystyle {\\sum\\limits_{x}} g(x) p_{X \\mid Y}(x \\mid y) {\\bf E}\\big[ g(X) \\mid Y = y \\big] = \\displaystyle {\\int_{-\\infty}^{+\\infty}} g(x) f_{X \\mid Y}(x \\mid y) dx {\\bf E}\\big[ g(X) \\mid Y = y \\big] = \\displaystyle {\\int_{-\\infty}^{+\\infty}} g(x) f_{X \\mid Y}(x \\mid y) dx","title":"Total probability and total expectation theorems"},{"location":"courses/6.431-probability/notes/#solved-problems-lecture-8-10","text":"","title":"Solved problems (Lecture 8 - 10)"},{"location":"courses/6.431-probability/notes/#10-buffons-needle-and-monte-carlo-simulation","text":"This problem is discussed in the text page 161, Example 3.11.","title":"10 Buffon's needle and Monte Carlo Simulation"},{"location":"courses/6.431-probability/notes/#lecture-11","text":"","title":"Lecture 11"},{"location":"courses/6.431-probability/notes/#lecture-12","text":"","title":"Lecture 12"},{"location":"courses/6.431-probability/notes/#covariance","text":"Definition: \\textbf{cov}(X, Y) = {\\bf E}\\big[(X - {\\bf E}[X]) (Y - {\\bf E}[Y])\\big] \\textbf{cov}(X, Y) = {\\bf E}\\big[(X - {\\bf E}[X]) (Y - {\\bf E}[Y])\\big] Covariance tell use whether two r.v.s tend to move together, both to low or both to high Two random variables X X and Y Y are independent indicates: \\textbf{cov}(X, Y) = 0 \\textbf{cov}(X, Y) = 0 , but the converse it not true.","title":"Covariance"},{"location":"courses/6.431-probability/notes/#covariance-properties","text":"","title":"Covariance properties"},{"location":"courses/6.431-probability/notes/#lecture-13","text":"","title":"Lecture 13"},{"location":"courses/6.431-probability/notes/#the-conditional-expectation-as-a-random-variable-bf-e-bigx-mid-ybigbf-e-bigx-mid-ybig","text":"Given a function h(x) = x^2 h(x) = x^2 for all x x , and a random variable X X . what is h(X) h(X) ? h(X) h(X) is a function of a random variable X X . h(X) h(X) itself is a random variable that take value x^2 x^2 if X X happens to take the value x x . (you should really understand this statement, it is crucial to understand the concept of conditional expectatioin) For discrete case: {\\bf E}\\big[X \\mid Y = y \\big] = \\displaystyle \\sum\\limits_{x} x p_{X \\mid Y}(x \\mid y) {\\bf E}\\big[X \\mid Y = y \\big] = \\displaystyle \\sum\\limits_{x} x p_{X \\mid Y}(x \\mid y) , in which different vaule of y y will give us the different value of conditional expectation of X X . Then we can really treat {\\bf E}\\big[X \\mid Y = y \\big] {\\bf E}\\big[X \\mid Y = y \\big] as a function of y y , noted as g(y) g(y) . From the above reasoning, because Y Y is a random variable, g(Y) g(Y) is also a reandom variable, that takes the value of g(y) = {\\bf E}\\big[X \\mid Y = y \\big] g(y) = {\\bf E}\\big[X \\mid Y = y \\big] if Y Y happens to take the value y y . g(Y) = {\\bf E}\\big[X \\mid Y\\big] g(Y) = {\\bf E}\\big[X \\mid Y\\big] , is a function of Y Y a random variable has a distribution, mean, variance, etc.","title":"The conditional expectation as a random variable {\\bf E} \\big[X \\mid Y\\big]{\\bf E} \\big[X \\mid Y\\big]"},{"location":"courses/6.431-probability/notes/#the-law-of-iterated-expectations","text":"By calculate {\\bf E}\\Big[{\\bf E}\\big[X \\mid Y\\big]\\Big] {\\bf E}\\Big[{\\bf E}\\big[X \\mid Y\\big]\\Big] , we could abtain that it is equal to {\\bf E}\\big[X\\big] {\\bf E}\\big[X\\big]","title":"The law of iterated expectations"},{"location":"courses/6.431-probability/notes/#the-conditional-variance-as-a-random-variable-textbfvarx-mid-y-ytextbfvarx-mid-y-y","text":"definition: \\textbf{Var}(X \\mid Y = y) = {\\bf E}\\Big[\\big(X - {\\bf E}\\big[X \\mid Y = y \\big]\\big)^2\\mid Y = y\\Big] \\textbf{Var}(X \\mid Y = y) = {\\bf E}\\Big[\\big(X - {\\bf E}\\big[X \\mid Y = y \\big]\\big)^2\\mid Y = y\\Big] \\textbf{Var}(X\\mid Y) \\textbf{Var}(X\\mid Y) is also a random variable, it takes the value \\textbf{Var}(X\\mid y) \\textbf{Var}(X\\mid y) when Y Y happens to take the value of y y . Total variance rule: \\textbf{Var}(X) = {\\bf E}\\Big[\\textbf{Var}(X \\mid Y)\\Big] + \\textbf{Var}\\Big({\\bf E}\\big[X\\mid Y\\big]\\Big) \\textbf{Var}(X) = {\\bf E}\\Big[\\textbf{Var}(X \\mid Y)\\Big] + \\textbf{Var}\\Big({\\bf E}\\big[X\\mid Y\\big]\\Big)","title":"The conditional variance as a random variable \\textbf{Var}(X \\mid Y = y)\\textbf{Var}(X \\mid Y = y)"},{"location":"courses/6.431-probability/notes/#lecture-14-introduction-to-bayesian-inference","text":"","title":"Lecture 14 Introduction to Bayesian Inference"},{"location":"courses/6.431-probability/notes/#the-bayesian-inference-framework","text":"Treat the unknown \\Theta \\Theta as a random variable. with prior p_{\\Theta} p_{\\Theta} or f_{\\Theta} f_{\\Theta} . With the observations X X , using Bayes rule to obtain the posterior probability of \\Theta \\Theta , P_{\\Theta|X}(\\cdot|X=x) P_{\\Theta|X}(\\cdot|X=x) . Once the posterior is available, the estimator can be obtained via maximize a posterioral probability (MAP) rule or leaset mean squares (LMS). The performance can be measured by \"probability of error\" or \"mean squared error\".","title":"The Bayesian inference framework"},{"location":"courses/6.431-probability/notes/#conditional-probability-of-error-and-total-probability-of-error","text":"The conditional probability of other estimators that made a mistake. One of the properties of MAP rule is the it guarantee smallest probability of error. The taotal probability of error is an abstract notion of error for MAP rule to estimator. It can be caculated using total probability theory.","title":"Conditional probability of error and total probability of error"},{"location":"courses/6.431-probability/notes/#discrete-unknowns-discrete-observation-example-8-exercise-discrete-unknowns","text":"Let \\Theta_1 \\Theta_1 and \\Theta_2 \\Theta_2 be some unobserved Bernoulli random variables and let be an observation. Conditional on X = x X = x , the posterior joint PMF of \\Theta_1 \\Theta_1 and \\Theta_2 \\Theta_2 is given by p_{\\Theta _1,\\Theta _2\\mid X}(\\theta _1,\\theta _2\\mid x) = \\begin{cases} 0.26, & \\mbox{if } \\theta _1=0, \\theta _2=0, \\\\ 0.26, & \\mbox{if } \\theta _1=0, \\theta _2=1, \\\\ 0.21, & \\mbox{if } \\theta _1=1, \\theta _2=0, \\\\ 0.27, & \\mbox{if } \\theta _1=1, \\theta _2=1, \\\\ 0, & \\mbox{otherwise.} \\end{cases} p_{\\Theta _1,\\Theta _2\\mid X}(\\theta _1,\\theta _2\\mid x) = \\begin{cases} 0.26, & \\mbox{if } \\theta _1=0, \\theta _2=0, \\\\ 0.26, & \\mbox{if } \\theta _1=0, \\theta _2=1, \\\\ 0.21, & \\mbox{if } \\theta _1=1, \\theta _2=0, \\\\ 0.27, & \\mbox{if } \\theta _1=1, \\theta _2=1, \\\\ 0, & \\mbox{otherwise.} \\end{cases} What is the estimate of (\\Theta_1, \\Theta_2) (\\Theta_1, \\Theta_2) provided by MAP rule? (1, 1), because the p_{\\Theta _1,\\Theta _2\\mid X}(1, 1\\mid x) p_{\\Theta _1,\\Theta _2\\mid X}(1, 1\\mid x) is the maximum of all. What is the MAP estimate of \\Theta_1 \\Theta_1 based on X X , that is, the one that maximizes p_{\\Theta_1 \\mid X}(\\theta_1 \\mid x) p_{\\Theta_1 \\mid X}(\\theta_1 \\mid x) ? 0, from the marginal PMF, you can see that p_{\\Theta_1 \\mid X}(0 \\mid x) p_{\\Theta_1 \\mid X}(0 \\mid x) is maximized The moral of this problem is that an estimate of $ \\Theta_1 $ obtained by identifying the maximum of the join PMF of all unknown random variables can be different from the MAP estimate of \\Theta_1 \\Theta_1 from the marginal PMF.","title":"Discrete unknowns, discrete observation example (8. Exercise: Discrete unknowns)"},{"location":"courses/6.431-probability/notes/#discrete-parameter-continuous-observation","text":"Digital signal transmission","title":"Discrete parameter, continuous observation"},{"location":"courses/6.431-probability/notes/#continous-parameter-continuous-observation","text":"Analog signal transmission","title":"continous parameter, continuous observation"},{"location":"courses/6.431-probability/notes/#lecture-15-linear-models-with-normal-noise","text":"","title":"Lecture 15 Linear models with normal noise"},{"location":"courses/6.431-probability/notes/#recognizing-normal-pdfs","text":"f_X(x) = c \\cdot e^{-(\\alpha x^2 + \\beta x + \\gamma)} f_X(x) = c \\cdot e^{-(\\alpha x^2 + \\beta x + \\gamma)} is a normal random variable with \\mu = -\\frac{\\beta}{2\\alpha} \\mu = -\\frac{\\beta}{2\\alpha} and variance \\frac{1}{2\\alpha} \\frac{1}{2\\alpha}","title":"recognizing normal PDFs"},{"location":"courses/6.431-probability/notes/#the-mean-squared-error","text":"The most important take away from this section is that for normal unknown and normal noise signal, X = \\Theta + W X = \\Theta + W , no matter which x_i x_i we observed, the mean squared error estimates {\\hat \\theta} {\\hat \\theta} are the same (the variance of \\Theta \\Theta ). In other words, the remaining uncertainty about \\theta \\theta after an observation is the same no matter what the observation is. The observation only determined the estimated mean of the random variable \\theta \\theta . (See the drawing at the end of the lecture)","title":"The mean squared error"},{"location":"courses/6.431-probability/notes/#measurement-estimate-and-learning","text":"How to measure the gravitational attraction constant","title":"Measurement, estimate, and learning"},{"location":"courses/6.431-probability/notes/#lecture-16-least-mean-square-lms-estimation","text":"","title":"Lecture 16 Least mean square (LMS) estimation"},{"location":"courses/6.431-probability/notes/#lms-estimation-without-any-observations","text":"Given the prior p_{\\Theta}(\\theta) p_{\\Theta}(\\theta) unknow random variable \\Theta \\Theta , what's you realy want to estimate? You may interested in a point estimate. You may interested in find the estimator. With no observation, to minimize the mean squared error (MSE), {\\bf E}\\Big[\\big(\\Theta - \\hat \\theta \\big)^2\\Big] {\\bf E}\\Big[\\big(\\Theta - \\hat \\theta \\big)^2\\Big] , the value \\hat \\theta = {\\bf E}\\big[X\\big] \\hat \\theta = {\\bf E}\\big[X\\big] minimize the error. and the minimum error equal to \\textbf{Var}(\\Theta) \\textbf{Var}(\\Theta) Optimal mean squared error: {\\bf E}\\Big[\\big(\\Theta - {\\bf E}\\big[\\Theta \\big] \\big)^2\\Big] = \\textbf{Var}(\\Theta) {\\bf E}\\Big[\\big(\\Theta - {\\bf E}\\big[\\Theta \\big] \\big)^2\\Big] = \\textbf{Var}(\\Theta) . Because {\\bf E}\\big[\\Theta \\big] {\\bf E}\\big[\\Theta \\big] is the optimal value that minimize the MSE, if we replace \\hat \\theta \\hat \\theta with {\\bf E}\\big[\\Theta \\big] {\\bf E}\\big[\\Theta \\big] in the expression MSE, it coincidently match the definition of variance. You can alse use the {\\bf E}\\Big[\\big(\\Theta - \\hat \\theta \\big)^2\\Big] = \\textbf{Var}(\\Theta - \\hat \\theta) + \\Big({\\bf E}\\big[\\Theta - \\hat \\theta \\big]\\Big)^2 {\\bf E}\\Big[\\big(\\Theta - \\hat \\theta \\big)^2\\Big] = \\textbf{Var}(\\Theta - \\hat \\theta) + \\Big({\\bf E}\\big[\\Theta - \\hat \\theta \\big]\\Big)^2 to derive \\hat \\theta = {\\bf E}\\big[X\\big] \\hat \\theta = {\\bf E}\\big[X\\big] is the optimal value.","title":"LMS estimation without any observations"},{"location":"courses/6.431-probability/notes/#lms-estimation-single-unknown-and-observation","text":"Goal: interested in a point estimate \\hat \\theta \\hat \\theta of unknow random variable \\Theta \\Theta , with prior p_{\\Theta}(\\theta) p_{\\Theta}(\\theta) . (Given observation X X ; model p_{X\\mid \\Theta}(x\\mid \\theta) p_{X\\mid \\Theta}(x\\mid \\theta) ) We want to minimize the MSE, because this time we have the particular observation X = x X = x , we now live in a conditional universe, we need to minimize the conditional MSE, {\\bf E}\\Big[\\big(\\Theta - {\\bf E}\\big[\\Theta \\big] \\big)^2 \\mid X = x \\Big] {\\bf E}\\Big[\\big(\\Theta - {\\bf E}\\big[\\Theta \\big] \\big)^2 \\mid X = x \\Big] , the optimal value is \\hat \\theta = {\\bf E}\\big[\\Theta \\mid X = x\\big] \\hat \\theta = {\\bf E}\\big[\\Theta \\mid X = x\\big] By the \"happens to take\" reasoning and the iterated expection rule, we can achieve the conclusion: that \\hat \\Theta_{LMS} = {\\bf E}\\big[\\Theta \\mid X\\big] \\hat \\Theta_{LMS} = {\\bf E}\\big[\\Theta \\mid X\\big] minimize the MSE {\\bf E}\\Big[\\big(\\Theta - g(X) \\big)^2 \\Big] {\\bf E}\\Big[\\big(\\Theta - g(X) \\big)^2 \\Big] over all estimators \\hat \\Theta = g(X) \\hat \\Theta = g(X) .","title":"LMS estimation; single unknown and observation"},{"location":"courses/6.431-probability/notes/#lms-performance-evaluation","text":"MSE: {\\bf E}\\Big[\\big(\\Theta - {\\bf E}\\big[\\Theta \\mid X = x\\big] \\big)^2 \\mid X = x \\Big] = \\textbf{Var}(\\Theta \\mid X = x) {\\bf E}\\Big[\\big(\\Theta - {\\bf E}\\big[\\Theta \\mid X = x\\big] \\big)^2 \\mid X = x \\Big] = \\textbf{Var}(\\Theta \\mid X = x) Expected performance of the design: {\\bf E}\\Big[\\big(\\Theta - {\\bf E}\\big[\\Theta \\mid X\\big] \\big)^2 \\Big] = {\\bf E}\\Big[\\textbf{Var}(\\Theta \\mid X)\\Big] {\\bf E}\\Big[\\big(\\Theta - {\\bf E}\\big[\\Theta \\mid X\\big] \\big)^2 \\Big] = {\\bf E}\\Big[\\textbf{Var}(\\Theta \\mid X)\\Big] LMS relavant to estimation (not hypothesis testing) Same as MAP if the posterior is unimodal and symmetric around the mean. e.g. when the posterior is normal (the case in \"linear-normal\" models)","title":"LMS performance evaluation"},{"location":"courses/6.431-probability/notes/#the-multidimensional-case","text":"","title":"The multidimensional case"},{"location":"courses/6.431-probability/notes/#lecture-17-linear-least-mean-squares-llms-estimation","text":"","title":"Lecture 17 Linear least mean squares (LLMS) estimation"},{"location":"courses/6.431-probability/notes/#lecture-18-inequalities-convergence-and-the-weak-law-of-large-numbers","text":"","title":"Lecture 18 Inequalities, convergence, and the Weak Law of Large Numbers"},{"location":"courses/6.431-probability/notes/#the-weak-law-of-large-numbers","text":"X_1, X_2, \\cdots, X_n X_1, X_2, \\cdots, X_n , i.i.d.; finite mean \\mu \\mu and variance \\sigma^2 \\sigma^2 Sample mean M_n M_n : it is not a constant, but a function of multiple random variables. You can understand it this way. In an experiment which you draw the exam score of student one by one, once you finish the first draw X_1 X_1 , the first sample is a fixed real number. But before your first draw, X_1 X_1 can be any score and it is a random variable. After you draw a total of n n scores, you have all the fixed value of X_1, X_2, \\cdots, X_n X_1, X_2, \\cdots, X_n , the sample mean is determined for this draw. However, abstractly, sample mean is a random variable, if you draw another n n example scores to get the sample mean, you get a different set of X_1, X_2, \\cdots, X_n X_1, X_2, \\cdots, X_n . The randomness comes from the randomness of different experiments, in each of the experiment, the sample X_i X_i is not random. {\\bf E}\\big[M_n\\big] =\\mu {\\bf E}\\big[M_n\\big] =\\mu . Two level of averaging. M_n M_n itself is averaging over n samples. Since these n samples are drawn randomly, taking the expectation of M_n M_n is averaging all possible sample means obtained through many (infinity) experiments. \\textbf{Var}(M_n) = \\frac{1}{n^2} \\cdot n \\cdot \\textbf{Var}(X_1) = \\frac{\\sigma^2}{n} \\textbf{Var}(M_n) = \\frac{1}{n^2} \\cdot n \\cdot \\textbf{Var}(X_1) = \\frac{\\sigma^2}{n} . Apply Chebyshev inequality: \\textbf{P}(|M_n - \\mu| \\ge \\epsilon) \\le \\frac{\\textbf{var}(M_n)}{\\epsilon^2} = \\frac{\\sigma^2}{n\\epsilon^2} \\rightarrow 0 \\textbf{P}(|M_n - \\mu| \\ge \\epsilon) \\le \\frac{\\textbf{var}(M_n)}{\\epsilon^2} = \\frac{\\sigma^2}{n\\epsilon^2} \\rightarrow 0 , as n \\rightarrow \\infty n \\rightarrow \\infty Weak Law of Large Numbers For \\epsilon > 0 \\epsilon > 0 , \\textbf{P}(|M_n - \\mu| \\ge \\epsilon) = \\textbf{P}\\Big(\\Big|\\frac{X_1 + X_2 + \\cdots + X_n}{n} - \\mu\\Big| \\ge \\epsilon \\Big) \\rightarrow 0 \\textbf{P}(|M_n - \\mu| \\ge \\epsilon) = \\textbf{P}\\Big(\\Big|\\frac{X_1 + X_2 + \\cdots + X_n}{n} - \\mu\\Big| \\ge \\epsilon \\Big) \\rightarrow 0 , as n \\rightarrow \\infty n \\rightarrow \\infty .","title":"The Weak Law of Large Numbers"},{"location":"courses/6.431-probability/notes/#interpreting-the-wlln","text":"One experiment many measurements X_i = \\mu + W_i X_i = \\mu + W_i , where W_i W_i is noise, \\textbf{E}[W_i] = 0 \\textbf{E}[W_i] = 0 , independent W_i W_i . Sample mean M_n M_n is unlikely to be far off from true mean \\mu \\mu Many independent repetitions of the same experiments event A A , with p = \\textbf{P}(A) p = \\textbf{P}(A) X_i X_i : indicator of event A A The sampel mean M_n M_n is the empirical frequency of event A A","title":"Interpreting the WLLN"},{"location":"courses/6.431-probability/notes/#application-of-wlln-polling","text":"The probability of error greater than \\epsilon \\epsilon is smaller than a certain probability. You can use Chebyshev inequality and WLLN to estimate how many samples you need to fulfill a specific error probability requirement.","title":"Application of WLLN - polling"},{"location":"courses/6.431-probability/notes/#convergence-in-probability","text":"Definition: A sequence Y_n Y_n converges in probability to a number a a if For any \\epsilon > 0 \\epsilon > 0 , \\displaystyle \\lim_{n\\rightarrow\\infty}\\textbf{P}(|Y_n - a| \\ge \\epsilon) = 0 \\displaystyle \\lim_{n\\rightarrow\\infty}\\textbf{P}(|Y_n - a| \\ge \\epsilon) = 0 * Comparison between ordinary convergence and convergence in probability:","title":"Convergence in probability"},{"location":"courses/6.431-probability/notes/#convergence-in-probability-examples","text":"convergence in probability doesn't imply convergence of the expectations. How to find what value it converges to? Make an educated conjecture about the limit \\tau \\tau , write {\\bf P}(|Y_n - \\tau| \\ge \\epsilon) {\\bf P}(|Y_n - \\tau| \\ge \\epsilon) , and derive the value of it to where you can observe from the expression that the probability \\to 0 \\to 0 , when n \\to \\infty n \\to \\infty .","title":"Convergence in probability examples"},{"location":"courses/6.431-probability/notes/#related-topics","text":"Better bounds/approximations on tail probabilities Markov and Chebyshev inequalities Chernoff bound Central limit theorem Different types of convergence Convergence in probability Convergence \"with probability 1\" Strong law of large numbers Convergence of a sequence of distributions (CDFs) to a limiting CDF.","title":"Related topics"},{"location":"courses/6.431-probability/notes/#lecture-19-the-central-limit-theorem-clt","text":"Considering the sum of random variable S_n = X_1 + X_2, + \\cdots + X_n S_n = X_1 + X_2, + \\cdots + X_n , ( X_1, X_2, \\cdots, X_n X_1, X_2, \\cdots, X_n are i.i.d. with finite mean \\mu \\mu and variance \\sigma^2 \\sigma^2 ) Z_n = \\frac{S_n - n\\mu}{\\sqrt{n}\\sigma} Z_n = \\frac{S_n - n\\mu}{\\sqrt{n}\\sigma} , then we have the central limit Theorem, Central Limit Theorem: For every z z : \\displaystyle \\lim_{n \\to \\infty}\\textbf{P}(Z_n \\le z) = \\textbf{P}(Z \\le z) \\displaystyle \\lim_{n \\to \\infty}\\textbf{P}(Z_n \\le z) = \\textbf{P}(Z \\le z)","title":"Lecture 19 The Central Limit Theorem (CLT)"},{"location":"courses/6.431-probability/notes/#what-exactly-does-the-clt-say-practice","text":"The practice of normal approximations: We have linear expression between S_n S_n and Z_n Z_n : S_n = \\sqrt n \\sigma Z_n + n\\mu S_n = \\sqrt n \\sigma Z_n + n\\mu , since Z_n Z_n can be treated as if it were normal, S_n S_n can be treated as if normal: N(n\\mu, n\\sigma^2) N(n\\mu, n\\sigma^2) Can we use the CLT when n is \"moderate\"? i.e. n = 30? Usually, yes. When the distribution of X X has common features with a normal distribution. symmetry and unimodality help","title":"What exactly does the CLT say? - Practice"},{"location":"courses/6.431-probability/notes/#central-limit-theory-examples","text":"\\textbf{P}(S_{\\color{red}n} \\le {\\color{red}a}) \\approx {\\color{red}b} \\textbf{P}(S_{\\color{red}n} \\le {\\color{red}a}) \\approx {\\color{red}b} Given two parameters, find the third. Package weight X_i X_i , i.i.d. exponential, \\lambda = 1/2 \\lambda = 1/2 , \\mu = \\sigma = 2 \\mu = \\sigma = 2 Load container with n = 100 n = 100 packages, what's the probability that the overall weight is heavier than 210? \\textbf{P}(S_n \\ge 210) = ? \\textbf{P}(S_n \\ge 210) = ? Load container with n = 100 n = 100 packages. Choose the \"capacity\" a so that \\textbf{P}(S_n \\ge a) \\approx 0.05 \\textbf{P}(S_n \\ge a) \\approx 0.05 Fix the capacity at 210, how large can n n be, so that \\textbf{P}(S_n \\ge 210) \\approx 0.05 \\textbf{P}(S_n \\ge 210) \\approx 0.05 Load container until wight exceeds 210, N N is the number of packages loaded, find \\textbf{P}(N > 100) \\textbf{P}(N > 100)","title":"Central Limit Theory examples"},{"location":"courses/6.431-probability/notes/#airline-booking","text":"For any given flight, an airline tries to sell as many tickets as possible. Suppose that on average, of ticket holders fail to show up, all independent of one another. Knowing this, an airline will sell more tickets than there are seats available (i.e., overbook the flight) and hope that there is a sufficient number of ticket holders who do not show up, to compensate for its overbooking. Using the Central Limit Theorem, determine, the maximum number of tickets an airline can sell on a flight with 400 seats so that it can be approximately confident that all ticket holders who do show up will be able to board the plane. Use the de Moivre-Laplace -correction in your calculations. Hint: You may have to solve numerically a quadratic equation. Solution: Each ticket can either be used or not used by the the passager. It can be modeled with the random variable: X \\sim \\operatorname{Bern} \\left({p}\\right) X \\sim \\operatorname{Bern} \\left({p}\\right) , p=0.8 p=0.8 , then the total passergers presented at airport is S_n = X_1 + X_2 + \\cdots + X_n S_n = X_1 + X_2 + \\cdots + X_n , which is a Binomial: S_n \\sim \\operatorname{B} \\left({n, p}\\right) S_n \\sim \\operatorname{B} \\left({n, p}\\right) . For binomial random variable, \\mu = np = 0.8n, \\sigma^2 = np(1-p) = 0.16n \\mu = np = 0.8n, \\sigma^2 = np(1-p) = 0.16n . Our requirements is \\textbf{P}(S_n \\le 400) \\approx 0.99 \\textbf{P}(S_n \\le 400) \\approx 0.99 . Normalize S_n S_n in the probability and treat the normalized random variable Z_n Z_n as a normal distribution, refer to the normal table, and solve the number n n . \\begin{align*} \\textbf{P}(S_n \\le 400) &\\approx 0.99 \\\\ \\textbf{P}\\Big(\\frac{S_n - 0.8n}{0.4\\sqrt{n}} \\le \\frac{400.5 - 0.8n}{0.4\\sqrt{n}}\\Big) &\\approx 0.99 \\\\ \\textbf{P}\\Big(Z_n \\le \\frac{400.5 - 0.8n}{0.4\\sqrt{n}}\\Big) &\\approx 0.99 \\\\ \\Phi\\Big(\\frac{400.5 - 0.8n}{0.4\\sqrt{n}}\\Big) &\\approx 0.99 \\\\ \\frac{400.5 - 0.8n}{0.4\\sqrt{n}} &= 2.33 \\end{align*} \\begin{align*} \\textbf{P}(S_n \\le 400) &\\approx 0.99 \\\\ \\textbf{P}\\Big(\\frac{S_n - 0.8n}{0.4\\sqrt{n}} \\le \\frac{400.5 - 0.8n}{0.4\\sqrt{n}}\\Big) &\\approx 0.99 \\\\ \\textbf{P}\\Big(Z_n \\le \\frac{400.5 - 0.8n}{0.4\\sqrt{n}}\\Big) &\\approx 0.99 \\\\ \\Phi\\Big(\\frac{400.5 - 0.8n}{0.4\\sqrt{n}}\\Big) &\\approx 0.99 \\\\ \\frac{400.5 - 0.8n}{0.4\\sqrt{n}} &= 2.33 \\end{align*} Solve n n to obtain n = 475 n = 475","title":"Airline booking"},{"location":"courses/6.431-probability/notes/#normal-approximation-to-the-binomial","text":"Take X_i X_i as independent Bernuolli random variables: X \\sim \\operatorname{Bern} \\left({p}\\right) X \\sim \\operatorname{Bern} \\left({p}\\right) , 0 < p < 1 0 < p < 1 , then S_n = X_1 + X_2 + \\cdots + X_n S_n = X_1 + X_2 + \\cdots + X_n is Binomial: X \\sim \\operatorname{B} \\left({n, p}\\right) X \\sim \\operatorname{B} \\left({n, p}\\right) . Binomial random variable S_n S_n have \\mu = np \\mu = np , \\sigma^2=np(1-p) \\sigma^2=np(1-p) . According CLT, The normalized random variable \\frac{S_n-np}{\\sqrt{np(1-p)}} \\frac{S_n-np}{\\sqrt{np(1-p)}} is a standard normal. In order to find the \\textbf{P}(S_n \\le 21) \\textbf{P}(S_n \\le 21) (given n = 36 n = 36 , p = 0.5 p = 0.5 ), we use the equvilent of events to transform the probability \\textbf{P}(S_n \\le 21) \\textbf{P}(S_n \\le 21) into another probability about a normal random vairable \\textbf{P}\\Big(\\frac{S_n - 18}{3} \\le \\frac{21-28}{3}\\Big) = \\textbf{P}(Z_n \\le 1) = \\Phi(1) = 0.8413 \\textbf{P}\\Big(\\frac{S_n - 18}{3} \\le \\frac{21-28}{3}\\Big) = \\textbf{P}(Z_n \\le 1) = \\Phi(1) = 0.8413 . The 1/2 correction for integer random variables. To get a improved estimate of binomial from normal, we can take the middle point of two integers. Instead using \\textbf{P}(S_n \\le 21) \\textbf{P}(S_n \\le 21) or \\textbf{P}(S_n \\lt 22) \\textbf{P}(S_n \\lt 22) , we use \\textbf{P}(S_n \\lt 21.5) \\textbf{P}(S_n \\lt 21.5) which is very accurate De Moivre-Laplace CLT to the binomial. To estimate \\textbf{P}(S_n=19) \\textbf{P}(S_n=19) , we take \\textbf{P}(18.5 \\le S_n \\le 19.5) \\textbf{P}(18.5 \\le S_n \\le 19.5) and get an accurate estimate.","title":"Normal approximation to the binomial"},{"location":"courses/6.431-probability/notes/#lecture-20-introduction-to-classical-statistics","text":"","title":"Lecture 20 Introduction to classical statistics"},{"location":"courses/6.431-probability/notes/#overview-of-the-classical-statistical-framework","text":"attributes Bayesian classical unkonwn \\Theta \\Theta is r.v. \\theta \\theta is a fixed value known p_\\Theta p_\\Theta prior distribution and samples X_i X_i only sample X_i X_i model p_{X \\mid \\Theta} p_{X \\mid \\Theta} , where observation is generated p_X(x; \\theta) p_X(x; \\theta) , \\theta \\theta is a real-valued parameter of the model, method use bayes rule to obtain p_{\\Theta \\mid X} p_{\\Theta \\mid X} , \\big(p_X = {\\int_{-\\infty}^{+\\infty}} p_\\Theta \\cdot p_{X \\mid \\Theta}d\\theta\\big) \\big(p_X = {\\int_{-\\infty}^{+\\infty}} p_\\Theta \\cdot p_{X \\mid \\Theta}d\\theta\\big) , then determind the estimate Design a estimator \\hat\\Theta \\hat\\Theta , to keep estimate error \\hat\\Theta - \\theta \\hat\\Theta - \\theta small estimator MAP, LMS ML","title":"Overview of the classical statistical framework"},{"location":"courses/6.431-probability/notes/#confidence-intervals-interpretation","text":"Every day, I try to estimate an unknown parameter using a fresh data set. I look at the data and then I use some formulas to calculate a 70% confidence interval, [\\hat\\Theta^-, \\hat\\Theta^+] [\\hat\\Theta^-, \\hat\\Theta^+] , based on the day's data.","title":"Confidence intervals interpretation"},{"location":"courses/6.431-probability/notes/#classical-statsistic-interpretation","text":"If today I got the confidence interval [0.41, 0.47] [0.41, 0.47] , that doesn't mean there is 70\\% 70\\% probability that the true value will be inside [0.41, 0.47] [0.41, 0.47] . What the confidence interval is that 70% of those confidence intervals you obtained in all those days will include the true value. The \"confidence\" (or probability) can be think of \\frac{\\text{All CIs that include the true value}}{\\text{All CIs that include the true value + CIs that exclude the true value}} \\frac{\\text{All CIs that include the true value}}{\\text{All CIs that include the true value + CIs that exclude the true value}} . You cannot speak about a particular interval, and once you do that, there is no randomness anymore, the true value is either inside the interval or not, there are no other probabilities so to speak.","title":"Classical statsistic interpretation"},{"location":"courses/6.431-probability/notes/#bayes-interpretation-bayesians-confidence-interval","text":"Alternatively, I decided to use a Bayesian approach, by viewing the unknown parameter, denoted by \\Theta \\Theta , as a continuous random variable and assuming a prior PDF for \\Theta \\Theta . I observe a specific value x x , calculate the posterior f_{\\Theta}{X}(\\theta|x) f_{\\Theta}{X}(\\theta|x) , and find out that \\begin{align*} \\int _{0.41}^{0.47} f_{\\Theta |X}(\\theta \\, |\\, x)\\, d\\theta =0.70. \\end{align*} \\begin{align*} \\int _{0.41}^{0.47} f_{\\Theta |X}(\\theta \\, |\\, x)\\, d\\theta =0.70. \\end{align*} This time, I can say that there is a probability 70\\% 70\\% that the unknown parameter is inside the (Bayesian) confidence interval [0.41, 0.47] [0.41, 0.47] .","title":"Bayes' interpretation (Bayesian's Confidence Interval)"},{"location":"courses/6.431-probability/notes/#confidence-intervals-for-the-estimation-of-the-mean","text":"I asked you to estimate the mean of i.i.d variables X_1, X_2, \\cdots, X_n X_1, X_2, \\cdots, X_n with true mean \\theta \\theta , and variance \\sigma^2 \\sigma^2 , and the estimate should achieve 95\\% 95\\% confidence interval. How you proceed with it? By Central Limit Theory, your estimate values \\hat\\Theta \\hat\\Theta is an normal distribution. By standardizing it, you get the standard normal Z_n = \\frac{\\hat\\Theta - \\theta}{\\sigma/\\sqrt{n}} Z_n = \\frac{\\hat\\Theta - \\theta}{\\sigma/\\sqrt{n}} . 95\\% 95\\% confidence interverl means standard normal is between symetric 95\\% 95\\% intervals [-b, b] [-b, b] . By looking up the normal table, b = 1.69 b = 1.69 corresponds to the probability 97.5\\% 97.5\\% , so \\textbf{P}(-1.69 \\le Z_n \\le 1.69) = 95\\% \\textbf{P}(-1.69 \\le Z_n \\le 1.69) = 95\\% thus \\textbf{P}\\big(\\frac{|\\hat\\Theta - \\theta|}{\\sigma/\\sqrt{n}} \\le 1.69\\big) = 95\\% \\textbf{P}\\big(\\frac{|\\hat\\Theta - \\theta|}{\\sigma/\\sqrt{n}} \\le 1.69\\big) = 95\\% , which can be rewrite as \\begin{align*} \\textbf{P}\\Big(\\hat\\Theta -\\frac{1.96\\sigma}{\\sqrt n} \\le \\theta \\le \\hat\\Theta +\\frac{1.96\\sigma}{\\sqrt n}\\Big) = 95\\%. \\end{align*} \\begin{align*} \\textbf{P}\\Big(\\hat\\Theta -\\frac{1.96\\sigma}{\\sqrt n} \\le \\theta \\le \\hat\\Theta +\\frac{1.96\\sigma}{\\sqrt n}\\Big) = 95\\%. \\end{align*} This is the way to construct the confidence interval. If you have the \\sigma \\sigma and n n , you have a concrete 95\\% 95\\% confidence interval to report. However, the \\sigma \\sigma is usually unknown, what you can do is to use the estimated \\hat\\Theta \\hat\\Theta to estimate the \\sigma \\sigma .","title":"Confidence intervals for the estimation of the mean"},{"location":"courses/6.431-probability/notes/#lecture-21","text":"","title":"Lecture 21"},{"location":"courses/6.431-probability/notes/#lecture-22","text":"","title":"Lecture 22"},{"location":"courses/6.431-probability/notes/#lecture-23","text":"","title":"Lecture 23"},{"location":"courses/6.431-probability/notes/#lecture-24","text":"","title":"Lecture 24"},{"location":"courses/6.431-probability/notes/#lecture-25","text":"","title":"Lecture 25"},{"location":"courses/6.431-probability/notes/#lecture-26","text":"","title":"Lecture 26"},{"location":"courses/9chap-dynamic-prog/notes/","text":"Nine Chapter Dynamic Programming \u00b6 Nine Chapter Dynamic Programming Course Lecture 1 Introduction to Dynamic Programming \u00b6 Problem Category Coin Change Unique Paths coordinate Jump Game \u52a8\u6001\u89c4\u5212\u9898\u76ee\u7279\u70b9: \u8ba1\u6570 \u6709\u591a\u5c11\u79cd\u65b9\u5f0f\u8d70\u5230\u53f3\u4e0b\u89d2 \u6709\u591a\u5c11\u79cd\u65b9\u6cd5\u9009\u51fak\u4e2a\u6570\u4f7f\u5f97\u548c\u662fSum \u6c42\u6700\u5927\u6700\u5c0f\u503c \u4ece\u5de6\u4e0a\u89d2\u8d70\u5230\u53f3\u4e0b\u89d2\u8def\u5f84\u7684\u6700\u5927\u6570\u5b57\u548c \u6700\u957f\u4e0a\u5347\u5b50\u5e8f\u5217\u957f\u5ea6 \u6c42\u5b58\u5728\u6027 \u53d6\u77f3\u5b50\u6e38\u620f\uff0c\u5148\u624b\u662f\u5426\u5fc5\u80dc \u80fd\u4e0d\u80fd\u9009\u51fak\u4e2a\u6570\u4f7f\u5f97\u548c\u662fSum \u72b6\u6001\u662f\u52a8\u6001\u89c4\u5212\u5b9a\u6d77\u795e\u9488\uff0c\u786e\u5b9a\u72b6\u6001\u9700\u8981\u4e24\u4e2a\u57fa\u672c\u610f\u8bc6\uff1a \u6700\u540e\u4e00\u6b65 \u5b50\u95ee\u9898 Four Ingredients for DP What's the state? start with the last step for the optimal solution decompose into subproblems Write the state transition? find the transition from subproblem relations Initial value and boundary conditions need to think careful in this step How can you compute the states? iteration directio forward computing Coin Change \u00b6 Imagine the last coin you can use and the minimum solution is found can be represented as f[amount] . It can be solved by solving the smaller problem first. we have f[amount] = min(f[amount], f[amount - last\\_coin] + 1) . The problem is we don't know which coin will be selected for the last one to reach the solution, so we have to iterate throught the coins to check every one of them. We expecting to see two for loops in our code. DP 4 ingredient: size of the dp array f[amount + 1] initial state: f[0] = 0 , amount 0 can use 0 coin. subproblem: f[amount] = min(f[amount], f[amount - last\\_coin] + 1) . results: f[amount] C++ class Solution { public : int coinChange ( vector < int >& coins , int amount ) { int n = coins . size (); int f [ amount + 1 ]; f [ 0 ] = 0 ; /* calculate the f[1], f[2], ... f[amount] */ for ( int i = 1 ; i <= amount ; i ++ ) { f [ i ] = INT_MAX ; /* small trick, set to invalid first */ for ( int j = 0 ; j < n ; j ++ ) { /* update states */ /* f[i] can select coins[j] && f[i - coins[j]] is possible && coins[j] is last coin */ if ( i >= coins [ j ] && f [ i - coins [ j ]] != INT_MAX && f [ i - coins [ j ]] + 1 < f [ i ]) { f [ i ] = f [ i - coins [ j ]] + 1 ; } } } return f [ amount ] == INT_MAX ? - 1 : f [ amount ]; } }; Alternative Solution class Solution { public : int coinChange ( vector < int >& coins , int amount ) { int n = coins . size (); int f [ amount + 1 ]; // f[i] represent the minimum counts to make up i amount // use INT_MAX to represent impossible case. f [ 0 ] = 0 ; for ( int i = 1 ; i <= amount ; i ++ ) { f [ i ] = INT_MAX ; for ( int j = 0 ; j < n ; j ++ ) { if ( i >= coins [ j ] && f [ i - coins [ j ]] != INT_MAX ) { f [ i ] = min ( f [ i ], f [ i - coins [ j ]] + 1 ); } } } return f [ amount ] == INT_MAX ? - 1 : f [ amount ]; } }; Unique Paths \u00b6 Solving smaller problem first than by accumulating the results from the smaller problems, we can solve the the overall problem. Use a 2-d array to record the result the smaller problem, we know for the position f[i][j] = f[i - 1][j] + f[i][j - 1] , which which mean the summation of number of path from above and from left. The initial state is the first row and the first column are all equal to 1 . C++ class Solution { public : /** * @param n, m: positive integer (1 <= n ,m <= 100) * @return an integer */ int uniquePaths ( int m , int n ) { int f [ m ][ n ] = { 0 }; for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { if ( i == 0 || j == 0 ) { f [ i ][ j ] = 1 ; } if ( i > 0 && j > 0 ) { f [ i ][ j ] = f [ i - 1 ][ j ] + f [ i ][ j - 1 ]; } } } return f [ m - 1 ][ n - 1 ]; } }; Jump Game \u00b6 Notice the problem statement \"Each element in the array represents your maximum jump length at that position.\" Solution 1 DP The problem have characteristics of the dynamic problem, that is it can be decomposed into smaller problem, the large problem can be solved using the result from solving smaller problems. We use a dp array f[i] to store whether it can jump from previous position to position i . if nums[i] + i >= j , it can also jump to position j . Looks like we are going to have a nested loop. the outer loop iterate to check whether can jump to each step j . inner loop check each step before j , namely the smaller size problem. C++ class Solution { public : bool canJump ( vector < int >& nums ) { int n = nums . size (); if ( n == 0 ) return true ; int f [ n ]; f [ 0 ] = true ; /* initialize */ for ( int j = 1 ; j < n ; j ++ ) { f [ j ] = false ; for ( int i = 0 ; i < j ; i ++ ) { if ( f [ i ] && nums [ i ] + i >= j ) { f [ j ] = true ; break ; } } } return f [ n - 1 ]; } }; Solution 2 Greedy Use a variable cur_max to maintain the possible maximum jump position, if the current position is less than the maximum possible jump, return flase. C++ class Solution { public : bool canJump ( vector < int >& nums ) { int n = nums . size (); if ( n == 0 ) return true ; int cur_max = nums [ 0 ]; /* maximum distance can reach from ith */ /* L.I.: current step (ith step) must <= cur_max jump */ for ( int i = 0 ; i < n ; i ++ ) { if ( i > cur_max ) /* must goes first */ return false ; if ( i + nums [ i ] > cur_max ) cur_max = i + nums [ i ]; } return true ; } }; Lecture 2 Dynamic Programming on Coordinates \u00b6 Problem Category Unique Paths II \u5750\u6807\u578b Paint House \u5e8f\u5217\u578b \uff0b \u72b6\u6001 Decode Ways \u5212\u5206\u578b Longest Increasing Continuous Subsequence \u5e8f\u5217\u578b / \u5212\u5206\u578b Minimum Path Sum \u5750\u6807\u578b Bomb Enemy \u5750\u6807\u578b Counting Bits \u5750\u6807\u578b \u5750\u6807\u578b\u52a8\u6001\u89c4\u5212 \u00b6 \u6700\u7b80\u5355\u7684\u52a8\u6001\u89c4\u5212\u7c7b\u578b \u7ed9\u5b9a\u4e00\u4e2a\u5e8f\u5217\u6216\u7f51\u683c \u9700\u8981\u627e\u5230\u5e8f\u5217\u4e2d\u67d0\u4e2a/\u4e9b\u5b50\u5e8f\u5217\u6216\u7f51\u683c\u4e2d\u7684\u67d0\u6761\u8def\u5f84 \u67d0\u79cd\u6027\u8d28\u6700\u5927/\u6700\u5c0f \u8ba1\u6570 \u5b58\u5728\u6027 \u52a8\u6001\u89c4\u5212\u65b9\u7a0b f[i] \u4e2d\u7684\u4e0b\u6807 i \u8868\u793a\u4ee5 a[i] \u4e3a\u7ed3\u5c3e\u7684\u6ee1\u8db3\u6761\u4ef6\u7684\u5b50\u5e8f\u5217\u7684\u6027\u8d28. f[i][j] \u4e2d\u7684 \u4e0b\u6807 i , j \u8868\u793a\u4ee5\u683c\u5b50 (i, j) \u4e3a\u7ed3\u5c3e\u7684\u6ee1\u8db3\u6761\u4ef6\u7684\u8def\u5f84\u7684\u6027\u8d28 \u6700\u5927\u503c/\u6700\u5c0f\u503c \u4e2a\u6570 \u662f\u5426\u5b58\u5728 \u5750\u6807\u578b\u52a8\u6001\u89c4\u5212\u7684\u521d\u59cb\u6761\u4ef6 f[0] \u5c31\u662f\u6307\u4ee5 a[0] \u4e3a\u7ed3\u5c3e\u7684\u5b50\u5e8f\u5217\u7684\u6027\u8d28 Unique Paths II \u00b6 C++ Naive DP class Solution { public : int uniquePathsWithObstacles ( vector < vector < int >>& obstacleGrid ) { int m = obstacleGrid . size (); int n = obstacleGrid [ 0 ]. size (); int i = 0 ; int j = 0 ; int flag = 0 ; vector < vector < int >> f ( m , vector < int > ( n )); if ( m == 0 || n == 0 ) { return 0 ; } if ( obstacleGrid [ 0 ][ 0 ] == 1 ) { return 0 ; } f [ 0 ][ 0 ] = 1 ; for ( i = 1 ; i < m ; i ++ ) { if ( obstacleGrid [ i ][ 0 ] == 1 ) { f [ i ][ 0 ] = 0 ; } else { f [ i ][ 0 ] = f [ i - 1 ][ 0 ]; } } for ( j = 1 ; j < n ; j ++ ) { if ( obstacleGrid [ 0 ][ j ] == 1 ) { f [ 0 ][ j ] = 0 ; } else { f [ 0 ][ j ] = f [ 0 ][ j - 1 ]; } } for ( i = 1 ; i < m ; i ++ ) { for ( j = 1 ; j < n ; j ++ ) { if ( obstacleGrid [ i ][ j ] == 1 ) { f [ i ][ j ] = 0 ; } else { f [ i ][ j ] = f [ i ][ j - 1 ] + f [ i - 1 ][ j ]; } } } return f [ m - 1 ][ n - 1 ]; } }; C++ Naive DP Refactored class Solution { public : int uniquePathsWithObstacles ( vector < vector < int >>& obstacleGrid ) { int m = obstacleGrid . size (); int n = m > 0 ? obstacleGrid [ 0 ]. size () : 0 ; if ( m == 0 && n == 0 ) { return 0 ; } vector < vector < int >> f ( m , vector < int > ( n , 0 )); f [ 0 ][ 0 ] = 1 ; if ( obstacleGrid [ 0 ][ 0 ] == 1 ) { return 0 ; } for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { if ( obstacleGrid [ i ][ j ] == 1 ) { f [ i ][ j ] = 0 ; } else { // whenever available if ( i > 0 ) { f [ i ][ j ] += f [ i - 1 ][ j ]; } if ( j > 0 ) { f [ i ][ j ] += f [ i ][ j - 1 ]; } } } } return f [ m - 1 ][ n - 1 ]; } }; Java O(n) space // Optimized using a rolling array or single row dp array instead of m x n. class Solution { public int uniquePathsWithObstacles ( int [][] a ) { int m = a . length ; int n = a [ 0 ] . length ; int dp [] = new int [ n ] ; dp [ 0 ] = 1 ; //T[i][j] = T[i-1][j] + T[i][j-1] for ( int i = 0 ; i < m ; i ++ ){ for ( int j = 0 ; j < n ; j ++ ){ if ( a [ i ][ j ] == 1 ){ dp [ j ] = 0 ; } else if ( j > 0 ){ dp [ j ] += dp [ j - 1 ] ; } } } return dp [ n - 1 ] ; } } Paint House \u00b6 DP last step: considering the optimal solution. The last paint must be one of the three colors and the paint cost is minimum. State: we can try to name f[i] the minimum cost of painting the first i houses, i = 0, 1, ... i-1 . However, we cannot know what color could be used for the last house. Change the state to: f[i][k] is the minium cost of painting the first i houses and the i-1 th element is painted as color k . This way, we can choose the last paint based on this piece of information. The state transition fomular: f[i][k] = f[i - 1][\\ell]_{\\ell != k} + costs[i - 1][k] f[i][k] = f[i - 1][\\ell]_{\\ell != k} + costs[i - 1][k] The result is minimum of the: f[i][0],\\ f[i][1],\\ f[i][2] f[i][0],\\ f[i][1],\\ f[i][2] class Solution { public : int minCost ( vector < vector < int >>& costs ) { int m = costs . size (); int f [ m + 1 ][ 3 ]; /* state: f[i][0] paint i house and the last (i - 1) house is red * /* initial value, paint 0 house cost 0 */ for ( int j = 0 ; j < 3 ; j ++ ) f [ 0 ][ j ] = 0 ; for ( int i = 1 ; i <= m ; i ++ ) { for ( int j = 0 ; j < 3 ; j ++ ) { // current color f [ i ][ j ] = INT_MAX ; for ( int k = 0 ; k < 3 ; k ++ ) { // painted color /* cannot paint same color for neighbor house */ if ( j == k ) continue ; // ith (index with i - 1) house is painted with color j if ( f [ i ][ j ] > f [ i - 1 ][ k ] + costs [ i - 1 ][ j ]) f [ i ][ j ] = f [ i - 1 ][ k ] + costs [ i - 1 ][ j ]; //f[i][j] = min(f[i - 1][k] + costs[i - 1][j], f[i][j]); } } } return min ( min ( f [ m ][ 0 ], f [ m ][ 1 ]), f [ m ][ 2 ]); } }; Decode Ways \u00b6 4 Ingredients Last step: A_0, A_1, A_2, ..., A_n-3, [A_n-2, A_n-1] A_0, A_1, A_2, ..., A_n-3, A_n-2, [A_n-1] State: f[n] : decode ways of first n letters. Smaller problem: f[n] = f[n - 1] + f[n - 2] | A_n-2,A_n-1 decodable. Init: f[0] = 1 Note Again the idea in this solution is to update the f[i][j] on demand, a technique that broadly used in 2-d coordinate based DP problems such as Bomb Enemy , Unique Paths II , and Minimum Path Sum . C++ DP class Solution { public : int numDecodings ( string s ) { int n = s . length (); int f [ n + 1 ] = { 0 }; if ( n == 0 ) return 0 ; f [ 0 ] = 1 ; for ( int i = 1 ; i <= n ; i ++ ) { int t = s [ i - 1 ] - '0' ; if ( t > 0 && t <= 9 ) { f [ i ] += f [ i - 1 ]; } if ( i > 1 ){ int q = ( s [ i - 2 ] - '0' ) * 10 + t ; if ( q >= 10 && q <= 26 ) { f [ i ] += f [ i - 2 ]; } } } return f [ n ]; } }; C++ DP O(1) class Solution { public : int numDecodings ( string s ) { if ( ! s . size () || s . front () == '0' ) return 0 ; // r2: decode ways of s[0, i-2] , r1: decode ways of s[0, i-1] int r1 = 1 , r2 = 1 ; // think it as a coordinate bases, not sequence based dp for ( int i = 1 ; i < s . size (); i ++ ) { // last char in s[0, i] is 0, cannot decode if ( s [ i ] == '0' ) r1 = 0 ; // two-digit letter, add r2 to r1, r2 get the previous r1 if ( s [ i - 1 ] == '1' || s [ i - 1 ] == '2' && s [ i ] <= '6' ) { r1 = r2 + r1 ; r2 = r1 - r2 ; } else { // one-digit letter, r2 get the previous r1 r2 = r1 ; } } return r1 ; } }; Longest Increasing Continuous Subsequence \u00b6 4 ingredients: Last step, last element a[n-1] could be in the result or not in the result. subproblem, suppose we have the LICS of the first n - 1 elements. represented as f[n-1] . base case and boundary condition, when no char in the string: f[0] = 1 . empty string have LICS length of 1. order of calculation, calculate small index first. Not a leetcode The problem is not a leetcode problem, the original problem ask for sequence that could be increase or decrease. Using index in DP problems Avoid using both index i - 1 and i + 1 in a loop invariance, otherwise you'll have problem in keeping the loop invariance. Compare the followings. Incorrect for ( int i = n - 1 ; i >= 0 ; i -- ) { f [ i ] = 1 ; if ( i < n - 1 && i > 0 && A [ i - 1 ] > A [ i ]) { f [ i ] = f [ i + 1 ] + 1 ; } res2 = max ( res2 , f [ i ]); } Correct for ( int i = n - 1 ; i >= 0 ; i -- ) { f [ i ] = 1 ; if ( i < n - 1 && A [ i ] > A [ i + 1 ]) { f [ i ] = f [ i + 1 ] + 1 ; } res2 = max ( res2 , f [ i ]); } Correct for ( int i = 0 ; i < n ; i ++ ) { f [ i ] = 1 ; if ( i > 0 && A [ i - 1 ] > A [ i ]) { f [ i ] = f [ i - 1 ] + 1 ; } res2 = max ( res2 , f [ i ]); } C++ DP solution class Solution { public : int longestIncreasingContinuousSubsequence ( vector < int >& A ) { // Write your code here int n = A . size (); int f [ n ]; int res1 = 0 ; int res2 = 0 ; for ( int i = 0 ; i < n ; i ++ ) { f [ i ] = 1 ; if ( i > 0 && A [ i - 1 ] < A [ i ]) { f [ i ] = f [ i - 1 ] + 1 ; } res1 = max ( res1 , f [ i ]); } for ( int i = n - 1 ; i >= 0 ; i -- ) { f [ i ] = 1 ; if ( i < n - 1 && A [ i ] > A [ i + 1 ]) { f [ i ] = f [ i + 1 ] + 1 ; } res2 = max ( res2 , f [ i ]); } return max ( res1 , res2 ); } }; Here we don't have to explicitly wirte f[i] = max(1, f[i - 1] + 1) , since we have the condition A[i - 1] < A[i] , we know the A[i] will be added to the f[i] , hence simply update f[i] = f[i - 1] + 1 directly. Minimum Path Sum \u00b6 This is coordinate based DP. We need to have f[i][j] to keep the minimum path sum to the grid[i][j] . Calculate f[i][j] from top to down and from left to right for each row. Space can be optimized. Notice we can do the init and first row and first column in nested loops, do not need use multiple for loops. C++ DP class Solution { public : int minPathSum ( vector < vector < int > > & grid ) { int m = grid . size (); int n = grid [ 0 ]. size (); if ( m == 0 || n == 0 ) return 0 ; int f [ m ][ n ]; for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { if ( i == 0 && j == 0 ) { f [ i ][ j ] = grid [ i ][ j ]; } if ( i == 0 && j > 0 ) { f [ i ][ j ] = f [ i ][ j - 1 ] + grid [ i ][ j ]; } if ( j == 0 && i > 0 ) { f [ i ][ j ] = f [ i - 1 ][ j ] + grid [ i ][ j ]; } if ( i > 0 && j > 0 ) { f [ i ][ j ] = min ( f [ i - 1 ][ j ], f [ i ][ j - 1 ]) + grid [ i ][ j ]; } } } return f [ m - 1 ][ n - 1 ]; } }; C++ DP Space O(n) class Solution { public : int minPathSum ( vector < vector < int > > & grid ) { int m = grid . size (); int n = grid [ 0 ]. size (); if ( m == 0 || n == 0 ) return 0 ; int f [ 2 ][ n ]; /* only two rows of status */ int prev = 1 ; int curr = 1 ; for ( int i = 0 ; i < m ; i ++ ) { // rolling the f array when move to a new row prev = curr ; curr = 1 - curr ; for ( int j = 0 ; j < n ; j ++ ) { f [ curr ][ j ] = grid [ i ][ j ]; if ( i == 0 && j > 0 ) { f [ i ][ j ] += f [ i ][ j - 1 ]; } if ( j == 0 && i > 0 ) { f [ curr ][ j ] += f [ prev ][ j ]; } if ( i > 0 && j > 0 ) { f [ curr ][ j ] += min ( f [ prev ][ j ], f [ curr ][ j - 1 ]); } } } return f [ curr ][ n - 1 ]; } }; Note \u5728\u5904\u7406\u4e8c\u7ef4\u5750\u6807\u578bDP\u95ee\u9898\u7684\u65f6\u5019\uff0c\u5bf9\u4e8e\u7b2c\u4e00\u884c\u6216\u8005\u7b2c\u4e00\u5217\u7684\u8ba1\u7b97\u6211\u4eec\u53ef\u4ee5\u628a\u5b83\u653e\u5230loop\u4e2d\u3002\u4f46\u662f\u8981\u52a0\u4e00\u4e2a\u6761\u4ef6\u3002 \u8fd9\u4e2a\u6761\u4ef6\u5c31\u662f if (i > 0) , \u8fd9\u4e2a\u6761\u4ef6\u4fdd\u8bc1\u4e86\u5728\u66f4\u65b0\u6570\u7ec4\u503c\u7684\u65f6\u5019\u4e0b\u6807\u4e0d\u4f1a\u8d8a\u754c\u3002\u5e76\u4e14\u5f88\u597d\u7684\u8be0\u91ca\u4e86\u6211\u4eec\u7684\u52a8\u673a\uff08\u5c31\u662f\u5bf9\u4e8e\u7b2c\u4e00\u884c\u7b2c\u4e00\u5217\u4ec5\u4ec5\u662f\u521d\u59cb\u5316)\u3002 \u5bf9\u4e8e1\u4e2d\u63d0\u5230\u7684trick\uff0c\u53e6\u5916\u4e00\u4e2a\u597d\u5904\u5c31\u662f\u5bf9\u4e8e\u7b2c\u4e00\u884c\u6216\u8005\u7b2c\u4e00\u5217\u6761\u4ef6\u5224\u65ad\u7684\u65f6\u5019\uff0c\u8fd9\u91cc\u8fd8\u6709\u4e00\u4e2a\u6280\u5de7\u5c31\u662f\u5148\u628a\u81ea\u8eab\u7684\u2018\u6027\u8d28\u2019\u6216\u8005\u2018\u503c\u2019\u52a0\u4e0a\uff0c \u5982\u679c\u6709\u4e0a\u4e00\u884c\u6211\u4eec\u518d\u53bb\u52a0\u4e0a\u4e00\u884c\u7684\u2018\u6027\u8d28\u2019\u6216\u2018\u503c\u2019\u3002\u8fd9\u91cc\u9700\u8981\u9006\u5411\u601d\u7ef4. \u5728\u591a\u79cd\u60c5\u51b5\u9700\u8981\u5206\u5f00\u8ba8\u8bba\u7684\u65f6\u5019\u6211\u4eec\u53ef\u4ee5\u5148\u2018\u4e00\u7968\u5426\u51b3\u2019\u6700\u660e\u663e\u60c5\u51b5\uff0c\u7136\u540e\u8ba8\u8bba\u5269\u4e0b\u7684\u60c5\u51b5\u3002 Bomb Enemy \u00b6 Breakdown the problem into smaller (simpler) problems. Take the special steps (i.e. different properties of the cell) as normal ones in the loop, deal with the special step when doing the calculation. class Solution { public : int maxKilledEnemies ( vector < vector < char >>& grid ) { int m = grid . size (); if ( m == 0 ) return 0 ; int n = grid [ 0 ]. size (); if ( n == 0 ) return 0 ; int f [ m ][ n ]; int res [ m ][ n ]; int ret = 0 ; for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { res [ i ][ j ] = 0 ; } } /* up */ for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { f [ i ][ j ] = 0 ; if ( grid [ i ][ j ] == 'W' ) { f [ i ][ j ] = 0 ; } else { if ( grid [ i ][ j ] == 'E' ) { f [ i ][ j ] = 1 ; } if ( i > 0 ) { f [ i ][ j ] += f [ i - 1 ][ j ]; } } res [ i ][ j ] += f [ i ][ j ]; } } /* down */ for ( int i = m - 1 ; i >= 0 ; i -- ) { for ( int j = 0 ; j < n ; j ++ ) { f [ i ][ j ] = 0 ; if ( grid [ i ][ j ] == 'W' ) { f [ i ][ j ] = 0 ; } else { if ( grid [ i ][ j ] == 'E' ) { f [ i ][ j ] = 1 ; } if ( i < m - 1 ) { f [ i ][ j ] += f [ i + 1 ][ j ]; } } res [ i ][ j ] += f [ i ][ j ]; } } /* left */ for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { f [ i ][ j ] = 0 ; if ( grid [ i ][ j ] == 'W' ) { f [ i ][ j ] = 0 ; } else { if ( grid [ i ][ j ] == 'E' ) { f [ i ][ j ] = 1 ; } if ( j > 0 ) { f [ i ][ j ] += f [ i ][ j - 1 ]; } } res [ i ][ j ] += f [ i ][ j ]; } } /* right */ for ( int i = 0 ; i < m ; i ++ ) { for ( int j = n - 1 ; j >= 0 ; j -- ) { f [ i ][ j ] = 0 ; if ( grid [ i ][ j ] == 'W' ) { f [ i ][ j ] = 0 ; } else { if ( grid [ i ][ j ] == 'E' ) { f [ i ][ j ] = 1 ; } if ( j < n - 1 ) { f [ i ][ j ] += f [ i ][ j + 1 ]; } } res [ i ][ j ] += f [ i ][ j ]; } } /* calculate resutls */ for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { if ( grid [ i ][ j ] == '0' ) if ( res [ i ][ j ] > ret ) ret = res [ i ][ j ]; } } return ret ; } }; Counting Bits \u00b6 Notice you can use the trick that i >> 1 == i / 2 to construct the subproblem i count 1 bits 1 1 2 1 3 2 6 2 12 2 25 3 50 3 class Solution { public : vector < int > countBits ( int num ) { vector < int > f ( num + 1 , 0 ); if ( num == 0 ) { return f ; } for ( int i = 1 ; i <= num ; i ++ ) { f [ i ] = f [ i >> 1 ] + ( i % 2 ); } return f ; } }; Lecture 3 Dynamic Programming on Sequences \u00b6 Problem Category Paint House II \u5e8f\u5217\u578b\uff0b\u72b6\u6001 House Robber \u5e8f\u5217\u578b\uff0b\u72b6\u6001 House Robber II \u5e8f\u5217\u578b\uff0b\u72b6\u6001 Best Time to Buy and Sell Stock Best Time to Buy and Sell Stock II Best Time to Buy and Sell Stock III \u5e8f\u5217\u578b\uff0b\u72b6\u6001 Best Time to Buy and Sell Stock IV \u5e8f\u5217\u578b\uff0b\u72b6\u6001 Longest Increasing Subsequence \u5e8f\u5217\u578b Russian Doll Envelopes \u5e8f\u5217\u578b\u52a8\u6001\u89c4\u5212 \u00b6 \u7ed9\u5b9a\u4e00\u4e2a\u5e8f\u5217 \u52a8\u6001\u89c4\u5212\u65b9\u7a0b f[i] \u4e2d\u7684\u4e0b\u6807 i \u8868\u793a\u524d i \u4e2a\u5143\u7d20 a[0], a[1], ..., a[i-1] \u7684\u67d0\u79cd\u6027\u8d28 \u5750\u6807\u578b\u7684 f[i] \u8868\u793a\u4ee5 a[i] \u4e3a\u7ed3\u5c3e\u7684\u67d0\u79cd\u6027\u8d28 \u521d\u59cb\u5316\u4e2d\uff0c f[0] \u8868\u793a\u7a7a\u5e8f\u5217\u7684\u6027\u8d28 \u5750\u6807\u578b\u52a8\u6001\u89c4\u5212\u7684\u521d\u59cb\u6761\u4ef6 f[0] \u5c31\u662f\u6307\u4ee5 a[0] \u4e3a\u7ed3\u5c3e\u7684\u5b50\u5e8f\u5217\u7684\u6027\u8d28 Paint House II \u00b6 the solution could be the same as the first verison, but it is O(n\\cdot k^2) O(n\\cdot k^2) . how to make it O(n\\cdot k) O(n\\cdot k) ? By analyzing the state transition equation, we observed that we want to find a minimum value of a set of numbers except one for each house. Put it in english, the min cost to paint i - 1 th house with color k , we want to have the min cost of painting all previous i - 1 houses and the i-1 th house cannot be painted as color k . f [ i ][ 1 ] = min { f [ i-1 ][ 2 ] + cost [ i-1 ][ 1 ] , f [ i-1 ][ 3 ] + cost [ i-1 ][ 1 ] , ..., f [ i-1 ][ K ] + cost [ i-1 ][ 1 ] } f [ i ][ 2 ] = min { f [ i-1 ][ 1 ] + cost [ i-1 ][ 2 ] , f [ i-1 ][ 3 ] + cost [ i-1 ][ 2 ] , ..., f [ i-1 ][ K ] + cost [ i-1 ][ 2 ] } ... f [ i ][ K ] = min { f [ i-1 ][ 1 ] + cost [ i-1 ][ K ] , f [ i-1 ][ 2 ] + cost [ i-1 ][ K ] , ..., f [ i-1 ][ K-1 ] + cost [ i-1 ][ K ] } We could optimize the solution upon this. Basically, we can maintain the first two minimum value of the set f[i-1][1], f[i-1][2], f[i-1][3], ..., f[i-1][K] , min1 and min2 and their index j1 and j2 . There are two cases, the first case is the house i-1 is painted with the same color correspoinding to the minimum value. in this case, we cannot chose to paint it with the color corresponding to the minimum cost, we can update the state using the second minimum. The second case is that we paint the i-1th house with color other than the color corresponding to the minimum cost to pain, we can update using the minimum value. C++ O(nk) class Solution { public : /** * @param costs n x k cost matrix * @return an integer, the minimum cost to paint all houses */ int minCostII ( vector < vector < int >>& costs ) { int m = costs . size (); if ( m == 0 ) return 0 ; int k = costs [ 0 ]. size (); if ( k == 0 ) return 0 ; int f [ m + 1 ][ k ]; int min1 ; int min2 ; int j1 = 0 , j2 = 0 ; /* init, 0 house cost nothing. */ for ( int j = 0 ; j < k ; j ++ ) f [ 0 ][ j ] = 0 ; for ( int i = 1 ; i <= m ; i ++ ) { min1 = min2 = INT_MAX ; /* from all the colors, find the min1 and min2 */ for ( int j = 0 ; j < k ; j ++ ) { /* get the min1 and min2 first */ if ( f [ i - 1 ][ j ] < min1 ) { min2 = min1 ; j2 = j1 ; min1 = f [ i - 1 ][ j ]; j1 = j ; } else if ( f [ i - 1 ][ j ] < min2 ) { min2 = f [ i - 1 ][ j ]; j2 = j ; } } /* update the states based on the min1 and min2 */ for ( int j = 0 ; j < k ; j ++ ) { if ( j != j1 ) { f [ i ][ j ] = f [ i - 1 ][ j1 ] + costs [ i - 1 ][ j ]; } else { f [ i ][ j ] = f [ i - 1 ][ j2 ] + costs [ i - 1 ][ j ]; } } } int res = INT_MAX ; for ( int j = 0 ; j < k ; j ++ ) { if ( f [ m ][ j ] < res ) res = f [ m ][ j ]; } return res ; } }; C++ O(nk^2) class Solution { public : int minCostII ( vector < vector < int >>& costs ) { int n = costs . size (); int k = n > 0 ? costs [ 0 ]. size () : 0 ; if ( n == 0 && k == 0 ) return 0 ; if ( n == 1 && k == 1 ) return costs [ 0 ][ 0 ]; vector < vector < int >> f ( n + 1 , vector < int > ( k , 0 )); for ( int i = 0 ; i < k ; i ++ ) { f [ 0 ][ i ] = 0 ; // 0 cost for 0 paint } for ( int i = 1 ; i <= n ; i ++ ) { for ( int j = 0 ; j < k ; j ++ ) { f [ i ][ j ] = INT_MAX ; for ( int c = 0 ; c < k ; c ++ ) { if ( j == c ) { continue ; } if ( f [ i ][ j ] > f [ i - 1 ][ c ] + costs [ i - 1 ][ j ]) { f [ i ][ j ] = f [ i - 1 ][ c ] + costs [ i - 1 ][ j ]; } } } } int res = INT_MAX ; for ( int i = 0 ; i < k ; i ++ ) { if ( f [ n ][ i ] < res ) res = f [ n ][ i ]; } return res ; } }; House Robber \u00b6 Start with the last step. The last house could be either robbed or not. If the last house a[i-1] is robbed, then we cannot rob a[i-2] . we have f[i] = f[i-2] + a[i-1] . If the last house a[i-1] is not robbed, then we can rob a[i-2] . Alternatively, we can skip it to rob a[i-3] . We have f[i] = f[n-1] , in which this f[n-1] could not let us know whether a[i-2] is robbed or not. We add the state of a[i-1] to the state transition equation. Thus we have: f[i][0] represent \"the maximum money for robbing the first i houses and the last house hasn't been robbed.\" f[i][1] represent \"the maximum money for robbing the first i houses and the last house has been robbed.\" state transition equations: f[i][0] = max(f[i-1][0], f[i-1][1]) f[i][1] = f[i-1][0] + a[i-1] C++ class Solution { public : int rob ( vector < int >& nums ) { int n = nums . size (); if ( n == 0 ) return 0 ; int f [ n + 1 ][ 2 ] = { 0 }; for ( int i = 1 ; i <= n ; i ++ ) { f [ i ][ 0 ] = max ( f [ i - 1 ][ 0 ], f [ i - 1 ][ 1 ]); f [ i ][ 1 ] = f [ i - 1 ][ 0 ] + nums [ i - 1 ]; } return max ( f [ n ][ 0 ], f [ n ][ 1 ]); } }; C++ optimized class Solution { public : long long houseRobber ( vector < int > A ) { int n = A . size (); if ( n == 0 ) return 0 ; long long f [ n + 1 ]; /* init */ f [ 0 ] = 0 ; f [ 1 ] = A [ 0 ]; for ( int i = 2 ; i <= n ; i ++ ) { f [ i ] = max ( f [ i - 1 ], f [ i - 2 ] + A [ i - 1 ]); } return f [ n ]; } }; Solution 2 yes - first i days, robber last day, no - first i days, not robber at last day. C++ class Solution { public : int rob ( vector < int >& nums ) { int n = nums . size (); if ( n == 0 ) return 0 ; int yes = 0 , no = 0 ; for ( int i = 1 ; i <= n ; i ++ ) { int tmp = no ; no = max ( yes , no ); yes = tmp + nums [ i - 1 ]; } return max ( yes , no ); } }; House Robber II \u00b6 Following House Robber , when we try to analyze the last step, the house i - 1 depends on the house i - 2 and the house 0 . How could we handle this? We could enumerate two cases, and reduce the probelm to House Rober house 0 is robbed and house i-1 is not. house i - 1 is robbed and house 0 is not. class Solution { public : int rob ( vector < int >& nums ) { int n = nums . size (); if ( n == 0 ) return 0 ; if ( n == 1 ) // edge case to deal with return nums [ 0 ]; vector < int > nums1 ( nums . begin () + 1 , nums . end ()); vector < int > nums2 ( nums . begin (), nums . end () - 1 ); return max ( rob_helper ( nums1 ), rob_helper ( nums2 )); } private : int rob_helper ( vector < int >& A ) { int n = A . size (); if ( n == 0 ) return 0 ; int f [ n + 1 ] = { 0 }; f [ 0 ] = 0 ; f [ 1 ] = A [ 0 ]; for ( int i = 2 ; i <= n ; i ++ ) { f [ i ] = max ( f [ i - 1 ], f [ i - 2 ] + A [ i - 1 ]); } return f [ n ]; } }; Best Time to Buy and Sell Stock \u00b6 Force youself to do it by only one pass. You'll find that you need two variables to record the minimum value currently found and the maximum profit currently found. class Solution { public : int maxProfit ( vector < int > & prices ) { int n = prices . size (); if ( n == 0 ) return 0 ; int min_price = INT_MAX ; int max_profit = 0 ; for ( int i = 0 ; i < n ; i ++ ) { if ( prices [ i ] < min_price ) { min_price = prices [ i ]; } else if ( prices [ i ] - min_price > max_profit ) { max_profit = prices [ i ] - min_price ; } } return max_profit ; } }; Best Time to Buy and Sell Stock II \u00b6 We can just buy and sell when ever the price is increased in a day. Once you can proof the day-by-day subproblem can form the solution. The solution becomes so easy. C++ class Solution { public : int maxProfit ( vector < int > & prices ) { int res = 0 ; for ( int i = 1 ; i < prices . size (); i ++ ) { if ( prices [ i - 1 ] < prices [ i ]) res += prices [ i ] - prices [ i - 1 ]; } return res ; } }; C++ Alternative class Solution { public : int maxProfit ( vector < int > & prices ) { int res = 0 ; for ( int i = 1 ; i < prices . size (); i ++ ) { res += max ( prices [ i ] - prices [ i - 1 ], 0 ) } return res ; } }; Best Time to Buy and Sell Stock III \u00b6 We can define 5 stages and write state transition equations based on it. C++ class Solution { public : /** * @param prices: Given an integer array * @return: Maximum profit */ int maxProfit ( vector < int > & A ) { int n = A . size (); if ( n == 0 ) { return 0 ; } int f [ n + 1 ][ 6 ]; /* init */ f [ 0 ][ 1 ] = 0 ; f [ 0 ][ 2 ] = f [ 0 ][ 3 ] = f [ 0 ][ 4 ] = f [ 0 ][ 5 ] = INT_MIN ; for ( int i = 1 ; i <= n ; i ++ ) { // stage 1, 3, 5: // f[i][j] = max{f[i - 1][j], f[i - 1][j - 1] + A[i - 1] - A[i - 2]} for ( int j = 1 ; j <= 5 ; j += 2 ) { f [ i ][ j ] = f [ i - 1 ][ j ]; if ( i > 1 && j > 1 && f [ i - 1 ][ j - 1 ] != INT_MIN ) { f [ i ][ j ] = max ( f [ i ][ j ], f [ i - 1 ][ j - 1 ] + A [ i - 1 ] - A [ i - 2 ]); } } // stage 2, 4: // f[i][j] = max{f[i - 1][j] + A[i - 1] - A[i - 2], f[i - 1][j - 1], f[i - 1][j - 2] + A[i - 1] - A[i - 2]} for ( int j = 2 ; j < 5 ; j += 2 ) { f [ i ][ j ] = f [ i - 1 ][ j - 1 ]; if ( i > 1 && f [ i - 1 ][ j ] != INT_MIN ) { f [ i ][ j ] = max ( f [ i ][ j ], f [ i - 1 ][ j ] + A [ i - 1 ] - A [ i - 2 ]); } if ( i > 1 && j > 2 && f [ i - 1 ][ j - 2 ] != INT_MIN ) { f [ i ][ j ] = max ( f [ i ][ j ], f [ i - 1 ][ j - 2 ] + A [ i - 1 ] - A [ i - 2 ]); } } } return max ( f [ n ][ 1 ], max ( f [ n ][ 3 ], f [ n ][ 5 ])); } }; Best Time to Buy and Sell Stock IV \u00b6 If k is larger then n/2 , it is equivalent to the II. This is a generalized solution from Best Time to Buy and Sell Stock III . class Solution { public : int maxProfit ( int K , vector < int > & A ) { int n = A . size (); if ( n == 0 ) { return 0 ; } int f [ n + 1 ][ 2 * K + 1 + 1 ]; /* special case need to take care of */ if ( K > ( n / 2 )) { int res = 0 ; for ( int i = 0 ; i + 1 < n ; i ++ ) { if ( A [ i + 1 ] - A [ i ] > 0 ) { res += A [ i + 1 ] - A [ i ]; } } return res ; } /* init */ f [ 0 ][ 1 ] = 0 ; for ( int k = 2 ; k <= 2 * K + 1 ; k ++ ) { f [ 0 ][ k ] = INT_MIN ; } for ( int i = 1 ; i <= n ; i ++ ) { for ( int j = 1 ; j <= 2 * K + 1 ; j += 2 ) { f [ i ][ j ] = f [ i - 1 ][ j ]; if ( i > 1 && j > 1 && f [ i - 1 ][ j - 1 ] != INT_MIN ) { f [ i ][ j ] = max ( f [ i ][ j ], f [ i - 1 ][ j - 1 ] + A [ i - 1 ] - A [ i - 2 ]); } } for ( int j = 2 ; j < 2 * K + 1 ; j += 2 ) { f [ i ][ j ] = f [ i - 1 ][ j - 1 ]; if ( i > 1 && f [ i - 1 ][ j ] != INT_MIN ) { f [ i ][ j ] = max ( f [ i ][ j ], f [ i - 1 ][ j ] + A [ i - 1 ] - A [ i - 2 ]); } if ( i > 1 && j > 2 && f [ i - 1 ][ j - 2 ] != INT_MIN ) { f [ i ][ j ] = max ( f [ i ][ j ], f [ i - 1 ][ j - 2 ] + A [ i - 1 ] - A [ i - 2 ]); } } } int res = INT_MIN ; for ( int j = 1 ; j <= 2 * K + 1 ; j += 2 ) { if ( f [ n ][ j ] > res ) res = f [ n ][ j ]; } return res ; } }; Longest Increasing Subsequence \u00b6 Comparing to the problem Longest Increasing Continuous Subsequence . If the subsequence is not continous. we have to enumerate each of the previous element befor A[j] . Solution 1 DP O(n^2) DP class Solution { public : int lengthOfLIS ( vector < int >& nums ) { int n = nums . size (); if ( n == 0 ) return 0 ; int res = 0 ; int f [ n ] = { 0 }; for ( int j = 0 ; j < n ; j ++ ) { /* case 1: a[j] is the subsequence */ f [ j ] = 1 ; /* case 2: LIS from a[0],...a[i] plus a[j] */ for ( int i = 0 ; i < j ; i ++ ) { if ( nums [ i ] < nums [ j ] && f [ i ] + 1 > f [ j ]) f [ j ] = f [ i ] + 1 ; } res = max ( res , f [ j ]); } return res ; } }; Solution 2 DP with binary search O(nlogn) To reduce the complexity, we can try to find if there is any redundant work we have been done. or some how we could use some order information to avoid some of the calculation. Focus on the real meaning of longest Increasing Subsequence. In fact, you are looking for the smallest value before A[i] that leads to the longest Increasing Subsequence so far. use the state f[i] to record the LIS of the array A[0], ... A[i -1] . If we are at f[j], j > i , we are looking for the largest f[i] value that have the smallest A[i] . DP with binary search class Solution { public : int longestIncreasingSubsequence ( vector < int > nums ) { int n = nums . size (); if ( n == 0 ) return 0 ; //int f[n]; // b[i]: when f value is i, smallest a value (ending value) int b [ n + 1 ]; int top = 0 ; b [ 0 ] = INT_MIN ; // O(n) for ( int i = 0 ; i < n ; i ++ ) { // b[0] ~ b[top] // last value b[j] which is smaller than A[i] int start = 0 , end = top ; int mid ; int j ; // O(lgn) while ( start <= end ) { mid = start + ( end - start ) / 2 ; if ( b [ mid ] < nums [ i ]) { j = mid ; start = mid + 1 ; } else { end = mid - 1 ; } } // b[i]: length is j (f value is j), smallest ending value. // A[i] is after it. // f[i] = j + 1 // b[j + 1]: length is (j + 1), smallest ending value. // A[i] // B[j + 1] >= A[i] b [ j + 1 ] = nums [ i ]; if ( j + 1 > top ) { top = j + 1 ; } } // b[top] stores the smallest ending value for an LIS return top ; } }; Solution 3 DP with binary search refactored In observing the fact that we can use extra space to keep the \"minimum elements see so far from nums that is the last element of LIS for the different length of such LISes\". Different from the regular DP solution, our extra space b is storing element from nums, and the element stored in b are not necessary in order. The index i of elements in b related to the length of a LIS whose last element is a[i] . specifically, i + 1 = length(LIS) . DP with binary search refactored class Solution { public : int lengthOfLIS ( vector < int >& nums ) { vector < int > b ; for ( int i = 0 ; i < nums . size (); ++ i ) { int begin = 0 , end = b . size (); while ( begin != end ) { int mid = begin + ( end - begin ) / 2 ; if ( b [ mid ] < nums [ i ]) { begin = mid + 1 ; } else { end = mid ; } } if ( begin == b . size ()) // nums[i] greater than all element in b b . push_back ( nums [ i ]); else // begin point to next element no less than the target nums[i]. b [ begin ] = nums [ i ]; } return b . size (); } }; Solution 4 C++ using lower_bound we can use the lower_bound to replace the binary search routine in the above solution. C++ using lower_bound class Solution { public : int lengthOfLIS ( vector < int >& nums ) { vector < int > b ; for ( auto a : nums ) { auto it = lower_bound ( b . begin (), b . end (), a ); if ( it == b . end ()) b . push_back ( a ); else * it = a ; } return b . size (); } }; Russian Doll Envelopes \u00b6","title":"Nine Chapter Dynamic Prog"},{"location":"courses/9chap-dynamic-prog/notes/#nine-chapter-dynamic-programming","text":"Nine Chapter Dynamic Programming Course","title":"Nine Chapter Dynamic Programming"},{"location":"courses/9chap-dynamic-prog/notes/#lecture-1-introduction-to-dynamic-programming","text":"Problem Category Coin Change Unique Paths coordinate Jump Game \u52a8\u6001\u89c4\u5212\u9898\u76ee\u7279\u70b9: \u8ba1\u6570 \u6709\u591a\u5c11\u79cd\u65b9\u5f0f\u8d70\u5230\u53f3\u4e0b\u89d2 \u6709\u591a\u5c11\u79cd\u65b9\u6cd5\u9009\u51fak\u4e2a\u6570\u4f7f\u5f97\u548c\u662fSum \u6c42\u6700\u5927\u6700\u5c0f\u503c \u4ece\u5de6\u4e0a\u89d2\u8d70\u5230\u53f3\u4e0b\u89d2\u8def\u5f84\u7684\u6700\u5927\u6570\u5b57\u548c \u6700\u957f\u4e0a\u5347\u5b50\u5e8f\u5217\u957f\u5ea6 \u6c42\u5b58\u5728\u6027 \u53d6\u77f3\u5b50\u6e38\u620f\uff0c\u5148\u624b\u662f\u5426\u5fc5\u80dc \u80fd\u4e0d\u80fd\u9009\u51fak\u4e2a\u6570\u4f7f\u5f97\u548c\u662fSum \u72b6\u6001\u662f\u52a8\u6001\u89c4\u5212\u5b9a\u6d77\u795e\u9488\uff0c\u786e\u5b9a\u72b6\u6001\u9700\u8981\u4e24\u4e2a\u57fa\u672c\u610f\u8bc6\uff1a \u6700\u540e\u4e00\u6b65 \u5b50\u95ee\u9898 Four Ingredients for DP What's the state? start with the last step for the optimal solution decompose into subproblems Write the state transition? find the transition from subproblem relations Initial value and boundary conditions need to think careful in this step How can you compute the states? iteration directio forward computing","title":"Lecture 1 Introduction to Dynamic Programming"},{"location":"courses/9chap-dynamic-prog/notes/#coin-change","text":"Imagine the last coin you can use and the minimum solution is found can be represented as f[amount] . It can be solved by solving the smaller problem first. we have f[amount] = min(f[amount], f[amount - last\\_coin] + 1) . The problem is we don't know which coin will be selected for the last one to reach the solution, so we have to iterate throught the coins to check every one of them. We expecting to see two for loops in our code. DP 4 ingredient: size of the dp array f[amount + 1] initial state: f[0] = 0 , amount 0 can use 0 coin. subproblem: f[amount] = min(f[amount], f[amount - last\\_coin] + 1) . results: f[amount] C++ class Solution { public : int coinChange ( vector < int >& coins , int amount ) { int n = coins . size (); int f [ amount + 1 ]; f [ 0 ] = 0 ; /* calculate the f[1], f[2], ... f[amount] */ for ( int i = 1 ; i <= amount ; i ++ ) { f [ i ] = INT_MAX ; /* small trick, set to invalid first */ for ( int j = 0 ; j < n ; j ++ ) { /* update states */ /* f[i] can select coins[j] && f[i - coins[j]] is possible && coins[j] is last coin */ if ( i >= coins [ j ] && f [ i - coins [ j ]] != INT_MAX && f [ i - coins [ j ]] + 1 < f [ i ]) { f [ i ] = f [ i - coins [ j ]] + 1 ; } } } return f [ amount ] == INT_MAX ? - 1 : f [ amount ]; } }; Alternative Solution class Solution { public : int coinChange ( vector < int >& coins , int amount ) { int n = coins . size (); int f [ amount + 1 ]; // f[i] represent the minimum counts to make up i amount // use INT_MAX to represent impossible case. f [ 0 ] = 0 ; for ( int i = 1 ; i <= amount ; i ++ ) { f [ i ] = INT_MAX ; for ( int j = 0 ; j < n ; j ++ ) { if ( i >= coins [ j ] && f [ i - coins [ j ]] != INT_MAX ) { f [ i ] = min ( f [ i ], f [ i - coins [ j ]] + 1 ); } } } return f [ amount ] == INT_MAX ? - 1 : f [ amount ]; } };","title":"Coin Change"},{"location":"courses/9chap-dynamic-prog/notes/#unique-paths","text":"Solving smaller problem first than by accumulating the results from the smaller problems, we can solve the the overall problem. Use a 2-d array to record the result the smaller problem, we know for the position f[i][j] = f[i - 1][j] + f[i][j - 1] , which which mean the summation of number of path from above and from left. The initial state is the first row and the first column are all equal to 1 . C++ class Solution { public : /** * @param n, m: positive integer (1 <= n ,m <= 100) * @return an integer */ int uniquePaths ( int m , int n ) { int f [ m ][ n ] = { 0 }; for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { if ( i == 0 || j == 0 ) { f [ i ][ j ] = 1 ; } if ( i > 0 && j > 0 ) { f [ i ][ j ] = f [ i - 1 ][ j ] + f [ i ][ j - 1 ]; } } } return f [ m - 1 ][ n - 1 ]; } };","title":"Unique Paths"},{"location":"courses/9chap-dynamic-prog/notes/#jump-game","text":"Notice the problem statement \"Each element in the array represents your maximum jump length at that position.\" Solution 1 DP The problem have characteristics of the dynamic problem, that is it can be decomposed into smaller problem, the large problem can be solved using the result from solving smaller problems. We use a dp array f[i] to store whether it can jump from previous position to position i . if nums[i] + i >= j , it can also jump to position j . Looks like we are going to have a nested loop. the outer loop iterate to check whether can jump to each step j . inner loop check each step before j , namely the smaller size problem. C++ class Solution { public : bool canJump ( vector < int >& nums ) { int n = nums . size (); if ( n == 0 ) return true ; int f [ n ]; f [ 0 ] = true ; /* initialize */ for ( int j = 1 ; j < n ; j ++ ) { f [ j ] = false ; for ( int i = 0 ; i < j ; i ++ ) { if ( f [ i ] && nums [ i ] + i >= j ) { f [ j ] = true ; break ; } } } return f [ n - 1 ]; } }; Solution 2 Greedy Use a variable cur_max to maintain the possible maximum jump position, if the current position is less than the maximum possible jump, return flase. C++ class Solution { public : bool canJump ( vector < int >& nums ) { int n = nums . size (); if ( n == 0 ) return true ; int cur_max = nums [ 0 ]; /* maximum distance can reach from ith */ /* L.I.: current step (ith step) must <= cur_max jump */ for ( int i = 0 ; i < n ; i ++ ) { if ( i > cur_max ) /* must goes first */ return false ; if ( i + nums [ i ] > cur_max ) cur_max = i + nums [ i ]; } return true ; } };","title":"Jump Game"},{"location":"courses/9chap-dynamic-prog/notes/#lecture-2-dynamic-programming-on-coordinates","text":"Problem Category Unique Paths II \u5750\u6807\u578b Paint House \u5e8f\u5217\u578b \uff0b \u72b6\u6001 Decode Ways \u5212\u5206\u578b Longest Increasing Continuous Subsequence \u5e8f\u5217\u578b / \u5212\u5206\u578b Minimum Path Sum \u5750\u6807\u578b Bomb Enemy \u5750\u6807\u578b Counting Bits \u5750\u6807\u578b","title":"Lecture 2 Dynamic Programming on Coordinates"},{"location":"courses/9chap-dynamic-prog/notes/#_1","text":"\u6700\u7b80\u5355\u7684\u52a8\u6001\u89c4\u5212\u7c7b\u578b \u7ed9\u5b9a\u4e00\u4e2a\u5e8f\u5217\u6216\u7f51\u683c \u9700\u8981\u627e\u5230\u5e8f\u5217\u4e2d\u67d0\u4e2a/\u4e9b\u5b50\u5e8f\u5217\u6216\u7f51\u683c\u4e2d\u7684\u67d0\u6761\u8def\u5f84 \u67d0\u79cd\u6027\u8d28\u6700\u5927/\u6700\u5c0f \u8ba1\u6570 \u5b58\u5728\u6027 \u52a8\u6001\u89c4\u5212\u65b9\u7a0b f[i] \u4e2d\u7684\u4e0b\u6807 i \u8868\u793a\u4ee5 a[i] \u4e3a\u7ed3\u5c3e\u7684\u6ee1\u8db3\u6761\u4ef6\u7684\u5b50\u5e8f\u5217\u7684\u6027\u8d28. f[i][j] \u4e2d\u7684 \u4e0b\u6807 i , j \u8868\u793a\u4ee5\u683c\u5b50 (i, j) \u4e3a\u7ed3\u5c3e\u7684\u6ee1\u8db3\u6761\u4ef6\u7684\u8def\u5f84\u7684\u6027\u8d28 \u6700\u5927\u503c/\u6700\u5c0f\u503c \u4e2a\u6570 \u662f\u5426\u5b58\u5728 \u5750\u6807\u578b\u52a8\u6001\u89c4\u5212\u7684\u521d\u59cb\u6761\u4ef6 f[0] \u5c31\u662f\u6307\u4ee5 a[0] \u4e3a\u7ed3\u5c3e\u7684\u5b50\u5e8f\u5217\u7684\u6027\u8d28","title":"\u5750\u6807\u578b\u52a8\u6001\u89c4\u5212"},{"location":"courses/9chap-dynamic-prog/notes/#unique-paths-ii","text":"C++ Naive DP class Solution { public : int uniquePathsWithObstacles ( vector < vector < int >>& obstacleGrid ) { int m = obstacleGrid . size (); int n = obstacleGrid [ 0 ]. size (); int i = 0 ; int j = 0 ; int flag = 0 ; vector < vector < int >> f ( m , vector < int > ( n )); if ( m == 0 || n == 0 ) { return 0 ; } if ( obstacleGrid [ 0 ][ 0 ] == 1 ) { return 0 ; } f [ 0 ][ 0 ] = 1 ; for ( i = 1 ; i < m ; i ++ ) { if ( obstacleGrid [ i ][ 0 ] == 1 ) { f [ i ][ 0 ] = 0 ; } else { f [ i ][ 0 ] = f [ i - 1 ][ 0 ]; } } for ( j = 1 ; j < n ; j ++ ) { if ( obstacleGrid [ 0 ][ j ] == 1 ) { f [ 0 ][ j ] = 0 ; } else { f [ 0 ][ j ] = f [ 0 ][ j - 1 ]; } } for ( i = 1 ; i < m ; i ++ ) { for ( j = 1 ; j < n ; j ++ ) { if ( obstacleGrid [ i ][ j ] == 1 ) { f [ i ][ j ] = 0 ; } else { f [ i ][ j ] = f [ i ][ j - 1 ] + f [ i - 1 ][ j ]; } } } return f [ m - 1 ][ n - 1 ]; } }; C++ Naive DP Refactored class Solution { public : int uniquePathsWithObstacles ( vector < vector < int >>& obstacleGrid ) { int m = obstacleGrid . size (); int n = m > 0 ? obstacleGrid [ 0 ]. size () : 0 ; if ( m == 0 && n == 0 ) { return 0 ; } vector < vector < int >> f ( m , vector < int > ( n , 0 )); f [ 0 ][ 0 ] = 1 ; if ( obstacleGrid [ 0 ][ 0 ] == 1 ) { return 0 ; } for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { if ( obstacleGrid [ i ][ j ] == 1 ) { f [ i ][ j ] = 0 ; } else { // whenever available if ( i > 0 ) { f [ i ][ j ] += f [ i - 1 ][ j ]; } if ( j > 0 ) { f [ i ][ j ] += f [ i ][ j - 1 ]; } } } } return f [ m - 1 ][ n - 1 ]; } }; Java O(n) space // Optimized using a rolling array or single row dp array instead of m x n. class Solution { public int uniquePathsWithObstacles ( int [][] a ) { int m = a . length ; int n = a [ 0 ] . length ; int dp [] = new int [ n ] ; dp [ 0 ] = 1 ; //T[i][j] = T[i-1][j] + T[i][j-1] for ( int i = 0 ; i < m ; i ++ ){ for ( int j = 0 ; j < n ; j ++ ){ if ( a [ i ][ j ] == 1 ){ dp [ j ] = 0 ; } else if ( j > 0 ){ dp [ j ] += dp [ j - 1 ] ; } } } return dp [ n - 1 ] ; } }","title":"Unique Paths II"},{"location":"courses/9chap-dynamic-prog/notes/#paint-house","text":"DP last step: considering the optimal solution. The last paint must be one of the three colors and the paint cost is minimum. State: we can try to name f[i] the minimum cost of painting the first i houses, i = 0, 1, ... i-1 . However, we cannot know what color could be used for the last house. Change the state to: f[i][k] is the minium cost of painting the first i houses and the i-1 th element is painted as color k . This way, we can choose the last paint based on this piece of information. The state transition fomular: f[i][k] = f[i - 1][\\ell]_{\\ell != k} + costs[i - 1][k] f[i][k] = f[i - 1][\\ell]_{\\ell != k} + costs[i - 1][k] The result is minimum of the: f[i][0],\\ f[i][1],\\ f[i][2] f[i][0],\\ f[i][1],\\ f[i][2] class Solution { public : int minCost ( vector < vector < int >>& costs ) { int m = costs . size (); int f [ m + 1 ][ 3 ]; /* state: f[i][0] paint i house and the last (i - 1) house is red * /* initial value, paint 0 house cost 0 */ for ( int j = 0 ; j < 3 ; j ++ ) f [ 0 ][ j ] = 0 ; for ( int i = 1 ; i <= m ; i ++ ) { for ( int j = 0 ; j < 3 ; j ++ ) { // current color f [ i ][ j ] = INT_MAX ; for ( int k = 0 ; k < 3 ; k ++ ) { // painted color /* cannot paint same color for neighbor house */ if ( j == k ) continue ; // ith (index with i - 1) house is painted with color j if ( f [ i ][ j ] > f [ i - 1 ][ k ] + costs [ i - 1 ][ j ]) f [ i ][ j ] = f [ i - 1 ][ k ] + costs [ i - 1 ][ j ]; //f[i][j] = min(f[i - 1][k] + costs[i - 1][j], f[i][j]); } } } return min ( min ( f [ m ][ 0 ], f [ m ][ 1 ]), f [ m ][ 2 ]); } };","title":"Paint House"},{"location":"courses/9chap-dynamic-prog/notes/#decode-ways","text":"4 Ingredients Last step: A_0, A_1, A_2, ..., A_n-3, [A_n-2, A_n-1] A_0, A_1, A_2, ..., A_n-3, A_n-2, [A_n-1] State: f[n] : decode ways of first n letters. Smaller problem: f[n] = f[n - 1] + f[n - 2] | A_n-2,A_n-1 decodable. Init: f[0] = 1 Note Again the idea in this solution is to update the f[i][j] on demand, a technique that broadly used in 2-d coordinate based DP problems such as Bomb Enemy , Unique Paths II , and Minimum Path Sum . C++ DP class Solution { public : int numDecodings ( string s ) { int n = s . length (); int f [ n + 1 ] = { 0 }; if ( n == 0 ) return 0 ; f [ 0 ] = 1 ; for ( int i = 1 ; i <= n ; i ++ ) { int t = s [ i - 1 ] - '0' ; if ( t > 0 && t <= 9 ) { f [ i ] += f [ i - 1 ]; } if ( i > 1 ){ int q = ( s [ i - 2 ] - '0' ) * 10 + t ; if ( q >= 10 && q <= 26 ) { f [ i ] += f [ i - 2 ]; } } } return f [ n ]; } }; C++ DP O(1) class Solution { public : int numDecodings ( string s ) { if ( ! s . size () || s . front () == '0' ) return 0 ; // r2: decode ways of s[0, i-2] , r1: decode ways of s[0, i-1] int r1 = 1 , r2 = 1 ; // think it as a coordinate bases, not sequence based dp for ( int i = 1 ; i < s . size (); i ++ ) { // last char in s[0, i] is 0, cannot decode if ( s [ i ] == '0' ) r1 = 0 ; // two-digit letter, add r2 to r1, r2 get the previous r1 if ( s [ i - 1 ] == '1' || s [ i - 1 ] == '2' && s [ i ] <= '6' ) { r1 = r2 + r1 ; r2 = r1 - r2 ; } else { // one-digit letter, r2 get the previous r1 r2 = r1 ; } } return r1 ; } };","title":"Decode Ways"},{"location":"courses/9chap-dynamic-prog/notes/#longest-increasing-continuous-subsequence","text":"4 ingredients: Last step, last element a[n-1] could be in the result or not in the result. subproblem, suppose we have the LICS of the first n - 1 elements. represented as f[n-1] . base case and boundary condition, when no char in the string: f[0] = 1 . empty string have LICS length of 1. order of calculation, calculate small index first. Not a leetcode The problem is not a leetcode problem, the original problem ask for sequence that could be increase or decrease. Using index in DP problems Avoid using both index i - 1 and i + 1 in a loop invariance, otherwise you'll have problem in keeping the loop invariance. Compare the followings. Incorrect for ( int i = n - 1 ; i >= 0 ; i -- ) { f [ i ] = 1 ; if ( i < n - 1 && i > 0 && A [ i - 1 ] > A [ i ]) { f [ i ] = f [ i + 1 ] + 1 ; } res2 = max ( res2 , f [ i ]); } Correct for ( int i = n - 1 ; i >= 0 ; i -- ) { f [ i ] = 1 ; if ( i < n - 1 && A [ i ] > A [ i + 1 ]) { f [ i ] = f [ i + 1 ] + 1 ; } res2 = max ( res2 , f [ i ]); } Correct for ( int i = 0 ; i < n ; i ++ ) { f [ i ] = 1 ; if ( i > 0 && A [ i - 1 ] > A [ i ]) { f [ i ] = f [ i - 1 ] + 1 ; } res2 = max ( res2 , f [ i ]); } C++ DP solution class Solution { public : int longestIncreasingContinuousSubsequence ( vector < int >& A ) { // Write your code here int n = A . size (); int f [ n ]; int res1 = 0 ; int res2 = 0 ; for ( int i = 0 ; i < n ; i ++ ) { f [ i ] = 1 ; if ( i > 0 && A [ i - 1 ] < A [ i ]) { f [ i ] = f [ i - 1 ] + 1 ; } res1 = max ( res1 , f [ i ]); } for ( int i = n - 1 ; i >= 0 ; i -- ) { f [ i ] = 1 ; if ( i < n - 1 && A [ i ] > A [ i + 1 ]) { f [ i ] = f [ i + 1 ] + 1 ; } res2 = max ( res2 , f [ i ]); } return max ( res1 , res2 ); } }; Here we don't have to explicitly wirte f[i] = max(1, f[i - 1] + 1) , since we have the condition A[i - 1] < A[i] , we know the A[i] will be added to the f[i] , hence simply update f[i] = f[i - 1] + 1 directly.","title":"Longest Increasing Continuous Subsequence"},{"location":"courses/9chap-dynamic-prog/notes/#minimum-path-sum","text":"This is coordinate based DP. We need to have f[i][j] to keep the minimum path sum to the grid[i][j] . Calculate f[i][j] from top to down and from left to right for each row. Space can be optimized. Notice we can do the init and first row and first column in nested loops, do not need use multiple for loops. C++ DP class Solution { public : int minPathSum ( vector < vector < int > > & grid ) { int m = grid . size (); int n = grid [ 0 ]. size (); if ( m == 0 || n == 0 ) return 0 ; int f [ m ][ n ]; for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { if ( i == 0 && j == 0 ) { f [ i ][ j ] = grid [ i ][ j ]; } if ( i == 0 && j > 0 ) { f [ i ][ j ] = f [ i ][ j - 1 ] + grid [ i ][ j ]; } if ( j == 0 && i > 0 ) { f [ i ][ j ] = f [ i - 1 ][ j ] + grid [ i ][ j ]; } if ( i > 0 && j > 0 ) { f [ i ][ j ] = min ( f [ i - 1 ][ j ], f [ i ][ j - 1 ]) + grid [ i ][ j ]; } } } return f [ m - 1 ][ n - 1 ]; } }; C++ DP Space O(n) class Solution { public : int minPathSum ( vector < vector < int > > & grid ) { int m = grid . size (); int n = grid [ 0 ]. size (); if ( m == 0 || n == 0 ) return 0 ; int f [ 2 ][ n ]; /* only two rows of status */ int prev = 1 ; int curr = 1 ; for ( int i = 0 ; i < m ; i ++ ) { // rolling the f array when move to a new row prev = curr ; curr = 1 - curr ; for ( int j = 0 ; j < n ; j ++ ) { f [ curr ][ j ] = grid [ i ][ j ]; if ( i == 0 && j > 0 ) { f [ i ][ j ] += f [ i ][ j - 1 ]; } if ( j == 0 && i > 0 ) { f [ curr ][ j ] += f [ prev ][ j ]; } if ( i > 0 && j > 0 ) { f [ curr ][ j ] += min ( f [ prev ][ j ], f [ curr ][ j - 1 ]); } } } return f [ curr ][ n - 1 ]; } }; Note \u5728\u5904\u7406\u4e8c\u7ef4\u5750\u6807\u578bDP\u95ee\u9898\u7684\u65f6\u5019\uff0c\u5bf9\u4e8e\u7b2c\u4e00\u884c\u6216\u8005\u7b2c\u4e00\u5217\u7684\u8ba1\u7b97\u6211\u4eec\u53ef\u4ee5\u628a\u5b83\u653e\u5230loop\u4e2d\u3002\u4f46\u662f\u8981\u52a0\u4e00\u4e2a\u6761\u4ef6\u3002 \u8fd9\u4e2a\u6761\u4ef6\u5c31\u662f if (i > 0) , \u8fd9\u4e2a\u6761\u4ef6\u4fdd\u8bc1\u4e86\u5728\u66f4\u65b0\u6570\u7ec4\u503c\u7684\u65f6\u5019\u4e0b\u6807\u4e0d\u4f1a\u8d8a\u754c\u3002\u5e76\u4e14\u5f88\u597d\u7684\u8be0\u91ca\u4e86\u6211\u4eec\u7684\u52a8\u673a\uff08\u5c31\u662f\u5bf9\u4e8e\u7b2c\u4e00\u884c\u7b2c\u4e00\u5217\u4ec5\u4ec5\u662f\u521d\u59cb\u5316)\u3002 \u5bf9\u4e8e1\u4e2d\u63d0\u5230\u7684trick\uff0c\u53e6\u5916\u4e00\u4e2a\u597d\u5904\u5c31\u662f\u5bf9\u4e8e\u7b2c\u4e00\u884c\u6216\u8005\u7b2c\u4e00\u5217\u6761\u4ef6\u5224\u65ad\u7684\u65f6\u5019\uff0c\u8fd9\u91cc\u8fd8\u6709\u4e00\u4e2a\u6280\u5de7\u5c31\u662f\u5148\u628a\u81ea\u8eab\u7684\u2018\u6027\u8d28\u2019\u6216\u8005\u2018\u503c\u2019\u52a0\u4e0a\uff0c \u5982\u679c\u6709\u4e0a\u4e00\u884c\u6211\u4eec\u518d\u53bb\u52a0\u4e0a\u4e00\u884c\u7684\u2018\u6027\u8d28\u2019\u6216\u2018\u503c\u2019\u3002\u8fd9\u91cc\u9700\u8981\u9006\u5411\u601d\u7ef4. \u5728\u591a\u79cd\u60c5\u51b5\u9700\u8981\u5206\u5f00\u8ba8\u8bba\u7684\u65f6\u5019\u6211\u4eec\u53ef\u4ee5\u5148\u2018\u4e00\u7968\u5426\u51b3\u2019\u6700\u660e\u663e\u60c5\u51b5\uff0c\u7136\u540e\u8ba8\u8bba\u5269\u4e0b\u7684\u60c5\u51b5\u3002","title":"Minimum Path Sum"},{"location":"courses/9chap-dynamic-prog/notes/#bomb-enemy","text":"Breakdown the problem into smaller (simpler) problems. Take the special steps (i.e. different properties of the cell) as normal ones in the loop, deal with the special step when doing the calculation. class Solution { public : int maxKilledEnemies ( vector < vector < char >>& grid ) { int m = grid . size (); if ( m == 0 ) return 0 ; int n = grid [ 0 ]. size (); if ( n == 0 ) return 0 ; int f [ m ][ n ]; int res [ m ][ n ]; int ret = 0 ; for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { res [ i ][ j ] = 0 ; } } /* up */ for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { f [ i ][ j ] = 0 ; if ( grid [ i ][ j ] == 'W' ) { f [ i ][ j ] = 0 ; } else { if ( grid [ i ][ j ] == 'E' ) { f [ i ][ j ] = 1 ; } if ( i > 0 ) { f [ i ][ j ] += f [ i - 1 ][ j ]; } } res [ i ][ j ] += f [ i ][ j ]; } } /* down */ for ( int i = m - 1 ; i >= 0 ; i -- ) { for ( int j = 0 ; j < n ; j ++ ) { f [ i ][ j ] = 0 ; if ( grid [ i ][ j ] == 'W' ) { f [ i ][ j ] = 0 ; } else { if ( grid [ i ][ j ] == 'E' ) { f [ i ][ j ] = 1 ; } if ( i < m - 1 ) { f [ i ][ j ] += f [ i + 1 ][ j ]; } } res [ i ][ j ] += f [ i ][ j ]; } } /* left */ for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { f [ i ][ j ] = 0 ; if ( grid [ i ][ j ] == 'W' ) { f [ i ][ j ] = 0 ; } else { if ( grid [ i ][ j ] == 'E' ) { f [ i ][ j ] = 1 ; } if ( j > 0 ) { f [ i ][ j ] += f [ i ][ j - 1 ]; } } res [ i ][ j ] += f [ i ][ j ]; } } /* right */ for ( int i = 0 ; i < m ; i ++ ) { for ( int j = n - 1 ; j >= 0 ; j -- ) { f [ i ][ j ] = 0 ; if ( grid [ i ][ j ] == 'W' ) { f [ i ][ j ] = 0 ; } else { if ( grid [ i ][ j ] == 'E' ) { f [ i ][ j ] = 1 ; } if ( j < n - 1 ) { f [ i ][ j ] += f [ i ][ j + 1 ]; } } res [ i ][ j ] += f [ i ][ j ]; } } /* calculate resutls */ for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { if ( grid [ i ][ j ] == '0' ) if ( res [ i ][ j ] > ret ) ret = res [ i ][ j ]; } } return ret ; } };","title":"Bomb Enemy"},{"location":"courses/9chap-dynamic-prog/notes/#counting-bits","text":"Notice you can use the trick that i >> 1 == i / 2 to construct the subproblem i count 1 bits 1 1 2 1 3 2 6 2 12 2 25 3 50 3 class Solution { public : vector < int > countBits ( int num ) { vector < int > f ( num + 1 , 0 ); if ( num == 0 ) { return f ; } for ( int i = 1 ; i <= num ; i ++ ) { f [ i ] = f [ i >> 1 ] + ( i % 2 ); } return f ; } };","title":"Counting Bits"},{"location":"courses/9chap-dynamic-prog/notes/#lecture-3-dynamic-programming-on-sequences","text":"Problem Category Paint House II \u5e8f\u5217\u578b\uff0b\u72b6\u6001 House Robber \u5e8f\u5217\u578b\uff0b\u72b6\u6001 House Robber II \u5e8f\u5217\u578b\uff0b\u72b6\u6001 Best Time to Buy and Sell Stock Best Time to Buy and Sell Stock II Best Time to Buy and Sell Stock III \u5e8f\u5217\u578b\uff0b\u72b6\u6001 Best Time to Buy and Sell Stock IV \u5e8f\u5217\u578b\uff0b\u72b6\u6001 Longest Increasing Subsequence \u5e8f\u5217\u578b Russian Doll Envelopes","title":"Lecture 3 Dynamic Programming on Sequences"},{"location":"courses/9chap-dynamic-prog/notes/#_2","text":"\u7ed9\u5b9a\u4e00\u4e2a\u5e8f\u5217 \u52a8\u6001\u89c4\u5212\u65b9\u7a0b f[i] \u4e2d\u7684\u4e0b\u6807 i \u8868\u793a\u524d i \u4e2a\u5143\u7d20 a[0], a[1], ..., a[i-1] \u7684\u67d0\u79cd\u6027\u8d28 \u5750\u6807\u578b\u7684 f[i] \u8868\u793a\u4ee5 a[i] \u4e3a\u7ed3\u5c3e\u7684\u67d0\u79cd\u6027\u8d28 \u521d\u59cb\u5316\u4e2d\uff0c f[0] \u8868\u793a\u7a7a\u5e8f\u5217\u7684\u6027\u8d28 \u5750\u6807\u578b\u52a8\u6001\u89c4\u5212\u7684\u521d\u59cb\u6761\u4ef6 f[0] \u5c31\u662f\u6307\u4ee5 a[0] \u4e3a\u7ed3\u5c3e\u7684\u5b50\u5e8f\u5217\u7684\u6027\u8d28","title":"\u5e8f\u5217\u578b\u52a8\u6001\u89c4\u5212"},{"location":"courses/9chap-dynamic-prog/notes/#paint-house-ii","text":"the solution could be the same as the first verison, but it is O(n\\cdot k^2) O(n\\cdot k^2) . how to make it O(n\\cdot k) O(n\\cdot k) ? By analyzing the state transition equation, we observed that we want to find a minimum value of a set of numbers except one for each house. Put it in english, the min cost to paint i - 1 th house with color k , we want to have the min cost of painting all previous i - 1 houses and the i-1 th house cannot be painted as color k . f [ i ][ 1 ] = min { f [ i-1 ][ 2 ] + cost [ i-1 ][ 1 ] , f [ i-1 ][ 3 ] + cost [ i-1 ][ 1 ] , ..., f [ i-1 ][ K ] + cost [ i-1 ][ 1 ] } f [ i ][ 2 ] = min { f [ i-1 ][ 1 ] + cost [ i-1 ][ 2 ] , f [ i-1 ][ 3 ] + cost [ i-1 ][ 2 ] , ..., f [ i-1 ][ K ] + cost [ i-1 ][ 2 ] } ... f [ i ][ K ] = min { f [ i-1 ][ 1 ] + cost [ i-1 ][ K ] , f [ i-1 ][ 2 ] + cost [ i-1 ][ K ] , ..., f [ i-1 ][ K-1 ] + cost [ i-1 ][ K ] } We could optimize the solution upon this. Basically, we can maintain the first two minimum value of the set f[i-1][1], f[i-1][2], f[i-1][3], ..., f[i-1][K] , min1 and min2 and their index j1 and j2 . There are two cases, the first case is the house i-1 is painted with the same color correspoinding to the minimum value. in this case, we cannot chose to paint it with the color corresponding to the minimum cost, we can update the state using the second minimum. The second case is that we paint the i-1th house with color other than the color corresponding to the minimum cost to pain, we can update using the minimum value. C++ O(nk) class Solution { public : /** * @param costs n x k cost matrix * @return an integer, the minimum cost to paint all houses */ int minCostII ( vector < vector < int >>& costs ) { int m = costs . size (); if ( m == 0 ) return 0 ; int k = costs [ 0 ]. size (); if ( k == 0 ) return 0 ; int f [ m + 1 ][ k ]; int min1 ; int min2 ; int j1 = 0 , j2 = 0 ; /* init, 0 house cost nothing. */ for ( int j = 0 ; j < k ; j ++ ) f [ 0 ][ j ] = 0 ; for ( int i = 1 ; i <= m ; i ++ ) { min1 = min2 = INT_MAX ; /* from all the colors, find the min1 and min2 */ for ( int j = 0 ; j < k ; j ++ ) { /* get the min1 and min2 first */ if ( f [ i - 1 ][ j ] < min1 ) { min2 = min1 ; j2 = j1 ; min1 = f [ i - 1 ][ j ]; j1 = j ; } else if ( f [ i - 1 ][ j ] < min2 ) { min2 = f [ i - 1 ][ j ]; j2 = j ; } } /* update the states based on the min1 and min2 */ for ( int j = 0 ; j < k ; j ++ ) { if ( j != j1 ) { f [ i ][ j ] = f [ i - 1 ][ j1 ] + costs [ i - 1 ][ j ]; } else { f [ i ][ j ] = f [ i - 1 ][ j2 ] + costs [ i - 1 ][ j ]; } } } int res = INT_MAX ; for ( int j = 0 ; j < k ; j ++ ) { if ( f [ m ][ j ] < res ) res = f [ m ][ j ]; } return res ; } }; C++ O(nk^2) class Solution { public : int minCostII ( vector < vector < int >>& costs ) { int n = costs . size (); int k = n > 0 ? costs [ 0 ]. size () : 0 ; if ( n == 0 && k == 0 ) return 0 ; if ( n == 1 && k == 1 ) return costs [ 0 ][ 0 ]; vector < vector < int >> f ( n + 1 , vector < int > ( k , 0 )); for ( int i = 0 ; i < k ; i ++ ) { f [ 0 ][ i ] = 0 ; // 0 cost for 0 paint } for ( int i = 1 ; i <= n ; i ++ ) { for ( int j = 0 ; j < k ; j ++ ) { f [ i ][ j ] = INT_MAX ; for ( int c = 0 ; c < k ; c ++ ) { if ( j == c ) { continue ; } if ( f [ i ][ j ] > f [ i - 1 ][ c ] + costs [ i - 1 ][ j ]) { f [ i ][ j ] = f [ i - 1 ][ c ] + costs [ i - 1 ][ j ]; } } } } int res = INT_MAX ; for ( int i = 0 ; i < k ; i ++ ) { if ( f [ n ][ i ] < res ) res = f [ n ][ i ]; } return res ; } };","title":"Paint House II"},{"location":"courses/9chap-dynamic-prog/notes/#house-robber","text":"Start with the last step. The last house could be either robbed or not. If the last house a[i-1] is robbed, then we cannot rob a[i-2] . we have f[i] = f[i-2] + a[i-1] . If the last house a[i-1] is not robbed, then we can rob a[i-2] . Alternatively, we can skip it to rob a[i-3] . We have f[i] = f[n-1] , in which this f[n-1] could not let us know whether a[i-2] is robbed or not. We add the state of a[i-1] to the state transition equation. Thus we have: f[i][0] represent \"the maximum money for robbing the first i houses and the last house hasn't been robbed.\" f[i][1] represent \"the maximum money for robbing the first i houses and the last house has been robbed.\" state transition equations: f[i][0] = max(f[i-1][0], f[i-1][1]) f[i][1] = f[i-1][0] + a[i-1] C++ class Solution { public : int rob ( vector < int >& nums ) { int n = nums . size (); if ( n == 0 ) return 0 ; int f [ n + 1 ][ 2 ] = { 0 }; for ( int i = 1 ; i <= n ; i ++ ) { f [ i ][ 0 ] = max ( f [ i - 1 ][ 0 ], f [ i - 1 ][ 1 ]); f [ i ][ 1 ] = f [ i - 1 ][ 0 ] + nums [ i - 1 ]; } return max ( f [ n ][ 0 ], f [ n ][ 1 ]); } }; C++ optimized class Solution { public : long long houseRobber ( vector < int > A ) { int n = A . size (); if ( n == 0 ) return 0 ; long long f [ n + 1 ]; /* init */ f [ 0 ] = 0 ; f [ 1 ] = A [ 0 ]; for ( int i = 2 ; i <= n ; i ++ ) { f [ i ] = max ( f [ i - 1 ], f [ i - 2 ] + A [ i - 1 ]); } return f [ n ]; } }; Solution 2 yes - first i days, robber last day, no - first i days, not robber at last day. C++ class Solution { public : int rob ( vector < int >& nums ) { int n = nums . size (); if ( n == 0 ) return 0 ; int yes = 0 , no = 0 ; for ( int i = 1 ; i <= n ; i ++ ) { int tmp = no ; no = max ( yes , no ); yes = tmp + nums [ i - 1 ]; } return max ( yes , no ); } };","title":"House Robber"},{"location":"courses/9chap-dynamic-prog/notes/#house-robber-ii","text":"Following House Robber , when we try to analyze the last step, the house i - 1 depends on the house i - 2 and the house 0 . How could we handle this? We could enumerate two cases, and reduce the probelm to House Rober house 0 is robbed and house i-1 is not. house i - 1 is robbed and house 0 is not. class Solution { public : int rob ( vector < int >& nums ) { int n = nums . size (); if ( n == 0 ) return 0 ; if ( n == 1 ) // edge case to deal with return nums [ 0 ]; vector < int > nums1 ( nums . begin () + 1 , nums . end ()); vector < int > nums2 ( nums . begin (), nums . end () - 1 ); return max ( rob_helper ( nums1 ), rob_helper ( nums2 )); } private : int rob_helper ( vector < int >& A ) { int n = A . size (); if ( n == 0 ) return 0 ; int f [ n + 1 ] = { 0 }; f [ 0 ] = 0 ; f [ 1 ] = A [ 0 ]; for ( int i = 2 ; i <= n ; i ++ ) { f [ i ] = max ( f [ i - 1 ], f [ i - 2 ] + A [ i - 1 ]); } return f [ n ]; } };","title":"House Robber II"},{"location":"courses/9chap-dynamic-prog/notes/#best-time-to-buy-and-sell-stock","text":"Force youself to do it by only one pass. You'll find that you need two variables to record the minimum value currently found and the maximum profit currently found. class Solution { public : int maxProfit ( vector < int > & prices ) { int n = prices . size (); if ( n == 0 ) return 0 ; int min_price = INT_MAX ; int max_profit = 0 ; for ( int i = 0 ; i < n ; i ++ ) { if ( prices [ i ] < min_price ) { min_price = prices [ i ]; } else if ( prices [ i ] - min_price > max_profit ) { max_profit = prices [ i ] - min_price ; } } return max_profit ; } };","title":"Best Time to Buy and Sell Stock"},{"location":"courses/9chap-dynamic-prog/notes/#best-time-to-buy-and-sell-stock-ii","text":"We can just buy and sell when ever the price is increased in a day. Once you can proof the day-by-day subproblem can form the solution. The solution becomes so easy. C++ class Solution { public : int maxProfit ( vector < int > & prices ) { int res = 0 ; for ( int i = 1 ; i < prices . size (); i ++ ) { if ( prices [ i - 1 ] < prices [ i ]) res += prices [ i ] - prices [ i - 1 ]; } return res ; } }; C++ Alternative class Solution { public : int maxProfit ( vector < int > & prices ) { int res = 0 ; for ( int i = 1 ; i < prices . size (); i ++ ) { res += max ( prices [ i ] - prices [ i - 1 ], 0 ) } return res ; } };","title":"Best Time to Buy and Sell Stock II"},{"location":"courses/9chap-dynamic-prog/notes/#best-time-to-buy-and-sell-stock-iii","text":"We can define 5 stages and write state transition equations based on it. C++ class Solution { public : /** * @param prices: Given an integer array * @return: Maximum profit */ int maxProfit ( vector < int > & A ) { int n = A . size (); if ( n == 0 ) { return 0 ; } int f [ n + 1 ][ 6 ]; /* init */ f [ 0 ][ 1 ] = 0 ; f [ 0 ][ 2 ] = f [ 0 ][ 3 ] = f [ 0 ][ 4 ] = f [ 0 ][ 5 ] = INT_MIN ; for ( int i = 1 ; i <= n ; i ++ ) { // stage 1, 3, 5: // f[i][j] = max{f[i - 1][j], f[i - 1][j - 1] + A[i - 1] - A[i - 2]} for ( int j = 1 ; j <= 5 ; j += 2 ) { f [ i ][ j ] = f [ i - 1 ][ j ]; if ( i > 1 && j > 1 && f [ i - 1 ][ j - 1 ] != INT_MIN ) { f [ i ][ j ] = max ( f [ i ][ j ], f [ i - 1 ][ j - 1 ] + A [ i - 1 ] - A [ i - 2 ]); } } // stage 2, 4: // f[i][j] = max{f[i - 1][j] + A[i - 1] - A[i - 2], f[i - 1][j - 1], f[i - 1][j - 2] + A[i - 1] - A[i - 2]} for ( int j = 2 ; j < 5 ; j += 2 ) { f [ i ][ j ] = f [ i - 1 ][ j - 1 ]; if ( i > 1 && f [ i - 1 ][ j ] != INT_MIN ) { f [ i ][ j ] = max ( f [ i ][ j ], f [ i - 1 ][ j ] + A [ i - 1 ] - A [ i - 2 ]); } if ( i > 1 && j > 2 && f [ i - 1 ][ j - 2 ] != INT_MIN ) { f [ i ][ j ] = max ( f [ i ][ j ], f [ i - 1 ][ j - 2 ] + A [ i - 1 ] - A [ i - 2 ]); } } } return max ( f [ n ][ 1 ], max ( f [ n ][ 3 ], f [ n ][ 5 ])); } };","title":"Best Time to Buy and Sell Stock III"},{"location":"courses/9chap-dynamic-prog/notes/#best-time-to-buy-and-sell-stock-iv","text":"If k is larger then n/2 , it is equivalent to the II. This is a generalized solution from Best Time to Buy and Sell Stock III . class Solution { public : int maxProfit ( int K , vector < int > & A ) { int n = A . size (); if ( n == 0 ) { return 0 ; } int f [ n + 1 ][ 2 * K + 1 + 1 ]; /* special case need to take care of */ if ( K > ( n / 2 )) { int res = 0 ; for ( int i = 0 ; i + 1 < n ; i ++ ) { if ( A [ i + 1 ] - A [ i ] > 0 ) { res += A [ i + 1 ] - A [ i ]; } } return res ; } /* init */ f [ 0 ][ 1 ] = 0 ; for ( int k = 2 ; k <= 2 * K + 1 ; k ++ ) { f [ 0 ][ k ] = INT_MIN ; } for ( int i = 1 ; i <= n ; i ++ ) { for ( int j = 1 ; j <= 2 * K + 1 ; j += 2 ) { f [ i ][ j ] = f [ i - 1 ][ j ]; if ( i > 1 && j > 1 && f [ i - 1 ][ j - 1 ] != INT_MIN ) { f [ i ][ j ] = max ( f [ i ][ j ], f [ i - 1 ][ j - 1 ] + A [ i - 1 ] - A [ i - 2 ]); } } for ( int j = 2 ; j < 2 * K + 1 ; j += 2 ) { f [ i ][ j ] = f [ i - 1 ][ j - 1 ]; if ( i > 1 && f [ i - 1 ][ j ] != INT_MIN ) { f [ i ][ j ] = max ( f [ i ][ j ], f [ i - 1 ][ j ] + A [ i - 1 ] - A [ i - 2 ]); } if ( i > 1 && j > 2 && f [ i - 1 ][ j - 2 ] != INT_MIN ) { f [ i ][ j ] = max ( f [ i ][ j ], f [ i - 1 ][ j - 2 ] + A [ i - 1 ] - A [ i - 2 ]); } } } int res = INT_MIN ; for ( int j = 1 ; j <= 2 * K + 1 ; j += 2 ) { if ( f [ n ][ j ] > res ) res = f [ n ][ j ]; } return res ; } };","title":"Best Time to Buy and Sell Stock IV"},{"location":"courses/9chap-dynamic-prog/notes/#longest-increasing-subsequence","text":"Comparing to the problem Longest Increasing Continuous Subsequence . If the subsequence is not continous. we have to enumerate each of the previous element befor A[j] . Solution 1 DP O(n^2) DP class Solution { public : int lengthOfLIS ( vector < int >& nums ) { int n = nums . size (); if ( n == 0 ) return 0 ; int res = 0 ; int f [ n ] = { 0 }; for ( int j = 0 ; j < n ; j ++ ) { /* case 1: a[j] is the subsequence */ f [ j ] = 1 ; /* case 2: LIS from a[0],...a[i] plus a[j] */ for ( int i = 0 ; i < j ; i ++ ) { if ( nums [ i ] < nums [ j ] && f [ i ] + 1 > f [ j ]) f [ j ] = f [ i ] + 1 ; } res = max ( res , f [ j ]); } return res ; } }; Solution 2 DP with binary search O(nlogn) To reduce the complexity, we can try to find if there is any redundant work we have been done. or some how we could use some order information to avoid some of the calculation. Focus on the real meaning of longest Increasing Subsequence. In fact, you are looking for the smallest value before A[i] that leads to the longest Increasing Subsequence so far. use the state f[i] to record the LIS of the array A[0], ... A[i -1] . If we are at f[j], j > i , we are looking for the largest f[i] value that have the smallest A[i] . DP with binary search class Solution { public : int longestIncreasingSubsequence ( vector < int > nums ) { int n = nums . size (); if ( n == 0 ) return 0 ; //int f[n]; // b[i]: when f value is i, smallest a value (ending value) int b [ n + 1 ]; int top = 0 ; b [ 0 ] = INT_MIN ; // O(n) for ( int i = 0 ; i < n ; i ++ ) { // b[0] ~ b[top] // last value b[j] which is smaller than A[i] int start = 0 , end = top ; int mid ; int j ; // O(lgn) while ( start <= end ) { mid = start + ( end - start ) / 2 ; if ( b [ mid ] < nums [ i ]) { j = mid ; start = mid + 1 ; } else { end = mid - 1 ; } } // b[i]: length is j (f value is j), smallest ending value. // A[i] is after it. // f[i] = j + 1 // b[j + 1]: length is (j + 1), smallest ending value. // A[i] // B[j + 1] >= A[i] b [ j + 1 ] = nums [ i ]; if ( j + 1 > top ) { top = j + 1 ; } } // b[top] stores the smallest ending value for an LIS return top ; } }; Solution 3 DP with binary search refactored In observing the fact that we can use extra space to keep the \"minimum elements see so far from nums that is the last element of LIS for the different length of such LISes\". Different from the regular DP solution, our extra space b is storing element from nums, and the element stored in b are not necessary in order. The index i of elements in b related to the length of a LIS whose last element is a[i] . specifically, i + 1 = length(LIS) . DP with binary search refactored class Solution { public : int lengthOfLIS ( vector < int >& nums ) { vector < int > b ; for ( int i = 0 ; i < nums . size (); ++ i ) { int begin = 0 , end = b . size (); while ( begin != end ) { int mid = begin + ( end - begin ) / 2 ; if ( b [ mid ] < nums [ i ]) { begin = mid + 1 ; } else { end = mid ; } } if ( begin == b . size ()) // nums[i] greater than all element in b b . push_back ( nums [ i ]); else // begin point to next element no less than the target nums[i]. b [ begin ] = nums [ i ]; } return b . size (); } }; Solution 4 C++ using lower_bound we can use the lower_bound to replace the binary search routine in the above solution. C++ using lower_bound class Solution { public : int lengthOfLIS ( vector < int >& nums ) { vector < int > b ; for ( auto a : nums ) { auto it = lower_bound ( b . begin (), b . end (), a ); if ( it == b . end ()) b . push_back ( a ); else * it = a ; } return b . size (); } };","title":"Longest Increasing Subsequence"},{"location":"courses/9chap-dynamic-prog/notes/#russian-doll-envelopes","text":"","title":"Russian Doll Envelopes"},{"location":"courses/cs224n/lec-notes/","text":"CS224N: Natural Language Processing with Deep Learning \u00b6 Lecture 1 Introduction to NLP and Deep Learning \u00b6 Representations of NLP levels: Semantics Traditional V.S. DL (rules v.s. sophisticated algorithm) Applications: Sentiment Analysis Question Answering system Dialogue agents / response generation Lecture 2 Word Vector Representations: word2vec \u00b6 \"one-hot\" representation, localist representation distributional similarity based representations \"You shall know a word by the company it keeps\u201d (J. R. Firth 1957:11)\" dense vector for each word type, chosen so that it is good at predicting other words appearing in its context (gets a bit recursive) Learning neural network word embeddings model p(\\text{context} | w_t) = ? p(\\text{context} | w_t) = ? loss function: J = 1 - p(w_{-t}|w_{t}) J = 1 - p(w_{-t}|w_{t}) , w_{-t} w_{-t} , context words that doesn't include word w_t w_t . word2vec Skip-grams (SG) - predict context words given target center words Continuous Bag of Words (CBOW) - predict target center word from bag-of-words context words 2 training methods hierarchical softmax negative sampling: tain binary logistic regression for a true pair versus a couple of noice pairs. Core ideas of SG prediction maximize the prediction of the model p(\\text{context} | w_t) = ? p(\\text{context} | w_t) = ? for all context words in the form of the cost function J(\\theta) J(\\theta) . cost function: $$ J'(\\theta) = \\prod_{t=1}^T\\prod_{\\substack{-m \\le j \\le m\\ j \\ne 0}} p(w_{t+j}|w_t; \\theta) $$ Negative log likelihood $$ J(\\theta) = -\\frac{1}{T} \\sum_{t=1}^T\\sum_{\\substack{-m \\le j \\le m\\ j \\ne 0}} \\log p(w_{t+j}|w_t) $$ softmax $$ p(o|c) = \\frac{\\exp(u_o^T v_c)}{\\sum_{w=1}^{v}\\exp(u_w^T v_c)} $$ What's really mean when you say train word2vec model optimize the parameter \\theta \\theta , which is a R^{2\\cdot d \\cdot V} R^{2\\cdot d \\cdot V} , d d is the word vector dimention, V V is the vacabular size, each word is represented by 2 vectors! Compute all vector gradients!!! Gradient calculation (lecture slides) Lecture 3 Advanced Word Vector Representations \u00b6 Compare count based and direct prediction count based: LSA, HAL (Lund & Burgess), COALS (Rohde et al), Hellinger-PCA (Lebret & Collobert) Fast training Efficient usage of statistics Primarily used to capture word similarity Disproportionate importance given to large counts direct prediction: NNLM, HLBL, RNN, Skip-gram/CBOW, (Bengio et al; Collobert & Weston; Huang et al; Mnih & Hinton; Mikolov et al;Mnih & Kavukcuoglu) Scales with corpus size Inefficient usage of statistics Can capture complex patterns beyond word similarity Generate improved performance on other tasks Combining the best of both worlds: GloVe Fast training Scalable to huge corpora Good performance even with small corpus, and small vectors How to evaluate word2vec? Intrinsic: Evaluation on a specific/intermediate subtask Fast to compute Helps to understand that system Not clear if really helpful unless correlation to real task is established Extrinsic: Evaluation on a real task Can take a long time to compute accuracy Unclear if the subsystem is the problem or its interaction or other subsystems If replacing exactly one subsystem with another improves accuracy --> Winning! Assignment 1 (Spring 2019) \u00b6 Singular Value Decomposition (SVD) is a kind of generalized PCA (Principal Components Analysis). Review materials \u00b6 Gradient Descent (SGD) Singular Value Decomposition (SVD) cross entropy loss max-margin loss Lecture 4 Word Window Classification and Neural Networks \u00b6 Window classification: Train softmax classifier by assigning a label to a center word and concatenating all word vectors surrounding it. max-margin loss; J(\\theta) = \\max(0, 1 - s + s_{corrupted}) J(\\theta) = \\max(0, 1 - s + s_{corrupted}) . s s is the good part, s_{corrupted} s_{corrupted} is the bad part, we would like the bad part is smaller than s - 1 s - 1 . backpropagation: insight: reuse the derivative computed previously Hadamard product ( \\circ, \\odot, \\otimes \\circ, \\odot, \\otimes ) Lecture 5 Backpropagation (Feb 24, 2019) \u00b6 Details of backpropagation \u00b6 The backprop algorithm is essentially compute the gradient (partial derivative) of the cost function with respect all the parameters, U, W, b, x U, W, b, x With the following setup: max-margin cost function: J = \\max(0, 1 - s + s_c) J = \\max(0, 1 - s + s_c) Scores: s = U^T f(Wx + b), s_c = U^T f(Wx_c + b) s = U^T f(Wx + b), s_c = U^T f(Wx_c + b) input: z = Wx + b z = Wx + b , hidden: a = f(z) a = f(z) , output: s = U^T a s = U^T a Derivatives: \\frac{\\partial s}{\\partial U} = \\frac{\\partial}{\\partial U} U^T a = a \\frac{\\partial s}{\\partial U} = \\frac{\\partial}{\\partial U} U^T a = a wrt one weight W_{ij} W_{ij} : \\frac{\\partial s}{\\partial W_{ij}} = \\delta_i x_j \\frac{\\partial s}{\\partial W_{ij}} = \\delta_i x_j , \\delta_i = U_i f'(z_i) x_j \\delta_i = U_i f'(z_i) x_j , where f'(z) = f(z)(1 - f(z)) f'(z) = f(z)(1 - f(z)) , f(x) f(x) is logistic function or sigmoid function. wrt all weights W W : \\frac{\\partial s}{\\partial W} = \\delta x^T \\frac{\\partial s}{\\partial W} = \\delta x^T wrt word vectors x x : \\frac{\\partial s}{\\partial x} = W^T\\delta \\frac{\\partial s}{\\partial x} = W^T\\delta Iterpretations of backpropagation using simple function. \u00b6 Lecture 6 Dependency Parsing (Feb 27, 2019) \u00b6 Lecture 8 \u00b6 Lecture 9 Machine Translation and Advanced Recurrent LSTMs and GRUs \u00b6 Reference \u00b6 Natural Language Processing with Python","title":"CS224N Lecture Notes"},{"location":"courses/cs224n/lec-notes/#cs224n-natural-language-processing-with-deep-learning","text":"","title":"CS224N: Natural Language Processing with Deep Learning"},{"location":"courses/cs224n/lec-notes/#lecture-1-introduction-to-nlp-and-deep-learning","text":"Representations of NLP levels: Semantics Traditional V.S. DL (rules v.s. sophisticated algorithm) Applications: Sentiment Analysis Question Answering system Dialogue agents / response generation","title":"Lecture 1 Introduction to NLP and Deep Learning"},{"location":"courses/cs224n/lec-notes/#lecture-2-word-vector-representations-word2vec","text":"\"one-hot\" representation, localist representation distributional similarity based representations \"You shall know a word by the company it keeps\u201d (J. R. Firth 1957:11)\" dense vector for each word type, chosen so that it is good at predicting other words appearing in its context (gets a bit recursive) Learning neural network word embeddings model p(\\text{context} | w_t) = ? p(\\text{context} | w_t) = ? loss function: J = 1 - p(w_{-t}|w_{t}) J = 1 - p(w_{-t}|w_{t}) , w_{-t} w_{-t} , context words that doesn't include word w_t w_t . word2vec Skip-grams (SG) - predict context words given target center words Continuous Bag of Words (CBOW) - predict target center word from bag-of-words context words 2 training methods hierarchical softmax negative sampling: tain binary logistic regression for a true pair versus a couple of noice pairs. Core ideas of SG prediction maximize the prediction of the model p(\\text{context} | w_t) = ? p(\\text{context} | w_t) = ? for all context words in the form of the cost function J(\\theta) J(\\theta) . cost function: $$ J'(\\theta) = \\prod_{t=1}^T\\prod_{\\substack{-m \\le j \\le m\\ j \\ne 0}} p(w_{t+j}|w_t; \\theta) $$ Negative log likelihood $$ J(\\theta) = -\\frac{1}{T} \\sum_{t=1}^T\\sum_{\\substack{-m \\le j \\le m\\ j \\ne 0}} \\log p(w_{t+j}|w_t) $$ softmax $$ p(o|c) = \\frac{\\exp(u_o^T v_c)}{\\sum_{w=1}^{v}\\exp(u_w^T v_c)} $$ What's really mean when you say train word2vec model optimize the parameter \\theta \\theta , which is a R^{2\\cdot d \\cdot V} R^{2\\cdot d \\cdot V} , d d is the word vector dimention, V V is the vacabular size, each word is represented by 2 vectors! Compute all vector gradients!!! Gradient calculation (lecture slides)","title":"Lecture 2 Word Vector Representations: word2vec"},{"location":"courses/cs224n/lec-notes/#lecture-3-advanced-word-vector-representations","text":"Compare count based and direct prediction count based: LSA, HAL (Lund & Burgess), COALS (Rohde et al), Hellinger-PCA (Lebret & Collobert) Fast training Efficient usage of statistics Primarily used to capture word similarity Disproportionate importance given to large counts direct prediction: NNLM, HLBL, RNN, Skip-gram/CBOW, (Bengio et al; Collobert & Weston; Huang et al; Mnih & Hinton; Mikolov et al;Mnih & Kavukcuoglu) Scales with corpus size Inefficient usage of statistics Can capture complex patterns beyond word similarity Generate improved performance on other tasks Combining the best of both worlds: GloVe Fast training Scalable to huge corpora Good performance even with small corpus, and small vectors How to evaluate word2vec? Intrinsic: Evaluation on a specific/intermediate subtask Fast to compute Helps to understand that system Not clear if really helpful unless correlation to real task is established Extrinsic: Evaluation on a real task Can take a long time to compute accuracy Unclear if the subsystem is the problem or its interaction or other subsystems If replacing exactly one subsystem with another improves accuracy --> Winning!","title":"Lecture 3 Advanced Word Vector Representations"},{"location":"courses/cs224n/lec-notes/#assignment-1-spring-2019","text":"Singular Value Decomposition (SVD) is a kind of generalized PCA (Principal Components Analysis).","title":"Assignment 1 (Spring 2019)"},{"location":"courses/cs224n/lec-notes/#review-materials","text":"Gradient Descent (SGD) Singular Value Decomposition (SVD) cross entropy loss max-margin loss","title":"Review materials"},{"location":"courses/cs224n/lec-notes/#lecture-4-word-window-classification-and-neural-networks","text":"Window classification: Train softmax classifier by assigning a label to a center word and concatenating all word vectors surrounding it. max-margin loss; J(\\theta) = \\max(0, 1 - s + s_{corrupted}) J(\\theta) = \\max(0, 1 - s + s_{corrupted}) . s s is the good part, s_{corrupted} s_{corrupted} is the bad part, we would like the bad part is smaller than s - 1 s - 1 . backpropagation: insight: reuse the derivative computed previously Hadamard product ( \\circ, \\odot, \\otimes \\circ, \\odot, \\otimes )","title":"Lecture 4 Word Window Classification and Neural Networks"},{"location":"courses/cs224n/lec-notes/#lecture-5-backpropagation-feb-24-2019","text":"","title":"Lecture 5 Backpropagation (Feb 24, 2019)"},{"location":"courses/cs224n/lec-notes/#details-of-backpropagation","text":"The backprop algorithm is essentially compute the gradient (partial derivative) of the cost function with respect all the parameters, U, W, b, x U, W, b, x With the following setup: max-margin cost function: J = \\max(0, 1 - s + s_c) J = \\max(0, 1 - s + s_c) Scores: s = U^T f(Wx + b), s_c = U^T f(Wx_c + b) s = U^T f(Wx + b), s_c = U^T f(Wx_c + b) input: z = Wx + b z = Wx + b , hidden: a = f(z) a = f(z) , output: s = U^T a s = U^T a Derivatives: \\frac{\\partial s}{\\partial U} = \\frac{\\partial}{\\partial U} U^T a = a \\frac{\\partial s}{\\partial U} = \\frac{\\partial}{\\partial U} U^T a = a wrt one weight W_{ij} W_{ij} : \\frac{\\partial s}{\\partial W_{ij}} = \\delta_i x_j \\frac{\\partial s}{\\partial W_{ij}} = \\delta_i x_j , \\delta_i = U_i f'(z_i) x_j \\delta_i = U_i f'(z_i) x_j , where f'(z) = f(z)(1 - f(z)) f'(z) = f(z)(1 - f(z)) , f(x) f(x) is logistic function or sigmoid function. wrt all weights W W : \\frac{\\partial s}{\\partial W} = \\delta x^T \\frac{\\partial s}{\\partial W} = \\delta x^T wrt word vectors x x : \\frac{\\partial s}{\\partial x} = W^T\\delta \\frac{\\partial s}{\\partial x} = W^T\\delta","title":"Details of backpropagation"},{"location":"courses/cs224n/lec-notes/#iterpretations-of-backpropagation-using-simple-function","text":"","title":"Iterpretations of backpropagation using simple function."},{"location":"courses/cs224n/lec-notes/#lecture-6-dependency-parsing-feb-27-2019","text":"","title":"Lecture 6 Dependency Parsing (Feb 27, 2019)"},{"location":"courses/cs224n/lec-notes/#lecture-8","text":"","title":"Lecture 8"},{"location":"courses/cs224n/lec-notes/#lecture-9-machine-translation-and-advanced-recurrent-lstms-and-grus","text":"","title":"Lecture 9 Machine Translation and Advanced Recurrent LSTMs and GRUs"},{"location":"courses/cs224n/lec-notes/#reference","text":"Natural Language Processing with Python","title":"Reference"},{"location":"courses/cs224n/write-up/","text":"CS224N NLP with Deep Learning \u00b6 Part 1 Word Embeddings Based on Distributional Semantic \u00b6 Date: April 20th 2019 If you have no experience in the Natrual Language Processing (NLP) field and asked to name a few practical NLP applications, what would you list? Google Translation or Amazon Echo voice assistant may first come into your mind. How did those products understand a sentence or human conversation? Engineers built computational models that understand our language as our brain does. To build such a computational model, the following components are necessary. A large corpus of text (input training data) A method to represent each word from the corpus (feature representation) A starting model that barely understands English at the beginning but can be improved by \"reading\" more words from the corpus (parametric function). An algorithm for the model to correct itself if it makes a mistake in understanding (learning algorithm/optimization method) A measurement that can qualify the mistake the model made (loss function) Introduction \u00b6 Starting with this post, I will write materials from my understanding of the Stanford CS224N. The plan is to journal down all the learning notes I learned about the above 5 components. The goal is to provide a systematic understand of the gist of the components for real applications. Part 1 is about language models and word embeddings. Part 2 discusses neural networks and backpropagation algorithm. Part 3 revisits the language model and introduces recurrent neural networks. Part 4 studies advanced RNN, Long short-term memory (LSTM), and gated recurrent networks (GRN). Word representations \u00b6 How do we represent word with meaning on a computer? Before 2013, wordNet and one-hot vector are most popular in word meaning representations. WordNet is a manually compiled thesaurus containing lists of synonym sets and hypernyms. Like most of the manual stuff, it is subjective, unscalable, inaccurate in computing word similarity, and impossible to maintain and keep up-to-data. One-hot vectors represent word meaning using discrete symbolic 1 1 s in a long stream of 0 0 of vector elements. It suffers from sparsity issues and many other drawbacks. We will not spend time on that outdated method. Instead, we will focus on the embedding method using the idea of word vector. The core idea of this embedding method is based on the remarkable insight on word meaning called distributional semantics . It conjectures that a word\u2019s meaning is given by the words that frequently appear close-by. It is proposed by J. R. Firth. Here is the famous quote: Quote \"You shall know a word by the company it keeps\" -- J. R. Firth 1957: 11 In this method, we will build a dense vector for each word, chosen so that it is similar to vectors of words that appear in similar contexts. word vector representation also called distributed representation , or word embeddings . A work vector might look likes this, \\mathrm{banking} = \\begin{pmatrix} 0.286\\\\ 0.792\\\\ \u22120.177\\\\ \u22120.107\\\\ 0.109\\\\ \u22120.542\\\\ 0.349\\\\ 0.271 \\end{pmatrix} \\mathrm{banking} = \\begin{pmatrix} 0.286\\\\ 0.792\\\\ \u22120.177\\\\ \u22120.107\\\\ 0.109\\\\ \u22120.542\\\\ 0.349\\\\ 0.271 \\end{pmatrix} word2vec \u00b6 Word2vec (Mikolov et al. 2013) is a framework for learning word vectors. Word2vec go through windows of words and calculate the probability of the context word given the center word (or vice versa) using the similarity of the word vectors. It keeps adjusting the word vectors to maximize this probability. Vividly, these two pictures show the general idea of how word2vec works. Formally, for a single prediction, the probability is P(o|c) P(o|c) , interpreted as the probability of the outer word o o given the center word c c . For the large corpus including T T words and each position t = 1, 2, \\cdots, T t = 1, 2, \\cdots, T , we predict context words within a window of fixed size m m , given center word w_t w_t . The model likelihood can be written as the following \\begin{equation*} \\mathrm{likelihood} = L(\\theta) = \\prod_{t=1}^T\\prod_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} p(w_{t+j}|w_t; \\theta) \\end{equation*} \\begin{equation*} \\mathrm{likelihood} = L(\\theta) = \\prod_{t=1}^T\\prod_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} p(w_{t+j}|w_t; \\theta) \\end{equation*} The objective function J(\\theta) J(\\theta) is the average negative log likelihood: \\begin{equation*} J(\\theta) = -\\frac{1}{T}\\log{L(\\theta)} = -\\frac{1}{T} \\sum_{t=1}^T\\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} \\log p(w_{t+j}|w_t; \\theta) \\end{equation*} \\begin{equation*} J(\\theta) = -\\frac{1}{T}\\log{L(\\theta)} = -\\frac{1}{T} \\sum_{t=1}^T\\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} \\log p(w_{t+j}|w_t; \\theta) \\end{equation*} We've defined the cost function by now. In order to minimize the loss, we need to know how p(w_{t+j}|w_t; \\theta) p(w_{t+j}|w_t; \\theta) can be calculated. One function we can use to calculate the probability value is the \\mathrm{softmax} \\mathrm{softmax} function. \\begin{equation*} \\mathrm{softmax}(x)_i = \\frac{\\exp(x_i)}{\\sum_{j=1}^{n}\\exp(x_j)} \\end{equation*} \\begin{equation*} \\mathrm{softmax}(x)_i = \\frac{\\exp(x_i)}{\\sum_{j=1}^{n}\\exp(x_j)} \\end{equation*} Particularly we will write the probability as the following format. \\begin{equation*} p(o|c) = \\frac{\\exp(u_o^T v_c)}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\end{equation*} \\begin{equation*} p(o|c) = \\frac{\\exp(u_o^T v_c)}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\end{equation*} There are several points need to be emphasized. Two vectors will be obtained for each individual word, one as center word v_w v_w , and the other context word u_w u_w . d d is the dimension of the word vector. V V is the vocabulary size. \\begin{equation*} \\theta = \\begin{bmatrix} v_{\\mathrm{aardvark}}\\\\ v_{\\mathrm{a}}\\\\ \\cdots \\\\ v_{\\mathrm{zebra}}\\\\ u_{\\mathrm{aardvark}}\\\\ u_{\\mathrm{a}}\\\\ \\cdots \\\\ u_{\\mathrm{zebra}}\\\\ \\end{bmatrix} \\in \\mathbb{R}^{2dV} \\end{equation*} \\begin{equation*} \\theta = \\begin{bmatrix} v_{\\mathrm{aardvark}}\\\\ v_{\\mathrm{a}}\\\\ \\cdots \\\\ v_{\\mathrm{zebra}}\\\\ u_{\\mathrm{aardvark}}\\\\ u_{\\mathrm{a}}\\\\ \\cdots \\\\ u_{\\mathrm{zebra}}\\\\ \\end{bmatrix} \\in \\mathbb{R}^{2dV} \\end{equation*} The dot product in the exponet compares similarity of o o and c c . Larger dot product indicates larger probability. The denorminator sum over entire vocabulary to give normalized probability distribution. The \\mathrm{softmax} \\mathrm{softmax} function maps aribitrary values x_i x_i to a probability distribution p_i p_i . \u201c \\mathrm{max} \\mathrm{max} \u201d because it amplifies the probability of the largest x_i x_i , \u201c \\mathrm{soft} \\mathrm{soft} \u201d because it still assigns some probabilities to smaller x_i x_i . It is very commonly used in deep learning. Gradient descent to optimize log likelihood loss function \u00b6 This section I will purely focus on how to derive the gradient of the log likelihood loss function with respect to center word using the chain rule. Once we have the computed gradients, we are ready to implement it in matrix form and train the word vectors. This model is called skip-gram. loss function in p(w_{t+j}|w_t) p(w_{t+j}|w_t) \\begin{equation*} J(\\theta) = -\\frac{1}{T} \\sum_{t=1}^T\\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} \\log p(w_{t+j}|w_t; \\theta) \\end{equation*} \\begin{equation*} J(\\theta) = -\\frac{1}{T} \\sum_{t=1}^T\\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} \\log p(w_{t+j}|w_t; \\theta) \\end{equation*} p(w_{t+j}|w_t) p(w_{t+j}|w_t) in softmax form \\begin{equation*} p(o|c) = \\frac{\\exp(u_o^T v_c)}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\end{equation*} \\begin{equation*} p(o|c) = \\frac{\\exp(u_o^T v_c)}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\end{equation*} We would like to find the following derivatives: \\begin{equation*} 1. \\frac{\\partial}{\\partial v_c} \\log p(o|c) \\\\ 2. \\frac{\\partial}{\\partial u_o} \\log p(o|c) \\end{equation*} \\begin{equation*} 1. \\frac{\\partial}{\\partial v_c} \\log p(o|c) \\\\ 2. \\frac{\\partial}{\\partial u_o} \\log p(o|c) \\end{equation*} Let's now start working with the first one, the derivative wrt. v_c v_c . \\begin{align*} \\frac{\\partial}{\\partial v_c} \\log p(o|c) & = \\frac{\\partial}{\\partial v_c} \\log{\\frac{\\exp(u_o^T v_c)}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)}} \\\\ &= \\frac{\\partial}{\\partial v_c} \\log{\\exp(u_o^T v_c)} - \\frac{\\partial}{\\partial v_c} \\log{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\;\\;\\;\\cdots (\\log\\frac{a}{b} = \\log a - \\log b) \\\\ & = \\frac{\\partial}{\\partial v_c} (u_o^T v_c) - \\frac{1}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\sum_{x=1}^{V} \\frac{\\partial}{\\partial v_c} \\exp(u_x^T v_c) \\;\\;\\;\\cdots \\mathrm{chain\\ rule}\\\\ & = u_o - \\frac{1}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\sum_{x=1}^{V} \\exp(u_x^{T} v_c) \\frac{\\partial}{\\partial v_c} (u_x^T v_c) \\;\\;\\;\\cdots \\mathrm{chain\\ rule}\\\\ & = u_o - \\frac{1}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\sum_{x=1}^{V} \\exp(u_x^{T} v_c) \\cdot u_x \\\\ & = u_o - \\sum_{x=1}^{V} \\frac{\\exp(u_x^{T} v_c)}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\cdot u_x \\\\ & = \\underbrace{u_o}_{\\mathrm{current}} - \\underbrace{\\sum_{x=1}^{V} p(x|c) \\cdot u_x}_{\\mathrm{expectation}} \\end{align*} \\begin{align*} \\frac{\\partial}{\\partial v_c} \\log p(o|c) & = \\frac{\\partial}{\\partial v_c} \\log{\\frac{\\exp(u_o^T v_c)}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)}} \\\\ &= \\frac{\\partial}{\\partial v_c} \\log{\\exp(u_o^T v_c)} - \\frac{\\partial}{\\partial v_c} \\log{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\;\\;\\;\\cdots (\\log\\frac{a}{b} = \\log a - \\log b) \\\\ & = \\frac{\\partial}{\\partial v_c} (u_o^T v_c) - \\frac{1}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\sum_{x=1}^{V} \\frac{\\partial}{\\partial v_c} \\exp(u_x^T v_c) \\;\\;\\;\\cdots \\mathrm{chain\\ rule}\\\\ & = u_o - \\frac{1}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\sum_{x=1}^{V} \\exp(u_x^{T} v_c) \\frac{\\partial}{\\partial v_c} (u_x^T v_c) \\;\\;\\;\\cdots \\mathrm{chain\\ rule}\\\\ & = u_o - \\frac{1}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\sum_{x=1}^{V} \\exp(u_x^{T} v_c) \\cdot u_x \\\\ & = u_o - \\sum_{x=1}^{V} \\frac{\\exp(u_x^{T} v_c)}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\cdot u_x \\\\ & = \\underbrace{u_o}_{\\mathrm{current}} - \\underbrace{\\sum_{x=1}^{V} p(x|c) \\cdot u_x}_{\\mathrm{expectation}} \\end{align*} The result is remarkable. It have great intuition in it. The gradient represent the slop in the multidimentional space that we should walk along to reach the optima. The result gradient we got tell us that the slop equals to the difference of current context vector \\boldsymbol{u}_o \\boldsymbol{u}_o and the expected context vector (the weighted average over all context vectors). It has nothing todo with the center word c c . To compute the gradient of J(\\theta) J(\\theta) with respect to the center word c c , you have to sum up all the gradients obtained from word windows when c c is the center word. The gradient with respect to the context word will be very similar, the chain rule is also handy in that case. Once all the gradient with respect to center words and context words are calculated. We can use gradient descent to update the model parameters, which in this case is all the word vectors. Because we have two vectors for each word, when update the parameters we will use the average of the two vectors to update. Gradient descent to optimize cross entropy loss function \u00b6 Alternatively, we could also use cross entropy loss function. CS224N 2017 assignment 1 requires to derive the gradient of cross entropy loss function. This section, we will go step by step to derive the gradient when using cross entropy loss function and \\mathrm{softmax} \\mathrm{softmax} activation function in the out put layer. the cross entropy function is defined as follows \\begin{equation*} \\mathrm{CE}(\\boldsymbol{y}, \\boldsymbol{\\hat y}) = - \\sum_i y_i \\log (\\hat y_i) \\end{equation*} \\begin{equation*} \\mathrm{CE}(\\boldsymbol{y}, \\boldsymbol{\\hat y}) = - \\sum_i y_i \\log (\\hat y_i) \\end{equation*} Notice the \\boldsymbol{y} \\boldsymbol{y} is the one-hot label vector, and \\boldsymbol{\\hat y} \\boldsymbol{\\hat y} is the predicted probability for all classes. The index i i is the index of the one-hot element in individual label vectors. Therefore the definition of cross entropy loss function is defined as followinng if we use \\mathrm{softmax} \\mathrm{softmax} to predict \\hat y_i \\hat y_i , \\begin{equation*} J_{\\mathrm{CE}}(\\theta) = - \\sum_i^N y_i \\log (\\hat y_i) = -\\sum_i^N y_i \\log (\\mathrm{softmax}(\\theta)_i) \\end{equation*} \\begin{equation*} J_{\\mathrm{CE}}(\\theta) = - \\sum_i^N y_i \\log (\\hat y_i) = -\\sum_i^N y_i \\log (\\mathrm{softmax}(\\theta)_i) \\end{equation*} The gradient of \\frac{\\partial}{\\partial \\theta}J_{\\mathrm{CE}}(\\theta) \\frac{\\partial}{\\partial \\theta}J_{\\mathrm{CE}}(\\theta) can be derived using chain rule. Because we will use the gradient of the \\mathrm{softmax} \\mathrm{softmax} for the derivation, Let's derive \\mathrm{softmax} \\mathrm{softmax} gradient first. \\begin{equation*} \\frac{\\partial \\hat y_i}{\\theta_j} = \\frac{\\partial }{\\theta_j}\\mathrm{softmax}(\\theta)_i = \\frac{\\partial }{\\theta_j} \\frac{e^{\\theta_i}}{\\sum_{k=1}^{n}e^{\\theta_k}} \\end{equation*} \\begin{equation*} \\frac{\\partial \\hat y_i}{\\theta_j} = \\frac{\\partial }{\\theta_j}\\mathrm{softmax}(\\theta)_i = \\frac{\\partial }{\\theta_j} \\frac{e^{\\theta_i}}{\\sum_{k=1}^{n}e^{\\theta_k}} \\end{equation*} Here we should use two tricks to derive this gradient. Quotient rule separate into 2 cases: when i = j i = j and i \\ne j i \\ne j . If i = j i = j , we have \\begin{align*} \\frac{\\partial }{\\theta_j} \\frac{e^{\\theta_i}}{\\sum_{k=1}^{n}e^{\\theta_k}} & = \\frac{e^{\\theta_i}\\sum_{k=1}^{n}e^{\\theta_k} - e^{\\theta_i}e^{\\theta_j} }{\\Big(\\sum_{k=1}^{n}e^{\\theta_k}\\Big)^2} \\\\ & = \\frac{e^{\\theta_i}\\Big(\\sum_{k=1}^{n}e^{\\theta_k} - e^{\\theta_j}\\Big)}{\\Big(\\sum_{k=1}^{n}e^{\\theta_k}\\Big)^2} \\\\ & = \\frac{e^{\\theta_i}}{\\sum_{k=1}^{n}e^{\\theta_k}} \\frac{\\sum_{k=1}^{n}e^{\\theta_k} - e^{\\theta_j}}{\\sum_{k=1}^{n}e^{\\theta_k}} \\\\ & = \\hat y_i(1 - \\hat y_j) \\end{align*} \\begin{align*} \\frac{\\partial }{\\theta_j} \\frac{e^{\\theta_i}}{\\sum_{k=1}^{n}e^{\\theta_k}} & = \\frac{e^{\\theta_i}\\sum_{k=1}^{n}e^{\\theta_k} - e^{\\theta_i}e^{\\theta_j} }{\\Big(\\sum_{k=1}^{n}e^{\\theta_k}\\Big)^2} \\\\ & = \\frac{e^{\\theta_i}\\Big(\\sum_{k=1}^{n}e^{\\theta_k} - e^{\\theta_j}\\Big)}{\\Big(\\sum_{k=1}^{n}e^{\\theta_k}\\Big)^2} \\\\ & = \\frac{e^{\\theta_i}}{\\sum_{k=1}^{n}e^{\\theta_k}} \\frac{\\sum_{k=1}^{n}e^{\\theta_k} - e^{\\theta_j}}{\\sum_{k=1}^{n}e^{\\theta_k}} \\\\ & = \\hat y_i(1 - \\hat y_j) \\end{align*} if i \\ne j i \\ne j , we have \\begin{align*} \\frac{\\partial }{\\theta_j} \\frac{e^{\\theta_i}}{\\sum_{k=1}^{n}e^{\\theta_k}} & = \\frac{0 - e^{\\theta_i}e^{\\theta_j} }{\\Big(\\sum_{k=1}^{n}e^{\\theta_k}\\Big)^2} \\\\ & = \\frac{e^{\\theta_i}}{\\sum_{k=1}^{n}e^{\\theta_k}} \\frac{ - e^{\\theta_j}}{\\sum_{k=1}^{n}e^{\\theta_k}} \\\\ & = -\\hat y_i \\hat y_j \\end{align*} \\begin{align*} \\frac{\\partial }{\\theta_j} \\frac{e^{\\theta_i}}{\\sum_{k=1}^{n}e^{\\theta_k}} & = \\frac{0 - e^{\\theta_i}e^{\\theta_j} }{\\Big(\\sum_{k=1}^{n}e^{\\theta_k}\\Big)^2} \\\\ & = \\frac{e^{\\theta_i}}{\\sum_{k=1}^{n}e^{\\theta_k}} \\frac{ - e^{\\theta_j}}{\\sum_{k=1}^{n}e^{\\theta_k}} \\\\ & = -\\hat y_i \\hat y_j \\end{align*} Now let calculate the gradient of the cross-entropy loss function. Notice the gradient is concerning the i i th parameter. This is because we can use the gradient of the \\mathrm{softmax} \\mathrm{softmax} (the \\frac{\\partial \\hat y_k}{\\partial \\theta_i} \\frac{\\partial \\hat y_k}{\\partial \\theta_i} term) conveniently. \\begin{align*} \\frac{\\partial J_{\\mathrm{CE}}(\\theta)}{\\partial \\theta_i} & = - \\sum_k^N y_k \\frac{\\partial \\log (\\hat y_k)}{\\partial \\theta_i} \\\\ & = - \\sum_k^N y_k \\frac{\\partial \\log (\\hat y_k)}{\\partial \\hat y_k} \\frac{\\partial \\hat y_k}{\\partial \\theta_i} \\\\ & = - \\sum_k^N y_k \\frac{1}{\\hat y_k} \\frac{\\partial \\hat y_k}{\\partial \\theta_i} \\\\ & = - \\Big(\\underbrace{y_k \\frac{1}{\\hat y_k} \\hat y_k(1 - \\hat y_i)}_{k = i} + \\underbrace{\\sum_{k\\ne i}^N y_k \\frac{1}{\\hat y_k} (-\\hat y_k \\hat y_i) \\Big)}_{k \\ne i} \\\\ & = - \\Big(y_i (1 - \\hat y_i) - \\sum_{k\\ne i}^N y_k \\hat y_i \\Big) \\\\ & = - y_i + y_i \\hat y_i + \\sum_{k\\ne i}^N y_k \\hat y_i \\\\ & = \\hat y_i\\Big(y_i + \\sum_{k\\ne i}^N y_k\\Big) - \\hat y_i \\\\ & = \\hat y_i \\cdot 1 - y_i = \\hat y_i - y_i \\end{align*} \\begin{align*} \\frac{\\partial J_{\\mathrm{CE}}(\\theta)}{\\partial \\theta_i} & = - \\sum_k^N y_k \\frac{\\partial \\log (\\hat y_k)}{\\partial \\theta_i} \\\\ & = - \\sum_k^N y_k \\frac{\\partial \\log (\\hat y_k)}{\\partial \\hat y_k} \\frac{\\partial \\hat y_k}{\\partial \\theta_i} \\\\ & = - \\sum_k^N y_k \\frac{1}{\\hat y_k} \\frac{\\partial \\hat y_k}{\\partial \\theta_i} \\\\ & = - \\Big(\\underbrace{y_k \\frac{1}{\\hat y_k} \\hat y_k(1 - \\hat y_i)}_{k = i} + \\underbrace{\\sum_{k\\ne i}^N y_k \\frac{1}{\\hat y_k} (-\\hat y_k \\hat y_i) \\Big)}_{k \\ne i} \\\\ & = - \\Big(y_i (1 - \\hat y_i) - \\sum_{k\\ne i}^N y_k \\hat y_i \\Big) \\\\ & = - y_i + y_i \\hat y_i + \\sum_{k\\ne i}^N y_k \\hat y_i \\\\ & = \\hat y_i\\Big(y_i + \\sum_{k\\ne i}^N y_k\\Big) - \\hat y_i \\\\ & = \\hat y_i \\cdot 1 - y_i = \\hat y_i - y_i \\end{align*} Write in vector form, we will have \\frac{\\partial J_{\\mathrm{CE}}(\\boldsymbol{y}, \\boldsymbol{\\hat y})}{\\partial \\boldsymbol{\\theta}} = \\boldsymbol{\\hat y} - \\boldsymbol{y}. \\frac{\\partial J_{\\mathrm{CE}}(\\boldsymbol{y}, \\boldsymbol{\\hat y})}{\\partial \\boldsymbol{\\theta}} = \\boldsymbol{\\hat y} - \\boldsymbol{y}. With this gradient, we can update the model parameters, namely the word vectors. In the next post, I will discuss neural networks, which have more layers than the word2vec. In neural networks, there are more parameters need to be trained and the gradients with respect all the parameters need to be derived. Skip-gram model \u00b6 Skip-gram model uses the center word to predict the surrounding. We have derived the gradient for the skip-gram model using log-likelihood loss function and \\mathrm{softmax} \\mathrm{softmax} . This section will derive the gradient for cross-entropy loss function. The probability output function will keep using the \\mathrm{softmax} \\mathrm{softmax} . Cross entropy loss function \u00b6 Since we have derived above that \\frac{\\partial J_{\\mathrm{CE}}(\\boldsymbol{y}, \\boldsymbol{\\hat y})}{\\partial \\boldsymbol{\\theta}} = \\boldsymbol{\\hat y} - \\boldsymbol{y} \\frac{\\partial J_{\\mathrm{CE}}(\\boldsymbol{y}, \\boldsymbol{\\hat y})}{\\partial \\boldsymbol{\\theta}} = \\boldsymbol{\\hat y} - \\boldsymbol{y} . In this case, the vector \\boldsymbol{\\theta} = [\\boldsymbol{u_1^{\\mathsf{T}}}\\boldsymbol{v_c}, \\boldsymbol{u_2^{\\mathsf{T}}}\\boldsymbol{v_c}, \\cdots, \\boldsymbol{u_W^{\\mathsf{T}}}\\boldsymbol{v_c}] \\boldsymbol{\\theta} = [\\boldsymbol{u_1^{\\mathsf{T}}}\\boldsymbol{v_c}, \\boldsymbol{u_2^{\\mathsf{T}}}\\boldsymbol{v_c}, \\cdots, \\boldsymbol{u_W^{\\mathsf{T}}}\\boldsymbol{v_c}] . Gradient for center word \u00b6 Borrow the above steps to derive the gradient with respect to \\boldsymbol{\\theta} \\boldsymbol{\\theta} , we have \\begin{align*} \\frac{\\partial }{\\partial \\boldsymbol{v_c}}J_{\\mathrm{softmax-CE}}(\\boldsymbol{o}, \\boldsymbol{v_c}, \\boldsymbol{U}) & = \\frac{\\partial J_{\\mathrm{CE}}(\\theta)}{\\partial \\theta_i} \\frac{\\partial \\theta_i}{\\boldsymbol{v_c}} \\\\ & = (\\hat y_i - y_i) \\boldsymbol{u_i^{\\mathsf{T}}} \\end{align*} \\begin{align*} \\frac{\\partial }{\\partial \\boldsymbol{v_c}}J_{\\mathrm{softmax-CE}}(\\boldsymbol{o}, \\boldsymbol{v_c}, \\boldsymbol{U}) & = \\frac{\\partial J_{\\mathrm{CE}}(\\theta)}{\\partial \\theta_i} \\frac{\\partial \\theta_i}{\\boldsymbol{v_c}} \\\\ & = (\\hat y_i - y_i) \\boldsymbol{u_i^{\\mathsf{T}}} \\end{align*} Notice here the \\boldsymbol{u_i} \\boldsymbol{u_i} is a column vector. And the derivative of \\boldsymbol{y} = \\boldsymbol{a}^{\\mathsf{T}}\\boldsymbol{x} \\boldsymbol{y} = \\boldsymbol{a}^{\\mathsf{T}}\\boldsymbol{x} with respect to \\boldsymbol{x} \\boldsymbol{x} is \\boldsymbol{a}^{\\mathsf{T}} \\boldsymbol{a}^{\\mathsf{T}} . Written in vector form, we will get \\begin{equation*} \\frac{\\partial J}{\\partial \\boldsymbol{v_c}} = \\boldsymbol{U}(\\boldsymbol{\\hat y} - \\boldsymbol{y}) \\end{equation*} \\begin{equation*} \\frac{\\partial J}{\\partial \\boldsymbol{v_c}} = \\boldsymbol{U}(\\boldsymbol{\\hat y} - \\boldsymbol{y}) \\end{equation*} In the above gradient, U = [\\boldsymbol{u_1}, \\boldsymbol{u_1}, \\cdots, \\boldsymbol{u_W}] U = [\\boldsymbol{u_1}, \\boldsymbol{u_1}, \\cdots, \\boldsymbol{u_W}] is the matrix of all the output vectors. \\boldsymbol{u_1} \\boldsymbol{u_1} is a column word vector. The component \\boldsymbol{\\hat y} - \\boldsymbol{y} \\boldsymbol{\\hat y} - \\boldsymbol{y} is a also a column vector with length of W W . The above gradient can be viewed as scaling each output vector \\boldsymbol{u_i} \\boldsymbol{u_i} by the scaler \\hat y_i - y. \\hat y_i - y. Alternatively, the gradient can also be wrote as distributive form \\begin{equation*} \\frac{\\partial J}{\\partial \\boldsymbol{v_c}} = \\boldsymbol{U}\\boldsymbol{\\hat y} - \\boldsymbol{U}\\boldsymbol{y} = -\\boldsymbol{u_i} + \\sum_{w=1}^{W}\\hat y_w \\boldsymbol{u_w} \\end{equation*} \\begin{equation*} \\frac{\\partial J}{\\partial \\boldsymbol{v_c}} = \\boldsymbol{U}\\boldsymbol{\\hat y} - \\boldsymbol{U}\\boldsymbol{y} = -\\boldsymbol{u_i} + \\sum_{w=1}^{W}\\hat y_w \\boldsymbol{u_w} \\end{equation*} The index i i in the above equation is corresponding to the index of the none zero element in the one-hot vector \\boldsymbol{y} \\boldsymbol{y} . Here we can see \\boldsymbol{y} \\boldsymbol{y} as the true label of the output word. Gradient for output word \u00b6 We can also calculate the gradient with respect to the output word. \\begin{align*} \\frac{\\partial }{\\partial \\boldsymbol{u_w}}J_{\\mathrm{softmax-CE}}(\\boldsymbol{o}, \\boldsymbol{v_c}, \\boldsymbol{U}) & = \\frac{\\partial J_{\\mathrm{CE}}(\\theta)}{\\partial \\theta_i} \\frac{\\partial \\theta_i}{\\boldsymbol{u_w}} \\\\ & = (\\hat y_i - y_i) \\boldsymbol{v_c} \\end{align*} \\begin{align*} \\frac{\\partial }{\\partial \\boldsymbol{u_w}}J_{\\mathrm{softmax-CE}}(\\boldsymbol{o}, \\boldsymbol{v_c}, \\boldsymbol{U}) & = \\frac{\\partial J_{\\mathrm{CE}}(\\theta)}{\\partial \\theta_i} \\frac{\\partial \\theta_i}{\\boldsymbol{u_w}} \\\\ & = (\\hat y_i - y_i) \\boldsymbol{v_c} \\end{align*} Notice here we apply \\frac{\\partial \\boldsymbol{a}^{\\mathsf{T}} \\boldsymbol{x}}{\\partial \\boldsymbol{a}} = x \\frac{\\partial \\boldsymbol{a}^{\\mathsf{T}} \\boldsymbol{x}}{\\partial \\boldsymbol{a}} = x . Writen the gradient in matrix format, we have \\begin{equation*} \\frac{\\partial J}{\\partial \\boldsymbol{U}} = \\boldsymbol{v_c}(\\boldsymbol{\\hat y} - \\boldsymbol{y})^{\\mathsf{T}} \\end{equation*} \\begin{equation*} \\frac{\\partial J}{\\partial \\boldsymbol{U}} = \\boldsymbol{v_c}(\\boldsymbol{\\hat y} - \\boldsymbol{y})^{\\mathsf{T}} \\end{equation*} Notice the shape of the gradient. It determines how the notation looks like. In the above notation notice the shape of the output word gradient \\frac{\\partial J}{\\partial \\boldsymbol{U}} \\frac{\\partial J}{\\partial \\boldsymbol{U}} is d \\times W d \\times W . As we will discussion in the next post that it is a convention to make the shape of gradient as the shape of the input vectors. in this case the shape of U U and \\frac{\\partial J}{\\partial \\boldsymbol{U}} \\frac{\\partial J}{\\partial \\boldsymbol{U}} are the same. Negative sampling \u00b6 The cost function for a single word prediction using nagative sampling is the following \\begin{align*} J_{\\mathrm{neg-sample}}(\\boldsymbol{o},\\boldsymbol{v_c},\\boldsymbol{U}) & = -\\log(\\sigma(\\boldsymbol{u_o}^{\\mathsf{T}} \\boldsymbol{v_c})) - \\sum_{k=1}^{K} \\log(\\sigma(-\\boldsymbol{u_k}^{\\mathsf{T}} \\boldsymbol{v_c})) \\end{align*} \\begin{align*} J_{\\mathrm{neg-sample}}(\\boldsymbol{o},\\boldsymbol{v_c},\\boldsymbol{U}) & = -\\log(\\sigma(\\boldsymbol{u_o}^{\\mathsf{T}} \\boldsymbol{v_c})) - \\sum_{k=1}^{K} \\log(\\sigma(-\\boldsymbol{u_k}^{\\mathsf{T}} \\boldsymbol{v_c})) \\end{align*} It comes from the original paper by Mikolov et al. \\sigma \\sigma is the sigmoid function \\sigma(x) = \\frac{1}{1+e^{-x}} \\sigma(x) = \\frac{1}{1+e^{-x}} . The ideas is to reduce the optimization computation by only sampling a small part of the vocabulary that have a lower probability of being context words of one another. The first lecture notes from CS224N discussed briefly the origin and intuition of the negative sampling loss function. Here we will focus on deriving the gradient and implementation ideas. With the fact that \\frac{\\mathrm{d}\\sigma(x)}{\\mathrm{d} x} = \\sigma(x)(1-\\sigma(x)) \\frac{\\mathrm{d}\\sigma(x)}{\\mathrm{d} x} = \\sigma(x)(1-\\sigma(x)) and the chain rule, it is not hard to derive the gradients result as following \\begin{align*} \\frac{\\partial J}{\\partial \\boldsymbol{v_c}} & = (\\sigma(\\boldsymbol{u_o}^{\\mathsf{T}}\\boldsymbol{v_c}) - 1)\\boldsymbol{u_o} - \\sum_{k=1}^{K}(\\sigma(-\\boldsymbol{u_k}^{\\mathsf{T}}\\boldsymbol{v_c}) - 1)\\boldsymbol{u_k} \\\\ \\frac{\\partial J}{\\partial \\boldsymbol{u_o}} & = (\\sigma(\\boldsymbol{u_o}^{\\mathsf{T}}\\boldsymbol{v_c}) - 1)\\boldsymbol{v_c} \\\\ \\frac{\\partial J}{\\partial \\boldsymbol{u_k}} & = -\\sigma(-\\boldsymbol{u_k}^{\\mathsf{T}}\\boldsymbol{v_c}) - 1)\\boldsymbol{v_c}, \\mathrm{for\\ all\\ } k = 1, 2, \\cdots, K \\end{align*} \\begin{align*} \\frac{\\partial J}{\\partial \\boldsymbol{v_c}} & = (\\sigma(\\boldsymbol{u_o}^{\\mathsf{T}}\\boldsymbol{v_c}) - 1)\\boldsymbol{u_o} - \\sum_{k=1}^{K}(\\sigma(-\\boldsymbol{u_k}^{\\mathsf{T}}\\boldsymbol{v_c}) - 1)\\boldsymbol{u_k} \\\\ \\frac{\\partial J}{\\partial \\boldsymbol{u_o}} & = (\\sigma(\\boldsymbol{u_o}^{\\mathsf{T}}\\boldsymbol{v_c}) - 1)\\boldsymbol{v_c} \\\\ \\frac{\\partial J}{\\partial \\boldsymbol{u_k}} & = -\\sigma(-\\boldsymbol{u_k}^{\\mathsf{T}}\\boldsymbol{v_c}) - 1)\\boldsymbol{v_c}, \\mathrm{for\\ all\\ } k = 1, 2, \\cdots, K \\end{align*} How to sample the \\boldsymbol{u_k} \\boldsymbol{u_k} in practice? The best known sampling method is based on the Unigram Model raise to the power of 3/4. Unigram Model is the counts of each word in the particular corpus (not the vocabulary). Gradient for all of word vectors \u00b6 Since skip-gram model is using one center word to predict all its context words. Given a word context size of m m , we obtain a set of context words [\\mathrm{word}_{c-m}, \\cdots, \\mathrm{word}_{c-1}, \\mathrm{word}_{c}, \\mathrm{word}_{c+1}, \\cdots, \\mathrm{word}_{c+m}] [\\mathrm{word}_{c-m}, \\cdots, \\mathrm{word}_{c-1}, \\mathrm{word}_{c}, \\mathrm{word}_{c+1}, \\cdots, \\mathrm{word}_{c+m}] . For each window, We need to predict 2m 2m context word given the center word. Denote the \"input\" and \"output\" word vectors for \\mathrm{word}_k \\mathrm{word}_k as \\boldsymbol{v}_k \\boldsymbol{v}_k and \\boldsymbol{u}_k \\boldsymbol{u}_k respectively. The cost for the entire context window with size m m centered around \\mathrm{word}_c \\mathrm{word}_c would be \\begin{equation*} J_{\\mathrm{skip-gram}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m}) = \\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} F(\\boldsymbol{u}_{c+j}, \\boldsymbol{v}_c). \\end{equation*} \\begin{equation*} J_{\\mathrm{skip-gram}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m}) = \\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} F(\\boldsymbol{u}_{c+j}, \\boldsymbol{v}_c). \\end{equation*} F F is a placeholder notation to represent the cost function given the center word for different model. Therefore for skip-gram, the gradients for the cost of one context window are \\begin{align*} \\frac{J_{\\mathrm{skip-gram}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{U}} & = \\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} \\frac{\\partial F(\\boldsymbol{u}_{c+j}, \\boldsymbol{v}_c)}{\\partial \\boldsymbol{v}_c},\\\\ \\frac{J_{\\mathrm{skip-gram}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{v}_c} & = \\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} \\frac{\\partial F(\\boldsymbol{u}_{c+j}, \\boldsymbol{v}_c)}{\\partial \\boldsymbol{v}_c}, \\\\ \\frac{J_{\\mathrm{skip-gram}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{v}_j} & = 0, \\forall j \\ne c \\end{align*} \\begin{align*} \\frac{J_{\\mathrm{skip-gram}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{U}} & = \\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} \\frac{\\partial F(\\boldsymbol{u}_{c+j}, \\boldsymbol{v}_c)}{\\partial \\boldsymbol{v}_c},\\\\ \\frac{J_{\\mathrm{skip-gram}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{v}_c} & = \\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} \\frac{\\partial F(\\boldsymbol{u}_{c+j}, \\boldsymbol{v}_c)}{\\partial \\boldsymbol{v}_c}, \\\\ \\frac{J_{\\mathrm{skip-gram}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{v}_j} & = 0, \\forall j \\ne c \\end{align*} CBOW model \u00b6 Continuous bag-of-words (CBOW) is using the context words to predect the center words. Different from skip-gram model, in CBOW, we will use 2m 2m context word vectors as we predict probability of a word is the center word. For a simple variant of CBOW, we could sum up all the 2m 2m word vectors in one context vector \\hat{\\boldsymbol{v}} \\hat{\\boldsymbol{v}} and use the similar cost function with \\mathrm{softmax} \\mathrm{softmax} as we did in skip-gram model. \\begin{equation*} \\hat{\\boldsymbol{v}} = \\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} \\boldsymbol{v}_{c+j}. \\end{equation*} \\begin{equation*} \\hat{\\boldsymbol{v}} = \\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} \\boldsymbol{v}_{c+j}. \\end{equation*} Similar to skip-gram, we have \\begin{align*} \\frac{J_{\\mathrm{CBOW}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{U}} & = \\frac{\\partial F(\\boldsymbol{u}_{c}, \\hat{\\boldsymbol{v}})}{\\partial \\boldsymbol{U}},\\\\ \\frac{J_{\\mathrm{CBOW}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{v}_j} & = \\frac{\\partial F(\\boldsymbol{u}_{c}, \\hat{\\boldsymbol{v}})}{\\partial \\boldsymbol{v}_j}, \\forall j \\in \\{c-m, \\cdots, c-1, c+1, \\cdots, c+m\\}\\\\ \\frac{J_{\\mathrm{CBOW}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{v}_j} & = 0, \\forall j \\notin \\{c-m, \\cdots, c-1, c+1, \\cdots, c+m\\}. \\end{align*} \\begin{align*} \\frac{J_{\\mathrm{CBOW}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{U}} & = \\frac{\\partial F(\\boldsymbol{u}_{c}, \\hat{\\boldsymbol{v}})}{\\partial \\boldsymbol{U}},\\\\ \\frac{J_{\\mathrm{CBOW}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{v}_j} & = \\frac{\\partial F(\\boldsymbol{u}_{c}, \\hat{\\boldsymbol{v}})}{\\partial \\boldsymbol{v}_j}, \\forall j \\in \\{c-m, \\cdots, c-1, c+1, \\cdots, c+m\\}\\\\ \\frac{J_{\\mathrm{CBOW}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{v}_j} & = 0, \\forall j \\notin \\{c-m, \\cdots, c-1, c+1, \\cdots, c+m\\}. \\end{align*} GloVe model \u00b6 TODO, the paper and lecture. Word vector evaluation \u00b6 Summary \u00b6 This post focused on gradient derivation for various word embedding models. The bootstrap model is based on the distributional semantic, by which to predict the probability of a word given some other words from the fixed corpus. We use the \\mathrm{softmax} \\mathrm{softmax} function to compute the probability. To update the word vectors, we introduce the likelihood function and derived its gradient. In that derivation, the parameter \\boldsymbol{\\theta} \\boldsymbol{\\theta} is the hyper parameters related to word vector that can be used to compute the probability. We continue to derive the gradient of the cross entropy loss function for a single word prediction. The result gradient of the cross entropy loss function is the key for all our later gradient derivation. After that we introduce the word2vec family of word embedding, skip-gram and CBOW models. In word2vec we use the dot product of the center word and its context words to compute the probability. We derive the gradient with respect to both center word and context word when using cross entropy loss function. Negative sampling is also discussed as it improves the computation cost by many factors. With the discussion of skip-gram model, CBOW model is easier to present. One thing you need to distinguish is that whether the gradient is for a particular word prediction, for the whole window of the words, or the over all objective for the corpus. For skip-gram, we first compute the gradient for each word prediction in the given context window, then sum up all the gradient to update the cost function of that window. We move the window over the corpus until finish all the word updates. This whole process is only one update. While using negative sampling, the process becomes more efficient, we don't have to go through all the window, only sampling K K windows with Unigram Model raise to the 3/4 3/4 power. In the simple CBOW model discussed, we add up all the context vectors first, then only update a single gradient for that window corresponding to the center word. We repeat this process for different windows to complete one update. Part 2 Neural Networks and Backpropagation \u00b6 Date: April 15th 2019 We discussed the softmax classifier in Part 1 and its major drawback that the classifier only gives linear decision boundaries. In Part 2, Neural Networks will be introduced to demonstrate that it can learn much more complex functions and nonlinear decision boundaries. Intro to Neural Networks \u00b6 Biological Neuron Mathematical Model Simplified Neuron The neuron can be modeled as a binary logistic regression unit as in the last row of the table above. It can be further simplified as following functions, \\begin{equation*} h_{w, b} (\\boldsymbol{x})=f(\\boldsymbol{w}^\\mathsf{T} \\boldsymbol{x} + b) \\\\ f(z) = \\frac{1}{1 + e^{-z}}. \\end{equation*} \\begin{equation*} h_{w, b} (\\boldsymbol{x})=f(\\boldsymbol{w}^\\mathsf{T} \\boldsymbol{x} + b) \\\\ f(z) = \\frac{1}{1 + e^{-z}}. \\end{equation*} \\boldsymbol{x} \\boldsymbol{x} is the inputs \\boldsymbol{w} \\boldsymbol{w} is the weights b b is a bias term h h is a hidden layer function f f is a nonlinear activation function (sigmoid, tanh, etc.) If we feed a vector of inputs through a bunch of logistic regression (sigmoid) functions, then we get a vector of outputs. The CS224N lecture note 3 section 1.2 have a complete derivation on multiple sigmoid units. But we don\u2019t have to decide ahead of time what variables these logistic regressions are trying to predict! It is the loss function that will direct what the intermediate hidden variables should be, so as to do a good job at predicting the targets for the next layer, etc. Forward Propagation in Matrix Notation \u00b6 In a multilayer neural network, not only did we have multiple sigmoid units, but we also have more than one layer. Let's explicitly write down the signal transformation (aka forward propagation) from one layer to another referring to this network from Andrew's ML course. We will use the following notation convention a^{(j)}_i a^{(j)}_i to represent the \"activation\" of unit i i in layer j j . W^{(j)} W^{(j)} to represent the matrix of weights controlling function mapping from layer j j to layer j + 1 j + 1 . The value at each node can be calculated as \\begin{equation*} a^{(2)}_1 = f(w^{(1)}_{10} x_0 + w^{(1)}_{11} x_1 + w^{(1)}_{12} x_2 + w^{(1)}_{13} x_3) \\\\ a^{(2)}_2 = f(w^{(1)}_{20} x_0 + w^{(1)}_{21} x_1 + w^{(1)}_{22} x_2 + w^{(1)}_{23} x_3) \\\\ a^{(2)}_3 = f(w^{(1)}_{30} x_0 + w^{(1)}_{31} x_1 + w^{(1)}_{32} x_2 + w^{(1)}_{33} x_3) \\\\ \\text{and} \\\\ h_{w} (\\boldsymbol{x}) = a^{(3)}_1 = f(w^{(2)}_{10} a^{(2)}_0 + w^{(2)}_{11} a^{(2)}_1 + w^{(2)}_{12} a^{(2)}_2 + w^{(2)}_{13} a^{(2)}_3) \\end{equation*} \\begin{equation*} a^{(2)}_1 = f(w^{(1)}_{10} x_0 + w^{(1)}_{11} x_1 + w^{(1)}_{12} x_2 + w^{(1)}_{13} x_3) \\\\ a^{(2)}_2 = f(w^{(1)}_{20} x_0 + w^{(1)}_{21} x_1 + w^{(1)}_{22} x_2 + w^{(1)}_{23} x_3) \\\\ a^{(2)}_3 = f(w^{(1)}_{30} x_0 + w^{(1)}_{31} x_1 + w^{(1)}_{32} x_2 + w^{(1)}_{33} x_3) \\\\ \\text{and} \\\\ h_{w} (\\boldsymbol{x}) = a^{(3)}_1 = f(w^{(2)}_{10} a^{(2)}_0 + w^{(2)}_{11} a^{(2)}_1 + w^{(2)}_{12} a^{(2)}_2 + w^{(2)}_{13} a^{(2)}_3) \\end{equation*} write the matrix W^{(j)} W^{(j)} explicity, \\begin{equation*} W^{(1)} = \\begin{bmatrix} w^{(1)}_{10} & w^{(1)}_{11} & w^{(1)}_{12} & w^{(1)}_{13} \\\\ w^{(1)}_{20} & w^{(1)}_{21} & w^{(1)}_{22} & w^{(1)}_{23} \\\\ w^{(1)}_{30} & w^{(1)}_{31} & w^{(1)}_{32} & w^{(1)}_{33} \\end{bmatrix} \\end{equation*} \\begin{equation*} W^{(1)} = \\begin{bmatrix} w^{(1)}_{10} & w^{(1)}_{11} & w^{(1)}_{12} & w^{(1)}_{13} \\\\ w^{(1)}_{20} & w^{(1)}_{21} & w^{(1)}_{22} & w^{(1)}_{23} \\\\ w^{(1)}_{30} & w^{(1)}_{31} & w^{(1)}_{32} & w^{(1)}_{33} \\end{bmatrix} \\end{equation*} \\begin{equation*} W^{(2)} = \\begin{bmatrix} w^{(2)}_{10} & w^{(2)}_{11} & w^{(2)}_{12} & w^{(2)}_{13} \\end{bmatrix} \\end{equation*} \\begin{equation*} W^{(2)} = \\begin{bmatrix} w^{(2)}_{10} & w^{(2)}_{11} & w^{(2)}_{12} & w^{(2)}_{13} \\end{bmatrix} \\end{equation*} With the above form, we can use matrix notation as \\begin{align*} & \\boldsymbol{a}^{(2)} = f(\\boldsymbol{W}^{(1)} \\boldsymbol{x}) \\\\ h_{w} (\\boldsymbol{x}) = & \\boldsymbol{a}^{(3)} = f(\\boldsymbol{W}^{(2)} \\boldsymbol{a}^{(2)}) \\end{align*} \\begin{align*} & \\boldsymbol{a}^{(2)} = f(\\boldsymbol{W}^{(1)} \\boldsymbol{x}) \\\\ h_{w} (\\boldsymbol{x}) = & \\boldsymbol{a}^{(3)} = f(\\boldsymbol{W}^{(2)} \\boldsymbol{a}^{(2)}) \\end{align*} We can see from above notations that if the network has s_j s_j units in layer j j and s_{j+1} s_{j+1} in layer j + 1 j + 1 , the matrix W^{(j)} W^{(j)} will be of dimention s_{j+1} \\times s_{j}+1 s_{j+1} \\times s_{j}+1 . It could be interpreted as \"the dimention of W^{(j)} W^{(j)} is the number of nodes in the next layer (layer j + 1 j + 1 ) \\times \\times the number of nodes in the current layer + + 1. Note that in cs224n the matrix notation is slightly different \\begin{align*} & \\boldsymbol{a}^{(2)} = f(\\boldsymbol{W}^{(1)} \\boldsymbol{x} + \\boldsymbol{b}^{(1)}) \\\\ h_{w} (\\boldsymbol{x}) = & \\boldsymbol{a}^{(3)} = f(\\boldsymbol{W}^{(2)} \\boldsymbol{a}^{(2)} + \\boldsymbol{b}^{(2)}) \\end{align*} \\begin{align*} & \\boldsymbol{a}^{(2)} = f(\\boldsymbol{W}^{(1)} \\boldsymbol{x} + \\boldsymbol{b}^{(1)}) \\\\ h_{w} (\\boldsymbol{x}) = & \\boldsymbol{a}^{(3)} = f(\\boldsymbol{W}^{(2)} \\boldsymbol{a}^{(2)} + \\boldsymbol{b}^{(2)}) \\end{align*} the different is from how we denote the bias. The two are enssentially the same, but be cautions that the matrix dimentions are different. Word Window Classification Using Neural Networks \u00b6 From now on, let's switch the notation to cs224n so that we can derive the backpropagation algorithm for word window classification and get more intuition about the backprop. The drawing of the neural nets in cs224n for word window classification is less dynamic and slitly different from the drawing from Andrew's ML class. The figure from cs224n may be slightly confusing at first, but it is good to understand it from this word window classification application. Forward propagation \u00b6 Firstly, the goal of this classification task is to classify whether the center word is a location. Similar to word2vec, we will go over all positions in a corpus. But this time, it will be supervised and only some positions should get a high score. The figure above illustrate the feed-forward process. We use the method by Collobert & Weston (2008, 2011). An unnormalized score will be calculated from the activation \\boldsymbol{a} = [a_1, a_2, a_3, \\cdots]^\\mathsf{T} \\boldsymbol{a} = [a_1, a_2, a_3, \\cdots]^\\mathsf{T} . \\begin{equation*} \\text{scroe}(\\boldsymbol{x}) = U^\\mathsf{T} \\boldsymbol{a} \\in \\mathbb{R} \\end{equation*} \\begin{equation*} \\text{scroe}(\\boldsymbol{x}) = U^\\mathsf{T} \\boldsymbol{a} \\in \\mathbb{R} \\end{equation*} We will use max-margin loss as our loss function. The training is essentially to find the optimal weights W W by minimize the max-margin loss \\begin{equation*} J = \\max(0, 1 - s + s_c) \\end{equation*} \\begin{equation*} J = \\max(0, 1 - s + s_c) \\end{equation*} s s is the score of a window that have a location in the center. s_c s_c is the score of a window that doesn't have a location in the center. For full objective function: Sample several corrupt windows per true one. Sum over all training windows. It is similar to negative sampling in word2vec. We will use gradient descent to update the parameter so as to minimize the loss function. The key is how to calculate the gradient with respect to the model parameters, namely \\nabla_{\\theta} J(\\theta) \\nabla_{\\theta} J(\\theta) . Here we use \\theta \\theta to represent the hyperthetic parameters, it can include the W W and other parameters of the model. \\begin{equation*} \\theta^{\\text{new}} = \\theta^{\\text{old}} - \\alpha\\nabla_{\\theta} J(\\theta) \\end{equation*} \\begin{equation*} \\theta^{\\text{new}} = \\theta^{\\text{old}} - \\alpha\\nabla_{\\theta} J(\\theta) \\end{equation*} Gradients and Jacobians Matrix \u00b6 At first, let us layout the input and all the equations in this simple neural network. Input Layer Hidden Layer Output Layer \\boldsymbol{x} \\boldsymbol{x} \\boldsymbol{z} = \\boldsymbol{W} \\boldsymbol{x} + \\boldsymbol{b}, \\\\ \\boldsymbol{h} = f(\\boldsymbol{z}) \\boldsymbol{z} = \\boldsymbol{W} \\boldsymbol{x} + \\boldsymbol{b}, \\\\ \\boldsymbol{h} = f(\\boldsymbol{z}) s = \\boldsymbol{u}^{\\mathsf{T}}\\boldsymbol{h} s = \\boldsymbol{u}^{\\mathsf{T}}\\boldsymbol{h} To update the parameters in this model, namely \\boldsymbol{x}, \\boldsymbol{W}, \\boldsymbol{b} \\boldsymbol{x}, \\boldsymbol{W}, \\boldsymbol{b} , we would like to compute the derivitavies of s s with respect to all these parameters. We have to use the chain rule to compute it. What chain rule says is that we can compute the partial derivatives of each individual functions and then multiply them together to get the derivative with respect the specific variable. For example, \\frac{\\partial s}{\\partial \\boldsymbol{b}} \\frac{\\partial s}{\\partial \\boldsymbol{b}} is computed as \\frac{\\partial s}{\\partial \\boldsymbol{b}} = \\frac{\\partial s}{\\partial \\boldsymbol{h}} \\cdot \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\cdot \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} \\frac{\\partial s}{\\partial \\boldsymbol{b}} = \\frac{\\partial s}{\\partial \\boldsymbol{h}} \\cdot \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\cdot \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} . This seems very easy to understand, but when it comes to implemenation in vectorized format, it become confusing for those who doesn't work on matrix calculus for quite a while like me. I want to get the points straight here. What exactly is \\frac{\\partial s}{\\partial \\boldsymbol{b}} \\frac{\\partial s}{\\partial \\boldsymbol{b}} and \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} . Note both \\boldsymbol{h} \\boldsymbol{h} and \\boldsymbol{z} \\boldsymbol{z} are vectors. To calculate these two gradient, simply remember the following two rules: Given a function with 1 output and n n inputs f(\\boldsymbol{x}) = f(x_1, x_2, \\cdots, x_n) f(\\boldsymbol{x}) = f(x_1, x_2, \\cdots, x_n) , it's gradient is a vector of partial derivatives with respect to each input (take the gradient element wise). $$ \\frac{\\partial f}{\\partial \\boldsymbol{x}} = \\Bigg [ \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\cdots, \\frac{\\partial f}{\\partial x_n}, \\Bigg ] $$ Given a function with m m output and n n inputs $$ \\boldsymbol{f}(\\boldsymbol{x}) = \\big [f_1(x_1, x_2, \\cdots, x_n), \\cdots, f_m(x_1, x_2, \\cdots, x_n) \\big ], $$ it's gradient is an m \\times n m \\times n matrix of partial derivatives (\\frac{\\partial \\boldsymbol{f}}{\\partial \\boldsymbol{x}})_{ij} = \\dfrac{\\partial f_i}{\\partial x_j} (\\frac{\\partial \\boldsymbol{f}}{\\partial \\boldsymbol{x}})_{ij} = \\dfrac{\\partial f_i}{\\partial x_j} . This matrix is also called Jacobian matrix. \\begin{equation*} \\frac{\\partial \\boldsymbol{f}}{\\partial \\boldsymbol{x}} = \\begin{bmatrix} \\dfrac{\\partial \\boldsymbol{f}}{\\partial x_1} & \\cdots & \\dfrac{\\partial \\boldsymbol{f}}{\\partial x_n} \\end{bmatrix} = \\begin{bmatrix} \\dfrac{\\partial f_1}{\\partial x_1} & \\cdots & \\dfrac{\\partial f_1}{\\partial x_n}\\\\ \\vdots & \\ddots & \\vdots\\\\ \\dfrac{\\partial f_m}{\\partial x_1} & \\cdots & \\dfrac{\\partial f_m}{\\partial x_n} \\end{bmatrix} \\end{equation*} \\begin{equation*} \\frac{\\partial \\boldsymbol{f}}{\\partial \\boldsymbol{x}} = \\begin{bmatrix} \\dfrac{\\partial \\boldsymbol{f}}{\\partial x_1} & \\cdots & \\dfrac{\\partial \\boldsymbol{f}}{\\partial x_n} \\end{bmatrix} = \\begin{bmatrix} \\dfrac{\\partial f_1}{\\partial x_1} & \\cdots & \\dfrac{\\partial f_1}{\\partial x_n}\\\\ \\vdots & \\ddots & \\vdots\\\\ \\dfrac{\\partial f_m}{\\partial x_1} & \\cdots & \\dfrac{\\partial f_m}{\\partial x_n} \\end{bmatrix} \\end{equation*} Computing gradients with the chain rule \u00b6 With these two rules we can calculate the partials. We will use \\frac{\\partial s}{\\partial \\boldsymbol{b}} \\frac{\\partial s}{\\partial \\boldsymbol{b}} as an example. \\begin{equation*} \\frac{\\partial s}{\\partial \\boldsymbol{b}} = \\frac{\\partial s}{\\partial \\boldsymbol{h}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} \\end{equation*} \\begin{equation*} \\frac{\\partial s}{\\partial \\boldsymbol{b}} = \\frac{\\partial s}{\\partial \\boldsymbol{h}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} \\end{equation*} \\frac{\\partial s}{\\partial \\boldsymbol{h}} = \\frac{\\partial}{\\partial \\boldsymbol{h}}(\\boldsymbol{u}^{\\mathsf{T}}\\boldsymbol{h}) = \\boldsymbol{u}^{\\mathsf{T}} \\frac{\\partial s}{\\partial \\boldsymbol{h}} = \\frac{\\partial}{\\partial \\boldsymbol{h}}(\\boldsymbol{u}^{\\mathsf{T}}\\boldsymbol{h}) = \\boldsymbol{u}^{\\mathsf{T}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} = \\frac{\\partial}{\\partial \\boldsymbol{z}}(f(\\boldsymbol{z})) = \\begin{pmatrix} f'(z_1) & & 0 \\\\ & \\ddots & \\\\ 0 & & f'(z_n) \\end{pmatrix} = \\mathrm{diag} (f' (\\boldsymbol{z})) \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} = \\frac{\\partial}{\\partial \\boldsymbol{z}}(f(\\boldsymbol{z})) = \\begin{pmatrix} f'(z_1) & & 0 \\\\ & \\ddots & \\\\ 0 & & f'(z_n) \\end{pmatrix} = \\mathrm{diag} (f' (\\boldsymbol{z})) \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} = \\frac{\\partial}{\\partial \\boldsymbol{b}}(\\boldsymbol{W}\\boldsymbol{x} + \\boldsymbol{b}) = \\boldsymbol{I} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} = \\frac{\\partial}{\\partial \\boldsymbol{b}}(\\boldsymbol{W}\\boldsymbol{x} + \\boldsymbol{b}) = \\boldsymbol{I} Therefore, \\begin{equation*} \\frac{\\partial s}{\\partial \\boldsymbol{b}} = \\boldsymbol{u}^{\\mathsf{T}} \\mathrm{diag}(f'(\\boldsymbol{z})) \\boldsymbol{I} = \\boldsymbol{u}^{\\mathsf{T}} \\circ f'(\\boldsymbol{z}) \\end{equation*} \\begin{equation*} \\frac{\\partial s}{\\partial \\boldsymbol{b}} = \\boldsymbol{u}^{\\mathsf{T}} \\mathrm{diag}(f'(\\boldsymbol{z})) \\boldsymbol{I} = \\boldsymbol{u}^{\\mathsf{T}} \\circ f'(\\boldsymbol{z}) \\end{equation*} Similarly we can calculate the partial with respect to \\boldsymbol{W} \\boldsymbol{W} and \\boldsymbol{x} \\boldsymbol{x} . Since the top layer partials are already calculated, we can reuese the results. We denote those reusable partials as \\delta \\delta meaning local error signal. \\begin{align*} \\frac{\\partial s}{\\partial \\boldsymbol{W}} & = \\frac{\\partial s}{\\partial \\boldsymbol{h}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{W}} \\\\ \\\\ \\frac{\\partial s}{\\partial \\boldsymbol{x}} & = \\frac{\\partial s}{\\partial \\boldsymbol{h}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{x}} \\\\ \\\\ \\frac{\\partial s}{\\partial \\boldsymbol{b}} & = \\underbrace{\\frac{\\partial s}{\\partial \\boldsymbol{h}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}}}_{\\mathrm{reuse}} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} = \\boldsymbol{\\delta} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} = \\boldsymbol{\\delta} = \\boldsymbol{u}^{\\mathsf{T}} \\circ f'(\\boldsymbol{z}) \\end{align*} \\begin{align*} \\frac{\\partial s}{\\partial \\boldsymbol{W}} & = \\frac{\\partial s}{\\partial \\boldsymbol{h}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{W}} \\\\ \\\\ \\frac{\\partial s}{\\partial \\boldsymbol{x}} & = \\frac{\\partial s}{\\partial \\boldsymbol{h}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{x}} \\\\ \\\\ \\frac{\\partial s}{\\partial \\boldsymbol{b}} & = \\underbrace{\\frac{\\partial s}{\\partial \\boldsymbol{h}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}}}_{\\mathrm{reuse}} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} = \\boldsymbol{\\delta} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} = \\boldsymbol{\\delta} = \\boldsymbol{u}^{\\mathsf{T}} \\circ f'(\\boldsymbol{z}) \\end{align*} Shape convention \u00b6 What does the shape of the derivatives looks like in practice? How can we make the chain rule computation efficient? According to the aforementioned gradient calculation rules, \\frac{\\partial s}{\\partial \\boldsymbol{W}} \\frac{\\partial s}{\\partial \\boldsymbol{W}} is a row vector. The chain rule gave \\begin{equation*} \\frac{\\partial s}{\\partial \\boldsymbol{W}} = \\boldsymbol{\\delta} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{W}}. \\end{equation*} \\begin{equation*} \\frac{\\partial s}{\\partial \\boldsymbol{W}} = \\boldsymbol{\\delta} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{W}}. \\end{equation*} We know from Jacobians that \\frac{\\partial\\boldsymbol{z}}{\\partial \\boldsymbol{W}} = \\frac{\\partial}{\\partial \\boldsymbol{W}}(\\boldsymbol{W}\\boldsymbol{x} + \\boldsymbol{b}) = \\boldsymbol{x}^{\\mathsf{T}} \\frac{\\partial\\boldsymbol{z}}{\\partial \\boldsymbol{W}} = \\frac{\\partial}{\\partial \\boldsymbol{W}}(\\boldsymbol{W}\\boldsymbol{x} + \\boldsymbol{b}) = \\boldsymbol{x}^{\\mathsf{T}} . We may arrived the result that \\frac{\\partial s}{\\partial \\boldsymbol{W}} = \\boldsymbol{\\delta} \\ \\boldsymbol{x}^{\\mathsf{T}} \\frac{\\partial s}{\\partial \\boldsymbol{W}} = \\boldsymbol{\\delta} \\ \\boldsymbol{x}^{\\mathsf{T}} . This is actually not quite wirte. The correct form should be \\begin{align*} \\underbrace{\\frac{\\partial s}{\\partial \\boldsymbol{W}}}_{n \\times m} = \\underbrace{\\boldsymbol{\\delta}^{\\mathsf{T}}}_{n \\times 1} \\ \\underbrace{\\boldsymbol{x}^{\\mathsf{T}}}_{1 \\times m}. \\end{align*} \\begin{align*} \\underbrace{\\frac{\\partial s}{\\partial \\boldsymbol{W}}}_{n \\times m} = \\underbrace{\\boldsymbol{\\delta}^{\\mathsf{T}}}_{n \\times 1} \\ \\underbrace{\\boldsymbol{x}^{\\mathsf{T}}}_{1 \\times m}. \\end{align*} Note You may wonder why is this form instead of the one we derived directly. The explanation from the CS224N is that we would like to follow the shape convention so as to make the chain rule implementation more efficient (matrix multiplication instead of loops). Different from the Jacobian form, shape convention states that the shape of the gradient is the same shape of parameters. The resolution here is to use Jacobian form as much as possible and reshape to follow the convention at the end. Because Jacobian form makes the chain rule easy and the shape convention makes the implementation of gradient descent easy. Computation graphs and backpropagation \u00b6 We have shown how to compute the partial derivatives using the chain rule. This is almost the backpropagation algorithm. If we want to add on to that and make the algorithm complete, the only thing we need is how to reuse gradient computed for higher layers. We leverage the computation graph to explain this. From the computation graph, you'll get the intuition of reusing the partial derivatives computed for higher layers in computing derivatives for lower layers so as to minimize computation. We represent our neural net equations as a computation graph as following: source nodes represent inputs interior nodes represent operations edges pass along result of the operation When following computation order to carry out the computations from inputs, it is forward propagation. Backpropagation is to pass along gradients backwards. This can be illustrated in the following computation graph. The partial derivative with respect to a parameter reflect how changing the parameter would effect the output value. The output value in the backprop is usually a loss function (or error). Intuitively, you can see backprop is push the error back to the lower layer through a bunch of operation nodes, the arrived error is a measure of the error at that particular layer, training is try to reduce this backprop-ed error by adjusting the local parameters, the effect of reducing the local error will be forward propagated to the output, the error at the output should also reduced. We use gradient descent to make this process to converge as soon as possible. To understand it better, let's look at a single node. We define \"local gradient\" for each node as the gradient of it's output with respect to it's input. By chain rule, the downstream gradient is equal to the multiplication of the upstream gradient and the local gradient. When having multipe local gradients, they are pushed back to each input using chain rule. CS224N lecture 4 slides have step by step example of backprop. From the example, we can got some intuitions about some nodes' effects. For example, when push back gradients along outward branches from a single node, the gradients should be sumed up; \"+\" node distributes the upstream gradient to each summand; \"max\" node simply routes the upstream gradients. When update gradient with backprop, you should compute all the gradients at once. With this computation graph notion, following routine captures the gist of backprop in a very decent manner. Fprop: visit nodes in topological sorted order Compute value of node given predecessors Bprop: initialize output gradient = 1 visit nodes in reverse order: Compute gradient wrt each node using gradient wrt successors \\{y_1, y_2, \\cdots, y_2\\} = \\mathrm{successors\\ of\\ } x \\{y_1, y_2, \\cdots, y_2\\} = \\mathrm{successors\\ of\\ } x \\begin{equation*}\\frac{\\partial z}{\\partial \\boldsymbol{x}} = \\sum_{i=i}^n \\frac{\\partial z}{\\partial y_i} \\frac{\\partial y_i}{\\partial x} \\end{equation*} \\begin{equation*}\\frac{\\partial z}{\\partial \\boldsymbol{x}} = \\sum_{i=i}^n \\frac{\\partial z}{\\partial y_i} \\frac{\\partial y_i}{\\partial x} \\end{equation*} If done correctly, the big O O complexity of forward propagation and backpropagation is the same. Automatic differentiation \u00b6 The gradient computation can be automatically inferred from the symbolic expression of the fprop. but this is not commonly used in practice. Each node type needs to know how to compute its output and how to compute the gradient wrt its inputs given the gradient wrt its output. Modern DL frameworks (Tensorflow, PyTorch, etc.) do backpropagation for you but mainly leave layer/node writer to hand-calculate the local derivative. Following is a simple demo of how to implement forward propagation and backpropagation. class MultiplyGate ( object ): def forward ( x , y ): z = x * y self . x = x # must keep these around! self . y = y return z def backword ( dz ): dx = self . y * dz # [dz/dx * dL/dz] dy = self . x * dz # [dz/dy * dL/dz] return [ dx , dy ] nonlinear activation functions: sigmoid, tanh, hard tanh, ReLU Learning rates: start 0.001. power of 10. halve the learning rate every k epochs. formula: lr = lr_0 e^{-k t} lr = lr_0 e^{-k t} for each epoch t t Regularization \u00b6 LFD starting point: L2 regularization Part 3 Language Model and RNN \u00b6 n-gram \u00b6 assumption: \\boldsymbol{x}^{(t+1)} \\boldsymbol{x}^{(t+1)} depends only on previous n-1 n-1 words. Sparsity problem, --> backoff (n-1)-gram practically, n has to be less than 5. Neural language model \u00b6 fixed-window neural language model Recurrent Neural Network","title":"CS224N Write-up"},{"location":"courses/cs224n/write-up/#cs224n-nlp-with-deep-learning","text":"","title":"CS224N NLP with Deep Learning"},{"location":"courses/cs224n/write-up/#part-1-word-embeddings-based-on-distributional-semantic","text":"Date: April 20th 2019 If you have no experience in the Natrual Language Processing (NLP) field and asked to name a few practical NLP applications, what would you list? Google Translation or Amazon Echo voice assistant may first come into your mind. How did those products understand a sentence or human conversation? Engineers built computational models that understand our language as our brain does. To build such a computational model, the following components are necessary. A large corpus of text (input training data) A method to represent each word from the corpus (feature representation) A starting model that barely understands English at the beginning but can be improved by \"reading\" more words from the corpus (parametric function). An algorithm for the model to correct itself if it makes a mistake in understanding (learning algorithm/optimization method) A measurement that can qualify the mistake the model made (loss function)","title":"Part 1 Word Embeddings Based on Distributional Semantic"},{"location":"courses/cs224n/write-up/#introduction","text":"Starting with this post, I will write materials from my understanding of the Stanford CS224N. The plan is to journal down all the learning notes I learned about the above 5 components. The goal is to provide a systematic understand of the gist of the components for real applications. Part 1 is about language models and word embeddings. Part 2 discusses neural networks and backpropagation algorithm. Part 3 revisits the language model and introduces recurrent neural networks. Part 4 studies advanced RNN, Long short-term memory (LSTM), and gated recurrent networks (GRN).","title":"Introduction"},{"location":"courses/cs224n/write-up/#word-representations","text":"How do we represent word with meaning on a computer? Before 2013, wordNet and one-hot vector are most popular in word meaning representations. WordNet is a manually compiled thesaurus containing lists of synonym sets and hypernyms. Like most of the manual stuff, it is subjective, unscalable, inaccurate in computing word similarity, and impossible to maintain and keep up-to-data. One-hot vectors represent word meaning using discrete symbolic 1 1 s in a long stream of 0 0 of vector elements. It suffers from sparsity issues and many other drawbacks. We will not spend time on that outdated method. Instead, we will focus on the embedding method using the idea of word vector. The core idea of this embedding method is based on the remarkable insight on word meaning called distributional semantics . It conjectures that a word\u2019s meaning is given by the words that frequently appear close-by. It is proposed by J. R. Firth. Here is the famous quote: Quote \"You shall know a word by the company it keeps\" -- J. R. Firth 1957: 11 In this method, we will build a dense vector for each word, chosen so that it is similar to vectors of words that appear in similar contexts. word vector representation also called distributed representation , or word embeddings . A work vector might look likes this, \\mathrm{banking} = \\begin{pmatrix} 0.286\\\\ 0.792\\\\ \u22120.177\\\\ \u22120.107\\\\ 0.109\\\\ \u22120.542\\\\ 0.349\\\\ 0.271 \\end{pmatrix} \\mathrm{banking} = \\begin{pmatrix} 0.286\\\\ 0.792\\\\ \u22120.177\\\\ \u22120.107\\\\ 0.109\\\\ \u22120.542\\\\ 0.349\\\\ 0.271 \\end{pmatrix}","title":"Word representations"},{"location":"courses/cs224n/write-up/#word2vec","text":"Word2vec (Mikolov et al. 2013) is a framework for learning word vectors. Word2vec go through windows of words and calculate the probability of the context word given the center word (or vice versa) using the similarity of the word vectors. It keeps adjusting the word vectors to maximize this probability. Vividly, these two pictures show the general idea of how word2vec works. Formally, for a single prediction, the probability is P(o|c) P(o|c) , interpreted as the probability of the outer word o o given the center word c c . For the large corpus including T T words and each position t = 1, 2, \\cdots, T t = 1, 2, \\cdots, T , we predict context words within a window of fixed size m m , given center word w_t w_t . The model likelihood can be written as the following \\begin{equation*} \\mathrm{likelihood} = L(\\theta) = \\prod_{t=1}^T\\prod_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} p(w_{t+j}|w_t; \\theta) \\end{equation*} \\begin{equation*} \\mathrm{likelihood} = L(\\theta) = \\prod_{t=1}^T\\prod_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} p(w_{t+j}|w_t; \\theta) \\end{equation*} The objective function J(\\theta) J(\\theta) is the average negative log likelihood: \\begin{equation*} J(\\theta) = -\\frac{1}{T}\\log{L(\\theta)} = -\\frac{1}{T} \\sum_{t=1}^T\\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} \\log p(w_{t+j}|w_t; \\theta) \\end{equation*} \\begin{equation*} J(\\theta) = -\\frac{1}{T}\\log{L(\\theta)} = -\\frac{1}{T} \\sum_{t=1}^T\\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} \\log p(w_{t+j}|w_t; \\theta) \\end{equation*} We've defined the cost function by now. In order to minimize the loss, we need to know how p(w_{t+j}|w_t; \\theta) p(w_{t+j}|w_t; \\theta) can be calculated. One function we can use to calculate the probability value is the \\mathrm{softmax} \\mathrm{softmax} function. \\begin{equation*} \\mathrm{softmax}(x)_i = \\frac{\\exp(x_i)}{\\sum_{j=1}^{n}\\exp(x_j)} \\end{equation*} \\begin{equation*} \\mathrm{softmax}(x)_i = \\frac{\\exp(x_i)}{\\sum_{j=1}^{n}\\exp(x_j)} \\end{equation*} Particularly we will write the probability as the following format. \\begin{equation*} p(o|c) = \\frac{\\exp(u_o^T v_c)}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\end{equation*} \\begin{equation*} p(o|c) = \\frac{\\exp(u_o^T v_c)}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\end{equation*} There are several points need to be emphasized. Two vectors will be obtained for each individual word, one as center word v_w v_w , and the other context word u_w u_w . d d is the dimension of the word vector. V V is the vocabulary size. \\begin{equation*} \\theta = \\begin{bmatrix} v_{\\mathrm{aardvark}}\\\\ v_{\\mathrm{a}}\\\\ \\cdots \\\\ v_{\\mathrm{zebra}}\\\\ u_{\\mathrm{aardvark}}\\\\ u_{\\mathrm{a}}\\\\ \\cdots \\\\ u_{\\mathrm{zebra}}\\\\ \\end{bmatrix} \\in \\mathbb{R}^{2dV} \\end{equation*} \\begin{equation*} \\theta = \\begin{bmatrix} v_{\\mathrm{aardvark}}\\\\ v_{\\mathrm{a}}\\\\ \\cdots \\\\ v_{\\mathrm{zebra}}\\\\ u_{\\mathrm{aardvark}}\\\\ u_{\\mathrm{a}}\\\\ \\cdots \\\\ u_{\\mathrm{zebra}}\\\\ \\end{bmatrix} \\in \\mathbb{R}^{2dV} \\end{equation*} The dot product in the exponet compares similarity of o o and c c . Larger dot product indicates larger probability. The denorminator sum over entire vocabulary to give normalized probability distribution. The \\mathrm{softmax} \\mathrm{softmax} function maps aribitrary values x_i x_i to a probability distribution p_i p_i . \u201c \\mathrm{max} \\mathrm{max} \u201d because it amplifies the probability of the largest x_i x_i , \u201c \\mathrm{soft} \\mathrm{soft} \u201d because it still assigns some probabilities to smaller x_i x_i . It is very commonly used in deep learning.","title":"word2vec"},{"location":"courses/cs224n/write-up/#gradient-descent-to-optimize-log-likelihood-loss-function","text":"This section I will purely focus on how to derive the gradient of the log likelihood loss function with respect to center word using the chain rule. Once we have the computed gradients, we are ready to implement it in matrix form and train the word vectors. This model is called skip-gram. loss function in p(w_{t+j}|w_t) p(w_{t+j}|w_t) \\begin{equation*} J(\\theta) = -\\frac{1}{T} \\sum_{t=1}^T\\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} \\log p(w_{t+j}|w_t; \\theta) \\end{equation*} \\begin{equation*} J(\\theta) = -\\frac{1}{T} \\sum_{t=1}^T\\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} \\log p(w_{t+j}|w_t; \\theta) \\end{equation*} p(w_{t+j}|w_t) p(w_{t+j}|w_t) in softmax form \\begin{equation*} p(o|c) = \\frac{\\exp(u_o^T v_c)}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\end{equation*} \\begin{equation*} p(o|c) = \\frac{\\exp(u_o^T v_c)}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\end{equation*} We would like to find the following derivatives: \\begin{equation*} 1. \\frac{\\partial}{\\partial v_c} \\log p(o|c) \\\\ 2. \\frac{\\partial}{\\partial u_o} \\log p(o|c) \\end{equation*} \\begin{equation*} 1. \\frac{\\partial}{\\partial v_c} \\log p(o|c) \\\\ 2. \\frac{\\partial}{\\partial u_o} \\log p(o|c) \\end{equation*} Let's now start working with the first one, the derivative wrt. v_c v_c . \\begin{align*} \\frac{\\partial}{\\partial v_c} \\log p(o|c) & = \\frac{\\partial}{\\partial v_c} \\log{\\frac{\\exp(u_o^T v_c)}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)}} \\\\ &= \\frac{\\partial}{\\partial v_c} \\log{\\exp(u_o^T v_c)} - \\frac{\\partial}{\\partial v_c} \\log{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\;\\;\\;\\cdots (\\log\\frac{a}{b} = \\log a - \\log b) \\\\ & = \\frac{\\partial}{\\partial v_c} (u_o^T v_c) - \\frac{1}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\sum_{x=1}^{V} \\frac{\\partial}{\\partial v_c} \\exp(u_x^T v_c) \\;\\;\\;\\cdots \\mathrm{chain\\ rule}\\\\ & = u_o - \\frac{1}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\sum_{x=1}^{V} \\exp(u_x^{T} v_c) \\frac{\\partial}{\\partial v_c} (u_x^T v_c) \\;\\;\\;\\cdots \\mathrm{chain\\ rule}\\\\ & = u_o - \\frac{1}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\sum_{x=1}^{V} \\exp(u_x^{T} v_c) \\cdot u_x \\\\ & = u_o - \\sum_{x=1}^{V} \\frac{\\exp(u_x^{T} v_c)}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\cdot u_x \\\\ & = \\underbrace{u_o}_{\\mathrm{current}} - \\underbrace{\\sum_{x=1}^{V} p(x|c) \\cdot u_x}_{\\mathrm{expectation}} \\end{align*} \\begin{align*} \\frac{\\partial}{\\partial v_c} \\log p(o|c) & = \\frac{\\partial}{\\partial v_c} \\log{\\frac{\\exp(u_o^T v_c)}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)}} \\\\ &= \\frac{\\partial}{\\partial v_c} \\log{\\exp(u_o^T v_c)} - \\frac{\\partial}{\\partial v_c} \\log{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\;\\;\\;\\cdots (\\log\\frac{a}{b} = \\log a - \\log b) \\\\ & = \\frac{\\partial}{\\partial v_c} (u_o^T v_c) - \\frac{1}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\sum_{x=1}^{V} \\frac{\\partial}{\\partial v_c} \\exp(u_x^T v_c) \\;\\;\\;\\cdots \\mathrm{chain\\ rule}\\\\ & = u_o - \\frac{1}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\sum_{x=1}^{V} \\exp(u_x^{T} v_c) \\frac{\\partial}{\\partial v_c} (u_x^T v_c) \\;\\;\\;\\cdots \\mathrm{chain\\ rule}\\\\ & = u_o - \\frac{1}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\sum_{x=1}^{V} \\exp(u_x^{T} v_c) \\cdot u_x \\\\ & = u_o - \\sum_{x=1}^{V} \\frac{\\exp(u_x^{T} v_c)}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\cdot u_x \\\\ & = \\underbrace{u_o}_{\\mathrm{current}} - \\underbrace{\\sum_{x=1}^{V} p(x|c) \\cdot u_x}_{\\mathrm{expectation}} \\end{align*} The result is remarkable. It have great intuition in it. The gradient represent the slop in the multidimentional space that we should walk along to reach the optima. The result gradient we got tell us that the slop equals to the difference of current context vector \\boldsymbol{u}_o \\boldsymbol{u}_o and the expected context vector (the weighted average over all context vectors). It has nothing todo with the center word c c . To compute the gradient of J(\\theta) J(\\theta) with respect to the center word c c , you have to sum up all the gradients obtained from word windows when c c is the center word. The gradient with respect to the context word will be very similar, the chain rule is also handy in that case. Once all the gradient with respect to center words and context words are calculated. We can use gradient descent to update the model parameters, which in this case is all the word vectors. Because we have two vectors for each word, when update the parameters we will use the average of the two vectors to update.","title":"Gradient descent to optimize log likelihood loss function"},{"location":"courses/cs224n/write-up/#gradient-descent-to-optimize-cross-entropy-loss-function","text":"Alternatively, we could also use cross entropy loss function. CS224N 2017 assignment 1 requires to derive the gradient of cross entropy loss function. This section, we will go step by step to derive the gradient when using cross entropy loss function and \\mathrm{softmax} \\mathrm{softmax} activation function in the out put layer. the cross entropy function is defined as follows \\begin{equation*} \\mathrm{CE}(\\boldsymbol{y}, \\boldsymbol{\\hat y}) = - \\sum_i y_i \\log (\\hat y_i) \\end{equation*} \\begin{equation*} \\mathrm{CE}(\\boldsymbol{y}, \\boldsymbol{\\hat y}) = - \\sum_i y_i \\log (\\hat y_i) \\end{equation*} Notice the \\boldsymbol{y} \\boldsymbol{y} is the one-hot label vector, and \\boldsymbol{\\hat y} \\boldsymbol{\\hat y} is the predicted probability for all classes. The index i i is the index of the one-hot element in individual label vectors. Therefore the definition of cross entropy loss function is defined as followinng if we use \\mathrm{softmax} \\mathrm{softmax} to predict \\hat y_i \\hat y_i , \\begin{equation*} J_{\\mathrm{CE}}(\\theta) = - \\sum_i^N y_i \\log (\\hat y_i) = -\\sum_i^N y_i \\log (\\mathrm{softmax}(\\theta)_i) \\end{equation*} \\begin{equation*} J_{\\mathrm{CE}}(\\theta) = - \\sum_i^N y_i \\log (\\hat y_i) = -\\sum_i^N y_i \\log (\\mathrm{softmax}(\\theta)_i) \\end{equation*} The gradient of \\frac{\\partial}{\\partial \\theta}J_{\\mathrm{CE}}(\\theta) \\frac{\\partial}{\\partial \\theta}J_{\\mathrm{CE}}(\\theta) can be derived using chain rule. Because we will use the gradient of the \\mathrm{softmax} \\mathrm{softmax} for the derivation, Let's derive \\mathrm{softmax} \\mathrm{softmax} gradient first. \\begin{equation*} \\frac{\\partial \\hat y_i}{\\theta_j} = \\frac{\\partial }{\\theta_j}\\mathrm{softmax}(\\theta)_i = \\frac{\\partial }{\\theta_j} \\frac{e^{\\theta_i}}{\\sum_{k=1}^{n}e^{\\theta_k}} \\end{equation*} \\begin{equation*} \\frac{\\partial \\hat y_i}{\\theta_j} = \\frac{\\partial }{\\theta_j}\\mathrm{softmax}(\\theta)_i = \\frac{\\partial }{\\theta_j} \\frac{e^{\\theta_i}}{\\sum_{k=1}^{n}e^{\\theta_k}} \\end{equation*} Here we should use two tricks to derive this gradient. Quotient rule separate into 2 cases: when i = j i = j and i \\ne j i \\ne j . If i = j i = j , we have \\begin{align*} \\frac{\\partial }{\\theta_j} \\frac{e^{\\theta_i}}{\\sum_{k=1}^{n}e^{\\theta_k}} & = \\frac{e^{\\theta_i}\\sum_{k=1}^{n}e^{\\theta_k} - e^{\\theta_i}e^{\\theta_j} }{\\Big(\\sum_{k=1}^{n}e^{\\theta_k}\\Big)^2} \\\\ & = \\frac{e^{\\theta_i}\\Big(\\sum_{k=1}^{n}e^{\\theta_k} - e^{\\theta_j}\\Big)}{\\Big(\\sum_{k=1}^{n}e^{\\theta_k}\\Big)^2} \\\\ & = \\frac{e^{\\theta_i}}{\\sum_{k=1}^{n}e^{\\theta_k}} \\frac{\\sum_{k=1}^{n}e^{\\theta_k} - e^{\\theta_j}}{\\sum_{k=1}^{n}e^{\\theta_k}} \\\\ & = \\hat y_i(1 - \\hat y_j) \\end{align*} \\begin{align*} \\frac{\\partial }{\\theta_j} \\frac{e^{\\theta_i}}{\\sum_{k=1}^{n}e^{\\theta_k}} & = \\frac{e^{\\theta_i}\\sum_{k=1}^{n}e^{\\theta_k} - e^{\\theta_i}e^{\\theta_j} }{\\Big(\\sum_{k=1}^{n}e^{\\theta_k}\\Big)^2} \\\\ & = \\frac{e^{\\theta_i}\\Big(\\sum_{k=1}^{n}e^{\\theta_k} - e^{\\theta_j}\\Big)}{\\Big(\\sum_{k=1}^{n}e^{\\theta_k}\\Big)^2} \\\\ & = \\frac{e^{\\theta_i}}{\\sum_{k=1}^{n}e^{\\theta_k}} \\frac{\\sum_{k=1}^{n}e^{\\theta_k} - e^{\\theta_j}}{\\sum_{k=1}^{n}e^{\\theta_k}} \\\\ & = \\hat y_i(1 - \\hat y_j) \\end{align*} if i \\ne j i \\ne j , we have \\begin{align*} \\frac{\\partial }{\\theta_j} \\frac{e^{\\theta_i}}{\\sum_{k=1}^{n}e^{\\theta_k}} & = \\frac{0 - e^{\\theta_i}e^{\\theta_j} }{\\Big(\\sum_{k=1}^{n}e^{\\theta_k}\\Big)^2} \\\\ & = \\frac{e^{\\theta_i}}{\\sum_{k=1}^{n}e^{\\theta_k}} \\frac{ - e^{\\theta_j}}{\\sum_{k=1}^{n}e^{\\theta_k}} \\\\ & = -\\hat y_i \\hat y_j \\end{align*} \\begin{align*} \\frac{\\partial }{\\theta_j} \\frac{e^{\\theta_i}}{\\sum_{k=1}^{n}e^{\\theta_k}} & = \\frac{0 - e^{\\theta_i}e^{\\theta_j} }{\\Big(\\sum_{k=1}^{n}e^{\\theta_k}\\Big)^2} \\\\ & = \\frac{e^{\\theta_i}}{\\sum_{k=1}^{n}e^{\\theta_k}} \\frac{ - e^{\\theta_j}}{\\sum_{k=1}^{n}e^{\\theta_k}} \\\\ & = -\\hat y_i \\hat y_j \\end{align*} Now let calculate the gradient of the cross-entropy loss function. Notice the gradient is concerning the i i th parameter. This is because we can use the gradient of the \\mathrm{softmax} \\mathrm{softmax} (the \\frac{\\partial \\hat y_k}{\\partial \\theta_i} \\frac{\\partial \\hat y_k}{\\partial \\theta_i} term) conveniently. \\begin{align*} \\frac{\\partial J_{\\mathrm{CE}}(\\theta)}{\\partial \\theta_i} & = - \\sum_k^N y_k \\frac{\\partial \\log (\\hat y_k)}{\\partial \\theta_i} \\\\ & = - \\sum_k^N y_k \\frac{\\partial \\log (\\hat y_k)}{\\partial \\hat y_k} \\frac{\\partial \\hat y_k}{\\partial \\theta_i} \\\\ & = - \\sum_k^N y_k \\frac{1}{\\hat y_k} \\frac{\\partial \\hat y_k}{\\partial \\theta_i} \\\\ & = - \\Big(\\underbrace{y_k \\frac{1}{\\hat y_k} \\hat y_k(1 - \\hat y_i)}_{k = i} + \\underbrace{\\sum_{k\\ne i}^N y_k \\frac{1}{\\hat y_k} (-\\hat y_k \\hat y_i) \\Big)}_{k \\ne i} \\\\ & = - \\Big(y_i (1 - \\hat y_i) - \\sum_{k\\ne i}^N y_k \\hat y_i \\Big) \\\\ & = - y_i + y_i \\hat y_i + \\sum_{k\\ne i}^N y_k \\hat y_i \\\\ & = \\hat y_i\\Big(y_i + \\sum_{k\\ne i}^N y_k\\Big) - \\hat y_i \\\\ & = \\hat y_i \\cdot 1 - y_i = \\hat y_i - y_i \\end{align*} \\begin{align*} \\frac{\\partial J_{\\mathrm{CE}}(\\theta)}{\\partial \\theta_i} & = - \\sum_k^N y_k \\frac{\\partial \\log (\\hat y_k)}{\\partial \\theta_i} \\\\ & = - \\sum_k^N y_k \\frac{\\partial \\log (\\hat y_k)}{\\partial \\hat y_k} \\frac{\\partial \\hat y_k}{\\partial \\theta_i} \\\\ & = - \\sum_k^N y_k \\frac{1}{\\hat y_k} \\frac{\\partial \\hat y_k}{\\partial \\theta_i} \\\\ & = - \\Big(\\underbrace{y_k \\frac{1}{\\hat y_k} \\hat y_k(1 - \\hat y_i)}_{k = i} + \\underbrace{\\sum_{k\\ne i}^N y_k \\frac{1}{\\hat y_k} (-\\hat y_k \\hat y_i) \\Big)}_{k \\ne i} \\\\ & = - \\Big(y_i (1 - \\hat y_i) - \\sum_{k\\ne i}^N y_k \\hat y_i \\Big) \\\\ & = - y_i + y_i \\hat y_i + \\sum_{k\\ne i}^N y_k \\hat y_i \\\\ & = \\hat y_i\\Big(y_i + \\sum_{k\\ne i}^N y_k\\Big) - \\hat y_i \\\\ & = \\hat y_i \\cdot 1 - y_i = \\hat y_i - y_i \\end{align*} Write in vector form, we will have \\frac{\\partial J_{\\mathrm{CE}}(\\boldsymbol{y}, \\boldsymbol{\\hat y})}{\\partial \\boldsymbol{\\theta}} = \\boldsymbol{\\hat y} - \\boldsymbol{y}. \\frac{\\partial J_{\\mathrm{CE}}(\\boldsymbol{y}, \\boldsymbol{\\hat y})}{\\partial \\boldsymbol{\\theta}} = \\boldsymbol{\\hat y} - \\boldsymbol{y}. With this gradient, we can update the model parameters, namely the word vectors. In the next post, I will discuss neural networks, which have more layers than the word2vec. In neural networks, there are more parameters need to be trained and the gradients with respect all the parameters need to be derived.","title":"Gradient descent to optimize cross entropy loss function"},{"location":"courses/cs224n/write-up/#skip-gram-model","text":"Skip-gram model uses the center word to predict the surrounding. We have derived the gradient for the skip-gram model using log-likelihood loss function and \\mathrm{softmax} \\mathrm{softmax} . This section will derive the gradient for cross-entropy loss function. The probability output function will keep using the \\mathrm{softmax} \\mathrm{softmax} .","title":"Skip-gram model"},{"location":"courses/cs224n/write-up/#cross-entropy-loss-function","text":"Since we have derived above that \\frac{\\partial J_{\\mathrm{CE}}(\\boldsymbol{y}, \\boldsymbol{\\hat y})}{\\partial \\boldsymbol{\\theta}} = \\boldsymbol{\\hat y} - \\boldsymbol{y} \\frac{\\partial J_{\\mathrm{CE}}(\\boldsymbol{y}, \\boldsymbol{\\hat y})}{\\partial \\boldsymbol{\\theta}} = \\boldsymbol{\\hat y} - \\boldsymbol{y} . In this case, the vector \\boldsymbol{\\theta} = [\\boldsymbol{u_1^{\\mathsf{T}}}\\boldsymbol{v_c}, \\boldsymbol{u_2^{\\mathsf{T}}}\\boldsymbol{v_c}, \\cdots, \\boldsymbol{u_W^{\\mathsf{T}}}\\boldsymbol{v_c}] \\boldsymbol{\\theta} = [\\boldsymbol{u_1^{\\mathsf{T}}}\\boldsymbol{v_c}, \\boldsymbol{u_2^{\\mathsf{T}}}\\boldsymbol{v_c}, \\cdots, \\boldsymbol{u_W^{\\mathsf{T}}}\\boldsymbol{v_c}] .","title":"Cross entropy loss function"},{"location":"courses/cs224n/write-up/#gradient-for-center-word","text":"Borrow the above steps to derive the gradient with respect to \\boldsymbol{\\theta} \\boldsymbol{\\theta} , we have \\begin{align*} \\frac{\\partial }{\\partial \\boldsymbol{v_c}}J_{\\mathrm{softmax-CE}}(\\boldsymbol{o}, \\boldsymbol{v_c}, \\boldsymbol{U}) & = \\frac{\\partial J_{\\mathrm{CE}}(\\theta)}{\\partial \\theta_i} \\frac{\\partial \\theta_i}{\\boldsymbol{v_c}} \\\\ & = (\\hat y_i - y_i) \\boldsymbol{u_i^{\\mathsf{T}}} \\end{align*} \\begin{align*} \\frac{\\partial }{\\partial \\boldsymbol{v_c}}J_{\\mathrm{softmax-CE}}(\\boldsymbol{o}, \\boldsymbol{v_c}, \\boldsymbol{U}) & = \\frac{\\partial J_{\\mathrm{CE}}(\\theta)}{\\partial \\theta_i} \\frac{\\partial \\theta_i}{\\boldsymbol{v_c}} \\\\ & = (\\hat y_i - y_i) \\boldsymbol{u_i^{\\mathsf{T}}} \\end{align*} Notice here the \\boldsymbol{u_i} \\boldsymbol{u_i} is a column vector. And the derivative of \\boldsymbol{y} = \\boldsymbol{a}^{\\mathsf{T}}\\boldsymbol{x} \\boldsymbol{y} = \\boldsymbol{a}^{\\mathsf{T}}\\boldsymbol{x} with respect to \\boldsymbol{x} \\boldsymbol{x} is \\boldsymbol{a}^{\\mathsf{T}} \\boldsymbol{a}^{\\mathsf{T}} . Written in vector form, we will get \\begin{equation*} \\frac{\\partial J}{\\partial \\boldsymbol{v_c}} = \\boldsymbol{U}(\\boldsymbol{\\hat y} - \\boldsymbol{y}) \\end{equation*} \\begin{equation*} \\frac{\\partial J}{\\partial \\boldsymbol{v_c}} = \\boldsymbol{U}(\\boldsymbol{\\hat y} - \\boldsymbol{y}) \\end{equation*} In the above gradient, U = [\\boldsymbol{u_1}, \\boldsymbol{u_1}, \\cdots, \\boldsymbol{u_W}] U = [\\boldsymbol{u_1}, \\boldsymbol{u_1}, \\cdots, \\boldsymbol{u_W}] is the matrix of all the output vectors. \\boldsymbol{u_1} \\boldsymbol{u_1} is a column word vector. The component \\boldsymbol{\\hat y} - \\boldsymbol{y} \\boldsymbol{\\hat y} - \\boldsymbol{y} is a also a column vector with length of W W . The above gradient can be viewed as scaling each output vector \\boldsymbol{u_i} \\boldsymbol{u_i} by the scaler \\hat y_i - y. \\hat y_i - y. Alternatively, the gradient can also be wrote as distributive form \\begin{equation*} \\frac{\\partial J}{\\partial \\boldsymbol{v_c}} = \\boldsymbol{U}\\boldsymbol{\\hat y} - \\boldsymbol{U}\\boldsymbol{y} = -\\boldsymbol{u_i} + \\sum_{w=1}^{W}\\hat y_w \\boldsymbol{u_w} \\end{equation*} \\begin{equation*} \\frac{\\partial J}{\\partial \\boldsymbol{v_c}} = \\boldsymbol{U}\\boldsymbol{\\hat y} - \\boldsymbol{U}\\boldsymbol{y} = -\\boldsymbol{u_i} + \\sum_{w=1}^{W}\\hat y_w \\boldsymbol{u_w} \\end{equation*} The index i i in the above equation is corresponding to the index of the none zero element in the one-hot vector \\boldsymbol{y} \\boldsymbol{y} . Here we can see \\boldsymbol{y} \\boldsymbol{y} as the true label of the output word.","title":"Gradient for center word"},{"location":"courses/cs224n/write-up/#gradient-for-output-word","text":"We can also calculate the gradient with respect to the output word. \\begin{align*} \\frac{\\partial }{\\partial \\boldsymbol{u_w}}J_{\\mathrm{softmax-CE}}(\\boldsymbol{o}, \\boldsymbol{v_c}, \\boldsymbol{U}) & = \\frac{\\partial J_{\\mathrm{CE}}(\\theta)}{\\partial \\theta_i} \\frac{\\partial \\theta_i}{\\boldsymbol{u_w}} \\\\ & = (\\hat y_i - y_i) \\boldsymbol{v_c} \\end{align*} \\begin{align*} \\frac{\\partial }{\\partial \\boldsymbol{u_w}}J_{\\mathrm{softmax-CE}}(\\boldsymbol{o}, \\boldsymbol{v_c}, \\boldsymbol{U}) & = \\frac{\\partial J_{\\mathrm{CE}}(\\theta)}{\\partial \\theta_i} \\frac{\\partial \\theta_i}{\\boldsymbol{u_w}} \\\\ & = (\\hat y_i - y_i) \\boldsymbol{v_c} \\end{align*} Notice here we apply \\frac{\\partial \\boldsymbol{a}^{\\mathsf{T}} \\boldsymbol{x}}{\\partial \\boldsymbol{a}} = x \\frac{\\partial \\boldsymbol{a}^{\\mathsf{T}} \\boldsymbol{x}}{\\partial \\boldsymbol{a}} = x . Writen the gradient in matrix format, we have \\begin{equation*} \\frac{\\partial J}{\\partial \\boldsymbol{U}} = \\boldsymbol{v_c}(\\boldsymbol{\\hat y} - \\boldsymbol{y})^{\\mathsf{T}} \\end{equation*} \\begin{equation*} \\frac{\\partial J}{\\partial \\boldsymbol{U}} = \\boldsymbol{v_c}(\\boldsymbol{\\hat y} - \\boldsymbol{y})^{\\mathsf{T}} \\end{equation*} Notice the shape of the gradient. It determines how the notation looks like. In the above notation notice the shape of the output word gradient \\frac{\\partial J}{\\partial \\boldsymbol{U}} \\frac{\\partial J}{\\partial \\boldsymbol{U}} is d \\times W d \\times W . As we will discussion in the next post that it is a convention to make the shape of gradient as the shape of the input vectors. in this case the shape of U U and \\frac{\\partial J}{\\partial \\boldsymbol{U}} \\frac{\\partial J}{\\partial \\boldsymbol{U}} are the same.","title":"Gradient for output word"},{"location":"courses/cs224n/write-up/#negative-sampling","text":"The cost function for a single word prediction using nagative sampling is the following \\begin{align*} J_{\\mathrm{neg-sample}}(\\boldsymbol{o},\\boldsymbol{v_c},\\boldsymbol{U}) & = -\\log(\\sigma(\\boldsymbol{u_o}^{\\mathsf{T}} \\boldsymbol{v_c})) - \\sum_{k=1}^{K} \\log(\\sigma(-\\boldsymbol{u_k}^{\\mathsf{T}} \\boldsymbol{v_c})) \\end{align*} \\begin{align*} J_{\\mathrm{neg-sample}}(\\boldsymbol{o},\\boldsymbol{v_c},\\boldsymbol{U}) & = -\\log(\\sigma(\\boldsymbol{u_o}^{\\mathsf{T}} \\boldsymbol{v_c})) - \\sum_{k=1}^{K} \\log(\\sigma(-\\boldsymbol{u_k}^{\\mathsf{T}} \\boldsymbol{v_c})) \\end{align*} It comes from the original paper by Mikolov et al. \\sigma \\sigma is the sigmoid function \\sigma(x) = \\frac{1}{1+e^{-x}} \\sigma(x) = \\frac{1}{1+e^{-x}} . The ideas is to reduce the optimization computation by only sampling a small part of the vocabulary that have a lower probability of being context words of one another. The first lecture notes from CS224N discussed briefly the origin and intuition of the negative sampling loss function. Here we will focus on deriving the gradient and implementation ideas. With the fact that \\frac{\\mathrm{d}\\sigma(x)}{\\mathrm{d} x} = \\sigma(x)(1-\\sigma(x)) \\frac{\\mathrm{d}\\sigma(x)}{\\mathrm{d} x} = \\sigma(x)(1-\\sigma(x)) and the chain rule, it is not hard to derive the gradients result as following \\begin{align*} \\frac{\\partial J}{\\partial \\boldsymbol{v_c}} & = (\\sigma(\\boldsymbol{u_o}^{\\mathsf{T}}\\boldsymbol{v_c}) - 1)\\boldsymbol{u_o} - \\sum_{k=1}^{K}(\\sigma(-\\boldsymbol{u_k}^{\\mathsf{T}}\\boldsymbol{v_c}) - 1)\\boldsymbol{u_k} \\\\ \\frac{\\partial J}{\\partial \\boldsymbol{u_o}} & = (\\sigma(\\boldsymbol{u_o}^{\\mathsf{T}}\\boldsymbol{v_c}) - 1)\\boldsymbol{v_c} \\\\ \\frac{\\partial J}{\\partial \\boldsymbol{u_k}} & = -\\sigma(-\\boldsymbol{u_k}^{\\mathsf{T}}\\boldsymbol{v_c}) - 1)\\boldsymbol{v_c}, \\mathrm{for\\ all\\ } k = 1, 2, \\cdots, K \\end{align*} \\begin{align*} \\frac{\\partial J}{\\partial \\boldsymbol{v_c}} & = (\\sigma(\\boldsymbol{u_o}^{\\mathsf{T}}\\boldsymbol{v_c}) - 1)\\boldsymbol{u_o} - \\sum_{k=1}^{K}(\\sigma(-\\boldsymbol{u_k}^{\\mathsf{T}}\\boldsymbol{v_c}) - 1)\\boldsymbol{u_k} \\\\ \\frac{\\partial J}{\\partial \\boldsymbol{u_o}} & = (\\sigma(\\boldsymbol{u_o}^{\\mathsf{T}}\\boldsymbol{v_c}) - 1)\\boldsymbol{v_c} \\\\ \\frac{\\partial J}{\\partial \\boldsymbol{u_k}} & = -\\sigma(-\\boldsymbol{u_k}^{\\mathsf{T}}\\boldsymbol{v_c}) - 1)\\boldsymbol{v_c}, \\mathrm{for\\ all\\ } k = 1, 2, \\cdots, K \\end{align*} How to sample the \\boldsymbol{u_k} \\boldsymbol{u_k} in practice? The best known sampling method is based on the Unigram Model raise to the power of 3/4. Unigram Model is the counts of each word in the particular corpus (not the vocabulary).","title":"Negative sampling"},{"location":"courses/cs224n/write-up/#gradient-for-all-of-word-vectors","text":"Since skip-gram model is using one center word to predict all its context words. Given a word context size of m m , we obtain a set of context words [\\mathrm{word}_{c-m}, \\cdots, \\mathrm{word}_{c-1}, \\mathrm{word}_{c}, \\mathrm{word}_{c+1}, \\cdots, \\mathrm{word}_{c+m}] [\\mathrm{word}_{c-m}, \\cdots, \\mathrm{word}_{c-1}, \\mathrm{word}_{c}, \\mathrm{word}_{c+1}, \\cdots, \\mathrm{word}_{c+m}] . For each window, We need to predict 2m 2m context word given the center word. Denote the \"input\" and \"output\" word vectors for \\mathrm{word}_k \\mathrm{word}_k as \\boldsymbol{v}_k \\boldsymbol{v}_k and \\boldsymbol{u}_k \\boldsymbol{u}_k respectively. The cost for the entire context window with size m m centered around \\mathrm{word}_c \\mathrm{word}_c would be \\begin{equation*} J_{\\mathrm{skip-gram}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m}) = \\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} F(\\boldsymbol{u}_{c+j}, \\boldsymbol{v}_c). \\end{equation*} \\begin{equation*} J_{\\mathrm{skip-gram}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m}) = \\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} F(\\boldsymbol{u}_{c+j}, \\boldsymbol{v}_c). \\end{equation*} F F is a placeholder notation to represent the cost function given the center word for different model. Therefore for skip-gram, the gradients for the cost of one context window are \\begin{align*} \\frac{J_{\\mathrm{skip-gram}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{U}} & = \\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} \\frac{\\partial F(\\boldsymbol{u}_{c+j}, \\boldsymbol{v}_c)}{\\partial \\boldsymbol{v}_c},\\\\ \\frac{J_{\\mathrm{skip-gram}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{v}_c} & = \\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} \\frac{\\partial F(\\boldsymbol{u}_{c+j}, \\boldsymbol{v}_c)}{\\partial \\boldsymbol{v}_c}, \\\\ \\frac{J_{\\mathrm{skip-gram}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{v}_j} & = 0, \\forall j \\ne c \\end{align*} \\begin{align*} \\frac{J_{\\mathrm{skip-gram}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{U}} & = \\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} \\frac{\\partial F(\\boldsymbol{u}_{c+j}, \\boldsymbol{v}_c)}{\\partial \\boldsymbol{v}_c},\\\\ \\frac{J_{\\mathrm{skip-gram}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{v}_c} & = \\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} \\frac{\\partial F(\\boldsymbol{u}_{c+j}, \\boldsymbol{v}_c)}{\\partial \\boldsymbol{v}_c}, \\\\ \\frac{J_{\\mathrm{skip-gram}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{v}_j} & = 0, \\forall j \\ne c \\end{align*}","title":"Gradient for all of word vectors"},{"location":"courses/cs224n/write-up/#cbow-model","text":"Continuous bag-of-words (CBOW) is using the context words to predect the center words. Different from skip-gram model, in CBOW, we will use 2m 2m context word vectors as we predict probability of a word is the center word. For a simple variant of CBOW, we could sum up all the 2m 2m word vectors in one context vector \\hat{\\boldsymbol{v}} \\hat{\\boldsymbol{v}} and use the similar cost function with \\mathrm{softmax} \\mathrm{softmax} as we did in skip-gram model. \\begin{equation*} \\hat{\\boldsymbol{v}} = \\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} \\boldsymbol{v}_{c+j}. \\end{equation*} \\begin{equation*} \\hat{\\boldsymbol{v}} = \\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} \\boldsymbol{v}_{c+j}. \\end{equation*} Similar to skip-gram, we have \\begin{align*} \\frac{J_{\\mathrm{CBOW}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{U}} & = \\frac{\\partial F(\\boldsymbol{u}_{c}, \\hat{\\boldsymbol{v}})}{\\partial \\boldsymbol{U}},\\\\ \\frac{J_{\\mathrm{CBOW}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{v}_j} & = \\frac{\\partial F(\\boldsymbol{u}_{c}, \\hat{\\boldsymbol{v}})}{\\partial \\boldsymbol{v}_j}, \\forall j \\in \\{c-m, \\cdots, c-1, c+1, \\cdots, c+m\\}\\\\ \\frac{J_{\\mathrm{CBOW}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{v}_j} & = 0, \\forall j \\notin \\{c-m, \\cdots, c-1, c+1, \\cdots, c+m\\}. \\end{align*} \\begin{align*} \\frac{J_{\\mathrm{CBOW}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{U}} & = \\frac{\\partial F(\\boldsymbol{u}_{c}, \\hat{\\boldsymbol{v}})}{\\partial \\boldsymbol{U}},\\\\ \\frac{J_{\\mathrm{CBOW}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{v}_j} & = \\frac{\\partial F(\\boldsymbol{u}_{c}, \\hat{\\boldsymbol{v}})}{\\partial \\boldsymbol{v}_j}, \\forall j \\in \\{c-m, \\cdots, c-1, c+1, \\cdots, c+m\\}\\\\ \\frac{J_{\\mathrm{CBOW}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{v}_j} & = 0, \\forall j \\notin \\{c-m, \\cdots, c-1, c+1, \\cdots, c+m\\}. \\end{align*}","title":"CBOW model"},{"location":"courses/cs224n/write-up/#glove-model","text":"TODO, the paper and lecture.","title":"GloVe model"},{"location":"courses/cs224n/write-up/#word-vector-evaluation","text":"","title":"Word vector evaluation"},{"location":"courses/cs224n/write-up/#summary","text":"This post focused on gradient derivation for various word embedding models. The bootstrap model is based on the distributional semantic, by which to predict the probability of a word given some other words from the fixed corpus. We use the \\mathrm{softmax} \\mathrm{softmax} function to compute the probability. To update the word vectors, we introduce the likelihood function and derived its gradient. In that derivation, the parameter \\boldsymbol{\\theta} \\boldsymbol{\\theta} is the hyper parameters related to word vector that can be used to compute the probability. We continue to derive the gradient of the cross entropy loss function for a single word prediction. The result gradient of the cross entropy loss function is the key for all our later gradient derivation. After that we introduce the word2vec family of word embedding, skip-gram and CBOW models. In word2vec we use the dot product of the center word and its context words to compute the probability. We derive the gradient with respect to both center word and context word when using cross entropy loss function. Negative sampling is also discussed as it improves the computation cost by many factors. With the discussion of skip-gram model, CBOW model is easier to present. One thing you need to distinguish is that whether the gradient is for a particular word prediction, for the whole window of the words, or the over all objective for the corpus. For skip-gram, we first compute the gradient for each word prediction in the given context window, then sum up all the gradient to update the cost function of that window. We move the window over the corpus until finish all the word updates. This whole process is only one update. While using negative sampling, the process becomes more efficient, we don't have to go through all the window, only sampling K K windows with Unigram Model raise to the 3/4 3/4 power. In the simple CBOW model discussed, we add up all the context vectors first, then only update a single gradient for that window corresponding to the center word. We repeat this process for different windows to complete one update.","title":"Summary"},{"location":"courses/cs224n/write-up/#part-2-neural-networks-and-backpropagation","text":"Date: April 15th 2019 We discussed the softmax classifier in Part 1 and its major drawback that the classifier only gives linear decision boundaries. In Part 2, Neural Networks will be introduced to demonstrate that it can learn much more complex functions and nonlinear decision boundaries.","title":"Part 2 Neural Networks and Backpropagation"},{"location":"courses/cs224n/write-up/#intro-to-neural-networks","text":"Biological Neuron Mathematical Model Simplified Neuron The neuron can be modeled as a binary logistic regression unit as in the last row of the table above. It can be further simplified as following functions, \\begin{equation*} h_{w, b} (\\boldsymbol{x})=f(\\boldsymbol{w}^\\mathsf{T} \\boldsymbol{x} + b) \\\\ f(z) = \\frac{1}{1 + e^{-z}}. \\end{equation*} \\begin{equation*} h_{w, b} (\\boldsymbol{x})=f(\\boldsymbol{w}^\\mathsf{T} \\boldsymbol{x} + b) \\\\ f(z) = \\frac{1}{1 + e^{-z}}. \\end{equation*} \\boldsymbol{x} \\boldsymbol{x} is the inputs \\boldsymbol{w} \\boldsymbol{w} is the weights b b is a bias term h h is a hidden layer function f f is a nonlinear activation function (sigmoid, tanh, etc.) If we feed a vector of inputs through a bunch of logistic regression (sigmoid) functions, then we get a vector of outputs. The CS224N lecture note 3 section 1.2 have a complete derivation on multiple sigmoid units. But we don\u2019t have to decide ahead of time what variables these logistic regressions are trying to predict! It is the loss function that will direct what the intermediate hidden variables should be, so as to do a good job at predicting the targets for the next layer, etc.","title":"Intro to Neural Networks"},{"location":"courses/cs224n/write-up/#forward-propagation-in-matrix-notation","text":"In a multilayer neural network, not only did we have multiple sigmoid units, but we also have more than one layer. Let's explicitly write down the signal transformation (aka forward propagation) from one layer to another referring to this network from Andrew's ML course. We will use the following notation convention a^{(j)}_i a^{(j)}_i to represent the \"activation\" of unit i i in layer j j . W^{(j)} W^{(j)} to represent the matrix of weights controlling function mapping from layer j j to layer j + 1 j + 1 . The value at each node can be calculated as \\begin{equation*} a^{(2)}_1 = f(w^{(1)}_{10} x_0 + w^{(1)}_{11} x_1 + w^{(1)}_{12} x_2 + w^{(1)}_{13} x_3) \\\\ a^{(2)}_2 = f(w^{(1)}_{20} x_0 + w^{(1)}_{21} x_1 + w^{(1)}_{22} x_2 + w^{(1)}_{23} x_3) \\\\ a^{(2)}_3 = f(w^{(1)}_{30} x_0 + w^{(1)}_{31} x_1 + w^{(1)}_{32} x_2 + w^{(1)}_{33} x_3) \\\\ \\text{and} \\\\ h_{w} (\\boldsymbol{x}) = a^{(3)}_1 = f(w^{(2)}_{10} a^{(2)}_0 + w^{(2)}_{11} a^{(2)}_1 + w^{(2)}_{12} a^{(2)}_2 + w^{(2)}_{13} a^{(2)}_3) \\end{equation*} \\begin{equation*} a^{(2)}_1 = f(w^{(1)}_{10} x_0 + w^{(1)}_{11} x_1 + w^{(1)}_{12} x_2 + w^{(1)}_{13} x_3) \\\\ a^{(2)}_2 = f(w^{(1)}_{20} x_0 + w^{(1)}_{21} x_1 + w^{(1)}_{22} x_2 + w^{(1)}_{23} x_3) \\\\ a^{(2)}_3 = f(w^{(1)}_{30} x_0 + w^{(1)}_{31} x_1 + w^{(1)}_{32} x_2 + w^{(1)}_{33} x_3) \\\\ \\text{and} \\\\ h_{w} (\\boldsymbol{x}) = a^{(3)}_1 = f(w^{(2)}_{10} a^{(2)}_0 + w^{(2)}_{11} a^{(2)}_1 + w^{(2)}_{12} a^{(2)}_2 + w^{(2)}_{13} a^{(2)}_3) \\end{equation*} write the matrix W^{(j)} W^{(j)} explicity, \\begin{equation*} W^{(1)} = \\begin{bmatrix} w^{(1)}_{10} & w^{(1)}_{11} & w^{(1)}_{12} & w^{(1)}_{13} \\\\ w^{(1)}_{20} & w^{(1)}_{21} & w^{(1)}_{22} & w^{(1)}_{23} \\\\ w^{(1)}_{30} & w^{(1)}_{31} & w^{(1)}_{32} & w^{(1)}_{33} \\end{bmatrix} \\end{equation*} \\begin{equation*} W^{(1)} = \\begin{bmatrix} w^{(1)}_{10} & w^{(1)}_{11} & w^{(1)}_{12} & w^{(1)}_{13} \\\\ w^{(1)}_{20} & w^{(1)}_{21} & w^{(1)}_{22} & w^{(1)}_{23} \\\\ w^{(1)}_{30} & w^{(1)}_{31} & w^{(1)}_{32} & w^{(1)}_{33} \\end{bmatrix} \\end{equation*} \\begin{equation*} W^{(2)} = \\begin{bmatrix} w^{(2)}_{10} & w^{(2)}_{11} & w^{(2)}_{12} & w^{(2)}_{13} \\end{bmatrix} \\end{equation*} \\begin{equation*} W^{(2)} = \\begin{bmatrix} w^{(2)}_{10} & w^{(2)}_{11} & w^{(2)}_{12} & w^{(2)}_{13} \\end{bmatrix} \\end{equation*} With the above form, we can use matrix notation as \\begin{align*} & \\boldsymbol{a}^{(2)} = f(\\boldsymbol{W}^{(1)} \\boldsymbol{x}) \\\\ h_{w} (\\boldsymbol{x}) = & \\boldsymbol{a}^{(3)} = f(\\boldsymbol{W}^{(2)} \\boldsymbol{a}^{(2)}) \\end{align*} \\begin{align*} & \\boldsymbol{a}^{(2)} = f(\\boldsymbol{W}^{(1)} \\boldsymbol{x}) \\\\ h_{w} (\\boldsymbol{x}) = & \\boldsymbol{a}^{(3)} = f(\\boldsymbol{W}^{(2)} \\boldsymbol{a}^{(2)}) \\end{align*} We can see from above notations that if the network has s_j s_j units in layer j j and s_{j+1} s_{j+1} in layer j + 1 j + 1 , the matrix W^{(j)} W^{(j)} will be of dimention s_{j+1} \\times s_{j}+1 s_{j+1} \\times s_{j}+1 . It could be interpreted as \"the dimention of W^{(j)} W^{(j)} is the number of nodes in the next layer (layer j + 1 j + 1 ) \\times \\times the number of nodes in the current layer + + 1. Note that in cs224n the matrix notation is slightly different \\begin{align*} & \\boldsymbol{a}^{(2)} = f(\\boldsymbol{W}^{(1)} \\boldsymbol{x} + \\boldsymbol{b}^{(1)}) \\\\ h_{w} (\\boldsymbol{x}) = & \\boldsymbol{a}^{(3)} = f(\\boldsymbol{W}^{(2)} \\boldsymbol{a}^{(2)} + \\boldsymbol{b}^{(2)}) \\end{align*} \\begin{align*} & \\boldsymbol{a}^{(2)} = f(\\boldsymbol{W}^{(1)} \\boldsymbol{x} + \\boldsymbol{b}^{(1)}) \\\\ h_{w} (\\boldsymbol{x}) = & \\boldsymbol{a}^{(3)} = f(\\boldsymbol{W}^{(2)} \\boldsymbol{a}^{(2)} + \\boldsymbol{b}^{(2)}) \\end{align*} the different is from how we denote the bias. The two are enssentially the same, but be cautions that the matrix dimentions are different.","title":"Forward Propagation in Matrix Notation"},{"location":"courses/cs224n/write-up/#word-window-classification-using-neural-networks","text":"From now on, let's switch the notation to cs224n so that we can derive the backpropagation algorithm for word window classification and get more intuition about the backprop. The drawing of the neural nets in cs224n for word window classification is less dynamic and slitly different from the drawing from Andrew's ML class. The figure from cs224n may be slightly confusing at first, but it is good to understand it from this word window classification application.","title":"Word Window Classification Using Neural Networks"},{"location":"courses/cs224n/write-up/#forward-propagation","text":"Firstly, the goal of this classification task is to classify whether the center word is a location. Similar to word2vec, we will go over all positions in a corpus. But this time, it will be supervised and only some positions should get a high score. The figure above illustrate the feed-forward process. We use the method by Collobert & Weston (2008, 2011). An unnormalized score will be calculated from the activation \\boldsymbol{a} = [a_1, a_2, a_3, \\cdots]^\\mathsf{T} \\boldsymbol{a} = [a_1, a_2, a_3, \\cdots]^\\mathsf{T} . \\begin{equation*} \\text{scroe}(\\boldsymbol{x}) = U^\\mathsf{T} \\boldsymbol{a} \\in \\mathbb{R} \\end{equation*} \\begin{equation*} \\text{scroe}(\\boldsymbol{x}) = U^\\mathsf{T} \\boldsymbol{a} \\in \\mathbb{R} \\end{equation*} We will use max-margin loss as our loss function. The training is essentially to find the optimal weights W W by minimize the max-margin loss \\begin{equation*} J = \\max(0, 1 - s + s_c) \\end{equation*} \\begin{equation*} J = \\max(0, 1 - s + s_c) \\end{equation*} s s is the score of a window that have a location in the center. s_c s_c is the score of a window that doesn't have a location in the center. For full objective function: Sample several corrupt windows per true one. Sum over all training windows. It is similar to negative sampling in word2vec. We will use gradient descent to update the parameter so as to minimize the loss function. The key is how to calculate the gradient with respect to the model parameters, namely \\nabla_{\\theta} J(\\theta) \\nabla_{\\theta} J(\\theta) . Here we use \\theta \\theta to represent the hyperthetic parameters, it can include the W W and other parameters of the model. \\begin{equation*} \\theta^{\\text{new}} = \\theta^{\\text{old}} - \\alpha\\nabla_{\\theta} J(\\theta) \\end{equation*} \\begin{equation*} \\theta^{\\text{new}} = \\theta^{\\text{old}} - \\alpha\\nabla_{\\theta} J(\\theta) \\end{equation*}","title":"Forward propagation"},{"location":"courses/cs224n/write-up/#gradients-and-jacobians-matrix","text":"At first, let us layout the input and all the equations in this simple neural network. Input Layer Hidden Layer Output Layer \\boldsymbol{x} \\boldsymbol{x} \\boldsymbol{z} = \\boldsymbol{W} \\boldsymbol{x} + \\boldsymbol{b}, \\\\ \\boldsymbol{h} = f(\\boldsymbol{z}) \\boldsymbol{z} = \\boldsymbol{W} \\boldsymbol{x} + \\boldsymbol{b}, \\\\ \\boldsymbol{h} = f(\\boldsymbol{z}) s = \\boldsymbol{u}^{\\mathsf{T}}\\boldsymbol{h} s = \\boldsymbol{u}^{\\mathsf{T}}\\boldsymbol{h} To update the parameters in this model, namely \\boldsymbol{x}, \\boldsymbol{W}, \\boldsymbol{b} \\boldsymbol{x}, \\boldsymbol{W}, \\boldsymbol{b} , we would like to compute the derivitavies of s s with respect to all these parameters. We have to use the chain rule to compute it. What chain rule says is that we can compute the partial derivatives of each individual functions and then multiply them together to get the derivative with respect the specific variable. For example, \\frac{\\partial s}{\\partial \\boldsymbol{b}} \\frac{\\partial s}{\\partial \\boldsymbol{b}} is computed as \\frac{\\partial s}{\\partial \\boldsymbol{b}} = \\frac{\\partial s}{\\partial \\boldsymbol{h}} \\cdot \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\cdot \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} \\frac{\\partial s}{\\partial \\boldsymbol{b}} = \\frac{\\partial s}{\\partial \\boldsymbol{h}} \\cdot \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\cdot \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} . This seems very easy to understand, but when it comes to implemenation in vectorized format, it become confusing for those who doesn't work on matrix calculus for quite a while like me. I want to get the points straight here. What exactly is \\frac{\\partial s}{\\partial \\boldsymbol{b}} \\frac{\\partial s}{\\partial \\boldsymbol{b}} and \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} . Note both \\boldsymbol{h} \\boldsymbol{h} and \\boldsymbol{z} \\boldsymbol{z} are vectors. To calculate these two gradient, simply remember the following two rules: Given a function with 1 output and n n inputs f(\\boldsymbol{x}) = f(x_1, x_2, \\cdots, x_n) f(\\boldsymbol{x}) = f(x_1, x_2, \\cdots, x_n) , it's gradient is a vector of partial derivatives with respect to each input (take the gradient element wise). $$ \\frac{\\partial f}{\\partial \\boldsymbol{x}} = \\Bigg [ \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\cdots, \\frac{\\partial f}{\\partial x_n}, \\Bigg ] $$ Given a function with m m output and n n inputs $$ \\boldsymbol{f}(\\boldsymbol{x}) = \\big [f_1(x_1, x_2, \\cdots, x_n), \\cdots, f_m(x_1, x_2, \\cdots, x_n) \\big ], $$ it's gradient is an m \\times n m \\times n matrix of partial derivatives (\\frac{\\partial \\boldsymbol{f}}{\\partial \\boldsymbol{x}})_{ij} = \\dfrac{\\partial f_i}{\\partial x_j} (\\frac{\\partial \\boldsymbol{f}}{\\partial \\boldsymbol{x}})_{ij} = \\dfrac{\\partial f_i}{\\partial x_j} . This matrix is also called Jacobian matrix. \\begin{equation*} \\frac{\\partial \\boldsymbol{f}}{\\partial \\boldsymbol{x}} = \\begin{bmatrix} \\dfrac{\\partial \\boldsymbol{f}}{\\partial x_1} & \\cdots & \\dfrac{\\partial \\boldsymbol{f}}{\\partial x_n} \\end{bmatrix} = \\begin{bmatrix} \\dfrac{\\partial f_1}{\\partial x_1} & \\cdots & \\dfrac{\\partial f_1}{\\partial x_n}\\\\ \\vdots & \\ddots & \\vdots\\\\ \\dfrac{\\partial f_m}{\\partial x_1} & \\cdots & \\dfrac{\\partial f_m}{\\partial x_n} \\end{bmatrix} \\end{equation*} \\begin{equation*} \\frac{\\partial \\boldsymbol{f}}{\\partial \\boldsymbol{x}} = \\begin{bmatrix} \\dfrac{\\partial \\boldsymbol{f}}{\\partial x_1} & \\cdots & \\dfrac{\\partial \\boldsymbol{f}}{\\partial x_n} \\end{bmatrix} = \\begin{bmatrix} \\dfrac{\\partial f_1}{\\partial x_1} & \\cdots & \\dfrac{\\partial f_1}{\\partial x_n}\\\\ \\vdots & \\ddots & \\vdots\\\\ \\dfrac{\\partial f_m}{\\partial x_1} & \\cdots & \\dfrac{\\partial f_m}{\\partial x_n} \\end{bmatrix} \\end{equation*}","title":"Gradients and Jacobians Matrix"},{"location":"courses/cs224n/write-up/#computing-gradients-with-the-chain-rule","text":"With these two rules we can calculate the partials. We will use \\frac{\\partial s}{\\partial \\boldsymbol{b}} \\frac{\\partial s}{\\partial \\boldsymbol{b}} as an example. \\begin{equation*} \\frac{\\partial s}{\\partial \\boldsymbol{b}} = \\frac{\\partial s}{\\partial \\boldsymbol{h}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} \\end{equation*} \\begin{equation*} \\frac{\\partial s}{\\partial \\boldsymbol{b}} = \\frac{\\partial s}{\\partial \\boldsymbol{h}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} \\end{equation*} \\frac{\\partial s}{\\partial \\boldsymbol{h}} = \\frac{\\partial}{\\partial \\boldsymbol{h}}(\\boldsymbol{u}^{\\mathsf{T}}\\boldsymbol{h}) = \\boldsymbol{u}^{\\mathsf{T}} \\frac{\\partial s}{\\partial \\boldsymbol{h}} = \\frac{\\partial}{\\partial \\boldsymbol{h}}(\\boldsymbol{u}^{\\mathsf{T}}\\boldsymbol{h}) = \\boldsymbol{u}^{\\mathsf{T}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} = \\frac{\\partial}{\\partial \\boldsymbol{z}}(f(\\boldsymbol{z})) = \\begin{pmatrix} f'(z_1) & & 0 \\\\ & \\ddots & \\\\ 0 & & f'(z_n) \\end{pmatrix} = \\mathrm{diag} (f' (\\boldsymbol{z})) \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} = \\frac{\\partial}{\\partial \\boldsymbol{z}}(f(\\boldsymbol{z})) = \\begin{pmatrix} f'(z_1) & & 0 \\\\ & \\ddots & \\\\ 0 & & f'(z_n) \\end{pmatrix} = \\mathrm{diag} (f' (\\boldsymbol{z})) \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} = \\frac{\\partial}{\\partial \\boldsymbol{b}}(\\boldsymbol{W}\\boldsymbol{x} + \\boldsymbol{b}) = \\boldsymbol{I} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} = \\frac{\\partial}{\\partial \\boldsymbol{b}}(\\boldsymbol{W}\\boldsymbol{x} + \\boldsymbol{b}) = \\boldsymbol{I} Therefore, \\begin{equation*} \\frac{\\partial s}{\\partial \\boldsymbol{b}} = \\boldsymbol{u}^{\\mathsf{T}} \\mathrm{diag}(f'(\\boldsymbol{z})) \\boldsymbol{I} = \\boldsymbol{u}^{\\mathsf{T}} \\circ f'(\\boldsymbol{z}) \\end{equation*} \\begin{equation*} \\frac{\\partial s}{\\partial \\boldsymbol{b}} = \\boldsymbol{u}^{\\mathsf{T}} \\mathrm{diag}(f'(\\boldsymbol{z})) \\boldsymbol{I} = \\boldsymbol{u}^{\\mathsf{T}} \\circ f'(\\boldsymbol{z}) \\end{equation*} Similarly we can calculate the partial with respect to \\boldsymbol{W} \\boldsymbol{W} and \\boldsymbol{x} \\boldsymbol{x} . Since the top layer partials are already calculated, we can reuese the results. We denote those reusable partials as \\delta \\delta meaning local error signal. \\begin{align*} \\frac{\\partial s}{\\partial \\boldsymbol{W}} & = \\frac{\\partial s}{\\partial \\boldsymbol{h}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{W}} \\\\ \\\\ \\frac{\\partial s}{\\partial \\boldsymbol{x}} & = \\frac{\\partial s}{\\partial \\boldsymbol{h}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{x}} \\\\ \\\\ \\frac{\\partial s}{\\partial \\boldsymbol{b}} & = \\underbrace{\\frac{\\partial s}{\\partial \\boldsymbol{h}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}}}_{\\mathrm{reuse}} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} = \\boldsymbol{\\delta} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} = \\boldsymbol{\\delta} = \\boldsymbol{u}^{\\mathsf{T}} \\circ f'(\\boldsymbol{z}) \\end{align*} \\begin{align*} \\frac{\\partial s}{\\partial \\boldsymbol{W}} & = \\frac{\\partial s}{\\partial \\boldsymbol{h}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{W}} \\\\ \\\\ \\frac{\\partial s}{\\partial \\boldsymbol{x}} & = \\frac{\\partial s}{\\partial \\boldsymbol{h}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{x}} \\\\ \\\\ \\frac{\\partial s}{\\partial \\boldsymbol{b}} & = \\underbrace{\\frac{\\partial s}{\\partial \\boldsymbol{h}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}}}_{\\mathrm{reuse}} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} = \\boldsymbol{\\delta} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} = \\boldsymbol{\\delta} = \\boldsymbol{u}^{\\mathsf{T}} \\circ f'(\\boldsymbol{z}) \\end{align*}","title":"Computing gradients with the chain rule"},{"location":"courses/cs224n/write-up/#shape-convention","text":"What does the shape of the derivatives looks like in practice? How can we make the chain rule computation efficient? According to the aforementioned gradient calculation rules, \\frac{\\partial s}{\\partial \\boldsymbol{W}} \\frac{\\partial s}{\\partial \\boldsymbol{W}} is a row vector. The chain rule gave \\begin{equation*} \\frac{\\partial s}{\\partial \\boldsymbol{W}} = \\boldsymbol{\\delta} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{W}}. \\end{equation*} \\begin{equation*} \\frac{\\partial s}{\\partial \\boldsymbol{W}} = \\boldsymbol{\\delta} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{W}}. \\end{equation*} We know from Jacobians that \\frac{\\partial\\boldsymbol{z}}{\\partial \\boldsymbol{W}} = \\frac{\\partial}{\\partial \\boldsymbol{W}}(\\boldsymbol{W}\\boldsymbol{x} + \\boldsymbol{b}) = \\boldsymbol{x}^{\\mathsf{T}} \\frac{\\partial\\boldsymbol{z}}{\\partial \\boldsymbol{W}} = \\frac{\\partial}{\\partial \\boldsymbol{W}}(\\boldsymbol{W}\\boldsymbol{x} + \\boldsymbol{b}) = \\boldsymbol{x}^{\\mathsf{T}} . We may arrived the result that \\frac{\\partial s}{\\partial \\boldsymbol{W}} = \\boldsymbol{\\delta} \\ \\boldsymbol{x}^{\\mathsf{T}} \\frac{\\partial s}{\\partial \\boldsymbol{W}} = \\boldsymbol{\\delta} \\ \\boldsymbol{x}^{\\mathsf{T}} . This is actually not quite wirte. The correct form should be \\begin{align*} \\underbrace{\\frac{\\partial s}{\\partial \\boldsymbol{W}}}_{n \\times m} = \\underbrace{\\boldsymbol{\\delta}^{\\mathsf{T}}}_{n \\times 1} \\ \\underbrace{\\boldsymbol{x}^{\\mathsf{T}}}_{1 \\times m}. \\end{align*} \\begin{align*} \\underbrace{\\frac{\\partial s}{\\partial \\boldsymbol{W}}}_{n \\times m} = \\underbrace{\\boldsymbol{\\delta}^{\\mathsf{T}}}_{n \\times 1} \\ \\underbrace{\\boldsymbol{x}^{\\mathsf{T}}}_{1 \\times m}. \\end{align*} Note You may wonder why is this form instead of the one we derived directly. The explanation from the CS224N is that we would like to follow the shape convention so as to make the chain rule implementation more efficient (matrix multiplication instead of loops). Different from the Jacobian form, shape convention states that the shape of the gradient is the same shape of parameters. The resolution here is to use Jacobian form as much as possible and reshape to follow the convention at the end. Because Jacobian form makes the chain rule easy and the shape convention makes the implementation of gradient descent easy.","title":"Shape convention"},{"location":"courses/cs224n/write-up/#computation-graphs-and-backpropagation","text":"We have shown how to compute the partial derivatives using the chain rule. This is almost the backpropagation algorithm. If we want to add on to that and make the algorithm complete, the only thing we need is how to reuse gradient computed for higher layers. We leverage the computation graph to explain this. From the computation graph, you'll get the intuition of reusing the partial derivatives computed for higher layers in computing derivatives for lower layers so as to minimize computation. We represent our neural net equations as a computation graph as following: source nodes represent inputs interior nodes represent operations edges pass along result of the operation When following computation order to carry out the computations from inputs, it is forward propagation. Backpropagation is to pass along gradients backwards. This can be illustrated in the following computation graph. The partial derivative with respect to a parameter reflect how changing the parameter would effect the output value. The output value in the backprop is usually a loss function (or error). Intuitively, you can see backprop is push the error back to the lower layer through a bunch of operation nodes, the arrived error is a measure of the error at that particular layer, training is try to reduce this backprop-ed error by adjusting the local parameters, the effect of reducing the local error will be forward propagated to the output, the error at the output should also reduced. We use gradient descent to make this process to converge as soon as possible. To understand it better, let's look at a single node. We define \"local gradient\" for each node as the gradient of it's output with respect to it's input. By chain rule, the downstream gradient is equal to the multiplication of the upstream gradient and the local gradient. When having multipe local gradients, they are pushed back to each input using chain rule. CS224N lecture 4 slides have step by step example of backprop. From the example, we can got some intuitions about some nodes' effects. For example, when push back gradients along outward branches from a single node, the gradients should be sumed up; \"+\" node distributes the upstream gradient to each summand; \"max\" node simply routes the upstream gradients. When update gradient with backprop, you should compute all the gradients at once. With this computation graph notion, following routine captures the gist of backprop in a very decent manner. Fprop: visit nodes in topological sorted order Compute value of node given predecessors Bprop: initialize output gradient = 1 visit nodes in reverse order: Compute gradient wrt each node using gradient wrt successors \\{y_1, y_2, \\cdots, y_2\\} = \\mathrm{successors\\ of\\ } x \\{y_1, y_2, \\cdots, y_2\\} = \\mathrm{successors\\ of\\ } x \\begin{equation*}\\frac{\\partial z}{\\partial \\boldsymbol{x}} = \\sum_{i=i}^n \\frac{\\partial z}{\\partial y_i} \\frac{\\partial y_i}{\\partial x} \\end{equation*} \\begin{equation*}\\frac{\\partial z}{\\partial \\boldsymbol{x}} = \\sum_{i=i}^n \\frac{\\partial z}{\\partial y_i} \\frac{\\partial y_i}{\\partial x} \\end{equation*} If done correctly, the big O O complexity of forward propagation and backpropagation is the same.","title":"Computation graphs and backpropagation"},{"location":"courses/cs224n/write-up/#automatic-differentiation","text":"The gradient computation can be automatically inferred from the symbolic expression of the fprop. but this is not commonly used in practice. Each node type needs to know how to compute its output and how to compute the gradient wrt its inputs given the gradient wrt its output. Modern DL frameworks (Tensorflow, PyTorch, etc.) do backpropagation for you but mainly leave layer/node writer to hand-calculate the local derivative. Following is a simple demo of how to implement forward propagation and backpropagation. class MultiplyGate ( object ): def forward ( x , y ): z = x * y self . x = x # must keep these around! self . y = y return z def backword ( dz ): dx = self . y * dz # [dz/dx * dL/dz] dy = self . x * dz # [dz/dy * dL/dz] return [ dx , dy ] nonlinear activation functions: sigmoid, tanh, hard tanh, ReLU Learning rates: start 0.001. power of 10. halve the learning rate every k epochs. formula: lr = lr_0 e^{-k t} lr = lr_0 e^{-k t} for each epoch t t","title":"Automatic differentiation"},{"location":"courses/cs224n/write-up/#regularization","text":"LFD starting point: L2 regularization","title":"Regularization"},{"location":"courses/cs224n/write-up/#part-3-language-model-and-rnn","text":"","title":"Part 3 Language Model and RNN"},{"location":"courses/cs224n/write-up/#n-gram","text":"assumption: \\boldsymbol{x}^{(t+1)} \\boldsymbol{x}^{(t+1)} depends only on previous n-1 n-1 words. Sparsity problem, --> backoff (n-1)-gram practically, n has to be less than 5.","title":"n-gram"},{"location":"courses/cs224n/write-up/#neural-language-model","text":"fixed-window neural language model Recurrent Neural Network","title":"Neural language model"},{"location":"courses/func-prog-in-scala/notes/","text":"Functional Programming Principles in Scala \u00b6 Week 1 \u00b6 Theory wise, pure imperative programming languages is limited by \"Von Neumann\" bottleneck Theory: one or more data types operations on these types laws that describe the relationship between values and operations Normally, a theory does not describe mutations (change something but keep the identity) Theories without Mutation Pylonomial String consequence in programming avoid mutations have a power way to abstract operations and functions. The substitution model lambda calculus only to expression don't have a side effect termination def loop : Int = loop unreduced arguments Call-by-name and call-by-value def test ( x : Int , y : Int ) = x * x test ( 2 , 3 ) test ( 3 + 4 , 8 ) test ( 7 , 2 * 4 ) test ( 3 + 4 , 2 * 4 ) CBV, CBN, and termination CBV termination --> CBN termination Not vice versa example def first ( x : Int , y : Int ) = x first ( 7 , loop ) enforce call by name => Conditional expressions if-else not a statement, just a expression. Value and definitions scala > def loop : Boolean = loop && loop loop : Boolean scala > def x = loop x : Boolean scala > val x = loop java . lang . StackOverflowError at . loop (< console >: 12 ) Square root with Newton's Method def abs ( x : Double ) = if ( x < 0 ) - x else x def sqrtIter ( guess : Double , x : Double ) : Double = if ( isGoodEnough ( guess , x )) guess else sqrtIter ( improve ( guess , x ), x ) def isGoodEnough ( guess : Double , x : Double ) = abs ( guess * guess - x ) < 0.001 def improve ( guess : Double , x : Double ) = ( guess + x / guess ) / 2 def sqrt ( x : Double ) = sqrtIter ( 1.0 , x ) Tail recursion: Implementation consideration. If a function calls itself as its last action, the function's stack frame can be reused. This is called tail recursion. Tail recursive function are iterative processes. (anotation: @tailrec) gcd is tail recursion def gcd ( a : Int , b : Int ) : Int = { if ( b == 0 ) a else gcd ( b , a % b ) } factorial is not def factorial ( n : Int ) = if ( n == 0 ) 1 else n * factorial ( n - 1 ) but we can rewrite factorial in tail recursion form. def factorial ( n : Int ) : Int = { def loop ( acc : Int , n : Int ) : Int = if ( n == 0 ) acc else loop ( acc * n , n - 1 ) loop ( 1 , n ) }","title":"Func Prog Principles in Scala"},{"location":"courses/func-prog-in-scala/notes/#functional-programming-principles-in-scala","text":"","title":"Functional Programming Principles in Scala"},{"location":"courses/func-prog-in-scala/notes/#week-1","text":"Theory wise, pure imperative programming languages is limited by \"Von Neumann\" bottleneck Theory: one or more data types operations on these types laws that describe the relationship between values and operations Normally, a theory does not describe mutations (change something but keep the identity) Theories without Mutation Pylonomial String consequence in programming avoid mutations have a power way to abstract operations and functions. The substitution model lambda calculus only to expression don't have a side effect termination def loop : Int = loop unreduced arguments Call-by-name and call-by-value def test ( x : Int , y : Int ) = x * x test ( 2 , 3 ) test ( 3 + 4 , 8 ) test ( 7 , 2 * 4 ) test ( 3 + 4 , 2 * 4 ) CBV, CBN, and termination CBV termination --> CBN termination Not vice versa example def first ( x : Int , y : Int ) = x first ( 7 , loop ) enforce call by name => Conditional expressions if-else not a statement, just a expression. Value and definitions scala > def loop : Boolean = loop && loop loop : Boolean scala > def x = loop x : Boolean scala > val x = loop java . lang . StackOverflowError at . loop (< console >: 12 ) Square root with Newton's Method def abs ( x : Double ) = if ( x < 0 ) - x else x def sqrtIter ( guess : Double , x : Double ) : Double = if ( isGoodEnough ( guess , x )) guess else sqrtIter ( improve ( guess , x ), x ) def isGoodEnough ( guess : Double , x : Double ) = abs ( guess * guess - x ) < 0.001 def improve ( guess : Double , x : Double ) = ( guess + x / guess ) / 2 def sqrt ( x : Double ) = sqrtIter ( 1.0 , x ) Tail recursion: Implementation consideration. If a function calls itself as its last action, the function's stack frame can be reused. This is called tail recursion. Tail recursive function are iterative processes. (anotation: @tailrec) gcd is tail recursion def gcd ( a : Int , b : Int ) : Int = { if ( b == 0 ) a else gcd ( b , a % b ) } factorial is not def factorial ( n : Int ) = if ( n == 0 ) 1 else n * factorial ( n - 1 ) but we can rewrite factorial in tail recursion form. def factorial ( n : Int ) : Int = { def loop ( acc : Int , n : Int ) : Int = if ( n == 0 ) acc else loop ( acc * n , n - 1 ) loop ( 1 , n ) }","title":"Week 1"},{"location":"leetcode/array/notes/","text":"Array \u00b6 Category 1 Remove/Contains Duplidate \u00b6 Contains Duplicate \u00b6 Contains Duplicate II \u00b6 Contains Duplicate III \u00b6 Find the Duplicate Number \u00b6 Remove Duplicates from Sorted Array \u00b6 Remove Duplicates from Sorted Array II \u00b6 Remove Duplicates from Sorted List II \u00b6 Remove Duplicates from Sorted List \u00b6 Move Zeroes \u00b6 Category 2 Matrix problems \u00b6 Spiral Matrix \u00b6 Spiral Matrix II \u00b6 Search a 2D Matrix \u00b6 Search a 2D Matrix II \u00b6 Rotate Image \u00b6 Range Sum Query 2D - Mutable \u00b6 See Range Sum Query 2D - Mutable Range Sum Query 2D - Immutable \u00b6 See Range Sum Query 2D - Immutable Maximal Square \u00b6 Maximal Rectangle \u00b6 Category 3 Subarray problems \u00b6 Type of subarray problem \u00b6 Find a subarray fulfill certain propertie, i.e maximum size. Split into subarrays that fulfill certain properties, i.e. sum greater than k. Two types of prefix sum \u00b6 There are two ways to calculate the prefix sum array. Take which ever conveniece for your when solving a problem. Option 1: sums.resize(n, 0); nums: [1, 2, 3, 4, 5, 6, 7, 8, 9] i j sums: [1, 3, 6, 10, 15, 21, 28, 36, 45] In this case, each element sums[i] in sums represent the cumulative sum for indexes [0, ..., i] . In other words, sum[i] represent cumulative sum up to element i inclusive. When you want to get the range sum rangeSum(i, j) , you can get it in the following way: rangeSum ( i , j ) = sums [ j ] - sums [ i - 1 ] // i > 0 rangeSum ( i , j ) = sums [ j ] // i == 0 work with this option is a little complex, to get the rangeSum(i, j) : rangeSum ( i , j ) = sums [ j ] - sums [ i ] + nums [ i ] // i >= 0 Option 2: sums.resize(n + 1, 0); In this case, each element sums[i] in sums represent the prefix sum of the first i elements in original array nums. When you want to get the range sum by rangeSum(i, j) , you can get it in the following way: rangeSum ( i , j ) = sums [ j + 1 ] - sums [ i ] // i >= 0 Using prefix sum with map \u00b6 One of the core trick in solving the following subarray problems is to build a map from prefix sum to array index for efficient lookup. For example, problems with keywaords \"maximum size equal to K\", \"differ by K\", or \"differ by multiple of k\" are solved using this trick. There are two hints. Hint When a map is used, it need to be initialized using <0, -1> . It is useful for handling some of the corner cases such as [-1, 1], -1 in the problem Maximum Size Subarray Sum Equals k . Hint It is usually easier to work with these problem when adding dummy element at the beginning of the array. For example: using sums[i] to represent the sum of first i element of array nums . Maximum Subarray \u00b6 Greedy solution C++ Greedy solution // why this greedy solution works? class Solution { public : int maxSubArray ( vector < int >& nums ) { int n = nums . size (); int sum = 0 ; int max = 0 ; if ( n == 0 ) return 0 ; max = nums [ 0 ]; for ( int i = 0 ; i < n ; i ++ ) { sum += nums [ i ]; max = sum > max ? sum : max ; sum = sum > 0 ? sum : 0 ; } return max ; } }; DP solution Note Why can not compare to f[i - 1]) to find the maximum. Because including the f[i - 1] will skip elements, the sum will not from a subarray, but sequence of numbers in the array. This is very similar to problems Longest Common Substring and Longest Common Subsequence C++ DP class Solution { public : int maxSubArray ( vector < int >& nums ) { int n = nums . size (); int res = INT_MIN ; int f [ n + 1 ] = { 0 }; // f[i] = maxSubArray of first i elements f [ 0 ] = 0 ; // initial value for ( int i = 1 ; i <= n ; i ++ ) { f [ i ] = max ( f [ i - 1 ] + nums [ i - 1 ], nums [ i - 1 ]); res = max ( f [ i ], res ); } return res ; } }; // Notice this is a coordinate based DP problem, the meaning of the index i // in nums and f are different. Kadane's solution This is a DP solution, it reduced the f array to two variables. Making the problem O(n) O(n) in space. Discuss about this solution , where it make use of the idea of global maximum and local maximum. Note The idea of global maximum and local maximum is very usefull to solve DP problems. The local maximum is the maximum sum of a continuous subarray, the global maximum is keep the maximum of the local mmaximum. C++ Kadane's solution class Solution { public : int maxSubArray ( vector < int >& nums ) { int n = nums . size (); int res = INT_MIN ; int curr = 0 ; for ( int i = 0 ; i < n ; i ++ ) { curr = max ( curr + nums [ i ], nums [ i ]); res = max ( curr , res ); } return res ; } }; Prefix sum solution The ideas is we have array sums, sums[i] = A[0] +, ... + A[i] , called prefix sum. With one for loop we can find the maxSum so far and the minSum before it. The difference is the possible results, we collect the maximum of those differences. C++ prefix sum solution public class Solution { public int maxSubArray ( int [] A ) { if ( A == null || A . length == 0 ){ return 0 ; } int max = Integer . MIN_VALUE , sum = 0 , minSum = 0 ; for ( int i = 0 ; i < A . length ; i ++ ) { sum += A [ i ]; max = Math . max ( max , sum - minSum ); minSum = Math . min ( minSum , sum ); } return max ; } }; Maximum Subarray II* \u00b6 Given an array of integers, find two non-overlapping subarrays which have the largest sum. The number in each subarray should be contiguous. Return the largest sum. Notice The subarray should contain at least one number Example For given [1, 3, -1, 2, -1, 2], the two subarrays are [1, 3] and [2, -1, 2] or [1, 3, -1, 2] and [2], they both have the largest sum 7. Prefix sum solution C++ Prefix sum solution class Solution { public : /* * @param nums: A list of integers * @return: An integer denotes the sum of max two non-overlapping subarrays */ int maxTwoSubArrays ( vector < int > nums ) { int n = nums . size (); int minSum = 0 ; int sums = 0 ; int maxSum = INT_MIN ; int left [ n ] = { 0 }; int right [ n ] = { 0 }; /* calculate the prefix sum */ for ( int i = 0 ; i < n ; i ++ ) { sums += nums [ i ]; maxSum = max ( maxSum , sums - minSum ); // minSum is previous calculated minSum = min ( minSum , sums ); left [ i ] = maxSum ; } /* calculate the postfix sum */ minSum = 0 ; sums = 0 ; maxSum = INT_MIN ; for ( int i = n - 1 ; i >= 0 ; i -- ) { sums += nums [ i ]; maxSum = max ( maxSum , sums - minSum ); minSum = min ( minSum , sums ); right [ i ] = maxSum ; } /* iterate the divider line, left[i] stored the maxSubArraySum * from nums[0] to nums[i], similar for right[i] */ maxSum = INT_MIN ; for ( int i = 0 ; i < n - 1 ; i ++ ) { maxSum = max ( maxSum , left [ i ] + right [ i + 1 ]); } return maxSum ; } }; Warning Cannot swap the highlighted lines. Because the maximum sum is calculated from current sum minus the previous minSum. Maximum Subarray III* \u00b6 Given an array of integers and a number k, find k non-overlapping subarrays which have the largest sum. The number in each subarray should be contiguous. Return the largest sum. Notice The subarray should contain at least one number Example Input: List = [-1,4,-2,3,-2,3] k = 2 Output: 8 Explanation: 4 + (3 + -2 + 3) = 8 DP solution Use the idea of global maximum and local maximum from Maximum Subarray . See this artical for detailed explaination of the solution. C++ class Solution { public : /** * My initial try: O(n^2 k) * Partitioning DP: f[n][k], maximum K subarrays of first n elements. * Last partition: A[j] ,... A[n - 1] * f[i][k] = max_{0 <= j < i}(f[j][k - 1] + MS(A[j] ,... A[i - 1])) * f[0][0] = * * Solution 2, O(nk) * local[i][k]: Max k subarray sum from \"first i elements\" that include nums[i] * global[i][k]: Max k subarray sum from \"first i elements\" that may not include nums[i] * * 2 cases: nums[i - 1] is kth subarray, nums[i - 1] belongs to kth subarray * local[i][k] = max(global[i - 1][k - 1], local[i - 1][k]) + nums[i - 1] * * 2 cases: not include nums[i - 1], include nums[i - 1] * global[i][k] = max(global[i - 1][k], local[i][k]) */ int maxSubArray ( vector < int > nums , int k ) { int n = nums . size (); int local [ n + 1 ][ k + 1 ] = { 0 }; int global [ n + 1 ][ k + 1 ] = { 0 }; for ( int j = 1 ; j <= k ; j ++ ) { // first j - 1 elements cannot form j groups, set to INT_MIN. local [ j - 1 ][ j ] = INT_MIN ; for ( int i = j ; i <= n ; i ++ ) { // must: i >= k. local [ i ][ j ] = max ( global [ i - 1 ][ j - 1 ], local [ i - 1 ][ j ]) + nums [ i - 1 ]; // the case when we divide k elements into k groups. if ( i == j ) { global [ i ][ j ] = local [ i ][ j ]; } else { global [ i ][ j ] = max ( global [ i - 1 ][ j ], local [ i ][ j ]); } } } return global [ n ][ k ]; } }; Maximum Subarray Difference* \u00b6 Given an array with integers. Find two non-overlapping subarrays A and B, which |SUM(A) - SUM(B)| is the largest. Return the largest difference. Notice The subarray should contain at least one number Example For [1, 2, -3, 1], return 6. Prefix sum solution We use the similar idea for problem Maximum Subarray II . We have to maintain four arrays. from forward maximum and minimum subarray sum and backward maximum and minimum subarray sum. C++ Prefix sum solution class Solution { public : /* * @param nums: A list of integers * @return: value of maximum difference between two subarrays */ int maxDiffSubArrays ( vector < int > nums ) { int n = nums . size (); int minSum = 0 ; int sums = 0 ; int maxSum = INT_MIN ; int left_max [ n ] = { 0 }; int left_min [ n ] = { 0 }; int right_max [ n ] = { 0 }; int right_min [ n ] = { 0 }; for ( int i = 0 ; i < n ; i ++ ) { sums += nums [ i ]; maxSum = max ( maxSum , sums - minSum ); minSum = min ( minSum , sums ); left_max [ i ] = maxSum ; //left_min[i] = minSum; } minSum = INT_MAX ; sums = 0 ; maxSum = 0 ; for ( int i = 0 ; i < n ; i ++ ) { sums += nums [ i ]; minSum = min ( minSum , sums - maxSum ); maxSum = max ( maxSum , sums ); //left_max[i] = maxSum; left_min [ i ] = minSum ; } minSum = 0 ; sums = 0 ; maxSum = INT_MIN ; for ( int i = n - 1 ; i >= 0 ; i -- ) { sums += nums [ i ]; maxSum = max ( maxSum , sums - minSum ); minSum = min ( minSum , sums ); right_max [ i ] = maxSum ; //right_min[i] = minSum; } minSum = INT_MAX ; sums = 0 ; maxSum = 0 ; for ( int i = n - 1 ; i >= 0 ; i -- ) { sums += nums [ i ]; minSum = min ( minSum , sums - maxSum ); maxSum = max ( maxSum , sums ); //right_max[i] = maxSum; right_min [ i ] = minSum ; } int diff = INT_MIN ; for ( int i = 0 ; i < n - 1 ; i ++ ) { diff = max ( left_max [ i ] - right_min [ i + 1 ], diff ); diff = max ( right_max [ i + 1 ] - left_min [ i ], diff ); } return diff ; } }; Maximum Product Subarray \u00b6 DP solution It is similar to the problem Maximum Subarray . Notice the negative number, min multiply a minus number could become the largest product. C++ class Solution { public : int maxProduct ( vector < int >& nums ) { int n = nums . size (); int max_pro [ n ] = { 0 }; int min_pro [ n ] = { 0 }; int result = nums [ 0 ]; max_pro [ 0 ] = nums [ 0 ]; min_pro [ 0 ] = nums [ 0 ]; for ( int i = 1 ; i < n ; i ++ ) { if ( nums [ i ] > 0 ) { max_pro [ i ] = max ( max_pro [ i - 1 ] * nums [ i ], nums [ i ]); min_pro [ i ] = min ( min_pro [ i - 1 ] * nums [ i ], nums [ i ]); } else { max_pro [ i ] = max ( min_pro [ i - 1 ] * nums [ i ], nums [ i ]); min_pro [ i ] = min ( max_pro [ i - 1 ] * nums [ i ], nums [ i ]); } result = max ( result , max_pro [ i ]); } return result ; } }; Constant space solution Without need to check whether nums[i] is positive is negative, we can just find the maximum or minium of three cases. C++ class Solution { public : /* * @param nums: An array of integers * @return: An integer */ int maxProduct ( vector < int > nums ) { int n = nums . size (); int res = nums [ 0 ]; int cur_max = nums [ 0 ]; int cur_min = nums [ 0 ]; for ( int i = 1 ; i < n ; i ++ ) { int tmp = cur_max ; cur_max = max ( max ( cur_max * nums [ i ], nums [ i ]), cur_min * nums [ i ]); cur_min = min ( min ( cur_min * nums [ i ], nums [ i ]), tmp * nums [ i ]); res = max ( res , cur_max ); } return res ; } }; Subarray Product Less Than K \u00b6 Subarray Sum* \u00b6 Given an integer array, find a subarray where the sum of numbers is zero. Your code should return the index of the first number and the index of the last number. Notice There is at least one subarray that it's sum equals to zero. Example Given [-3, 1, 2, -3, 4], return [0, 2] or [1, 3]. Hash solution use a hash table to keep the prefix sum. Once we see another prefix sum that exists in the hash table, we discovered the subarray that sums to zero. However, pay attention to the indexing, because it requires to return the original array's index. C++ class Solution { public : /** * @param nums: A list of integers * @return: A list of integers includes the index of the first number * and the index of the last number */ vector < int > subarraySum ( vector < int > nums ){ int n = nums . size (); vector < int > res ( 2 , 0 ); int sum = 0 ; unordered_map < int , int > map ; map [ 0 ] = - 1 ; //important for ( int i = 0 ; i < n ; i ++ ) { sum += nums [ i ]; if ( map . count ( sum ) != 0 ) { res [ 0 ] = map [ sum ] + 1 ; res [ 1 ] = i ; break ; } map [ sum ] = i ; } return res ; } }; // test cases // [-1, 2, 3, -3] A // [0,-1, 1, 4, 1] sum // i j Note Pay attention to the initial value and initializethe map[0] = -1 ; This can be validated with an edge case. The time complexity is O(n) Prefix sum solution Calculate the prefix sum first and then use the prefix sum to find the subarray. This solution is O(n^2) O(n^2) class Solution { public : vector < int > subarraySum ( vector < int > nums ){ int n = nums . size (); vector < int > res ( 2 , 0 ); vector < int > sum ( n + 1 , 0 ); sum [ 0 ] = 0 ; for ( int i = 1 ; i <= n ; i ++ ) { sum [ i ] = sum [ i - 1 ] + nums [ i - 1 ]; } for ( int i = 0 ; i < n ; i ++ ) { for ( int j = i ; j <= n ; j ++ ) { if ( j > 1 && sum [ j ] - sum [ i ] == 0 ) { res [ 0 ] = i ; res [ 1 ] = j - 1 ; break ; } } } return res ; } }; Minimum Size Subarray Sum \u00b6 Accumulative sum solution Using accumulative sum and another moving pointer to check both the sum and the length of the subarray. C++ class Solution { public : int minSubArrayLen ( int s , vector < int >& nums ) { int n = nums . size (); if ( n == 0 ) return 0 ; int sum = 0 ; int res = INT_MAX ; int left = 0 ; for ( int i = 0 ; i < n ; i ++ ) { sum += nums [ i ]; while ( sum >= s ) { res = min ( res , i - left + 1 ); sum -= nums [ left ++ ]; } } return res != INT_MAX ? res : 0 ; } }; Maximum Size Subarray Sum Equals k \u00b6 Similar to Continuous Subarray Sum Hash solution Use a hash table to keep <sums, i> entries. Look up using sum - k . We only add to the hash table for the first time a value is appeared. It ensures the length of the found subarray is the largest. Notice you also have to initialize the hash with value <0, -1> to handle the edge case. C++ class Solution { public : int maxSubArrayLen ( vector < int >& nums , int k ) { int n = nums . size (); if ( n == 0 ) return 0 ; unordered_map < int , int > map ; int sum = 0 ; int left = 0 ; int res = INT_MIN ; map [ 0 ] = - 1 ; for ( int i = 0 ; i < n ; i ++ ) { sum += nums [ i ]; if ( map . count ( sum - k ) != 0 ) { left = map [ sum - k ] + 1 ; res = max ( res , ( i - left + 1 )); } if ( map . count ( sum ) == 0 ) { map [ sum ] = i ; } } return res != INT_MIN ? res : 0 ; } }; /* test cases: 1. [-1, 1], -1 \u5982\u679c\u6ca1\u6709\u521d\u59cb\u5316hash\uff0c\u8fd9\u4e2acase\u5c31\u4f1a\u9519\u8bef [-1, 0] sums 2. [-1], 0 3. [-1], -1 4. if return the result will be 1 if there is no res variable [1, 1, 0], 1 [1, 2, 2], 1 */ Subarray Sum Equals K \u00b6 Prefix sum solution Use prefix sum to find the subarray sum. Two pointer to check all the possible subarray sum. C++ prefix sum solution public class Solution { public int subarraySum ( int [] nums , int k ) { int count = 0 ; int [] sum = new int [ nums . length + 1 ]; sum [ 0 ] = 0 ; for ( int i = 1 ; i <= nums . length ; i ++ ) sum [ i ] = sum [ i - 1 ] + nums [ i - 1 ]; for ( int start = 0 ; start < nums . length ; start ++ ) { for ( int end = start + 1 ; end <= nums . length ; end ++ ) { if ( sum [ end ] - sum [ start ] == k ) count ++ ; } } return count ; } } Hash solution Use a map to store the prefix sum and a counter. The idea is while calculating prefix sums, if we find an sums - k exist in the map, we found one of target subarray. The subtilty is for a particular prefix sum, there might be multiple earlier prefix sums differ from it by k. We should take this into account. Compare to the hash solution for problem Subarray Sum . /* k = 2 i = 1, 2, 3 sum = 1, 2, 3 cnt = 0, 1, 2 key = 1, 2, 3 val = 1, 1, 1 the reason that the cnt += map[sum - k], not cnt += 1 is that the prefix sum \"sum - k\" has been shown up for total of map[sum - k] times. All those prefix sum could be result of distinct subarrays between current prefix sum and previous prefix sum \"sum - k\" */ C++ class Solution { public : int subarraySum ( vector < int >& nums , int k ) { int n = nums . size (); // key=prefix sum, val=appearance unordered_map < int , int > map ; int cnt = 0 ; int sum = 0 ; map [ 0 ] = 1 ; for ( int i = 0 ; i < n ; i ++ ) { sum += nums [ i ]; if ( map . count ( sum - k ) != 0 ) { cnt += map [ sum - k ]; } map [ sum ] += 1 ; } return cnt ; } }; Warning Notice you have to initialize the map[0] = 1 ; this is because for cases such as [1, 1, 1] , when i = 1 , sum = 2 , [1,1] should be counted as one subarray. Without setting map[0] = 1 at first hand, it will give incorrect results. Max Sum of Subarry No Larger Than K* \u00b6 This problem in geeksforgeeks as \"Maximum sum subarray having sum less than or equal to given sum\". It has been discussed here. This problem is the basis to solve the problem 363. Max Sum of Rectangle No Larger Than K. Solution 1 using prefix sum and set calculate prefix and using a set to store individual prefix sum, ( vector also works). In each iteration, we lookup the value preSum - k in the set. Notice we can use binary search to find the smallest element that >= preSum - k . We can use lower_bound to achieve that. Notice if it is asking the sum less than k we have to use upper_bound C++ int maxSumSubarryNoLargerThanK ( int A [], int n , int k ) { set < int > preSumSet ; preSumSet . insert ( 0 ); int res = 0 , preSum = 0 ; for ( int i = 0 ; i < n ; ++ i ) { preSum += A [ i ]; set < int >:: iterator siter = preSumSet . lower_bound ( preSum - k ); if ( siter != preSumSet . end () { res = max ( res , preSum - * siter ); } preSumSet . insert ( preSum ); } return res ; } Max Sum of Rectangle No Larger Than K \u00b6 Solution 1 iterate the wide of the matrix and using prefix sum and set lower_bound . To optimize it with the brute force solution, you will find this problem is a combination of the problem Maximum Sum Rectangular Submatrix in Matrix and problem Max Sum of Subarry No Larger Than K. From the problem Max Sum of Subarry No Larger Than K, we have to enumerate the width of the sub-matrix and sum up all row elements and get an array of length m , m is the number of rows of the matrix. Then apply the method. C++ class Solution { public : int maxSumSubmatrix ( vector < vector < int >>& matrix , int k ) { if ( matrix . empty ()) return 0 ; int m = matrix . size (); int n = m ? matrix [ 0 ]. size () : 0 ; int res = INT_MIN ; for ( int l = 0 ; l < n ; ++ l ) { vector < int > sums ( m , 0 ); for ( int r = l ; r < n ; ++ r ) { for ( int i = 0 ; i < m ; ++ i ) { sums [ i ] += matrix [ i ][ r ]; } set < int > preSumSet ; preSumSet . insert ( 0 ); int preSum = 0 , curMax = INT_MIN ; for ( int sum : sums ) { preSum += sum ; set < int >:: iterator it = preSumSet . lower_bound ( preSum - k ); if ( it != preSumSet . end ()) { curMax = max ( curMax , preSum - * it ); } preSumSet . insert ( preSum ); } res = max ( res , curMax ); } } return res ; } }; Note The complexity is n\u22c5n\u22c5(m+m\\log m)=O(n\u22c5n\u22c5m\\log m) n\u22c5n\u22c5(m+m\\log m)=O(n\u22c5n\u22c5m\\log m) Notice the use of lower_bound, this function return iterator point to element greater than or equal to the value curSum - k, if use upper_bound, it will return iterator points to element greater than curSum - k, which would miss the equal to K case. Solution 2 using merge sort The idea is similar that solution 1. Instead of calculate preSum on the fly, we finish calculation and pass it to a mergeSort routine. The use mergeSort here is to find the A[j] - A[i] <= k efficiently, O(nlogn) . The complexity is n\u22c5n\u22c5(m+m\u22c5\\log m)=O(n\u22c5n\u22c5m\u22c5\\log m) C++ class Solution { public : int maxSumSubmatrix ( vector < vector < int >>& matrix , int k ) { int m = matrix . size (); int n = m ? matrix [ 0 ]. size () : 0 ; int res = INT_MIN ; vector < long long > sums ( m + 1 , 0 ); for ( int l = 0 ; l < n ; ++ l ) { vector < long long > sumInRow ( m , 0 ); for ( int r = l ; r < n ; ++ r ) { for ( int i = 0 ; i < m ; ++ i ) { sumInRow [ i ] += matrix [ i ][ r ]; sums [ i + 1 ] = sums [ i ] + sumInRow [ i ]; } res = max ( res , mergeSort ( sums , 0 , m + 1 , k )); if ( res == k ) return k ; } } return res ; } int mergeSort ( vector < long long >& sums , int start , int end , int k ) { if ( end == start + 1 ) return INT_MIN ; int mid = start + ( end - start ) / 2 ; int res = mergeSort ( sums , start , mid , k ); if ( res == k ) return k ; res = max ( res , mergeSort ( sums , mid , end , k )); if ( res == k ) return k ; long long cache [ end - start ]; int j = mid , c = 0 , t = mid ; for ( int i = start ; i < mid ; ++ i ) { /* search first time sums[j] - sums[i] > k */ while ( j < end && sums [ j ] - sums [ i ] <= k ) ++ j ; /* sums[j - 1] - sums[i] <= k, make sure j - 1 is in right side */ if ( j - 1 >= mid ) { res = max ( res , ( int )( sums [ j - 1 ] - sums [ i ])); if ( res == k ) return k ; } /* parallel merge */ while ( t < end && sums [ t ] < sums [ i ]) { cache [ c ++ ] = sums [ t ++ ]; } cache [ c ++ ] = sums [ i ]; } /* parallel merge */ for ( int i = start ; i < t ; ++ i ) { sums [ i ] = cache [ i - start ]; } return res ; } }; Maximum Sum Rectangular Submatrix in Matrix* \u00b6 Subarray Sum Closest* \u00b6 Shortest Unsorted Continuous Subarray (Count inversions) \u00b6 Solution 1 using merge sort This problem is the basic of using merge sort to solve lots of hard problems. C++ long long merge_and_count ( int A [], int start , int end ) { if ( end - start <= 1 ) return 0 ; long long count = 0 ; int mid = start + ( end - start ) / 2 ; count = merge_and_count ( A , start , mid ) + merge_and_count ( A , mid , end ); int j = mid ; int cache [ end - start ]; for ( int i = start , k = 0 ; i < mid ; i ++ ) { while ( j < end && A [ i ] > A [ j ]) { cache [ k ++ ] = A [ j ++ ]; count += mid - i ; } cache [ k ++ ] = A [ i ]; } // copy cache[0, j - start) to A[start, j) // if j < end, then A[j, end) in final place. for ( int i = start ; i < j ; i ++ ) { A [ i ] = cache [ i - start ]; } return count ; } Count Inversion (course assignment) \u00b6 Solution 1 using merge sort This problem is the basic of using merge sort to solve lots of hard problems. C++ long long merge_and_count ( int A [], int start , int end ) { if ( end - start <= 1 ) return 0 ; long long count = 0 ; int mid = start + ( end - start ) / 2 ; count = merge_and_count ( A , start , mid ) + merge_and_count ( A , mid , end ); int j = mid ; int cache [ end - start ]; for ( int i = start , k = 0 ; i < mid ; i ++ ) { while ( j < end && A [ i ] > A [ j ]) { cache [ k ++ ] = A [ j ++ ]; count += mid - i ; } cache [ k ++ ] = A [ i ]; } // copy cache[0, j - start) to A[start, j) // if j < end, then A[j, end) in final place. for ( int i = start ; i < j ; i ++ ) { A [ i ] = cache [ i - start ]; } return count ; } 315. Count of Smaller Numbers After Self \u00b6 Solution 1 Merge sort One important point to remember is you have to create pairs out of the array element and its index, because during merge sort, when we count each value, we don't know where to put those count values in the result vector. The second merge solutions run much faster than the first one. C++ Merge sort class Solution { public : vector < int > countSmaller ( vector < int >& nums ) { int n = nums . size (); vector < int > res ( n , 0 ); vector < pair < int , int > > vp ; for ( int i = 0 ; i < n ; i ++ ) { vp . emplace_back ( nums [ i ], i ); } merge_sort_count ( vp , 0 , n , res ); return res ; } private : void merge_sort_count ( vector < pair < int , int > >& nums , int start , int end , vector < int >& res ) { if ( end - start <= 1 ) return ; int mid = start + ( end - start ) / 2 ; merge_sort_count ( nums , start , mid , res ); merge_sort_count ( nums , mid , end , res ); vector < pair < int , int > > cache ( end - start , pair < int , int > ( 0 , 0 )); int j = mid , k = 0 , t = mid ; for ( int i = start ; i < mid ; i ++ ) { j = mid ; while ( j < end && nums [ i ]. first > nums [ j ]. first ) { // found smaller elements res [ nums [ i ]. second ] ++ ; j ++ ; } while ( t < end && nums [ i ]. first > nums [ t ]. first ) { cache [ k ++ ] = nums [ t ++ ]; } cache [ k ++ ] = nums [ i ]; } for ( int i = start ; i < j ; i ++ ) { nums [ i ] = cache [ i - start ]; } return ; } }; C++ more efficient Merge sort class Solution { public : vector < int > countSmaller ( vector < int >& nums ) { int n = nums . size (); vector < int > res ( n , 0 ); vector < pair < int , int > > vp ; for ( int i = 0 ; i < n ; i ++ ) { vp . emplace_back ( nums [ i ], i ); } mergeSort ( vp , 0 , n , res ); return res ; } void mergeSort ( vector < pair < int , int >>& x , int start , int end , vector < int >& res ) { if ( end - start <= 1 ) return ; int mid = start + ( end - start ) / 2 ; mergeSort ( x , start , mid , res ); mergeSort ( x , mid , end , res ); vector < pair < int , int > > cache ( end - start , pair < int , int > ( 0 , 0 )); int i = start , j = mid , k = 0 ; while ( i < mid && j < end ) { if ( x [ i ]. first <= x [ j ]. first ) { cache [ k ++ ] = x [ i ]; res [ x [ i ]. second ] += j - mid ; ++ i ; } else { cache [ k ++ ] = x [ j ++ ]; } } while ( i < mid ) { cache [ k ++ ] = x [ i ]; res [ x [ i ]. second ] += end - mid ; ++ i ; } while ( j < end ) cache [ k ++ ] = x [ j ++ ]; for ( i = start , k = 0 ; i < end ; ++ i , ++ k ) { x [ i ] = cache [ k ]; } } }; C++ BST class Solution { public : class TreeNode { public : int val , smallerCnt ; TreeNode * left , * right ; TreeNode ( int v , int s ) : left ( NULL ), right ( NULL ), val ( v ), smallerCnt ( s ){} }; vector < int > countSmaller ( vector < int >& nums ) { int n = nums . size (); if ( n == 0 ) return {}; vector < int > res ( n , 0 ); TreeNode * root = NULL ; for ( int i = n - 1 ; i >= 0 ; -- i ) root = insert ( root , nums [ i ], i , 0 , res ); return res ; } private : TreeNode * insert ( TreeNode * node , int val , int idx , int preSum , vector < int >& res ) { if ( node == NULL ) { node = new TreeNode ( val , 0 ); res [ idx ] = preSum ; } else if ( node -> val > val ) { node -> smallerCnt ++ ; node -> left = insert ( node -> left , val , idx , preSum , res ); } else { node -> right = insert ( node -> right , val , idx , preSum + node -> smallerCnt + (( node -> val < val ) ? 1 : 0 ), res ); } return node ; } }; Continuous Subarray Sum \u00b6 Hash solution Once see a multiple of K, you should consider the modulor operation % The values put into to the hash only for the first time, this is similar to the case in the problem Maximum Size Subarray Sum Equals k. C++ Hash soution class Solution { public : bool checkSubarraySum ( vector < int >& nums , int k ) { int n = nums . size (); if ( n == 0 ) return false ; unordered_map < int , int > map ; int sum = 0 ; map [ 0 ] = - 1 ; // test case [0, 0], 0 for ( int i = 0 ; i < n ; i ++ ) { sum += nums [ i ]; if ( k != 0 ) sum = sum % k ; if ( map . count ( sum ) != 0 ) { if ( i - map [ sum ] > 1 ) { return true ; } } else { map [ sum ] = i ; } } return false ; } }; Contiguous Array \u00b6 Similar problems: Continuous Subarray Sum Maximum Size Subarray Sum Equals k Hash solution This problem is very similar to the problem Continuous Subarray Sum . However, there is a trick to calculate the cummulative sum, treat 0 as -1 . C++ class Solution { public : int findMaxLength ( vector < int >& nums ) { int n = nums . size (); int res = 0 ; int cnt = 0 ; unordered_map < int , int > map ; map [ 0 ] = - 1 ; // test case: [0, 1] for ( int i = 0 ; i < n ; i ++ ) { cnt += nums [ i ] == 0 ? - 1 : 1 ; if ( map . count ( cnt ) != 0 ) { res = max ( res , i - map [ cnt ]); } else { map [ cnt ] = i ; } } return res ; } }; Split Array with Equal Sum \u00b6 Cummulative sum soluiton Because of the symetric property of the head subarray and trailing subarray, we can calculate cumulative sum from both direction. This can help to fix the index i and k . we can enumerate the index j in between. C++ cummulateive sum solution class Solution { public : bool splitArray ( vector < int >& nums ) { int n = nums . size (); int sum1 [ n ] = { 0 }; int sum2 [ n ] = { 0 }; sum1 [ 0 ] = nums [ 0 ]; for ( int i = 1 ; i < n ; i ++ ) { sum1 [ i ] = sum1 [ i - 1 ] + nums [ i ]; } sum2 [ n - 1 ] = nums [ n - 1 ]; for ( int i = n - 2 ; i >= 0 ; i -- ) { sum2 [ i ] = sum2 [ i + 1 ] + nums [ i ]; } // notice the index bounds for ( int i = 1 ; i < n - 5 ; i ++ ) { for ( int k = n - 2 ; k > i + 3 ; k -- ) { if ( sum1 [ i ] - nums [ i ] == sum2 [ k ] - nums [ k ]) { for ( int j = i + 2 ; j < k - 1 ; j ++ ) { int sumij = sum1 [ j ] - nums [ j ] - sum1 [ i ]; int sumjk = sum2 [ j ] - nums [ j ] - sum2 [ k ]; if ( sumij == sumjk ) { return true ; } } } } } return false ; } }; Split Array Largest Sum \u00b6 Similar problems: Copy Books . DP solution Notice the edge case: [1, INT_MAX] , use double will can avoid integer overflow. C++ DP /** * equivalent to the lintcode copy books problem * * last step: mth subarray A[j], ..., A[i - 1]. * State: f[m][n]: minmax sum of m subarrays that include n elements * Equation: f[m][n] = min_{0<=j<n}(max(f[m - 1][j], sum(A[j], ..., A[n - 1]))) * Init: f[0][n] = INT_MAX; * f[0][0] = 0; * NB: notice a special case: [1, 2147483247], 2 * the sum will overflow in the state update, You use a double type */ class Solution { public : int splitArray ( vector < int >& nums , int m ) { int n = nums . size (); double f [ m + 1 ][ n + 1 ]; f [ 0 ][ 0 ] = 0 ; for ( int i = 1 ; i <= n ; i ++ ) { f [ 0 ][ i ] = INT_MAX ; } double sum = 0 ; for ( int k = 1 ; k <= m ; k ++ ) { f [ k ][ 0 ] = 0 ; for ( int i = 1 ; i <= n ; i ++ ) { sum = 0 ; f [ k ][ i ] = INT_MAX ; for ( int j = i ; j >= 0 ; j -- ) { //j = i, mean sum = 0. f [ k ][ i ] = min ( f [ k ][ i ], max ( f [ k - 1 ][ j ], sum )); if ( j > 0 ) { sum += nums [ j - 1 ]; } } } } return f [ m ][ n ]; } }; Copy Books* \u00b6 DP solution There are i books, consider the last copier, he can copy A[j], ..., A[i-1] . The first k-1 copier copy A[0], ..., A[j - 1] . Define state: f[k][i] , meaning the k-th copier copy i books. State transition equation: f[k][i] = \\min_{0 \\le j \\le i} \\max(f[k - 1][j], A[j] + ... + A[i - 1]) f[k][i] = \\min_{0 \\le j \\le i} \\max(f[k - 1][j], A[j] + ... + A[i - 1]) C++ DP solution class Solution { public : /** * last step: last copier copy A[j], ... A[i-1] * first k-1 copier --> A[0], ... A[j - 1]. * f[k][i]: k copier copy i books. * f[k][i] = \\min_{0 \\le j \\le i} \\max(f[k - 1][j], A[j] + ... + A[i - 1]) */ int copyBooks ( vector < int > & pages , int K ) { // write your code here int n = pages . size (); if ( n == 0 ) { return 0 ; } if ( K > n ) { K = n ; } int f [ K + 1 ][ n + 1 ]; /* init */ f [ 0 ][ 0 ] = 0 ; for ( int j = 1 ; j <= n ; j ++ ) { f [ 0 ][ j ] = INT_MAX ; } int sum = 0 ; for ( int k = 1 ; k <= K ; k ++ ) { f [ k ][ 0 ] = 0 ; for ( int i = 1 ; i <= n ; i ++ ) { sum = 0 ; f [ k ][ i ] = INT_MAX ; for ( int j = i ; j >= 0 ; j -- ) { f [ k ][ i ] = min ( f [ k ][ i ], max ( f [ k - 1 ][ j ], sum )); if ( j > 0 ) { sum += pages [ j - 1 ]; } } } } return f [ K ][ n ]; } }; Note We have to enumerate the index j , the highlighted code used a clever technique to optimize this task. It enumerate j backwards. While this seems impossible at the first glance, how can you calculate the states from right to left in DP? Notice the index j is in the upper row (row k-1 ). Once we are in the k -th row, the values in the k-1 -th row are all given. Maximum Average Subarray I \u00b6 Prefix sum solution class Solution { public : double findMaxAverage ( vector < int >& nums , int k ) { int n = nums . size (); double sums [ n ] = { 0 }; double max_avg = INT_MIN ; sums [ 0 ] = nums [ 0 ]; for ( int i = 1 ; i < n ; i ++ ) { sums [ i ] = sums [ i - 1 ] + nums [ i ]; } for ( int i = k - 1 ; i < n ; i ++ ) { double avg = ( sums [ i ] - sums [ i - k + 1 ] + nums [ i - k + 1 ]) / k ; max_avg = max ( max_avg , avg ); } return max_avg ; } }; Maximum Average Subarray II \u00b6 Prefix sum solution This is still a brute force solution. time complexity: O(n^2) O(n^2) space complexity: O(n) O(n) C++ prefix sum solution class Solution { public : double findMaxAverage ( vector < int >& nums , int k ) { int n = nums . size (); vector < int > sums = nums ; for ( int i = 1 ; i < n ; ++ i ) { sums [ i ] = sums [ i - 1 ] + nums [ i ]; } double res = ( double ) sums [ k - 1 ] / k ; for ( int i = k ; i < n ; ++ i ) { double t = sums [ i ]; if ( t > res * ( i + 1 )) res = t / ( i + 1 ); for ( int j = 0 ; j < i - k + 1 ; ++ j ) { t = sums [ i ] - sums [ j ]; if ( t > res * ( i - j )) res = t / ( i - j ); } } return res ; } }; Space optimized solution We could avoid using the prefix sum array and only use two variables to record the prefix sum at any particular instance. One for record prefix sum of first i elements. Another for the inner loop to check whether removing an element from the beginning will make a new maximum value or not. C++ space optimized class Solution { public : double findMaxAverage ( vector < int >& nums , int k ) { int n = nums . size (); /* range is half open */ double sumsAll = accumulate ( nums . begin (), nums . begin () + k , 0 ); double sums = sumsAll , res = sumsAll / k ; for ( int i = k ; i < n ; ++ i ) { sumsAll += nums [ i ]; sums = sumsAll ; if ( sums > res * ( i + 1 )) res = sums / ( i + 1 ); for ( int j = 0 ; j < i - k + 1 ; ++ j ) { sums -= nums [ j ]; if ( sums > res * ( i - j )) res = sums / ( i - j ); } } return res ; } }; Binary search solution The key question to answer in order to solve this problem using binary search is that what condition we should use to serve the similar effect of cutting the input space in half in the original binary search. The answer is we can test whether it is possible to have an average value of subarray whose length is greater than or equal to k in the upper half. C++ binary search soluiton class Solution { public : double findMaxAverage ( vector < int >& nums , int k ) { int n = nums . size (); double upper = INT_MIN , lower = INT_MAX ; for ( auto num : nums ) { upper = max ( upper , ( double ) num ); lower = min ( lower , ( double ) num ); } while ( lower + 0.00001 < upper ) { double mid = lower + ( upper - lower ) / 2 ; if ( isLarger ( nums , mid , k )) { // is average value >= mid? lower = mid ; } else { upper = mid ; } } return lower ; } /* return true if a greater average value is possible */ bool isLarger ( vector < int >& nums , double mid , int k ) { int n = nums . size (); double sums = 0 , prev = 0 , prev_min = 0 ; for ( int i = 0 ; i < k ; i ++ ) { sums += nums [ i ] - mid ; } if ( sums >= 0 ) { return true ; } /* * we keep looking for whether a subarray sum of length >= k in array * \"sums\" is possible to be greater than zero. If such a subarray exist, * it means that the target average value is greater than the \"mid\" * value. We look at the front part of sums that at least k element * apart from i. If we can find the minimum of the sums[0, 1, ..., i - k] * and check if sums[i] - min(sum[0, 1, ..., i - k]) >= 0. If this is the * case, it indicate there exist a subarray of length >= k with sum * greater than 0 in sums, we can return ture, otherwise, false. */ for ( int i = k ; i < n ; i ++ ) { sums += nums [ i ] - mid ; prev += nums [ i - k ] - mid ; prev_min = min ( prev_min , prev ); if ( sums >= prev_min ) return true ; } return false ; } }; Note Notice the initial value of prev_min is set to 0 not INT_MAX; Try to understand why set the initial value of prev_min to INT_MAX cannot pass the test case: [8,9,3,1,8,3,0,6,9,2] , 8. Deque solution C++ deque solution class Solution { public : double findMaxAverage ( vector < int >& nums , int k ) { int n = nums . size (); vector < double > sums ( n , 0 ); deque < int > q ; sums [ 0 ] = nums [ 0 ]; for ( int i = 1 ; i < n ; ++ i ) sums [ i ] = sums [ i - 1 ] + nums [ i ]; double res = sums [ n - 1 ] / n ; for ( int j = k - 1 ; j < n ; ++ j ) { while ( q . size () >= 2 && density ( sums , q [ q . size () - 2 ], q . back () - 1 ) >= density ( sums , q . back (), j - k )) { q . pop_back (); } q . push_back ( j - k + 1 ); while ( q . size () >= 2 && density ( sums , q [ 0 ], j ) <= density ( sums , q [ 1 ], j )) { q . pop_front (); } res = max ( res , density ( sums , q . front (), j )); } return res ; } private : double density ( vector < double >& sums , int l , int r ) { if ( l == 0 ) return sums [ r ] / ( r + 1 ); return ( sums [ r ] - sums [ l - 1 ]) / ( r - l + 1 ); } }; Range Sum Query - Immutable \u00b6 Prefix sum solution Use prefix sum to record the accumulative sum of the array in the constructor. The algorithm is O(n) O(n) in space and O(1) O(1) in time. C++ class NumArray { private : vector < int > sums ; int n ; public : NumArray ( vector < int > nums ) { n = nums . size (); sums . resize ( n + 1 , 0 ); sums [ 0 ] = 0 ; for ( int i = 1 ; i <= n ; ++ i ) { sums [ i ] = sums [ i - 1 ] + nums [ i - 1 ]; } } int sumRange ( int i , int j ) { return sums [ j + 1 ] - sums [ i ]; } }; /** * Your NumArray object will be instantiated and called as such: * NumArray obj = new NumArray(nums); * int param_1 = obj.sumRange(i,j); */ Range Sum Query - Mutable \u00b6 Segment tree solution Using segment tree, the solution is given at Leetcode Solution . C++ segment tree class NumArray { private : vector < int > tree ; int n ; public : NumArray ( vector < int > nums ) { n = nums . size (); tree . resize ( 2 * n , 0 ); for ( int i = n , j = 0 ; i < 2 * n ; ++ i , ++ j ) { tree [ i ] = nums [ j ]; } for ( int i = n - 1 ; i > 0 ; -- i ) { tree [ i ] = tree [ 2 * i ] + tree [ 2 * i + 1 ]; } } void update ( int i , int val ) { int pos = n + i ; int left = 0 ; int right = 0 ; tree [ pos ] = val ; while ( pos > 0 ) { left = pos ; right = pos ; if ( pos % 2 == 0 ) { right = pos + 1 ; } if ( pos % 2 == 1 ) { left = pos - 1 ; } tree [ pos / 2 ] = tree [ left ] + tree [ right ]; pos /= 2 ; } } int sumRange ( int i , int j ) { int left = i + n ; int right = j + n ; int sum = 0 ; while ( left <= right ) { if ( left % 2 == 1 ) { sum += tree [ left ]; left ++ ; } if ( right % 2 == 0 ) { sum += tree [ right ]; right -- ; } left /= 2 ; right /= 2 ; } return sum ; } }; /** * Your NumArray object will be instantiated and called as such: * NumArray obj = new NumArray(nums); * obj.update(i,val); * int param_2 = obj.sumRange(i,j); */ Binary Indexed Tree solution I Using Binary indexed tree, we are able to solve it optimally in O(\\log n) O(\\log n) . The solution originally from here class NumArray { private : vector < int > arr ; vector < int > BIT ; int n ; public : NumArray ( vector < int > nums ) { arr = nums ; n = nums . size (); BIT . resize ( n + 1 , 0 ); for ( int i = 0 ; i < n ; i ++ ) { init ( i , arr [ i ]); } } void init ( int i , int val ) { i ++ ; while ( i <= n ) { BIT [ i ] += val ; // BIT[i] = nums[0, i - 1]; i += i & ( - i ); } } void update ( int i , int val ) { int diff = val - arr [ i ]; arr [ i ] = val ; init ( i , diff ); } int sumRange ( int i , int j ) { return getSum ( j ) - getSum ( i - 1 ); } int getSum ( int i ) { i ++ ; int sum = 0 ; while ( i > 0 ) { sum += BIT [ i ]; i -= i & ( - i ); } return sum ; } }; /** * Your NumArray object will be instantiated and called as such: * NumArray obj = new NumArray(nums); * obj.update(i,val); * int param_2 = obj.sumRange(i,j); */ Binary Indexed Tree solution II Similar to the above solution, We have combined the init and update . To make it consistant with the solution with problem [Range Sum Query 2D - Mutable] class NumArray { private : vector < int > arr ; vector < int > BIT ; int n ; public : NumArray ( vector < int > nums ) { n = nums . size (); BIT . resize ( n + 1 , 0 ); arr . resize ( n , 0 ); for ( int i = 0 ; i < n ; i ++ ) { update ( i , nums [ i ]); } } /* We can combine the init and update like this */ void update ( int i , int val ) { int diff = val - arr [ i ]; arr [ i ] = val ; // here we initialize arr[i] i ++ ; while ( i <= n ) { BIT [ i ] += diff ; // BIT[i] = nums[0, i - 1]; i += i & ( - i ); } } int sumRange ( int i , int j ) { return getSum ( j ) - getSum ( i - 1 ); } int getSum ( int i ) { i ++ ; int sum = 0 ; while ( i > 0 ) { sum += BIT [ i ]; i -= i & ( - i ); } return sum ; } }; /** * Your NumArray object will be instantiated and called as such: * NumArray obj = new NumArray(nums); * obj.update(i,val); * int param_2 = obj.sumRange(i,j); */ Range Sum Query 2D - Immutable \u00b6 Prefix sum solution Extended from the 1d array, we can use the prefix sum of the 2d matrix. we use extra space to store the accumulative sum of the submatrix with upper left coordinate (0, 0) and lower right coordinate (i, j) . C++ prefix sum solution class NumMatrix { private : vector < vector < int > > dp ; public : NumMatrix ( vector < vector < int >> matrix ) { int m = matrix . size (); if ( m == 0 ) return ; int n = matrix [ 0 ]. size (); //dp = vector<vector<int> (m + 1, vector<int>(n + 1, 0)); dp . resize ( m + 1 , vector < int > ( n + 1 , 0 )); for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { dp [ i + 1 ][ j + 1 ] = dp [ i ][ j + 1 ] + dp [ i + 1 ][ j ] + matrix [ i ][ j ] - dp [ i ][ j ]; } } } int sumRegion ( int row1 , int col1 , int row2 , int col2 ) { return dp [ row2 + 1 ][ col2 + 1 ] - dp [ row2 + 1 ][ col1 ] - dp [ row1 ][ col2 + 1 ] + dp [ row1 ][ col1 ]; } }; /** * Your NumMatrix object will be instantiated and called as such: * NumMatrix obj = new NumMatrix(matrix); * int param_1 = obj.sumRegion(row1,col1,row2,col2); */ Note This version create the m + 1 by n + 1 dp array to record the prefix sum. The code is clean and elegant. Alternative prefix sum solution The idea is the same, in the following solution, we have a m by n 2d array to record the accumulative sum. See how complex the code is. C++ prefix sum solution class NumMatrix { private : vector < vector < int >> dp ; int m ; int n ; public : NumMatrix ( vector < vector < int >> matrix ) { m = matrix . size (); if ( m == 0 ) return ; n = matrix [ 0 ]. size (); dp . resize ( m , vector < int > ( n , 0 )); for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { dp [ i ][ j ] = matrix [ i ][ j ]; if ( i > 0 ) { dp [ i ][ j ] += dp [ i - 1 ][ j ]; } if ( j > 0 ) { dp [ i ][ j ] += dp [ i ][ j - 1 ]; } if ( i > 0 && j > 0 ) { dp [ i ][ j ] -= dp [ i - 1 ][ j - 1 ]; } } } } int sumRegion ( int row1 , int col1 , int row2 , int col2 ) { int res = 0 ; res = dp [ row2 ][ col2 ]; if ( row1 > 0 ) { res -= dp [ row1 - 1 ][ col2 ]; } if ( col1 > 0 ) { res -= dp [ row2 ][ col1 - 1 ]; } if ( row1 > 0 && col1 > 0 ) { res += dp [ row1 - 1 ][ col1 - 1 ]; } return res ; } }; /** * Your NumMatrix object will be instantiated and called as such: * NumMatrix obj = new NumMatrix(matrix); * int param_1 = obj.sumRegion(row1,col1,row2,col2); */ Note This version is a little complex. However, the way it was written reflects a very important practice when operating on a 2d array, that is: to check the validation of the array. Range Sum Query 2D - Mutable \u00b6 Binary Indexed Tree solution We use 2D version of Binary Index Tree. Some of the explaination can be found at Topcoder tutorial C++ BIT solution class NumMatrix { private : vector < vector < int > > nums ; vector < vector < int > > tree ; int m ; int n ; public : NumMatrix ( vector < vector < int >> matrix ) { if ( matrix . size () == 0 || matrix [ 0 ]. size () == 0 ) return ; m = matrix . size (); n = matrix [ 0 ]. size (); tree . resize ( m + 1 , vector < int > ( n + 1 , 0 )); nums . resize ( m , vector < int > ( n , 0 )); for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { update ( i , j , matrix [ i ][ j ]); } } } void update ( int row , int col , int val ) { if ( m == 0 || n == 0 ) return ; int diff = val - nums [ row ][ col ]; nums [ row ][ col ] = val ; for ( int i = row + 1 ; i <= m ; i += i & ( - i )) { for ( int j = col + 1 ; j <= n ; j += j & ( - j )) { tree [ i ][ j ] += diff ; } } } int sumRegion ( int row1 , int col1 , int row2 , int col2 ) { if ( m == 0 || n == 0 ) return 0 ; return getSum ( row2 + 1 , col2 + 1 ) - getSum ( row1 , col2 + 1 ) - getSum ( row2 + 1 , col1 ) + getSum ( row1 , col1 ); } int getSum ( int row , int col ) { int sum = 0 ; for ( int i = row ; i > 0 ; i -= i & ( - i )) { for ( int j = col ; j > 0 ; j -= j & ( - j )) { sum += tree [ i ][ j ]; } } return sum ; } }; /** * Your NumMatrix object will be instantiated and called as such: * NumMatrix obj = new NumMatrix(matrix); * obj.update(row,col,val); * int param_2 = obj.sumRegion(row1,col1,row2,col2); */ Count of Range Sum \u00b6 Solution 1 Merge sort using inplace_merge() The core is to figure out how to calculate the result while merging. It is based on the fact that the left half and right half are all sorted. Using the ordering information we are able to locate two points in the right half j and k , between which will fulfill the requirement. Several important points need to be made. 1) calculation of prefix sum of the array. The length is n + 1 not n ? 2) the range passed to the merge subroutine are open-end [start, end) . The base case of the subrouine. It return zero becuase the case has been counted in the for loop, we don't need to count it again. Not because the base case is 0 . C++ class Solution { public : int countRangeSum ( vector < int >& nums , int lower , int upper ) { int n = nums . size (); vector < long > sums ( n + 1 , 0 ); for ( int i = 0 ; i < n ; i ++ ) sums [ i + 1 ] = sums [ i ] + nums [ i ]; return mergeSort ( sums , 0 , n + 1 , lower , upper ); } int mergeSort ( vector < long >& sums , int start , int end , int lower , int upper ) { if ( end - start <= 1 ) return 0 ; // note the meaning of this 0. int mid = start + ( end - start ) / 2 ; count = mergeSort ( sums , start , mid , lower , upper ) + mergeSort ( sums , mid , end , lower , upper ); int m = mid , n = mid , count = 0 ; for ( int i = start ; i < mid ; i ++ ) { while ( m < end && sums [ m ] - sums [ i ] < lower ) m ++ ; while ( n < end && sums [ n ] - sums [ i ] <= upper ) n ++ ; count += n - m ; } inplace_merge ( sums . begin () + start , sums . begin () + mid , sums . begin () + end ); return count ; } }; Solution 2 Merge sort using tmp buffer cache Here is how the count is making sense. |--------------|-------------------| sums: |start |mid |end |---|----------|------|------|-----| i j k Because sums[j] - sums[i] >= lower, and sums[k] - sums[i] > upper, So for the subarray start with i, ending index in [j, k), the range sum is in [lower, upper]. Notice k should not be included. C++ class Solution { public : int countRangeSum ( vector < int >& nums , int lower , int upper ) { int n = nums . size (); long sums [ n + 1 ] = { 0 }; if ( n == 0 ) return 0 ; for ( int i = 0 ; i < n ; i ++ ) { sums [ i + 1 ] = sums [ i ] + nums [ i ]; } /* n + 1 is the one pass the last element of sums */ return countByMergeSort ( sums , 0 , n + 1 , lower , upper ); } /* This function will return sorted array sums[start], ... sums[end - 1] */ int countByMergeSort ( long sums [], int start , int end , int lower , int upper ) { if ( end - start <= 1 ) return 0 ; int mid = start + ( end - start ) / 2 ; int count = countByMergeSort ( sums , start , mid , lower , upper ) + countByMergeSort ( sums , mid , end , lower , upper ); long cache [ end - start ] = { 0 }; int j = mid , k = mid , t = mid ; for ( int i = start , r = 0 ; i < mid ; ++ i , ++ r ) { while ( k < end && sums [ k ] - sums [ i ] < lower ) k ++ ; while ( j < end && sums [ j ] - sums [ i ] <= upper ) j ++ ; count += j - k ; /* calculate the result */ /* Merge left and right to get sorted array {sums[start], .. sums[end - 1]}. * Because left part of sums[start] to sums[mid] are already sorted, * use cache here to merge prefix of the right part: sum[mid] to sums[t] * with left part upto sums[i] for all i = {start, mid - 1}. */ while ( t < end && sums [ t ] < sums [ i ]) cache [ r ++ ] = sums [ t ++ ]; cache [ r ] = sums [ i ]; } /* after this for loop, cache will have partially sorted array * cache = sums_left = {sums[start], ... sums[t - 1]} element * of which will be in their final sorted positions. * array sums_right = {sums[t], sums[end - 1]} is also * in their final sorted positions. */ /* Since the sums_left is sorted, it have size of t - start, * here we copy exactly t - start element from cache to sums. */ for ( int i = start ; i < t ; i ++ ) sums [ i ] = cache [ i - start ]; return count ; } }; Solution 3 BST Solution 4 BIT Maximum Sum of Two Non-Overlapping Subarrays \u00b6 Brute Force Iterate class Solution { public : int maxSumTwoNoOverlap ( vector < int >& A , int L , int M ) { int n = A . size (); if ( L == 0 || M == 0 ) { return 0 ; } vector < int > preSum ( n , 0 ); preSum [ 0 ] = A [ 0 ]; for ( int i = 1 ; i < n ; i ++ ) { preSum [ i ] = preSum [ i - 1 ] + A [ i ]; } int res = 0 ; // iterate the L using index i for ( int i = 0 ; i < n - L + 1 ; i ++ ) { int Lsum = 0 ; if ( i == 0 ) { Lsum = preSum [ i + L - 1 ]; } else { Lsum = preSum [ i + L - 1 ] - preSum [ i - 1 ]; } int Msum = 0 ; // iterate the left M array using index j for ( int j = 0 ; j < i - M ; j ++ ) { int tmp = 0 ; if ( j == 0 ) { tmp = preSum [ j + M - 1 ]; } else { tmp = preSum [ j + M - 1 ] - preSum [ j - 1 ]; } Msum = max ( Msum , tmp ); } // iterate the right M array using index j for ( int j = i + L ; j < n - M + 1 ; j ++ ) { Msum = max ( Msum , preSum [ j + M - 1 ] - preSum [ j - 1 ]); } res = max ( res , Msum + Lsum ); } return res ; } }; One pass class Solution { public : int maxSumTwoNoOverlap ( vector < int >& A , int L , int M ) { int n = A . size (); if ( L == 0 || M == 0 ) { return 0 ; } vector < int > preSum ( n , 0 ); preSum [ 0 ] = A [ 0 ]; for ( int i = 1 ; i < n ; i ++ ) { preSum [ i ] = preSum [ i - 1 ] + A [ i ]; } int res = INT_MIN ; int Lmax = INT_MIN ; int Mmax = INT_MIN ; // for ( int i = L + M ; i <= n ; i ++ ) { // L is front, M is back if ( i == L + M ) { Lmax = preSum [ L - 1 ]; } else { Lmax = max ( Lmax , preSum [ i - M - 1 ] - preSum [ i - L - M - 1 ]); } // M is front, L is back if ( i == L + M ) { Mmax = preSum [ M - 1 ]; } else { Mmax = max ( Mmax , preSum ( i - L - 1 ) - preSum [ i - M - L - 1 ]); } res = max ({ res , Lmax + preSum [ i - 1 ] - preSum [ i - M - 1 ], Mmax + preSum [ i - 1 ] - preSum [ i - L - 1 ]}) } return res ; } }; DP solution //TODO Maximum Sum of 3 Non-Overlapping Subarrays \u00b6 Category 4 K Sum problems \u00b6 Two Sum \u00b6 Two Sum II - Input array is sorted \u00b6 Two Sum III - Data structure design \u00b6 Two Sum IV - Input is a BST \u00b6 3Sum \u00b6 3Sum Closest \u00b6 3Sum Smaller \u00b6 4Sum \u00b6 4Sum II \u00b6 K Sum \u00b6 Target Sum \u00b6 Category 5 Array partition problems \u00b6 Category 6 Interval and range problems \u00b6 Summary Ranges \u00b6 Missing Ranges \u00b6 Insert Interval \u00b6 Merge Intervals \u00b6 Range Addition \u00b6 Range Addition II \u00b6 My Calendar I \u00b6 My Calendar II \u00b6 My Calendar III \u00b6 Meeting Rooms I \u00b6 Meeting Rooms II \u00b6 Number of Airplanes in the Sky \u00b6 Cagegory 7 2D arry (matrix, grid) problems \u00b6 Perfect Rectangle \u00b6 Trapping Rain Water \u00b6 Trapping Rain Water II \u00b6 Container With Most Water \u00b6 Largest Rectangle in Histogram \u00b6 Maximal Rectangle \u00b6 Maximal Square \u00b6 The Skyline Problem \u00b6 Smallest Rectangle Enclosing Black Pixels \u00b6 Rectangle Area \u00b6 Max Sum of Rectangle No Larger Than K \u00b6 Category 8 stock buying problems \u00b6 Most consistent ways of dealing with the series of stock problems 121. Best Time to Buy and Sell Stock \u00b6 Solution 1 O(n) one pass to find the minimum and in the meantime, find the max profit. C++ class Solution { public : int maxProfit ( vector < int > & prices ) { int n = prices . size (); if ( n == 0 ) return 0 ; int res = 0 , low = prices [ 0 ]; for ( int i = 1 ; i < n ; i ++ ) { res = max ( res , prices [ i ] - low ); low = min ( low , prices [ i ]); } return res ; } }; 122. Best Time to Buy and Sell Stock II \u00b6 Solution 1 Greedy since you can buy as many times as you can C++ class Solution { public : int maxProfit ( vector < int > & prices ) { int n = prices . size (); int res = 0 ; for ( int i = 0 ; i < n - 1 ; i ++ ) { if ( prices [ i + 1 ] - prices [ i ]) { res += prices [ i + 1 ] - prices [ i ]; } } return res ; } }; 123. Best Time to Buy and Sell Stock III \u00b6 You can now buy at most twice. how to max the profit. Solution 1 Dynamic programming 5 stages: 1. before buy the first <-- optimal solution could be at this stage 2. hold the first 3. sell the first <-- or at this stage, only bought once, 4. hold the second 5. sell the second <-- or at this stage, bought twice. C++ class Solution { public : int maxProfit ( vector < int > & A ) { //1, 3, 5: // f[i][j] = max{f[i - 1][j], f[i - 1][j - 1] + A[i - 1] - A[i - 2]} // 2, 4: // f[i][j] = max{f[i - 1][j] + A[i - 1] - A[i - 2], // f[i - 1][j - 1], f[i - 1][j - 2] + A[i - 1] - A[i - 2]} int n = A . size (); if ( n == 0 ) { return 0 ; } int f [ n + 1 ][ 6 ]; /* init */ f [ 0 ][ 1 ] = 0 ; f [ 0 ][ 2 ] = f [ 0 ][ 3 ] = f [ 0 ][ 4 ] = f [ 0 ][ 5 ] = INT_MIN ; for ( int i = 1 ; i <= n ; i ++ ) { for ( int j = 1 ; j <= 5 ; j += 2 ) { f [ i ][ j ] = f [ i - 1 ][ j ]; if ( i > 1 && j > 1 && f [ i - 1 ][ j - 1 ] != INT_MIN ) { f [ i ][ j ] = max ( f [ i ][ j ], f [ i - 1 ][ j - 1 ] + A [ i - 1 ] - A [ i - 2 ]); } } for ( int j = 2 ; j < 5 ; j += 2 ) { f [ i ][ j ] = f [ i - 1 ][ j - 1 ]; if ( i > 1 && f [ i - 1 ][ j ] != INT_MIN ) { f [ i ][ j ] = max ( f [ i ][ j ], f [ i - 1 ][ j ] + A [ i - 1 ] - A [ i - 2 ]); } if ( i > 1 && j > 2 && f [ i - 1 ][ j - 2 ] != INT_MIN ) { f [ i ][ j ] = max ( f [ i ][ j ], f [ i - 1 ][ j - 2 ] + A [ i - 1 ] - A [ i - 2 ]); } } } return max ( f [ n ][ 1 ], max ( f [ n ][ 3 ], f [ n ][ 5 ])); } }; Solution 2 Use T[i][k][j] to represent the maximum profit of first i days if we allow at most k transactions and the current number of stocks at hand is j ( j == 0, 1 because hold two stocks at the same time is not allowed). So we have: T [ i ][ 2 ][ 0 ] = max ( T [ i - 1 ][ 2 ][ 0 ] , T [ i - 1 ][ 2 ][ 1 ] + prices [ i - 1 ] ); T [ i ][ 2 ][ 1 ] = max ( T [ i - 1 ][ 2 ][ 1 ] , T [ i - 1 ][ 1 ][ 0 ] - prices [ i - 1 ] ); T [ i ][ 1 ][ 0 ] = max ( T [ i - 1 ][ 1 ][ 0 ] , T [ i - 1 ][ 1 ][ 1 ] + prices [ i - 1 ] ); T [ i ][ 1 ][ 1 ] = max ( T [ i - 1 ][ 1 ][ 1 ] , T [ i - 1 ][ 0 ][ 0 ] - prices [ i - 1 ] ); Think: How to ensure you fourmular cover all the possible values? 188. Best Time to Buy and Sell Stock IV \u00b6 309. Best Time to Buy and Sell Stock with Cooldown \u00b6 714. Best Time to Buy and Sell Stock with Transaction Fee \u00b6 Determine the buy data and sell data of maximum profit (DD 139) \u00b6","title":"Array"},{"location":"leetcode/array/notes/#array","text":"","title":"Array"},{"location":"leetcode/array/notes/#category-1-removecontains-duplidate","text":"","title":"Category 1 Remove/Contains Duplidate"},{"location":"leetcode/array/notes/#contains-duplicate","text":"","title":"Contains Duplicate"},{"location":"leetcode/array/notes/#contains-duplicate-ii","text":"","title":"Contains Duplicate II"},{"location":"leetcode/array/notes/#contains-duplicate-iii","text":"","title":"Contains Duplicate III"},{"location":"leetcode/array/notes/#find-the-duplicate-number","text":"","title":"Find the Duplicate Number"},{"location":"leetcode/array/notes/#remove-duplicates-from-sorted-array","text":"","title":"Remove Duplicates from Sorted Array"},{"location":"leetcode/array/notes/#remove-duplicates-from-sorted-array-ii","text":"","title":"Remove Duplicates from Sorted Array II"},{"location":"leetcode/array/notes/#remove-duplicates-from-sorted-list-ii","text":"","title":"Remove Duplicates from Sorted List II"},{"location":"leetcode/array/notes/#remove-duplicates-from-sorted-list","text":"","title":"Remove Duplicates from Sorted List"},{"location":"leetcode/array/notes/#move-zeroes","text":"","title":"Move Zeroes"},{"location":"leetcode/array/notes/#category-2-matrix-problems","text":"","title":"Category 2 Matrix problems"},{"location":"leetcode/array/notes/#spiral-matrix","text":"","title":"Spiral Matrix"},{"location":"leetcode/array/notes/#spiral-matrix-ii","text":"","title":"Spiral Matrix II"},{"location":"leetcode/array/notes/#search-a-2d-matrix","text":"","title":"Search a 2D Matrix"},{"location":"leetcode/array/notes/#search-a-2d-matrix-ii","text":"","title":"Search a 2D Matrix II"},{"location":"leetcode/array/notes/#rotate-image","text":"","title":"Rotate Image"},{"location":"leetcode/array/notes/#range-sum-query-2d-mutable","text":"See Range Sum Query 2D - Mutable","title":"Range Sum Query 2D - Mutable"},{"location":"leetcode/array/notes/#range-sum-query-2d-immutable","text":"See Range Sum Query 2D - Immutable","title":"Range Sum Query 2D - Immutable"},{"location":"leetcode/array/notes/#maximal-square","text":"","title":"Maximal Square"},{"location":"leetcode/array/notes/#maximal-rectangle","text":"","title":"Maximal Rectangle"},{"location":"leetcode/array/notes/#category-3-subarray-problems","text":"","title":"Category 3 Subarray problems"},{"location":"leetcode/array/notes/#type-of-subarray-problem","text":"Find a subarray fulfill certain propertie, i.e maximum size. Split into subarrays that fulfill certain properties, i.e. sum greater than k.","title":"Type of subarray problem"},{"location":"leetcode/array/notes/#two-types-of-prefix-sum","text":"There are two ways to calculate the prefix sum array. Take which ever conveniece for your when solving a problem. Option 1: sums.resize(n, 0); nums: [1, 2, 3, 4, 5, 6, 7, 8, 9] i j sums: [1, 3, 6, 10, 15, 21, 28, 36, 45] In this case, each element sums[i] in sums represent the cumulative sum for indexes [0, ..., i] . In other words, sum[i] represent cumulative sum up to element i inclusive. When you want to get the range sum rangeSum(i, j) , you can get it in the following way: rangeSum ( i , j ) = sums [ j ] - sums [ i - 1 ] // i > 0 rangeSum ( i , j ) = sums [ j ] // i == 0 work with this option is a little complex, to get the rangeSum(i, j) : rangeSum ( i , j ) = sums [ j ] - sums [ i ] + nums [ i ] // i >= 0 Option 2: sums.resize(n + 1, 0); In this case, each element sums[i] in sums represent the prefix sum of the first i elements in original array nums. When you want to get the range sum by rangeSum(i, j) , you can get it in the following way: rangeSum ( i , j ) = sums [ j + 1 ] - sums [ i ] // i >= 0","title":"Two types of prefix sum"},{"location":"leetcode/array/notes/#using-prefix-sum-with-map","text":"One of the core trick in solving the following subarray problems is to build a map from prefix sum to array index for efficient lookup. For example, problems with keywaords \"maximum size equal to K\", \"differ by K\", or \"differ by multiple of k\" are solved using this trick. There are two hints. Hint When a map is used, it need to be initialized using <0, -1> . It is useful for handling some of the corner cases such as [-1, 1], -1 in the problem Maximum Size Subarray Sum Equals k . Hint It is usually easier to work with these problem when adding dummy element at the beginning of the array. For example: using sums[i] to represent the sum of first i element of array nums .","title":"Using prefix sum with map"},{"location":"leetcode/array/notes/#maximum-subarray","text":"Greedy solution C++ Greedy solution // why this greedy solution works? class Solution { public : int maxSubArray ( vector < int >& nums ) { int n = nums . size (); int sum = 0 ; int max = 0 ; if ( n == 0 ) return 0 ; max = nums [ 0 ]; for ( int i = 0 ; i < n ; i ++ ) { sum += nums [ i ]; max = sum > max ? sum : max ; sum = sum > 0 ? sum : 0 ; } return max ; } }; DP solution Note Why can not compare to f[i - 1]) to find the maximum. Because including the f[i - 1] will skip elements, the sum will not from a subarray, but sequence of numbers in the array. This is very similar to problems Longest Common Substring and Longest Common Subsequence C++ DP class Solution { public : int maxSubArray ( vector < int >& nums ) { int n = nums . size (); int res = INT_MIN ; int f [ n + 1 ] = { 0 }; // f[i] = maxSubArray of first i elements f [ 0 ] = 0 ; // initial value for ( int i = 1 ; i <= n ; i ++ ) { f [ i ] = max ( f [ i - 1 ] + nums [ i - 1 ], nums [ i - 1 ]); res = max ( f [ i ], res ); } return res ; } }; // Notice this is a coordinate based DP problem, the meaning of the index i // in nums and f are different. Kadane's solution This is a DP solution, it reduced the f array to two variables. Making the problem O(n) O(n) in space. Discuss about this solution , where it make use of the idea of global maximum and local maximum. Note The idea of global maximum and local maximum is very usefull to solve DP problems. The local maximum is the maximum sum of a continuous subarray, the global maximum is keep the maximum of the local mmaximum. C++ Kadane's solution class Solution { public : int maxSubArray ( vector < int >& nums ) { int n = nums . size (); int res = INT_MIN ; int curr = 0 ; for ( int i = 0 ; i < n ; i ++ ) { curr = max ( curr + nums [ i ], nums [ i ]); res = max ( curr , res ); } return res ; } }; Prefix sum solution The ideas is we have array sums, sums[i] = A[0] +, ... + A[i] , called prefix sum. With one for loop we can find the maxSum so far and the minSum before it. The difference is the possible results, we collect the maximum of those differences. C++ prefix sum solution public class Solution { public int maxSubArray ( int [] A ) { if ( A == null || A . length == 0 ){ return 0 ; } int max = Integer . MIN_VALUE , sum = 0 , minSum = 0 ; for ( int i = 0 ; i < A . length ; i ++ ) { sum += A [ i ]; max = Math . max ( max , sum - minSum ); minSum = Math . min ( minSum , sum ); } return max ; } };","title":"Maximum Subarray"},{"location":"leetcode/array/notes/#maximum-subarray-ii","text":"Given an array of integers, find two non-overlapping subarrays which have the largest sum. The number in each subarray should be contiguous. Return the largest sum. Notice The subarray should contain at least one number Example For given [1, 3, -1, 2, -1, 2], the two subarrays are [1, 3] and [2, -1, 2] or [1, 3, -1, 2] and [2], they both have the largest sum 7. Prefix sum solution C++ Prefix sum solution class Solution { public : /* * @param nums: A list of integers * @return: An integer denotes the sum of max two non-overlapping subarrays */ int maxTwoSubArrays ( vector < int > nums ) { int n = nums . size (); int minSum = 0 ; int sums = 0 ; int maxSum = INT_MIN ; int left [ n ] = { 0 }; int right [ n ] = { 0 }; /* calculate the prefix sum */ for ( int i = 0 ; i < n ; i ++ ) { sums += nums [ i ]; maxSum = max ( maxSum , sums - minSum ); // minSum is previous calculated minSum = min ( minSum , sums ); left [ i ] = maxSum ; } /* calculate the postfix sum */ minSum = 0 ; sums = 0 ; maxSum = INT_MIN ; for ( int i = n - 1 ; i >= 0 ; i -- ) { sums += nums [ i ]; maxSum = max ( maxSum , sums - minSum ); minSum = min ( minSum , sums ); right [ i ] = maxSum ; } /* iterate the divider line, left[i] stored the maxSubArraySum * from nums[0] to nums[i], similar for right[i] */ maxSum = INT_MIN ; for ( int i = 0 ; i < n - 1 ; i ++ ) { maxSum = max ( maxSum , left [ i ] + right [ i + 1 ]); } return maxSum ; } }; Warning Cannot swap the highlighted lines. Because the maximum sum is calculated from current sum minus the previous minSum.","title":"Maximum Subarray II*"},{"location":"leetcode/array/notes/#maximum-subarray-iii","text":"Given an array of integers and a number k, find k non-overlapping subarrays which have the largest sum. The number in each subarray should be contiguous. Return the largest sum. Notice The subarray should contain at least one number Example Input: List = [-1,4,-2,3,-2,3] k = 2 Output: 8 Explanation: 4 + (3 + -2 + 3) = 8 DP solution Use the idea of global maximum and local maximum from Maximum Subarray . See this artical for detailed explaination of the solution. C++ class Solution { public : /** * My initial try: O(n^2 k) * Partitioning DP: f[n][k], maximum K subarrays of first n elements. * Last partition: A[j] ,... A[n - 1] * f[i][k] = max_{0 <= j < i}(f[j][k - 1] + MS(A[j] ,... A[i - 1])) * f[0][0] = * * Solution 2, O(nk) * local[i][k]: Max k subarray sum from \"first i elements\" that include nums[i] * global[i][k]: Max k subarray sum from \"first i elements\" that may not include nums[i] * * 2 cases: nums[i - 1] is kth subarray, nums[i - 1] belongs to kth subarray * local[i][k] = max(global[i - 1][k - 1], local[i - 1][k]) + nums[i - 1] * * 2 cases: not include nums[i - 1], include nums[i - 1] * global[i][k] = max(global[i - 1][k], local[i][k]) */ int maxSubArray ( vector < int > nums , int k ) { int n = nums . size (); int local [ n + 1 ][ k + 1 ] = { 0 }; int global [ n + 1 ][ k + 1 ] = { 0 }; for ( int j = 1 ; j <= k ; j ++ ) { // first j - 1 elements cannot form j groups, set to INT_MIN. local [ j - 1 ][ j ] = INT_MIN ; for ( int i = j ; i <= n ; i ++ ) { // must: i >= k. local [ i ][ j ] = max ( global [ i - 1 ][ j - 1 ], local [ i - 1 ][ j ]) + nums [ i - 1 ]; // the case when we divide k elements into k groups. if ( i == j ) { global [ i ][ j ] = local [ i ][ j ]; } else { global [ i ][ j ] = max ( global [ i - 1 ][ j ], local [ i ][ j ]); } } } return global [ n ][ k ]; } };","title":"Maximum Subarray III*"},{"location":"leetcode/array/notes/#maximum-subarray-difference","text":"Given an array with integers. Find two non-overlapping subarrays A and B, which |SUM(A) - SUM(B)| is the largest. Return the largest difference. Notice The subarray should contain at least one number Example For [1, 2, -3, 1], return 6. Prefix sum solution We use the similar idea for problem Maximum Subarray II . We have to maintain four arrays. from forward maximum and minimum subarray sum and backward maximum and minimum subarray sum. C++ Prefix sum solution class Solution { public : /* * @param nums: A list of integers * @return: value of maximum difference between two subarrays */ int maxDiffSubArrays ( vector < int > nums ) { int n = nums . size (); int minSum = 0 ; int sums = 0 ; int maxSum = INT_MIN ; int left_max [ n ] = { 0 }; int left_min [ n ] = { 0 }; int right_max [ n ] = { 0 }; int right_min [ n ] = { 0 }; for ( int i = 0 ; i < n ; i ++ ) { sums += nums [ i ]; maxSum = max ( maxSum , sums - minSum ); minSum = min ( minSum , sums ); left_max [ i ] = maxSum ; //left_min[i] = minSum; } minSum = INT_MAX ; sums = 0 ; maxSum = 0 ; for ( int i = 0 ; i < n ; i ++ ) { sums += nums [ i ]; minSum = min ( minSum , sums - maxSum ); maxSum = max ( maxSum , sums ); //left_max[i] = maxSum; left_min [ i ] = minSum ; } minSum = 0 ; sums = 0 ; maxSum = INT_MIN ; for ( int i = n - 1 ; i >= 0 ; i -- ) { sums += nums [ i ]; maxSum = max ( maxSum , sums - minSum ); minSum = min ( minSum , sums ); right_max [ i ] = maxSum ; //right_min[i] = minSum; } minSum = INT_MAX ; sums = 0 ; maxSum = 0 ; for ( int i = n - 1 ; i >= 0 ; i -- ) { sums += nums [ i ]; minSum = min ( minSum , sums - maxSum ); maxSum = max ( maxSum , sums ); //right_max[i] = maxSum; right_min [ i ] = minSum ; } int diff = INT_MIN ; for ( int i = 0 ; i < n - 1 ; i ++ ) { diff = max ( left_max [ i ] - right_min [ i + 1 ], diff ); diff = max ( right_max [ i + 1 ] - left_min [ i ], diff ); } return diff ; } };","title":"Maximum Subarray Difference*"},{"location":"leetcode/array/notes/#maximum-product-subarray","text":"DP solution It is similar to the problem Maximum Subarray . Notice the negative number, min multiply a minus number could become the largest product. C++ class Solution { public : int maxProduct ( vector < int >& nums ) { int n = nums . size (); int max_pro [ n ] = { 0 }; int min_pro [ n ] = { 0 }; int result = nums [ 0 ]; max_pro [ 0 ] = nums [ 0 ]; min_pro [ 0 ] = nums [ 0 ]; for ( int i = 1 ; i < n ; i ++ ) { if ( nums [ i ] > 0 ) { max_pro [ i ] = max ( max_pro [ i - 1 ] * nums [ i ], nums [ i ]); min_pro [ i ] = min ( min_pro [ i - 1 ] * nums [ i ], nums [ i ]); } else { max_pro [ i ] = max ( min_pro [ i - 1 ] * nums [ i ], nums [ i ]); min_pro [ i ] = min ( max_pro [ i - 1 ] * nums [ i ], nums [ i ]); } result = max ( result , max_pro [ i ]); } return result ; } }; Constant space solution Without need to check whether nums[i] is positive is negative, we can just find the maximum or minium of three cases. C++ class Solution { public : /* * @param nums: An array of integers * @return: An integer */ int maxProduct ( vector < int > nums ) { int n = nums . size (); int res = nums [ 0 ]; int cur_max = nums [ 0 ]; int cur_min = nums [ 0 ]; for ( int i = 1 ; i < n ; i ++ ) { int tmp = cur_max ; cur_max = max ( max ( cur_max * nums [ i ], nums [ i ]), cur_min * nums [ i ]); cur_min = min ( min ( cur_min * nums [ i ], nums [ i ]), tmp * nums [ i ]); res = max ( res , cur_max ); } return res ; } };","title":"Maximum Product Subarray"},{"location":"leetcode/array/notes/#subarray-product-less-than-k","text":"","title":"Subarray Product Less Than K"},{"location":"leetcode/array/notes/#subarray-sum","text":"Given an integer array, find a subarray where the sum of numbers is zero. Your code should return the index of the first number and the index of the last number. Notice There is at least one subarray that it's sum equals to zero. Example Given [-3, 1, 2, -3, 4], return [0, 2] or [1, 3]. Hash solution use a hash table to keep the prefix sum. Once we see another prefix sum that exists in the hash table, we discovered the subarray that sums to zero. However, pay attention to the indexing, because it requires to return the original array's index. C++ class Solution { public : /** * @param nums: A list of integers * @return: A list of integers includes the index of the first number * and the index of the last number */ vector < int > subarraySum ( vector < int > nums ){ int n = nums . size (); vector < int > res ( 2 , 0 ); int sum = 0 ; unordered_map < int , int > map ; map [ 0 ] = - 1 ; //important for ( int i = 0 ; i < n ; i ++ ) { sum += nums [ i ]; if ( map . count ( sum ) != 0 ) { res [ 0 ] = map [ sum ] + 1 ; res [ 1 ] = i ; break ; } map [ sum ] = i ; } return res ; } }; // test cases // [-1, 2, 3, -3] A // [0,-1, 1, 4, 1] sum // i j Note Pay attention to the initial value and initializethe map[0] = -1 ; This can be validated with an edge case. The time complexity is O(n) Prefix sum solution Calculate the prefix sum first and then use the prefix sum to find the subarray. This solution is O(n^2) O(n^2) class Solution { public : vector < int > subarraySum ( vector < int > nums ){ int n = nums . size (); vector < int > res ( 2 , 0 ); vector < int > sum ( n + 1 , 0 ); sum [ 0 ] = 0 ; for ( int i = 1 ; i <= n ; i ++ ) { sum [ i ] = sum [ i - 1 ] + nums [ i - 1 ]; } for ( int i = 0 ; i < n ; i ++ ) { for ( int j = i ; j <= n ; j ++ ) { if ( j > 1 && sum [ j ] - sum [ i ] == 0 ) { res [ 0 ] = i ; res [ 1 ] = j - 1 ; break ; } } } return res ; } };","title":"Subarray Sum*"},{"location":"leetcode/array/notes/#minimum-size-subarray-sum","text":"Accumulative sum solution Using accumulative sum and another moving pointer to check both the sum and the length of the subarray. C++ class Solution { public : int minSubArrayLen ( int s , vector < int >& nums ) { int n = nums . size (); if ( n == 0 ) return 0 ; int sum = 0 ; int res = INT_MAX ; int left = 0 ; for ( int i = 0 ; i < n ; i ++ ) { sum += nums [ i ]; while ( sum >= s ) { res = min ( res , i - left + 1 ); sum -= nums [ left ++ ]; } } return res != INT_MAX ? res : 0 ; } };","title":"Minimum Size Subarray Sum"},{"location":"leetcode/array/notes/#maximum-size-subarray-sum-equals-k","text":"Similar to Continuous Subarray Sum Hash solution Use a hash table to keep <sums, i> entries. Look up using sum - k . We only add to the hash table for the first time a value is appeared. It ensures the length of the found subarray is the largest. Notice you also have to initialize the hash with value <0, -1> to handle the edge case. C++ class Solution { public : int maxSubArrayLen ( vector < int >& nums , int k ) { int n = nums . size (); if ( n == 0 ) return 0 ; unordered_map < int , int > map ; int sum = 0 ; int left = 0 ; int res = INT_MIN ; map [ 0 ] = - 1 ; for ( int i = 0 ; i < n ; i ++ ) { sum += nums [ i ]; if ( map . count ( sum - k ) != 0 ) { left = map [ sum - k ] + 1 ; res = max ( res , ( i - left + 1 )); } if ( map . count ( sum ) == 0 ) { map [ sum ] = i ; } } return res != INT_MIN ? res : 0 ; } }; /* test cases: 1. [-1, 1], -1 \u5982\u679c\u6ca1\u6709\u521d\u59cb\u5316hash\uff0c\u8fd9\u4e2acase\u5c31\u4f1a\u9519\u8bef [-1, 0] sums 2. [-1], 0 3. [-1], -1 4. if return the result will be 1 if there is no res variable [1, 1, 0], 1 [1, 2, 2], 1 */","title":"Maximum Size Subarray Sum Equals k"},{"location":"leetcode/array/notes/#subarray-sum-equals-k","text":"Prefix sum solution Use prefix sum to find the subarray sum. Two pointer to check all the possible subarray sum. C++ prefix sum solution public class Solution { public int subarraySum ( int [] nums , int k ) { int count = 0 ; int [] sum = new int [ nums . length + 1 ]; sum [ 0 ] = 0 ; for ( int i = 1 ; i <= nums . length ; i ++ ) sum [ i ] = sum [ i - 1 ] + nums [ i - 1 ]; for ( int start = 0 ; start < nums . length ; start ++ ) { for ( int end = start + 1 ; end <= nums . length ; end ++ ) { if ( sum [ end ] - sum [ start ] == k ) count ++ ; } } return count ; } } Hash solution Use a map to store the prefix sum and a counter. The idea is while calculating prefix sums, if we find an sums - k exist in the map, we found one of target subarray. The subtilty is for a particular prefix sum, there might be multiple earlier prefix sums differ from it by k. We should take this into account. Compare to the hash solution for problem Subarray Sum . /* k = 2 i = 1, 2, 3 sum = 1, 2, 3 cnt = 0, 1, 2 key = 1, 2, 3 val = 1, 1, 1 the reason that the cnt += map[sum - k], not cnt += 1 is that the prefix sum \"sum - k\" has been shown up for total of map[sum - k] times. All those prefix sum could be result of distinct subarrays between current prefix sum and previous prefix sum \"sum - k\" */ C++ class Solution { public : int subarraySum ( vector < int >& nums , int k ) { int n = nums . size (); // key=prefix sum, val=appearance unordered_map < int , int > map ; int cnt = 0 ; int sum = 0 ; map [ 0 ] = 1 ; for ( int i = 0 ; i < n ; i ++ ) { sum += nums [ i ]; if ( map . count ( sum - k ) != 0 ) { cnt += map [ sum - k ]; } map [ sum ] += 1 ; } return cnt ; } }; Warning Notice you have to initialize the map[0] = 1 ; this is because for cases such as [1, 1, 1] , when i = 1 , sum = 2 , [1,1] should be counted as one subarray. Without setting map[0] = 1 at first hand, it will give incorrect results.","title":"Subarray Sum Equals K"},{"location":"leetcode/array/notes/#max-sum-of-subarry-no-larger-than-k","text":"This problem in geeksforgeeks as \"Maximum sum subarray having sum less than or equal to given sum\". It has been discussed here. This problem is the basis to solve the problem 363. Max Sum of Rectangle No Larger Than K. Solution 1 using prefix sum and set calculate prefix and using a set to store individual prefix sum, ( vector also works). In each iteration, we lookup the value preSum - k in the set. Notice we can use binary search to find the smallest element that >= preSum - k . We can use lower_bound to achieve that. Notice if it is asking the sum less than k we have to use upper_bound C++ int maxSumSubarryNoLargerThanK ( int A [], int n , int k ) { set < int > preSumSet ; preSumSet . insert ( 0 ); int res = 0 , preSum = 0 ; for ( int i = 0 ; i < n ; ++ i ) { preSum += A [ i ]; set < int >:: iterator siter = preSumSet . lower_bound ( preSum - k ); if ( siter != preSumSet . end () { res = max ( res , preSum - * siter ); } preSumSet . insert ( preSum ); } return res ; }","title":"Max Sum of Subarry No Larger Than K*"},{"location":"leetcode/array/notes/#max-sum-of-rectangle-no-larger-than-k","text":"Solution 1 iterate the wide of the matrix and using prefix sum and set lower_bound . To optimize it with the brute force solution, you will find this problem is a combination of the problem Maximum Sum Rectangular Submatrix in Matrix and problem Max Sum of Subarry No Larger Than K. From the problem Max Sum of Subarry No Larger Than K, we have to enumerate the width of the sub-matrix and sum up all row elements and get an array of length m , m is the number of rows of the matrix. Then apply the method. C++ class Solution { public : int maxSumSubmatrix ( vector < vector < int >>& matrix , int k ) { if ( matrix . empty ()) return 0 ; int m = matrix . size (); int n = m ? matrix [ 0 ]. size () : 0 ; int res = INT_MIN ; for ( int l = 0 ; l < n ; ++ l ) { vector < int > sums ( m , 0 ); for ( int r = l ; r < n ; ++ r ) { for ( int i = 0 ; i < m ; ++ i ) { sums [ i ] += matrix [ i ][ r ]; } set < int > preSumSet ; preSumSet . insert ( 0 ); int preSum = 0 , curMax = INT_MIN ; for ( int sum : sums ) { preSum += sum ; set < int >:: iterator it = preSumSet . lower_bound ( preSum - k ); if ( it != preSumSet . end ()) { curMax = max ( curMax , preSum - * it ); } preSumSet . insert ( preSum ); } res = max ( res , curMax ); } } return res ; } }; Note The complexity is n\u22c5n\u22c5(m+m\\log m)=O(n\u22c5n\u22c5m\\log m) n\u22c5n\u22c5(m+m\\log m)=O(n\u22c5n\u22c5m\\log m) Notice the use of lower_bound, this function return iterator point to element greater than or equal to the value curSum - k, if use upper_bound, it will return iterator points to element greater than curSum - k, which would miss the equal to K case. Solution 2 using merge sort The idea is similar that solution 1. Instead of calculate preSum on the fly, we finish calculation and pass it to a mergeSort routine. The use mergeSort here is to find the A[j] - A[i] <= k efficiently, O(nlogn) . The complexity is n\u22c5n\u22c5(m+m\u22c5\\log m)=O(n\u22c5n\u22c5m\u22c5\\log m) C++ class Solution { public : int maxSumSubmatrix ( vector < vector < int >>& matrix , int k ) { int m = matrix . size (); int n = m ? matrix [ 0 ]. size () : 0 ; int res = INT_MIN ; vector < long long > sums ( m + 1 , 0 ); for ( int l = 0 ; l < n ; ++ l ) { vector < long long > sumInRow ( m , 0 ); for ( int r = l ; r < n ; ++ r ) { for ( int i = 0 ; i < m ; ++ i ) { sumInRow [ i ] += matrix [ i ][ r ]; sums [ i + 1 ] = sums [ i ] + sumInRow [ i ]; } res = max ( res , mergeSort ( sums , 0 , m + 1 , k )); if ( res == k ) return k ; } } return res ; } int mergeSort ( vector < long long >& sums , int start , int end , int k ) { if ( end == start + 1 ) return INT_MIN ; int mid = start + ( end - start ) / 2 ; int res = mergeSort ( sums , start , mid , k ); if ( res == k ) return k ; res = max ( res , mergeSort ( sums , mid , end , k )); if ( res == k ) return k ; long long cache [ end - start ]; int j = mid , c = 0 , t = mid ; for ( int i = start ; i < mid ; ++ i ) { /* search first time sums[j] - sums[i] > k */ while ( j < end && sums [ j ] - sums [ i ] <= k ) ++ j ; /* sums[j - 1] - sums[i] <= k, make sure j - 1 is in right side */ if ( j - 1 >= mid ) { res = max ( res , ( int )( sums [ j - 1 ] - sums [ i ])); if ( res == k ) return k ; } /* parallel merge */ while ( t < end && sums [ t ] < sums [ i ]) { cache [ c ++ ] = sums [ t ++ ]; } cache [ c ++ ] = sums [ i ]; } /* parallel merge */ for ( int i = start ; i < t ; ++ i ) { sums [ i ] = cache [ i - start ]; } return res ; } };","title":"Max Sum of Rectangle No Larger Than K"},{"location":"leetcode/array/notes/#maximum-sum-rectangular-submatrix-in-matrix","text":"","title":"Maximum Sum Rectangular Submatrix in Matrix*"},{"location":"leetcode/array/notes/#subarray-sum-closest","text":"","title":"Subarray Sum Closest*"},{"location":"leetcode/array/notes/#shortest-unsorted-continuous-subarray-count-inversions","text":"Solution 1 using merge sort This problem is the basic of using merge sort to solve lots of hard problems. C++ long long merge_and_count ( int A [], int start , int end ) { if ( end - start <= 1 ) return 0 ; long long count = 0 ; int mid = start + ( end - start ) / 2 ; count = merge_and_count ( A , start , mid ) + merge_and_count ( A , mid , end ); int j = mid ; int cache [ end - start ]; for ( int i = start , k = 0 ; i < mid ; i ++ ) { while ( j < end && A [ i ] > A [ j ]) { cache [ k ++ ] = A [ j ++ ]; count += mid - i ; } cache [ k ++ ] = A [ i ]; } // copy cache[0, j - start) to A[start, j) // if j < end, then A[j, end) in final place. for ( int i = start ; i < j ; i ++ ) { A [ i ] = cache [ i - start ]; } return count ; }","title":"Shortest Unsorted Continuous Subarray (Count inversions)"},{"location":"leetcode/array/notes/#count-inversion-course-assignment","text":"Solution 1 using merge sort This problem is the basic of using merge sort to solve lots of hard problems. C++ long long merge_and_count ( int A [], int start , int end ) { if ( end - start <= 1 ) return 0 ; long long count = 0 ; int mid = start + ( end - start ) / 2 ; count = merge_and_count ( A , start , mid ) + merge_and_count ( A , mid , end ); int j = mid ; int cache [ end - start ]; for ( int i = start , k = 0 ; i < mid ; i ++ ) { while ( j < end && A [ i ] > A [ j ]) { cache [ k ++ ] = A [ j ++ ]; count += mid - i ; } cache [ k ++ ] = A [ i ]; } // copy cache[0, j - start) to A[start, j) // if j < end, then A[j, end) in final place. for ( int i = start ; i < j ; i ++ ) { A [ i ] = cache [ i - start ]; } return count ; }","title":"Count Inversion (course assignment)"},{"location":"leetcode/array/notes/#315-count-of-smaller-numbers-after-self","text":"Solution 1 Merge sort One important point to remember is you have to create pairs out of the array element and its index, because during merge sort, when we count each value, we don't know where to put those count values in the result vector. The second merge solutions run much faster than the first one. C++ Merge sort class Solution { public : vector < int > countSmaller ( vector < int >& nums ) { int n = nums . size (); vector < int > res ( n , 0 ); vector < pair < int , int > > vp ; for ( int i = 0 ; i < n ; i ++ ) { vp . emplace_back ( nums [ i ], i ); } merge_sort_count ( vp , 0 , n , res ); return res ; } private : void merge_sort_count ( vector < pair < int , int > >& nums , int start , int end , vector < int >& res ) { if ( end - start <= 1 ) return ; int mid = start + ( end - start ) / 2 ; merge_sort_count ( nums , start , mid , res ); merge_sort_count ( nums , mid , end , res ); vector < pair < int , int > > cache ( end - start , pair < int , int > ( 0 , 0 )); int j = mid , k = 0 , t = mid ; for ( int i = start ; i < mid ; i ++ ) { j = mid ; while ( j < end && nums [ i ]. first > nums [ j ]. first ) { // found smaller elements res [ nums [ i ]. second ] ++ ; j ++ ; } while ( t < end && nums [ i ]. first > nums [ t ]. first ) { cache [ k ++ ] = nums [ t ++ ]; } cache [ k ++ ] = nums [ i ]; } for ( int i = start ; i < j ; i ++ ) { nums [ i ] = cache [ i - start ]; } return ; } }; C++ more efficient Merge sort class Solution { public : vector < int > countSmaller ( vector < int >& nums ) { int n = nums . size (); vector < int > res ( n , 0 ); vector < pair < int , int > > vp ; for ( int i = 0 ; i < n ; i ++ ) { vp . emplace_back ( nums [ i ], i ); } mergeSort ( vp , 0 , n , res ); return res ; } void mergeSort ( vector < pair < int , int >>& x , int start , int end , vector < int >& res ) { if ( end - start <= 1 ) return ; int mid = start + ( end - start ) / 2 ; mergeSort ( x , start , mid , res ); mergeSort ( x , mid , end , res ); vector < pair < int , int > > cache ( end - start , pair < int , int > ( 0 , 0 )); int i = start , j = mid , k = 0 ; while ( i < mid && j < end ) { if ( x [ i ]. first <= x [ j ]. first ) { cache [ k ++ ] = x [ i ]; res [ x [ i ]. second ] += j - mid ; ++ i ; } else { cache [ k ++ ] = x [ j ++ ]; } } while ( i < mid ) { cache [ k ++ ] = x [ i ]; res [ x [ i ]. second ] += end - mid ; ++ i ; } while ( j < end ) cache [ k ++ ] = x [ j ++ ]; for ( i = start , k = 0 ; i < end ; ++ i , ++ k ) { x [ i ] = cache [ k ]; } } }; C++ BST class Solution { public : class TreeNode { public : int val , smallerCnt ; TreeNode * left , * right ; TreeNode ( int v , int s ) : left ( NULL ), right ( NULL ), val ( v ), smallerCnt ( s ){} }; vector < int > countSmaller ( vector < int >& nums ) { int n = nums . size (); if ( n == 0 ) return {}; vector < int > res ( n , 0 ); TreeNode * root = NULL ; for ( int i = n - 1 ; i >= 0 ; -- i ) root = insert ( root , nums [ i ], i , 0 , res ); return res ; } private : TreeNode * insert ( TreeNode * node , int val , int idx , int preSum , vector < int >& res ) { if ( node == NULL ) { node = new TreeNode ( val , 0 ); res [ idx ] = preSum ; } else if ( node -> val > val ) { node -> smallerCnt ++ ; node -> left = insert ( node -> left , val , idx , preSum , res ); } else { node -> right = insert ( node -> right , val , idx , preSum + node -> smallerCnt + (( node -> val < val ) ? 1 : 0 ), res ); } return node ; } };","title":"315. Count of Smaller Numbers After Self"},{"location":"leetcode/array/notes/#continuous-subarray-sum","text":"Hash solution Once see a multiple of K, you should consider the modulor operation % The values put into to the hash only for the first time, this is similar to the case in the problem Maximum Size Subarray Sum Equals k. C++ Hash soution class Solution { public : bool checkSubarraySum ( vector < int >& nums , int k ) { int n = nums . size (); if ( n == 0 ) return false ; unordered_map < int , int > map ; int sum = 0 ; map [ 0 ] = - 1 ; // test case [0, 0], 0 for ( int i = 0 ; i < n ; i ++ ) { sum += nums [ i ]; if ( k != 0 ) sum = sum % k ; if ( map . count ( sum ) != 0 ) { if ( i - map [ sum ] > 1 ) { return true ; } } else { map [ sum ] = i ; } } return false ; } };","title":"Continuous Subarray Sum"},{"location":"leetcode/array/notes/#contiguous-array","text":"Similar problems: Continuous Subarray Sum Maximum Size Subarray Sum Equals k Hash solution This problem is very similar to the problem Continuous Subarray Sum . However, there is a trick to calculate the cummulative sum, treat 0 as -1 . C++ class Solution { public : int findMaxLength ( vector < int >& nums ) { int n = nums . size (); int res = 0 ; int cnt = 0 ; unordered_map < int , int > map ; map [ 0 ] = - 1 ; // test case: [0, 1] for ( int i = 0 ; i < n ; i ++ ) { cnt += nums [ i ] == 0 ? - 1 : 1 ; if ( map . count ( cnt ) != 0 ) { res = max ( res , i - map [ cnt ]); } else { map [ cnt ] = i ; } } return res ; } };","title":"Contiguous Array"},{"location":"leetcode/array/notes/#split-array-with-equal-sum","text":"Cummulative sum soluiton Because of the symetric property of the head subarray and trailing subarray, we can calculate cumulative sum from both direction. This can help to fix the index i and k . we can enumerate the index j in between. C++ cummulateive sum solution class Solution { public : bool splitArray ( vector < int >& nums ) { int n = nums . size (); int sum1 [ n ] = { 0 }; int sum2 [ n ] = { 0 }; sum1 [ 0 ] = nums [ 0 ]; for ( int i = 1 ; i < n ; i ++ ) { sum1 [ i ] = sum1 [ i - 1 ] + nums [ i ]; } sum2 [ n - 1 ] = nums [ n - 1 ]; for ( int i = n - 2 ; i >= 0 ; i -- ) { sum2 [ i ] = sum2 [ i + 1 ] + nums [ i ]; } // notice the index bounds for ( int i = 1 ; i < n - 5 ; i ++ ) { for ( int k = n - 2 ; k > i + 3 ; k -- ) { if ( sum1 [ i ] - nums [ i ] == sum2 [ k ] - nums [ k ]) { for ( int j = i + 2 ; j < k - 1 ; j ++ ) { int sumij = sum1 [ j ] - nums [ j ] - sum1 [ i ]; int sumjk = sum2 [ j ] - nums [ j ] - sum2 [ k ]; if ( sumij == sumjk ) { return true ; } } } } } return false ; } };","title":"Split Array with Equal Sum"},{"location":"leetcode/array/notes/#split-array-largest-sum","text":"Similar problems: Copy Books . DP solution Notice the edge case: [1, INT_MAX] , use double will can avoid integer overflow. C++ DP /** * equivalent to the lintcode copy books problem * * last step: mth subarray A[j], ..., A[i - 1]. * State: f[m][n]: minmax sum of m subarrays that include n elements * Equation: f[m][n] = min_{0<=j<n}(max(f[m - 1][j], sum(A[j], ..., A[n - 1]))) * Init: f[0][n] = INT_MAX; * f[0][0] = 0; * NB: notice a special case: [1, 2147483247], 2 * the sum will overflow in the state update, You use a double type */ class Solution { public : int splitArray ( vector < int >& nums , int m ) { int n = nums . size (); double f [ m + 1 ][ n + 1 ]; f [ 0 ][ 0 ] = 0 ; for ( int i = 1 ; i <= n ; i ++ ) { f [ 0 ][ i ] = INT_MAX ; } double sum = 0 ; for ( int k = 1 ; k <= m ; k ++ ) { f [ k ][ 0 ] = 0 ; for ( int i = 1 ; i <= n ; i ++ ) { sum = 0 ; f [ k ][ i ] = INT_MAX ; for ( int j = i ; j >= 0 ; j -- ) { //j = i, mean sum = 0. f [ k ][ i ] = min ( f [ k ][ i ], max ( f [ k - 1 ][ j ], sum )); if ( j > 0 ) { sum += nums [ j - 1 ]; } } } } return f [ m ][ n ]; } };","title":"Split Array Largest Sum"},{"location":"leetcode/array/notes/#copy-books","text":"DP solution There are i books, consider the last copier, he can copy A[j], ..., A[i-1] . The first k-1 copier copy A[0], ..., A[j - 1] . Define state: f[k][i] , meaning the k-th copier copy i books. State transition equation: f[k][i] = \\min_{0 \\le j \\le i} \\max(f[k - 1][j], A[j] + ... + A[i - 1]) f[k][i] = \\min_{0 \\le j \\le i} \\max(f[k - 1][j], A[j] + ... + A[i - 1]) C++ DP solution class Solution { public : /** * last step: last copier copy A[j], ... A[i-1] * first k-1 copier --> A[0], ... A[j - 1]. * f[k][i]: k copier copy i books. * f[k][i] = \\min_{0 \\le j \\le i} \\max(f[k - 1][j], A[j] + ... + A[i - 1]) */ int copyBooks ( vector < int > & pages , int K ) { // write your code here int n = pages . size (); if ( n == 0 ) { return 0 ; } if ( K > n ) { K = n ; } int f [ K + 1 ][ n + 1 ]; /* init */ f [ 0 ][ 0 ] = 0 ; for ( int j = 1 ; j <= n ; j ++ ) { f [ 0 ][ j ] = INT_MAX ; } int sum = 0 ; for ( int k = 1 ; k <= K ; k ++ ) { f [ k ][ 0 ] = 0 ; for ( int i = 1 ; i <= n ; i ++ ) { sum = 0 ; f [ k ][ i ] = INT_MAX ; for ( int j = i ; j >= 0 ; j -- ) { f [ k ][ i ] = min ( f [ k ][ i ], max ( f [ k - 1 ][ j ], sum )); if ( j > 0 ) { sum += pages [ j - 1 ]; } } } } return f [ K ][ n ]; } }; Note We have to enumerate the index j , the highlighted code used a clever technique to optimize this task. It enumerate j backwards. While this seems impossible at the first glance, how can you calculate the states from right to left in DP? Notice the index j is in the upper row (row k-1 ). Once we are in the k -th row, the values in the k-1 -th row are all given.","title":"Copy Books*"},{"location":"leetcode/array/notes/#maximum-average-subarray-i","text":"Prefix sum solution class Solution { public : double findMaxAverage ( vector < int >& nums , int k ) { int n = nums . size (); double sums [ n ] = { 0 }; double max_avg = INT_MIN ; sums [ 0 ] = nums [ 0 ]; for ( int i = 1 ; i < n ; i ++ ) { sums [ i ] = sums [ i - 1 ] + nums [ i ]; } for ( int i = k - 1 ; i < n ; i ++ ) { double avg = ( sums [ i ] - sums [ i - k + 1 ] + nums [ i - k + 1 ]) / k ; max_avg = max ( max_avg , avg ); } return max_avg ; } };","title":"Maximum Average Subarray I"},{"location":"leetcode/array/notes/#maximum-average-subarray-ii","text":"Prefix sum solution This is still a brute force solution. time complexity: O(n^2) O(n^2) space complexity: O(n) O(n) C++ prefix sum solution class Solution { public : double findMaxAverage ( vector < int >& nums , int k ) { int n = nums . size (); vector < int > sums = nums ; for ( int i = 1 ; i < n ; ++ i ) { sums [ i ] = sums [ i - 1 ] + nums [ i ]; } double res = ( double ) sums [ k - 1 ] / k ; for ( int i = k ; i < n ; ++ i ) { double t = sums [ i ]; if ( t > res * ( i + 1 )) res = t / ( i + 1 ); for ( int j = 0 ; j < i - k + 1 ; ++ j ) { t = sums [ i ] - sums [ j ]; if ( t > res * ( i - j )) res = t / ( i - j ); } } return res ; } }; Space optimized solution We could avoid using the prefix sum array and only use two variables to record the prefix sum at any particular instance. One for record prefix sum of first i elements. Another for the inner loop to check whether removing an element from the beginning will make a new maximum value or not. C++ space optimized class Solution { public : double findMaxAverage ( vector < int >& nums , int k ) { int n = nums . size (); /* range is half open */ double sumsAll = accumulate ( nums . begin (), nums . begin () + k , 0 ); double sums = sumsAll , res = sumsAll / k ; for ( int i = k ; i < n ; ++ i ) { sumsAll += nums [ i ]; sums = sumsAll ; if ( sums > res * ( i + 1 )) res = sums / ( i + 1 ); for ( int j = 0 ; j < i - k + 1 ; ++ j ) { sums -= nums [ j ]; if ( sums > res * ( i - j )) res = sums / ( i - j ); } } return res ; } }; Binary search solution The key question to answer in order to solve this problem using binary search is that what condition we should use to serve the similar effect of cutting the input space in half in the original binary search. The answer is we can test whether it is possible to have an average value of subarray whose length is greater than or equal to k in the upper half. C++ binary search soluiton class Solution { public : double findMaxAverage ( vector < int >& nums , int k ) { int n = nums . size (); double upper = INT_MIN , lower = INT_MAX ; for ( auto num : nums ) { upper = max ( upper , ( double ) num ); lower = min ( lower , ( double ) num ); } while ( lower + 0.00001 < upper ) { double mid = lower + ( upper - lower ) / 2 ; if ( isLarger ( nums , mid , k )) { // is average value >= mid? lower = mid ; } else { upper = mid ; } } return lower ; } /* return true if a greater average value is possible */ bool isLarger ( vector < int >& nums , double mid , int k ) { int n = nums . size (); double sums = 0 , prev = 0 , prev_min = 0 ; for ( int i = 0 ; i < k ; i ++ ) { sums += nums [ i ] - mid ; } if ( sums >= 0 ) { return true ; } /* * we keep looking for whether a subarray sum of length >= k in array * \"sums\" is possible to be greater than zero. If such a subarray exist, * it means that the target average value is greater than the \"mid\" * value. We look at the front part of sums that at least k element * apart from i. If we can find the minimum of the sums[0, 1, ..., i - k] * and check if sums[i] - min(sum[0, 1, ..., i - k]) >= 0. If this is the * case, it indicate there exist a subarray of length >= k with sum * greater than 0 in sums, we can return ture, otherwise, false. */ for ( int i = k ; i < n ; i ++ ) { sums += nums [ i ] - mid ; prev += nums [ i - k ] - mid ; prev_min = min ( prev_min , prev ); if ( sums >= prev_min ) return true ; } return false ; } }; Note Notice the initial value of prev_min is set to 0 not INT_MAX; Try to understand why set the initial value of prev_min to INT_MAX cannot pass the test case: [8,9,3,1,8,3,0,6,9,2] , 8. Deque solution C++ deque solution class Solution { public : double findMaxAverage ( vector < int >& nums , int k ) { int n = nums . size (); vector < double > sums ( n , 0 ); deque < int > q ; sums [ 0 ] = nums [ 0 ]; for ( int i = 1 ; i < n ; ++ i ) sums [ i ] = sums [ i - 1 ] + nums [ i ]; double res = sums [ n - 1 ] / n ; for ( int j = k - 1 ; j < n ; ++ j ) { while ( q . size () >= 2 && density ( sums , q [ q . size () - 2 ], q . back () - 1 ) >= density ( sums , q . back (), j - k )) { q . pop_back (); } q . push_back ( j - k + 1 ); while ( q . size () >= 2 && density ( sums , q [ 0 ], j ) <= density ( sums , q [ 1 ], j )) { q . pop_front (); } res = max ( res , density ( sums , q . front (), j )); } return res ; } private : double density ( vector < double >& sums , int l , int r ) { if ( l == 0 ) return sums [ r ] / ( r + 1 ); return ( sums [ r ] - sums [ l - 1 ]) / ( r - l + 1 ); } };","title":"Maximum Average Subarray II"},{"location":"leetcode/array/notes/#range-sum-query-immutable","text":"Prefix sum solution Use prefix sum to record the accumulative sum of the array in the constructor. The algorithm is O(n) O(n) in space and O(1) O(1) in time. C++ class NumArray { private : vector < int > sums ; int n ; public : NumArray ( vector < int > nums ) { n = nums . size (); sums . resize ( n + 1 , 0 ); sums [ 0 ] = 0 ; for ( int i = 1 ; i <= n ; ++ i ) { sums [ i ] = sums [ i - 1 ] + nums [ i - 1 ]; } } int sumRange ( int i , int j ) { return sums [ j + 1 ] - sums [ i ]; } }; /** * Your NumArray object will be instantiated and called as such: * NumArray obj = new NumArray(nums); * int param_1 = obj.sumRange(i,j); */","title":"Range Sum Query - Immutable"},{"location":"leetcode/array/notes/#range-sum-query-mutable","text":"Segment tree solution Using segment tree, the solution is given at Leetcode Solution . C++ segment tree class NumArray { private : vector < int > tree ; int n ; public : NumArray ( vector < int > nums ) { n = nums . size (); tree . resize ( 2 * n , 0 ); for ( int i = n , j = 0 ; i < 2 * n ; ++ i , ++ j ) { tree [ i ] = nums [ j ]; } for ( int i = n - 1 ; i > 0 ; -- i ) { tree [ i ] = tree [ 2 * i ] + tree [ 2 * i + 1 ]; } } void update ( int i , int val ) { int pos = n + i ; int left = 0 ; int right = 0 ; tree [ pos ] = val ; while ( pos > 0 ) { left = pos ; right = pos ; if ( pos % 2 == 0 ) { right = pos + 1 ; } if ( pos % 2 == 1 ) { left = pos - 1 ; } tree [ pos / 2 ] = tree [ left ] + tree [ right ]; pos /= 2 ; } } int sumRange ( int i , int j ) { int left = i + n ; int right = j + n ; int sum = 0 ; while ( left <= right ) { if ( left % 2 == 1 ) { sum += tree [ left ]; left ++ ; } if ( right % 2 == 0 ) { sum += tree [ right ]; right -- ; } left /= 2 ; right /= 2 ; } return sum ; } }; /** * Your NumArray object will be instantiated and called as such: * NumArray obj = new NumArray(nums); * obj.update(i,val); * int param_2 = obj.sumRange(i,j); */ Binary Indexed Tree solution I Using Binary indexed tree, we are able to solve it optimally in O(\\log n) O(\\log n) . The solution originally from here class NumArray { private : vector < int > arr ; vector < int > BIT ; int n ; public : NumArray ( vector < int > nums ) { arr = nums ; n = nums . size (); BIT . resize ( n + 1 , 0 ); for ( int i = 0 ; i < n ; i ++ ) { init ( i , arr [ i ]); } } void init ( int i , int val ) { i ++ ; while ( i <= n ) { BIT [ i ] += val ; // BIT[i] = nums[0, i - 1]; i += i & ( - i ); } } void update ( int i , int val ) { int diff = val - arr [ i ]; arr [ i ] = val ; init ( i , diff ); } int sumRange ( int i , int j ) { return getSum ( j ) - getSum ( i - 1 ); } int getSum ( int i ) { i ++ ; int sum = 0 ; while ( i > 0 ) { sum += BIT [ i ]; i -= i & ( - i ); } return sum ; } }; /** * Your NumArray object will be instantiated and called as such: * NumArray obj = new NumArray(nums); * obj.update(i,val); * int param_2 = obj.sumRange(i,j); */ Binary Indexed Tree solution II Similar to the above solution, We have combined the init and update . To make it consistant with the solution with problem [Range Sum Query 2D - Mutable] class NumArray { private : vector < int > arr ; vector < int > BIT ; int n ; public : NumArray ( vector < int > nums ) { n = nums . size (); BIT . resize ( n + 1 , 0 ); arr . resize ( n , 0 ); for ( int i = 0 ; i < n ; i ++ ) { update ( i , nums [ i ]); } } /* We can combine the init and update like this */ void update ( int i , int val ) { int diff = val - arr [ i ]; arr [ i ] = val ; // here we initialize arr[i] i ++ ; while ( i <= n ) { BIT [ i ] += diff ; // BIT[i] = nums[0, i - 1]; i += i & ( - i ); } } int sumRange ( int i , int j ) { return getSum ( j ) - getSum ( i - 1 ); } int getSum ( int i ) { i ++ ; int sum = 0 ; while ( i > 0 ) { sum += BIT [ i ]; i -= i & ( - i ); } return sum ; } }; /** * Your NumArray object will be instantiated and called as such: * NumArray obj = new NumArray(nums); * obj.update(i,val); * int param_2 = obj.sumRange(i,j); */","title":"Range Sum Query - Mutable"},{"location":"leetcode/array/notes/#range-sum-query-2d-immutable_1","text":"Prefix sum solution Extended from the 1d array, we can use the prefix sum of the 2d matrix. we use extra space to store the accumulative sum of the submatrix with upper left coordinate (0, 0) and lower right coordinate (i, j) . C++ prefix sum solution class NumMatrix { private : vector < vector < int > > dp ; public : NumMatrix ( vector < vector < int >> matrix ) { int m = matrix . size (); if ( m == 0 ) return ; int n = matrix [ 0 ]. size (); //dp = vector<vector<int> (m + 1, vector<int>(n + 1, 0)); dp . resize ( m + 1 , vector < int > ( n + 1 , 0 )); for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { dp [ i + 1 ][ j + 1 ] = dp [ i ][ j + 1 ] + dp [ i + 1 ][ j ] + matrix [ i ][ j ] - dp [ i ][ j ]; } } } int sumRegion ( int row1 , int col1 , int row2 , int col2 ) { return dp [ row2 + 1 ][ col2 + 1 ] - dp [ row2 + 1 ][ col1 ] - dp [ row1 ][ col2 + 1 ] + dp [ row1 ][ col1 ]; } }; /** * Your NumMatrix object will be instantiated and called as such: * NumMatrix obj = new NumMatrix(matrix); * int param_1 = obj.sumRegion(row1,col1,row2,col2); */ Note This version create the m + 1 by n + 1 dp array to record the prefix sum. The code is clean and elegant. Alternative prefix sum solution The idea is the same, in the following solution, we have a m by n 2d array to record the accumulative sum. See how complex the code is. C++ prefix sum solution class NumMatrix { private : vector < vector < int >> dp ; int m ; int n ; public : NumMatrix ( vector < vector < int >> matrix ) { m = matrix . size (); if ( m == 0 ) return ; n = matrix [ 0 ]. size (); dp . resize ( m , vector < int > ( n , 0 )); for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { dp [ i ][ j ] = matrix [ i ][ j ]; if ( i > 0 ) { dp [ i ][ j ] += dp [ i - 1 ][ j ]; } if ( j > 0 ) { dp [ i ][ j ] += dp [ i ][ j - 1 ]; } if ( i > 0 && j > 0 ) { dp [ i ][ j ] -= dp [ i - 1 ][ j - 1 ]; } } } } int sumRegion ( int row1 , int col1 , int row2 , int col2 ) { int res = 0 ; res = dp [ row2 ][ col2 ]; if ( row1 > 0 ) { res -= dp [ row1 - 1 ][ col2 ]; } if ( col1 > 0 ) { res -= dp [ row2 ][ col1 - 1 ]; } if ( row1 > 0 && col1 > 0 ) { res += dp [ row1 - 1 ][ col1 - 1 ]; } return res ; } }; /** * Your NumMatrix object will be instantiated and called as such: * NumMatrix obj = new NumMatrix(matrix); * int param_1 = obj.sumRegion(row1,col1,row2,col2); */ Note This version is a little complex. However, the way it was written reflects a very important practice when operating on a 2d array, that is: to check the validation of the array.","title":"Range Sum Query 2D - Immutable"},{"location":"leetcode/array/notes/#range-sum-query-2d-mutable_1","text":"Binary Indexed Tree solution We use 2D version of Binary Index Tree. Some of the explaination can be found at Topcoder tutorial C++ BIT solution class NumMatrix { private : vector < vector < int > > nums ; vector < vector < int > > tree ; int m ; int n ; public : NumMatrix ( vector < vector < int >> matrix ) { if ( matrix . size () == 0 || matrix [ 0 ]. size () == 0 ) return ; m = matrix . size (); n = matrix [ 0 ]. size (); tree . resize ( m + 1 , vector < int > ( n + 1 , 0 )); nums . resize ( m , vector < int > ( n , 0 )); for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { update ( i , j , matrix [ i ][ j ]); } } } void update ( int row , int col , int val ) { if ( m == 0 || n == 0 ) return ; int diff = val - nums [ row ][ col ]; nums [ row ][ col ] = val ; for ( int i = row + 1 ; i <= m ; i += i & ( - i )) { for ( int j = col + 1 ; j <= n ; j += j & ( - j )) { tree [ i ][ j ] += diff ; } } } int sumRegion ( int row1 , int col1 , int row2 , int col2 ) { if ( m == 0 || n == 0 ) return 0 ; return getSum ( row2 + 1 , col2 + 1 ) - getSum ( row1 , col2 + 1 ) - getSum ( row2 + 1 , col1 ) + getSum ( row1 , col1 ); } int getSum ( int row , int col ) { int sum = 0 ; for ( int i = row ; i > 0 ; i -= i & ( - i )) { for ( int j = col ; j > 0 ; j -= j & ( - j )) { sum += tree [ i ][ j ]; } } return sum ; } }; /** * Your NumMatrix object will be instantiated and called as such: * NumMatrix obj = new NumMatrix(matrix); * obj.update(row,col,val); * int param_2 = obj.sumRegion(row1,col1,row2,col2); */","title":"Range Sum Query 2D - Mutable"},{"location":"leetcode/array/notes/#count-of-range-sum","text":"Solution 1 Merge sort using inplace_merge() The core is to figure out how to calculate the result while merging. It is based on the fact that the left half and right half are all sorted. Using the ordering information we are able to locate two points in the right half j and k , between which will fulfill the requirement. Several important points need to be made. 1) calculation of prefix sum of the array. The length is n + 1 not n ? 2) the range passed to the merge subroutine are open-end [start, end) . The base case of the subrouine. It return zero becuase the case has been counted in the for loop, we don't need to count it again. Not because the base case is 0 . C++ class Solution { public : int countRangeSum ( vector < int >& nums , int lower , int upper ) { int n = nums . size (); vector < long > sums ( n + 1 , 0 ); for ( int i = 0 ; i < n ; i ++ ) sums [ i + 1 ] = sums [ i ] + nums [ i ]; return mergeSort ( sums , 0 , n + 1 , lower , upper ); } int mergeSort ( vector < long >& sums , int start , int end , int lower , int upper ) { if ( end - start <= 1 ) return 0 ; // note the meaning of this 0. int mid = start + ( end - start ) / 2 ; count = mergeSort ( sums , start , mid , lower , upper ) + mergeSort ( sums , mid , end , lower , upper ); int m = mid , n = mid , count = 0 ; for ( int i = start ; i < mid ; i ++ ) { while ( m < end && sums [ m ] - sums [ i ] < lower ) m ++ ; while ( n < end && sums [ n ] - sums [ i ] <= upper ) n ++ ; count += n - m ; } inplace_merge ( sums . begin () + start , sums . begin () + mid , sums . begin () + end ); return count ; } }; Solution 2 Merge sort using tmp buffer cache Here is how the count is making sense. |--------------|-------------------| sums: |start |mid |end |---|----------|------|------|-----| i j k Because sums[j] - sums[i] >= lower, and sums[k] - sums[i] > upper, So for the subarray start with i, ending index in [j, k), the range sum is in [lower, upper]. Notice k should not be included. C++ class Solution { public : int countRangeSum ( vector < int >& nums , int lower , int upper ) { int n = nums . size (); long sums [ n + 1 ] = { 0 }; if ( n == 0 ) return 0 ; for ( int i = 0 ; i < n ; i ++ ) { sums [ i + 1 ] = sums [ i ] + nums [ i ]; } /* n + 1 is the one pass the last element of sums */ return countByMergeSort ( sums , 0 , n + 1 , lower , upper ); } /* This function will return sorted array sums[start], ... sums[end - 1] */ int countByMergeSort ( long sums [], int start , int end , int lower , int upper ) { if ( end - start <= 1 ) return 0 ; int mid = start + ( end - start ) / 2 ; int count = countByMergeSort ( sums , start , mid , lower , upper ) + countByMergeSort ( sums , mid , end , lower , upper ); long cache [ end - start ] = { 0 }; int j = mid , k = mid , t = mid ; for ( int i = start , r = 0 ; i < mid ; ++ i , ++ r ) { while ( k < end && sums [ k ] - sums [ i ] < lower ) k ++ ; while ( j < end && sums [ j ] - sums [ i ] <= upper ) j ++ ; count += j - k ; /* calculate the result */ /* Merge left and right to get sorted array {sums[start], .. sums[end - 1]}. * Because left part of sums[start] to sums[mid] are already sorted, * use cache here to merge prefix of the right part: sum[mid] to sums[t] * with left part upto sums[i] for all i = {start, mid - 1}. */ while ( t < end && sums [ t ] < sums [ i ]) cache [ r ++ ] = sums [ t ++ ]; cache [ r ] = sums [ i ]; } /* after this for loop, cache will have partially sorted array * cache = sums_left = {sums[start], ... sums[t - 1]} element * of which will be in their final sorted positions. * array sums_right = {sums[t], sums[end - 1]} is also * in their final sorted positions. */ /* Since the sums_left is sorted, it have size of t - start, * here we copy exactly t - start element from cache to sums. */ for ( int i = start ; i < t ; i ++ ) sums [ i ] = cache [ i - start ]; return count ; } }; Solution 3 BST Solution 4 BIT","title":"Count of Range Sum"},{"location":"leetcode/array/notes/#maximum-sum-of-two-non-overlapping-subarrays","text":"Brute Force Iterate class Solution { public : int maxSumTwoNoOverlap ( vector < int >& A , int L , int M ) { int n = A . size (); if ( L == 0 || M == 0 ) { return 0 ; } vector < int > preSum ( n , 0 ); preSum [ 0 ] = A [ 0 ]; for ( int i = 1 ; i < n ; i ++ ) { preSum [ i ] = preSum [ i - 1 ] + A [ i ]; } int res = 0 ; // iterate the L using index i for ( int i = 0 ; i < n - L + 1 ; i ++ ) { int Lsum = 0 ; if ( i == 0 ) { Lsum = preSum [ i + L - 1 ]; } else { Lsum = preSum [ i + L - 1 ] - preSum [ i - 1 ]; } int Msum = 0 ; // iterate the left M array using index j for ( int j = 0 ; j < i - M ; j ++ ) { int tmp = 0 ; if ( j == 0 ) { tmp = preSum [ j + M - 1 ]; } else { tmp = preSum [ j + M - 1 ] - preSum [ j - 1 ]; } Msum = max ( Msum , tmp ); } // iterate the right M array using index j for ( int j = i + L ; j < n - M + 1 ; j ++ ) { Msum = max ( Msum , preSum [ j + M - 1 ] - preSum [ j - 1 ]); } res = max ( res , Msum + Lsum ); } return res ; } }; One pass class Solution { public : int maxSumTwoNoOverlap ( vector < int >& A , int L , int M ) { int n = A . size (); if ( L == 0 || M == 0 ) { return 0 ; } vector < int > preSum ( n , 0 ); preSum [ 0 ] = A [ 0 ]; for ( int i = 1 ; i < n ; i ++ ) { preSum [ i ] = preSum [ i - 1 ] + A [ i ]; } int res = INT_MIN ; int Lmax = INT_MIN ; int Mmax = INT_MIN ; // for ( int i = L + M ; i <= n ; i ++ ) { // L is front, M is back if ( i == L + M ) { Lmax = preSum [ L - 1 ]; } else { Lmax = max ( Lmax , preSum [ i - M - 1 ] - preSum [ i - L - M - 1 ]); } // M is front, L is back if ( i == L + M ) { Mmax = preSum [ M - 1 ]; } else { Mmax = max ( Mmax , preSum ( i - L - 1 ) - preSum [ i - M - L - 1 ]); } res = max ({ res , Lmax + preSum [ i - 1 ] - preSum [ i - M - 1 ], Mmax + preSum [ i - 1 ] - preSum [ i - L - 1 ]}) } return res ; } }; DP solution //TODO","title":"Maximum Sum of Two Non-Overlapping Subarrays"},{"location":"leetcode/array/notes/#maximum-sum-of-3-non-overlapping-subarrays","text":"","title":"Maximum Sum of 3 Non-Overlapping Subarrays"},{"location":"leetcode/array/notes/#category-4-k-sum-problems","text":"","title":"Category 4 K Sum problems"},{"location":"leetcode/array/notes/#two-sum","text":"","title":"Two Sum"},{"location":"leetcode/array/notes/#two-sum-ii-input-array-is-sorted","text":"","title":"Two Sum II - Input array is sorted"},{"location":"leetcode/array/notes/#two-sum-iii-data-structure-design","text":"","title":"Two Sum III - Data structure design"},{"location":"leetcode/array/notes/#two-sum-iv-input-is-a-bst","text":"","title":"Two Sum IV - Input is a BST"},{"location":"leetcode/array/notes/#3sum","text":"","title":"3Sum"},{"location":"leetcode/array/notes/#3sum-closest","text":"","title":"3Sum Closest"},{"location":"leetcode/array/notes/#3sum-smaller","text":"","title":"3Sum Smaller"},{"location":"leetcode/array/notes/#4sum","text":"","title":"4Sum"},{"location":"leetcode/array/notes/#4sum-ii","text":"","title":"4Sum II"},{"location":"leetcode/array/notes/#k-sum","text":"","title":"K Sum"},{"location":"leetcode/array/notes/#target-sum","text":"","title":"Target Sum"},{"location":"leetcode/array/notes/#category-5-array-partition-problems","text":"","title":"Category 5 Array partition problems"},{"location":"leetcode/array/notes/#category-6-interval-and-range-problems","text":"","title":"Category 6 Interval and range problems"},{"location":"leetcode/array/notes/#summary-ranges","text":"","title":"Summary Ranges"},{"location":"leetcode/array/notes/#missing-ranges","text":"","title":"Missing Ranges"},{"location":"leetcode/array/notes/#insert-interval","text":"","title":"Insert Interval"},{"location":"leetcode/array/notes/#merge-intervals","text":"","title":"Merge Intervals"},{"location":"leetcode/array/notes/#range-addition","text":"","title":"Range Addition"},{"location":"leetcode/array/notes/#range-addition-ii","text":"","title":"Range Addition II"},{"location":"leetcode/array/notes/#my-calendar-i","text":"","title":"My Calendar I"},{"location":"leetcode/array/notes/#my-calendar-ii","text":"","title":"My Calendar II"},{"location":"leetcode/array/notes/#my-calendar-iii","text":"","title":"My Calendar III"},{"location":"leetcode/array/notes/#meeting-rooms-i","text":"","title":"Meeting Rooms I"},{"location":"leetcode/array/notes/#meeting-rooms-ii","text":"","title":"Meeting Rooms II"},{"location":"leetcode/array/notes/#number-of-airplanes-in-the-sky","text":"","title":"Number of Airplanes in the Sky"},{"location":"leetcode/array/notes/#cagegory-7-2d-arry-matrix-grid-problems","text":"","title":"Cagegory 7 2D arry (matrix, grid) problems"},{"location":"leetcode/array/notes/#perfect-rectangle","text":"","title":"Perfect Rectangle"},{"location":"leetcode/array/notes/#trapping-rain-water","text":"","title":"Trapping Rain Water"},{"location":"leetcode/array/notes/#trapping-rain-water-ii","text":"","title":"Trapping Rain Water II"},{"location":"leetcode/array/notes/#container-with-most-water","text":"","title":"Container With Most Water"},{"location":"leetcode/array/notes/#largest-rectangle-in-histogram","text":"","title":"Largest Rectangle in Histogram"},{"location":"leetcode/array/notes/#maximal-rectangle_1","text":"","title":"Maximal Rectangle"},{"location":"leetcode/array/notes/#maximal-square_1","text":"","title":"Maximal Square"},{"location":"leetcode/array/notes/#the-skyline-problem","text":"","title":"The Skyline Problem"},{"location":"leetcode/array/notes/#smallest-rectangle-enclosing-black-pixels","text":"","title":"Smallest Rectangle Enclosing Black Pixels"},{"location":"leetcode/array/notes/#rectangle-area","text":"","title":"Rectangle Area"},{"location":"leetcode/array/notes/#max-sum-of-rectangle-no-larger-than-k_1","text":"","title":"Max Sum of Rectangle No Larger Than K"},{"location":"leetcode/array/notes/#category-8-stock-buying-problems","text":"Most consistent ways of dealing with the series of stock problems","title":"Category 8 stock buying problems"},{"location":"leetcode/array/notes/#121-best-time-to-buy-and-sell-stock","text":"Solution 1 O(n) one pass to find the minimum and in the meantime, find the max profit. C++ class Solution { public : int maxProfit ( vector < int > & prices ) { int n = prices . size (); if ( n == 0 ) return 0 ; int res = 0 , low = prices [ 0 ]; for ( int i = 1 ; i < n ; i ++ ) { res = max ( res , prices [ i ] - low ); low = min ( low , prices [ i ]); } return res ; } };","title":"121. Best Time to Buy and Sell Stock"},{"location":"leetcode/array/notes/#122-best-time-to-buy-and-sell-stock-ii","text":"Solution 1 Greedy since you can buy as many times as you can C++ class Solution { public : int maxProfit ( vector < int > & prices ) { int n = prices . size (); int res = 0 ; for ( int i = 0 ; i < n - 1 ; i ++ ) { if ( prices [ i + 1 ] - prices [ i ]) { res += prices [ i + 1 ] - prices [ i ]; } } return res ; } };","title":"122. Best Time to Buy and Sell Stock II"},{"location":"leetcode/array/notes/#123-best-time-to-buy-and-sell-stock-iii","text":"You can now buy at most twice. how to max the profit. Solution 1 Dynamic programming 5 stages: 1. before buy the first <-- optimal solution could be at this stage 2. hold the first 3. sell the first <-- or at this stage, only bought once, 4. hold the second 5. sell the second <-- or at this stage, bought twice. C++ class Solution { public : int maxProfit ( vector < int > & A ) { //1, 3, 5: // f[i][j] = max{f[i - 1][j], f[i - 1][j - 1] + A[i - 1] - A[i - 2]} // 2, 4: // f[i][j] = max{f[i - 1][j] + A[i - 1] - A[i - 2], // f[i - 1][j - 1], f[i - 1][j - 2] + A[i - 1] - A[i - 2]} int n = A . size (); if ( n == 0 ) { return 0 ; } int f [ n + 1 ][ 6 ]; /* init */ f [ 0 ][ 1 ] = 0 ; f [ 0 ][ 2 ] = f [ 0 ][ 3 ] = f [ 0 ][ 4 ] = f [ 0 ][ 5 ] = INT_MIN ; for ( int i = 1 ; i <= n ; i ++ ) { for ( int j = 1 ; j <= 5 ; j += 2 ) { f [ i ][ j ] = f [ i - 1 ][ j ]; if ( i > 1 && j > 1 && f [ i - 1 ][ j - 1 ] != INT_MIN ) { f [ i ][ j ] = max ( f [ i ][ j ], f [ i - 1 ][ j - 1 ] + A [ i - 1 ] - A [ i - 2 ]); } } for ( int j = 2 ; j < 5 ; j += 2 ) { f [ i ][ j ] = f [ i - 1 ][ j - 1 ]; if ( i > 1 && f [ i - 1 ][ j ] != INT_MIN ) { f [ i ][ j ] = max ( f [ i ][ j ], f [ i - 1 ][ j ] + A [ i - 1 ] - A [ i - 2 ]); } if ( i > 1 && j > 2 && f [ i - 1 ][ j - 2 ] != INT_MIN ) { f [ i ][ j ] = max ( f [ i ][ j ], f [ i - 1 ][ j - 2 ] + A [ i - 1 ] - A [ i - 2 ]); } } } return max ( f [ n ][ 1 ], max ( f [ n ][ 3 ], f [ n ][ 5 ])); } }; Solution 2 Use T[i][k][j] to represent the maximum profit of first i days if we allow at most k transactions and the current number of stocks at hand is j ( j == 0, 1 because hold two stocks at the same time is not allowed). So we have: T [ i ][ 2 ][ 0 ] = max ( T [ i - 1 ][ 2 ][ 0 ] , T [ i - 1 ][ 2 ][ 1 ] + prices [ i - 1 ] ); T [ i ][ 2 ][ 1 ] = max ( T [ i - 1 ][ 2 ][ 1 ] , T [ i - 1 ][ 1 ][ 0 ] - prices [ i - 1 ] ); T [ i ][ 1 ][ 0 ] = max ( T [ i - 1 ][ 1 ][ 0 ] , T [ i - 1 ][ 1 ][ 1 ] + prices [ i - 1 ] ); T [ i ][ 1 ][ 1 ] = max ( T [ i - 1 ][ 1 ][ 1 ] , T [ i - 1 ][ 0 ][ 0 ] - prices [ i - 1 ] ); Think: How to ensure you fourmular cover all the possible values?","title":"123. Best Time to Buy and Sell Stock III"},{"location":"leetcode/array/notes/#188-best-time-to-buy-and-sell-stock-iv","text":"","title":"188. Best Time to Buy and Sell Stock IV"},{"location":"leetcode/array/notes/#309-best-time-to-buy-and-sell-stock-with-cooldown","text":"","title":"309. Best Time to Buy and Sell Stock with Cooldown"},{"location":"leetcode/array/notes/#714-best-time-to-buy-and-sell-stock-with-transaction-fee","text":"","title":"714. Best Time to Buy and Sell Stock with Transaction Fee"},{"location":"leetcode/array/notes/#determine-the-buy-data-and-sell-data-of-maximum-profit-dd-139","text":"","title":"Determine the buy data and sell data of maximum profit (DD 139)"},{"location":"leetcode/backtracking/notes/","text":"Backtracking \u00b6 introduction \u00b6 Backtracking algorithm can be used to generate all the subsets of a given set, all the permutation of a given sequence, and all the combinations of k elements from a given set with n elements. The algorithms are very similar but differ in some unique property of each problem. Subnets \u00b6 How many subsets there are for a set with n elements? ( 2^n 2^n ) How to generate all the subsets if there are NO duplicates in the set? (See problem Subsets ) How to generate all the UNIQUE subsets if there are duplicates in the set? (See problem Subsets II ) What are the application of subset? Permutation \u00b6 How many permutations (number of ordering) of a sequence with n elements? ( n! n! ) How to generate all the permutations if there are NO duplicates in the set? (See problem Permutations ) How to generate all the UNIQUE permutations if there are duplicates in the set? (See problem Permutations II ) What are the applications of permutation? (upper bound of sorting algorithms) Combination \u00b6 How many combinations of k elements from a set of n elements? ( \\binom{n}{k} \\binom{n}{k} ) How to derive the formula \\binom{n}{k} = \\frac{n!}{k! (n - k)!} \\binom{n}{k} = \\frac{n!}{k! (n - k)!} ? Choose k elements from the set one after another without putting back: n (n - 1) \\dots (n - k + 1) = \\frac{n!}{(n - k)!} n (n - 1) \\dots (n - k + 1) = \\frac{n!}{(n - k)!} Calculate the number of ways to choose k elements as \\binom{n}{k} \\binom{n}{k} , then order the k elements: \\binom{n}{k} \\times k! \\binom{n}{k} \\times k! Equalize the two give us: \\binom{n}{k} \\times k! \\binom{n}{k} \\times k! What\u2019s the relation between the subset and combination? For k from 0 to n , get all the combinations, all those combinations will form the powerset of the original set of n elements. We have \\sum_{k = 0}^{n}\\binom{n}{k} = \\binom{n}{0} + \\binom{n}{1}, \\dots, \\binom{n}{n} = 2^n\\text{(number of all set)} \\sum_{k = 0}^{n}\\binom{n}{k} = \\binom{n}{0} + \\binom{n}{1}, \\dots, \\binom{n}{n} = 2^n\\text{(number of all set)} How to generate all the combination if there are NO duplicates in the set? (See problem Combinations ) How to generate all the UNIQUE combinations if there are duplicates in the set? (See problem Combination Sum , Combination Sum II , Combination Sum III , Backpack VI ) What are the applications of combination? (calculate the upper bound of sorting algorithms) Partition \u00b6 How many ways can we partition a set of n elements into r groups, with the i -th group have n_i n_i elements. ( \\frac{n!}{n_1! n_2!, \\dots, n_r!} \\frac{n!}{n_1! n_2!, \\dots, n_r!} ) We will look at the problem this way: note the numbfer of different ways of partitioning as C C , each partition have n_1, n_2, \\dots, n_r n_1, n_2, \\dots, n_r elements. We align n_1, n_2, \\dots, n_r n_1, n_2, \\dots, n_r together to form a n elements sequence and there are n! n! of such sequences. Remeber for each partition, there are n_1! n_1! different sequences. So that we have so we have C \\cdot n_1! \\cdot n_2! \\dots n_r! = n! C \\cdot n_1! \\cdot n_2! \\dots n_r! = n! , and \\frac{n!}{n_1! n_2!, \\dots, n_r!} \\frac{n!}{n_1! n_2!, \\dots, n_r!} When the r = 2 , the partition problem essentially becomes a combination problem. What are the applications of partition? (calculate the upper bound of sorting algorithms) Summary \u00b6 Subset , permutation , and combination problems can be solved using a single code template. Other problems that are involving multiple steps, and asking for multiple eligible results that fulfill certain criteria, could also use this template. The subtlety arise when deal with replicates element in the problems. There are some general principles that I summarized for solving it. The recursive helper function prototype: helper ( vector < int > nums , int n , [ int cur ], vector < int > & subset , vector < vector < int >> & result ); The \" select action \" could be done through update the index int cur for each level of recursive call. Each for iteration (invoke helper function) is to \" select \" an element in this level, it recursively adding more elements to the temporary result. To remove duplicate result if duplicate element presented, we first need to be clear about where the duplicate results come from. e.g. In generating the power subsets, the duplicated results due to repeatedly selecting the same element from different original position. (consecutive positions, since it is sorted first). Therefore, the idea to avoid duplicate results is to not select the duplicate elements for a second time to form the same pattern, we use the following template check statement to achieve this. if ( i != cur && nums [ i ] == nums [ i - 1 ]){ continue ; } 4 elements of backtracking What is the iteration? (the for loop, the same level of node in the tree) What is the recursion? (what index will be advanced?) What is counted in the result? (what should be pushed back?) When is the result returned? (the cutting condition) traceability of backtracking By using recursion and iteration together in this backtrack technique, we can imagine the problem as growing a recursive tree. To grow the node at the same level, an index in a for loop can do the job. To grow a child of a node, a recursive function should be called. More specifically, we can advance the index by passing it into the recursive function. This index increment let the tree grow vertically. Problems \u00b6 Remove Invalid Parenthesis \u00b6 Decide what to search Relation with BFS Subsets \u00b6 C++ class Solution { public : vector < vector < int >> subsets ( vector < int >& nums ) { int n = nums . size (); vector < vector < int > > results ; vector < int > subset ; helper ( nums , n , 0 , subset , results ); return results ; } /* helper to get the permutation from curr to n - 1, total is n - curr */ void helper ( vector < int > nums , int n , int curr , vector < int >& subset , vector < vector < int > >& results ) { results . push_back ( subset ); for ( int i = curr ; i < n ; i ++ ) { subset . push_back ( nums [ i ]); helper ( nums , n , i + 1 , subset , results ); subset . pop_back (); } } }; Python class Solution ( object ): def subsets ( self , nums ): \"\"\" :type nums: List[int] :rtype: List[List[int]] \"\"\" n = len ( nums ) results = [] self . helper ( nums , n , 0 , [], results ) return results def helper ( self , nums , n , curr , currSubset , results ): results . append ( list ( currSubset )) for i in range ( curr , n ): currSubset . append ( nums [ i ]) self . helper ( nums , n , i + 1 , currSubset , results ) currSubset . pop () Subsets II \u00b6 C++ class Solution { public : vector < vector < int >> subsetsWithDup ( vector < int >& nums ) { int n = nums . size (); vector < vector < int >> results ; vector < int > subset ; sort ( nums . begin (), nums . end ()); helper ( nums , n , 0 , subset , results ); return results ; } void helper ( vector < int > nums , int n , int curr , vector < int >& subset , vector < vector < int >>& results ) { results . push_back ( subset ); for ( int i = curr ; i < n ; i ++ ) { if ( i > curr && nums [ i ] == nums [ i - 1 ]) { continue ; } subset . push_back ( nums [ i ]); helper ( nums , n , i + 1 , subset , results ); subset . pop_back (); } } }; Python class Solution ( object ): def subsetsWithDup ( self , nums ): \"\"\" :type nums: List[int] :rtype: List[List[int]] \"\"\" n = len ( nums ) results = [] nums . sort () self . helper ( nums , n , 0 , [], results ) return results def helper ( self , nums , n , curr , currSubset , results ): results . append ( list ( currSubset )) for i in range ( curr , n ): if ( i > curr ) and ( nums [ i ] == nums [ i - 1 ]): continue currSubset . append ( nums [ i ]) self . helper ( nums , n , i + 1 , currSubset , results ) currSubset . pop () Permutations \u00b6 classic Backtracking class Solution { public : vector < vector < int > > permute ( vector < int > nums ) { vector < vector < int > > results ; vector < int > permutation ; int n = nums . size (); if ( n == 0 ) return results ; sort ( nums . begin (), nums . end ()); helper ( nums , n , permutation , results ); return results ; } void helper ( vector < int > nums , int n , vector < int > & permutation , vector < vector < int > > & results ) { if ( permutation . size () == n ) { results . push_back ( permutation ); return ; } for ( int i = 0 ; i < n ; i ++ ) { vector < int >:: iterator it ; it = find ( permutation . begin (), permutation . end (), nums [ i ]); if ( it == permutation . end ()){ permutation . push_back ( nums [ i ]); helper ( nums , n , permutation , results ); permutation . pop_back (); } } } }; use 'visited' variable class Solution { public : vector < vector < int >> permute ( vector < int >& nums ) { int n = nums . size (); vector < vector < int > > results ; vector < int > permute ; vector < bool > visited ( n , false ); helper ( nums , n , visited , permute , results ); return results ; } void helper ( vector < int > nums , int n , vector < bool > visited , vector < int >& permutation , vector < vector < int > >& results ) { if ( permutation . size () == n ) { results . push_back ( permutation ); return ; } for ( int i = 0 ; i < n ; i ++ ) { if ( visited [ i ]) { continue ; } visited [ i ] = true ; permutation . push_back ( nums [ i ]); helper ( nums , n , visited , permutation , results ); permutation . pop_back (); visited [ i ] = false ; } } }; Permutations II \u00b6 The line 27 is very tricky and important. It can be understand as following: using !visited[i - 1] makes sure when duplicates are selected, the order is ascending (index from small to large). However, using visited[i - 1] means the descending order. You cannot using the find to check whether the element is presetend, you have to use the visited \"bit map\" to record the states. C++ class Solution { public : vector < vector < int >> permuteUnique ( vector < int >& nums ) { int n = nums . size (); vector < vector < int >> results ; vector < int > permutation ; vector < bool > visited ( n , 0 ); if ( n == 0 ) return results ; sort ( nums . begin (), nums . end ()); helper ( nums , n , visited , permutation , results ); return results ; } void helper ( vector < int > nums , int n , vector < bool > visited , vector < int >& permutation , vector < vector < int >>& results ) { if ( permutation . size () == n ) { results . push_back ( permutation ); return ; } for ( int i = 0 ; i < n ; i ++ ) { if ( visited [ i ] || ( i > 0 && nums [ i ] == nums [ i - 1 ] && ! visited [ i - 1 ])) { //if (visited[i] == 1 || (i != 0 && nums[i] == nums[i - 1] && visited[i - 1])) { // why this also work? // duplicated element only add once in each recursion(i == 0). // e.g. 1 2 3 4 4 4 5 6 7. each 4 is selected in different level of recursion, // the above equality check avoid adding duplicate element in the same level. (from different index) // if we don't add this check, the duplicates could from different index continue ; } visited [ i ] = true ; permutation . push_back ( nums [ i ]); helper ( nums , n , visited , permutation , results ); permutation . pop_back (); visited [ i ] = false ; } } }; Combinations \u00b6 Notice the = in the for loop, here the i is not array index, but number we are enumerating. C++ class Solution { public : vector < vector < int >> combine ( int n , int k ) { vector < vector < int >> results ; vector < int > comb ; helper ( n , k , 1 , comb , results ); return results ; } void helper ( int n , int k , int cur , vector < int >& comb , vector < vector < int >>& results ) { if ( comb . size () == k ) { results . push_back ( comb ); return ; } for ( int i = cur ; i <= n ; i ++ ) { comb . push_back ( i ); helper ( n , k , i + 1 , comb , results ); comb . pop_back (); } } }; Combination Sum \u00b6 Notice in line 23, the recursion is on i again even though it has been selected already. class Solution { public : vector < vector < int >> combinationSum ( vector < int >& candidates , int target ) { vector < vector < int >> results ; vector < int > result ; sort ( candidates . begin (), candidates . end ()); helper ( candidates , 0 , target , result , results ); return results ; } void helper ( vector < int > nums , int cur , int target , vector < int >& result , vector < vector < int >>& results ) { if ( target == 0 ) { results . push_back ( result ); return ; } for ( int i = cur ; i < nums . size (); i ++ ) { if ( nums [ i ] <= target ) { result . push_back ( nums [ i ]); helper ( nums , i , target - nums [ i ], result , results ); result . pop_back (); } } } }; Combination Sum II \u00b6 Notice in this problem, duplicate number could be in the original array, don't worry, we will treat the same element in different position differently. Notice how this code can be changed from the Combinations Sum, compare to the solution we only add the if check in line 21, and call the hlepr function with i + 1 in line 27. class Solution { public : vector < vector < int >> combinationSum2 ( vector < int >& candidates , int target ) { vector < vector < int >> results ; vector < int > result ; sort ( candidates . begin (), candidates . end ()); helper ( candidates , 0 , target , result , results ); return results ; } void helper ( vector < int > nums , int cur , int target , vector < int >& result , vector < vector < int >>& results ) { if ( target == 0 ) { results . push_back ( result ); return ; } for ( int i = cur ; i < nums . size (); i ++ ) { if ( i > cur && nums [ i ] == nums [ i - 1 ]) { continue ; } if ( nums [ i ] <= target ) { result . push_back ( nums [ i ]); helper ( nums , i + 1 , target - nums [ i ], result , results ); result . pop_back (); } } } }; Combination Sum III \u00b6 Very similar to the Combination Sum . Instead of giving an array, this problem gives numbers from 1 to 9 and no duplicate selection allowed. The meaning of this problem is also similar to the problem K Sum , while K Sum is asking \"how many\", so it is solved using Dynamic Programming. differ with Combination Sum in the for loop bounds and the recursive call, remember in Combination Sum , the recursive function called with i instead of i + 1 because duplication is allowed in that problem. class Solution { public : vector < vector < int >> combinationSum2 ( vector < int >& candidates , int target ) { vector < vector < int >> results ; vector < int > result ; sort ( candidates . begin (), candidates . end ()); helper ( candidates , 0 , target , result , results ); return results ; } void helper ( vector < int > nums , int cur , int target , vector < int >& result , vector < vector < int >>& results ) { if ( target == 0 ) { results . push_back ( result ); return ; } for ( int i = cur ; i < nums . size (); i ++ ) { if ( i > cur && nums [ i ] == nums [ i - 1 ]) { continue ; } if ( nums [ i ] <= target ) { result . push_back ( nums [ i ]); helper ( nums , i + 1 , target - nums [ i ], result , results ); result . pop_back (); } } } }; Combination Sum IV (Backpack VI) \u00b6 \u8fd9\u91cc\u53ef\u4ee5\u968f\u4fbf\u53d6\uff0c\u4f3c\u4e4e\u9898\u76ee\u53d8\u5f97\u65e0\u6cd5\u4e0b\u624b\uff0c\u8003\u8651\u201c\u6700\u540e\u4e00\u6b65\u201d\u8fd9\u4e2a\u6280\u5de7\u4e0d\u80fd\u7528\u4e86\uff0c\u56e0\u4e3a\u6700\u540e\u4e00\u6b65\u53ef\u4ee5\u662f\u4efb\u610f\u4e00\u4e2a\u4e86\u3002 \u4f46\u4ecd\u7136\u53ef\u4ee5\u7528\u5b50\u95ee\u9898\u6765\u8003\u8651\u3002\u5148\u4e0d\u7ba1\u6700\u540e\u4e00\u6b65\u662f\u54ea\u4e00\u4e2a\uff0c\u6700\u540e\u4e00\u6b65\u4e4b\u524d\u7684\u76f8\u52a0\u7684\u603b\u548c\u4e00\u5b9a\u662f Target - x . \u8fd9\u6837\u5c31\u8f6c\u5316\u6210\u4e00\u4e2a\u5b50\u95ee\u9898\u53ef\u4ee5\u7528DP\u6765\u505a\u3002 \u5177\u4f53\u505a\u6cd5\u6211\u4eec\u53ef\u4ee5\u5bf9\u4e8e\u6bcf\u4e00\u4e2a\u5c0f\u4e8e\u201c\u603b\u627f\u91cd\u201d\u7684\u91cd\u91cf\u8fdb\u884c\u679a\u4e3e\u6700\u540e\u4e00\u6b65 x \u3002\u53ef\u80fd\u7684 x \u662f A[0], ..., A[i - 1] \u4e2d\u4efb\u610f\u4e00\u4e2a. Must initialize f[i] = 0 . Because some of the state won't be updated indicates that they are not possible to fill). class Solution { public : int backPackVI ( vector < int >& nums , int T ) { int n = nums . size (); if ( n == 0 ) { return 0 ; } int f [ T + 1 ]; f [ 0 ] = 1 ; /* for each sub problem */ for ( int i = 1 ; i <= T ; i ++ ) { f [ i ] = 0 ; /* enumerate the last step */ for ( int j = 0 ; j < n ; j ++ ) { if ( i >= nums [ j ]) { f [ i ] += f [ i - nums [ j ]]; } } } return f [ T ]; } }; Note \u5373\u4f7f\u662f\u7b80\u5355\u7684\u4e00\u7ef4\u80cc\u5305\uff0c\u4f9d\u7136\u662f\u603b\u627f\u91cd\u653e\u5165\u72b6\u6001(\u5373\u6240\u5f00\u6570\u7ec4\u4e0e\u603b\u627f\u91cd\u76f8\u5173) Print one such combination solution Suppose we also interested in print one of the possible solution. How could we change the code? f[i] : \u5b58\u591a\u5c11\u79cd\u65b9\u5f0f pi[i] : \u5982\u679c f[i] >= 1 , \u6700\u540e\u4e00\u4e2a\u6570\u5b57\u53ef\u4ee5\u662f pi[i] Print one such combination class Solution { public : int backPackVI ( vector < int >& nums , int T ) { int n = nums . size (); if ( n == 0 ) { return 0 ; } int f [ T + 1 ]; /* pi[i]: \u5982\u679ci\u53ef\u62fc\u51fa(f[i] >= 1), \u6700\u540e\u4e00\u4e2a\u662fpi[i] */ int pi [ T + 1 ]; f [ 0 ] = 1 ; for ( int i = 1 ; i <= T ; i ++ ) { f [ i ] = 0 ; for ( int j = 0 ; j < n ; j ++ ) { if ( i >= nums [ j ]) { f [ i ] += f [ i - nums [ j ]]; /* \u6700\u540e\u4e00\u4e2a\u662fnums[j]\u7684\u53ef\u62fc\u51fai */ if ( f [ i - nums [ j ]] > 0 ) { /* \u7eaa\u5f55\u4e0b\u6765 */ pi [ i ] = nums [ j ]; } } } } if ( f [ T ] > 0 ) { int i = T ; cout << i << \"=\" << endl ; while ( i != 0 ) { // sum is i now; // last number is pi[i] // previuos sum is i - pi[i] cout << pi [ i ] << endl ; i -= pi [ i ]; } } return f [ T ]; } };","title":"Backtracking"},{"location":"leetcode/backtracking/notes/#backtracking","text":"","title":"Backtracking"},{"location":"leetcode/backtracking/notes/#introduction","text":"Backtracking algorithm can be used to generate all the subsets of a given set, all the permutation of a given sequence, and all the combinations of k elements from a given set with n elements. The algorithms are very similar but differ in some unique property of each problem.","title":"introduction"},{"location":"leetcode/backtracking/notes/#subnets","text":"How many subsets there are for a set with n elements? ( 2^n 2^n ) How to generate all the subsets if there are NO duplicates in the set? (See problem Subsets ) How to generate all the UNIQUE subsets if there are duplicates in the set? (See problem Subsets II ) What are the application of subset?","title":"Subnets"},{"location":"leetcode/backtracking/notes/#permutation","text":"How many permutations (number of ordering) of a sequence with n elements? ( n! n! ) How to generate all the permutations if there are NO duplicates in the set? (See problem Permutations ) How to generate all the UNIQUE permutations if there are duplicates in the set? (See problem Permutations II ) What are the applications of permutation? (upper bound of sorting algorithms)","title":"Permutation"},{"location":"leetcode/backtracking/notes/#combination","text":"How many combinations of k elements from a set of n elements? ( \\binom{n}{k} \\binom{n}{k} ) How to derive the formula \\binom{n}{k} = \\frac{n!}{k! (n - k)!} \\binom{n}{k} = \\frac{n!}{k! (n - k)!} ? Choose k elements from the set one after another without putting back: n (n - 1) \\dots (n - k + 1) = \\frac{n!}{(n - k)!} n (n - 1) \\dots (n - k + 1) = \\frac{n!}{(n - k)!} Calculate the number of ways to choose k elements as \\binom{n}{k} \\binom{n}{k} , then order the k elements: \\binom{n}{k} \\times k! \\binom{n}{k} \\times k! Equalize the two give us: \\binom{n}{k} \\times k! \\binom{n}{k} \\times k! What\u2019s the relation between the subset and combination? For k from 0 to n , get all the combinations, all those combinations will form the powerset of the original set of n elements. We have \\sum_{k = 0}^{n}\\binom{n}{k} = \\binom{n}{0} + \\binom{n}{1}, \\dots, \\binom{n}{n} = 2^n\\text{(number of all set)} \\sum_{k = 0}^{n}\\binom{n}{k} = \\binom{n}{0} + \\binom{n}{1}, \\dots, \\binom{n}{n} = 2^n\\text{(number of all set)} How to generate all the combination if there are NO duplicates in the set? (See problem Combinations ) How to generate all the UNIQUE combinations if there are duplicates in the set? (See problem Combination Sum , Combination Sum II , Combination Sum III , Backpack VI ) What are the applications of combination? (calculate the upper bound of sorting algorithms)","title":"Combination"},{"location":"leetcode/backtracking/notes/#partition","text":"How many ways can we partition a set of n elements into r groups, with the i -th group have n_i n_i elements. ( \\frac{n!}{n_1! n_2!, \\dots, n_r!} \\frac{n!}{n_1! n_2!, \\dots, n_r!} ) We will look at the problem this way: note the numbfer of different ways of partitioning as C C , each partition have n_1, n_2, \\dots, n_r n_1, n_2, \\dots, n_r elements. We align n_1, n_2, \\dots, n_r n_1, n_2, \\dots, n_r together to form a n elements sequence and there are n! n! of such sequences. Remeber for each partition, there are n_1! n_1! different sequences. So that we have so we have C \\cdot n_1! \\cdot n_2! \\dots n_r! = n! C \\cdot n_1! \\cdot n_2! \\dots n_r! = n! , and \\frac{n!}{n_1! n_2!, \\dots, n_r!} \\frac{n!}{n_1! n_2!, \\dots, n_r!} When the r = 2 , the partition problem essentially becomes a combination problem. What are the applications of partition? (calculate the upper bound of sorting algorithms)","title":"Partition"},{"location":"leetcode/backtracking/notes/#summary","text":"Subset , permutation , and combination problems can be solved using a single code template. Other problems that are involving multiple steps, and asking for multiple eligible results that fulfill certain criteria, could also use this template. The subtlety arise when deal with replicates element in the problems. There are some general principles that I summarized for solving it. The recursive helper function prototype: helper ( vector < int > nums , int n , [ int cur ], vector < int > & subset , vector < vector < int >> & result ); The \" select action \" could be done through update the index int cur for each level of recursive call. Each for iteration (invoke helper function) is to \" select \" an element in this level, it recursively adding more elements to the temporary result. To remove duplicate result if duplicate element presented, we first need to be clear about where the duplicate results come from. e.g. In generating the power subsets, the duplicated results due to repeatedly selecting the same element from different original position. (consecutive positions, since it is sorted first). Therefore, the idea to avoid duplicate results is to not select the duplicate elements for a second time to form the same pattern, we use the following template check statement to achieve this. if ( i != cur && nums [ i ] == nums [ i - 1 ]){ continue ; } 4 elements of backtracking What is the iteration? (the for loop, the same level of node in the tree) What is the recursion? (what index will be advanced?) What is counted in the result? (what should be pushed back?) When is the result returned? (the cutting condition) traceability of backtracking By using recursion and iteration together in this backtrack technique, we can imagine the problem as growing a recursive tree. To grow the node at the same level, an index in a for loop can do the job. To grow a child of a node, a recursive function should be called. More specifically, we can advance the index by passing it into the recursive function. This index increment let the tree grow vertically.","title":"Summary"},{"location":"leetcode/backtracking/notes/#problems","text":"","title":"Problems"},{"location":"leetcode/backtracking/notes/#remove-invalid-parenthesis","text":"Decide what to search Relation with BFS","title":"Remove Invalid Parenthesis"},{"location":"leetcode/backtracking/notes/#subsets","text":"C++ class Solution { public : vector < vector < int >> subsets ( vector < int >& nums ) { int n = nums . size (); vector < vector < int > > results ; vector < int > subset ; helper ( nums , n , 0 , subset , results ); return results ; } /* helper to get the permutation from curr to n - 1, total is n - curr */ void helper ( vector < int > nums , int n , int curr , vector < int >& subset , vector < vector < int > >& results ) { results . push_back ( subset ); for ( int i = curr ; i < n ; i ++ ) { subset . push_back ( nums [ i ]); helper ( nums , n , i + 1 , subset , results ); subset . pop_back (); } } }; Python class Solution ( object ): def subsets ( self , nums ): \"\"\" :type nums: List[int] :rtype: List[List[int]] \"\"\" n = len ( nums ) results = [] self . helper ( nums , n , 0 , [], results ) return results def helper ( self , nums , n , curr , currSubset , results ): results . append ( list ( currSubset )) for i in range ( curr , n ): currSubset . append ( nums [ i ]) self . helper ( nums , n , i + 1 , currSubset , results ) currSubset . pop ()","title":"Subsets"},{"location":"leetcode/backtracking/notes/#subsets-ii","text":"C++ class Solution { public : vector < vector < int >> subsetsWithDup ( vector < int >& nums ) { int n = nums . size (); vector < vector < int >> results ; vector < int > subset ; sort ( nums . begin (), nums . end ()); helper ( nums , n , 0 , subset , results ); return results ; } void helper ( vector < int > nums , int n , int curr , vector < int >& subset , vector < vector < int >>& results ) { results . push_back ( subset ); for ( int i = curr ; i < n ; i ++ ) { if ( i > curr && nums [ i ] == nums [ i - 1 ]) { continue ; } subset . push_back ( nums [ i ]); helper ( nums , n , i + 1 , subset , results ); subset . pop_back (); } } }; Python class Solution ( object ): def subsetsWithDup ( self , nums ): \"\"\" :type nums: List[int] :rtype: List[List[int]] \"\"\" n = len ( nums ) results = [] nums . sort () self . helper ( nums , n , 0 , [], results ) return results def helper ( self , nums , n , curr , currSubset , results ): results . append ( list ( currSubset )) for i in range ( curr , n ): if ( i > curr ) and ( nums [ i ] == nums [ i - 1 ]): continue currSubset . append ( nums [ i ]) self . helper ( nums , n , i + 1 , currSubset , results ) currSubset . pop ()","title":"Subsets II"},{"location":"leetcode/backtracking/notes/#permutations","text":"classic Backtracking class Solution { public : vector < vector < int > > permute ( vector < int > nums ) { vector < vector < int > > results ; vector < int > permutation ; int n = nums . size (); if ( n == 0 ) return results ; sort ( nums . begin (), nums . end ()); helper ( nums , n , permutation , results ); return results ; } void helper ( vector < int > nums , int n , vector < int > & permutation , vector < vector < int > > & results ) { if ( permutation . size () == n ) { results . push_back ( permutation ); return ; } for ( int i = 0 ; i < n ; i ++ ) { vector < int >:: iterator it ; it = find ( permutation . begin (), permutation . end (), nums [ i ]); if ( it == permutation . end ()){ permutation . push_back ( nums [ i ]); helper ( nums , n , permutation , results ); permutation . pop_back (); } } } }; use 'visited' variable class Solution { public : vector < vector < int >> permute ( vector < int >& nums ) { int n = nums . size (); vector < vector < int > > results ; vector < int > permute ; vector < bool > visited ( n , false ); helper ( nums , n , visited , permute , results ); return results ; } void helper ( vector < int > nums , int n , vector < bool > visited , vector < int >& permutation , vector < vector < int > >& results ) { if ( permutation . size () == n ) { results . push_back ( permutation ); return ; } for ( int i = 0 ; i < n ; i ++ ) { if ( visited [ i ]) { continue ; } visited [ i ] = true ; permutation . push_back ( nums [ i ]); helper ( nums , n , visited , permutation , results ); permutation . pop_back (); visited [ i ] = false ; } } };","title":"Permutations"},{"location":"leetcode/backtracking/notes/#permutations-ii","text":"The line 27 is very tricky and important. It can be understand as following: using !visited[i - 1] makes sure when duplicates are selected, the order is ascending (index from small to large). However, using visited[i - 1] means the descending order. You cannot using the find to check whether the element is presetend, you have to use the visited \"bit map\" to record the states. C++ class Solution { public : vector < vector < int >> permuteUnique ( vector < int >& nums ) { int n = nums . size (); vector < vector < int >> results ; vector < int > permutation ; vector < bool > visited ( n , 0 ); if ( n == 0 ) return results ; sort ( nums . begin (), nums . end ()); helper ( nums , n , visited , permutation , results ); return results ; } void helper ( vector < int > nums , int n , vector < bool > visited , vector < int >& permutation , vector < vector < int >>& results ) { if ( permutation . size () == n ) { results . push_back ( permutation ); return ; } for ( int i = 0 ; i < n ; i ++ ) { if ( visited [ i ] || ( i > 0 && nums [ i ] == nums [ i - 1 ] && ! visited [ i - 1 ])) { //if (visited[i] == 1 || (i != 0 && nums[i] == nums[i - 1] && visited[i - 1])) { // why this also work? // duplicated element only add once in each recursion(i == 0). // e.g. 1 2 3 4 4 4 5 6 7. each 4 is selected in different level of recursion, // the above equality check avoid adding duplicate element in the same level. (from different index) // if we don't add this check, the duplicates could from different index continue ; } visited [ i ] = true ; permutation . push_back ( nums [ i ]); helper ( nums , n , visited , permutation , results ); permutation . pop_back (); visited [ i ] = false ; } } };","title":"Permutations II"},{"location":"leetcode/backtracking/notes/#combinations","text":"Notice the = in the for loop, here the i is not array index, but number we are enumerating. C++ class Solution { public : vector < vector < int >> combine ( int n , int k ) { vector < vector < int >> results ; vector < int > comb ; helper ( n , k , 1 , comb , results ); return results ; } void helper ( int n , int k , int cur , vector < int >& comb , vector < vector < int >>& results ) { if ( comb . size () == k ) { results . push_back ( comb ); return ; } for ( int i = cur ; i <= n ; i ++ ) { comb . push_back ( i ); helper ( n , k , i + 1 , comb , results ); comb . pop_back (); } } };","title":"Combinations"},{"location":"leetcode/backtracking/notes/#combination-sum","text":"Notice in line 23, the recursion is on i again even though it has been selected already. class Solution { public : vector < vector < int >> combinationSum ( vector < int >& candidates , int target ) { vector < vector < int >> results ; vector < int > result ; sort ( candidates . begin (), candidates . end ()); helper ( candidates , 0 , target , result , results ); return results ; } void helper ( vector < int > nums , int cur , int target , vector < int >& result , vector < vector < int >>& results ) { if ( target == 0 ) { results . push_back ( result ); return ; } for ( int i = cur ; i < nums . size (); i ++ ) { if ( nums [ i ] <= target ) { result . push_back ( nums [ i ]); helper ( nums , i , target - nums [ i ], result , results ); result . pop_back (); } } } };","title":"Combination Sum"},{"location":"leetcode/backtracking/notes/#combination-sum-ii","text":"Notice in this problem, duplicate number could be in the original array, don't worry, we will treat the same element in different position differently. Notice how this code can be changed from the Combinations Sum, compare to the solution we only add the if check in line 21, and call the hlepr function with i + 1 in line 27. class Solution { public : vector < vector < int >> combinationSum2 ( vector < int >& candidates , int target ) { vector < vector < int >> results ; vector < int > result ; sort ( candidates . begin (), candidates . end ()); helper ( candidates , 0 , target , result , results ); return results ; } void helper ( vector < int > nums , int cur , int target , vector < int >& result , vector < vector < int >>& results ) { if ( target == 0 ) { results . push_back ( result ); return ; } for ( int i = cur ; i < nums . size (); i ++ ) { if ( i > cur && nums [ i ] == nums [ i - 1 ]) { continue ; } if ( nums [ i ] <= target ) { result . push_back ( nums [ i ]); helper ( nums , i + 1 , target - nums [ i ], result , results ); result . pop_back (); } } } };","title":"Combination Sum II"},{"location":"leetcode/backtracking/notes/#combination-sum-iii","text":"Very similar to the Combination Sum . Instead of giving an array, this problem gives numbers from 1 to 9 and no duplicate selection allowed. The meaning of this problem is also similar to the problem K Sum , while K Sum is asking \"how many\", so it is solved using Dynamic Programming. differ with Combination Sum in the for loop bounds and the recursive call, remember in Combination Sum , the recursive function called with i instead of i + 1 because duplication is allowed in that problem. class Solution { public : vector < vector < int >> combinationSum2 ( vector < int >& candidates , int target ) { vector < vector < int >> results ; vector < int > result ; sort ( candidates . begin (), candidates . end ()); helper ( candidates , 0 , target , result , results ); return results ; } void helper ( vector < int > nums , int cur , int target , vector < int >& result , vector < vector < int >>& results ) { if ( target == 0 ) { results . push_back ( result ); return ; } for ( int i = cur ; i < nums . size (); i ++ ) { if ( i > cur && nums [ i ] == nums [ i - 1 ]) { continue ; } if ( nums [ i ] <= target ) { result . push_back ( nums [ i ]); helper ( nums , i + 1 , target - nums [ i ], result , results ); result . pop_back (); } } } };","title":"Combination Sum III"},{"location":"leetcode/backtracking/notes/#combination-sum-iv-backpack-vi","text":"\u8fd9\u91cc\u53ef\u4ee5\u968f\u4fbf\u53d6\uff0c\u4f3c\u4e4e\u9898\u76ee\u53d8\u5f97\u65e0\u6cd5\u4e0b\u624b\uff0c\u8003\u8651\u201c\u6700\u540e\u4e00\u6b65\u201d\u8fd9\u4e2a\u6280\u5de7\u4e0d\u80fd\u7528\u4e86\uff0c\u56e0\u4e3a\u6700\u540e\u4e00\u6b65\u53ef\u4ee5\u662f\u4efb\u610f\u4e00\u4e2a\u4e86\u3002 \u4f46\u4ecd\u7136\u53ef\u4ee5\u7528\u5b50\u95ee\u9898\u6765\u8003\u8651\u3002\u5148\u4e0d\u7ba1\u6700\u540e\u4e00\u6b65\u662f\u54ea\u4e00\u4e2a\uff0c\u6700\u540e\u4e00\u6b65\u4e4b\u524d\u7684\u76f8\u52a0\u7684\u603b\u548c\u4e00\u5b9a\u662f Target - x . \u8fd9\u6837\u5c31\u8f6c\u5316\u6210\u4e00\u4e2a\u5b50\u95ee\u9898\u53ef\u4ee5\u7528DP\u6765\u505a\u3002 \u5177\u4f53\u505a\u6cd5\u6211\u4eec\u53ef\u4ee5\u5bf9\u4e8e\u6bcf\u4e00\u4e2a\u5c0f\u4e8e\u201c\u603b\u627f\u91cd\u201d\u7684\u91cd\u91cf\u8fdb\u884c\u679a\u4e3e\u6700\u540e\u4e00\u6b65 x \u3002\u53ef\u80fd\u7684 x \u662f A[0], ..., A[i - 1] \u4e2d\u4efb\u610f\u4e00\u4e2a. Must initialize f[i] = 0 . Because some of the state won't be updated indicates that they are not possible to fill). class Solution { public : int backPackVI ( vector < int >& nums , int T ) { int n = nums . size (); if ( n == 0 ) { return 0 ; } int f [ T + 1 ]; f [ 0 ] = 1 ; /* for each sub problem */ for ( int i = 1 ; i <= T ; i ++ ) { f [ i ] = 0 ; /* enumerate the last step */ for ( int j = 0 ; j < n ; j ++ ) { if ( i >= nums [ j ]) { f [ i ] += f [ i - nums [ j ]]; } } } return f [ T ]; } }; Note \u5373\u4f7f\u662f\u7b80\u5355\u7684\u4e00\u7ef4\u80cc\u5305\uff0c\u4f9d\u7136\u662f\u603b\u627f\u91cd\u653e\u5165\u72b6\u6001(\u5373\u6240\u5f00\u6570\u7ec4\u4e0e\u603b\u627f\u91cd\u76f8\u5173) Print one such combination solution Suppose we also interested in print one of the possible solution. How could we change the code? f[i] : \u5b58\u591a\u5c11\u79cd\u65b9\u5f0f pi[i] : \u5982\u679c f[i] >= 1 , \u6700\u540e\u4e00\u4e2a\u6570\u5b57\u53ef\u4ee5\u662f pi[i] Print one such combination class Solution { public : int backPackVI ( vector < int >& nums , int T ) { int n = nums . size (); if ( n == 0 ) { return 0 ; } int f [ T + 1 ]; /* pi[i]: \u5982\u679ci\u53ef\u62fc\u51fa(f[i] >= 1), \u6700\u540e\u4e00\u4e2a\u662fpi[i] */ int pi [ T + 1 ]; f [ 0 ] = 1 ; for ( int i = 1 ; i <= T ; i ++ ) { f [ i ] = 0 ; for ( int j = 0 ; j < n ; j ++ ) { if ( i >= nums [ j ]) { f [ i ] += f [ i - nums [ j ]]; /* \u6700\u540e\u4e00\u4e2a\u662fnums[j]\u7684\u53ef\u62fc\u51fai */ if ( f [ i - nums [ j ]] > 0 ) { /* \u7eaa\u5f55\u4e0b\u6765 */ pi [ i ] = nums [ j ]; } } } } if ( f [ T ] > 0 ) { int i = T ; cout << i << \"=\" << endl ; while ( i != 0 ) { // sum is i now; // last number is pi[i] // previuos sum is i - pi[i] cout << pi [ i ] << endl ; i -= pi [ i ]; } } return f [ T ]; } };","title":"Combination Sum IV (Backpack VI)"},{"location":"leetcode/binary-search/notes/","text":"Binary Search \u00b6 Binary search problem characteristics \u00b6 Ordered binary search. You need to find index or array element where ordering info is available, either explicitly (sorted array) or implicitly (partially sorted or other special info). monotony pattern. If the ordering info isn't available, but you can exclude \"all\" the possible cases from left or right by a condition camparing f(mid) to the target . Binary search problem solving techniques \u00b6 Clarify that you are trying to find the first one or find the last one. Clarify that you are trying to move the index or move the value (i.e. kth smallest number in multiplicative table). Use an \"ordering abstraction\" vless(target, f(mid)) . This ordering abstraction will produce a boolean array that indicate the ordering information between the target value and the f(mid) . Decide whether the left or right part the f(mid) should fall into. The principle to determine the predicate is simple: don't rule out the possible result (maintain the loop invariant). Shrink the range accordingly based on the predicate decided in step 3. Test the case that the search range is small, such as only have one or two elements. Binary search practical use case \u00b6 Find whether the given target is in the array. Find the the position of the first value equal to the given target. Find the insertion position of the given target in the array. Find the the position of the last value equal to the given target. Find the total number of x in a sorted array. Find the last element less than the target. Find the first element greater than the target. Binary search in C++ STL \u00b6 lower_bound : return iterator point to the element no less than the target. upper_bound : return iterator point to the element greater than the target. equal_range : return a pair of iterators, first of which is lower_bound , second is upper_bound . binary_search : return true if an element equivalent to val is found, and false otherwise. Caveat of binary search implementation \u00b6 Specify the range: [start, end) or [start, end] ? C++ STL used [start, end) to denote a range, which bring in many conveniences. We will stick on this convention. Which while loop condition? start < end ? start <= end ? start != end ? start + 1 < end ? The calculation of the mid . mid = start + (end - start) / 2 or mid = (start + end) / 2 ? To proof mid is always in the range [begin, end) . The \"bisection\": start = mid + 1 , start = mid , or end = mid - 1 or end = mid ? Where is the result? start ? end ? How to make sure? A \"universial\" binary search implementation \u00b6 Despite the above caveas, just remember that there are two version of binary search one can write based on the range [begin, end) and [begin, end] . Iterator type in C++ using the former, it have many benefits in reduce the code complexity. Among all the binary search implementation you have seen, the following one is the most powerful version and it equivalent to C++ STL lower_bound algorithm. /** * return index to an element no less than x. Be more specifically, if there is * an element in the given array equal to x, it returns the index of first such * element; if there is no element that is equal to x, it returns the index * where x can be inserted into the position without changing the ordering of * the elements. * * All possible return value for calling this function with array.size() == n is * [0, 1, ..., n - 1, n] * */ size_t binary_search ( int x , vector < int >& array , size_t n ) { size_t begin = 0 , end = n ; while ( begin != end ) { size_t mid = begin + ( end - begin ) / 2 ; if ( array [ mid ] < x ) { begin = mid + 1 ; } else { end = mid ; } } return begin ; } Important observations about this implementation \u00b6 mid cannot less than begin , they can be equal. This will ensure begin = mid + 1 in the if statement at least reduce the size of [begin, end] by 1. Informal proof: if (array[mid] < x) , it indicate x can only possible be in array[mid + 1, mid + 2, ... n - 1]. mid + 1 is at least 1 greater than begin. mid and end never equal inside the while loop, mid < end is always hold. This will ensure end = mid in the else statement at least reduce the size of [begin, end] by 1. Informal proof: we have begain < end , so begin + end < 2 * end , thus (begin + end) / 2 < end , because integer divisioin truncate down, mid = (begin + end) / 2 always less than end. begin and end never cross. Informal proof: Inside the while loop, at the begining, we have begin < end . If the current iteration executes the if condition, begin = mid + 1 at most advance begin to end but not exceed end . If it execute the else condition, end = mid would at worst case change end point to the minimum value of mid , because we have begin <= mid . Thus, we can conclude that executing the statement end = mid will not change end less than begin , at worst equal to begin . Claims regarding this binary search routine \u00b6 The range [begin, end) is used, which comply to the convention used in C++ iterator. It is impossible that mid == end . If they are equal, array[n] is invalid memory access. We use the loop condition while (begin != end) to indicate that once the loop terminates, we have begin == end . By checking whether begin is a valid index to the array or not, we can know whether x is greater than all the elements in the array or not. If we want to check whether x is found in the array, we simply check array[begin] == x . However, this condition is based on the assumption that begin < end initially. Considering that, using while (begin < end) is better if you cannot ensure begin < end before the loop. Setting begin = mid + 1 reduces the size of the remaining interested sub-array and maintains the invariant, which is if x in the array, x is in [begin, end) . Setting end = mid reduces the size of the remaining interested sub-array (mid never equal to end) and maintains the invariant, which is if x in the array, x is in [begin, end) . This claim is a little hard to absorb. On way to understand is like the following: ~~Because we need keep searching x in the range [begin, mid] if we get in the else statement. In the else case there are two possibilities: 1) array[mid] > x . 2) array[mid] = x . For 1) it indicates x is in [begin, mid) , setting end = mid maintains the loop invariant correctly, which is that x is in the shrinked range. For the 2) it is a little complex. If array[mid] is the only element equal to x, setting end = mid appears violate the loop invariant by exclude x from the range [begin, end) . however, remember array[mid] is the only element equal to x, after the while loop, begin = end , we have the x found by begin even though theoretically [begin, end) is already an empty range since begin = end and array[begin] = array[end] = x . If there are more values are equal to x before and after the element array[mid] the loop will always end up finding the first x value in the array. If we use end = mid + 1 . Try test case [1, 3, 5, 7] , with x = 0 . deadloop will accur. i.e. begin = 0, mid = 1, end = 2 . Category 1 Binary search basics and binary search on special array (i.e. rotated sorted) \u00b6 To solve this type of binary search problem. You should focus on the following: Come up test cases to verify your solution. Be able to find which side to drop for each iteration. Be extremly careful \"off by 1\" bugs. (1. reasoning: is mid value possible to be the solution or not. 2. exercise test cases: especially short ones) 34. Search for a Range \u00b6 C++ Use STL function class Solution { public : vector < int > searchRange ( vector < int >& nums , int target ) { vector < int > res ( 2 , - 1 ); int low = lower_bound ( nums . begin (), nums . end (), target ) - nums . begin (); int high = upper_bound ( nums . begin (), nums . end (), target ) - nums . begin (); if ( low == high ) return res ; return { low , hight - 1 }; } }; C++ Implementation of binary search class Solution { public : vector < int > searchRange ( vector < int >& nums , int target ) { vector < int > res ( 2 , - 1 ); int low = lower_bound ( nums , target ); //int high = lower_bound(nums, target + 1); // also works. int high = upper_bound ( nums , target ); if ( low == high ) { return res ; } return { low , high - 1 }; } int lower_bound ( vector < int >& nums , int target ) { if ( nums . size () == 0 ) return 0 ; int l = 0 , r = nums . size (); while ( l < r ) { int m = l + ( r - l ) / 2 ; if ( nums [ m ] < target ) { l = m + 1 ; } else { r = m ; } } return l ; } int upper_bound ( vector < int >& nums , int target ) { if ( nums . size () == 0 ) return 0 ; int l = 0 , r = nums . size (); while ( l < r ) { int m = l + ( r - l ) / 2 ; if ( nums [ m ] <= target ) { l = m + 1 ; } else { r = m ; } } return l ; } }; Python solution class Solution ( object ): def searchRange ( self , nums , target ): \"\"\" :type nums: List[int] :type target: int :rtype: List[int] \"\"\" if len ( nums ) == 0 : return [ - 1 , - 1 ] begin = 0 end = len ( nums ) while begin != end : mid = begin + ( end - begin ) / 2 if nums [ mid ] < target : begin = mid + 1 else : end = mid if begin == len ( nums ): return [ - 1 , - 1 ] if nums [ begin ] == target : lower = begin else : lower = - 1 begin = 0 end = len ( nums ) while begin != end : mid = begin + ( end - begin ) / 2 if nums [ mid ] <= target : begin = mid + 1 else : end = mid if nums [ begin - 1 ] == target : upper = begin - 1 else : upper = - 1 return [ lower , upper ] 35. Search Insert Position \u00b6 C++ solution lower_bound class Solution { public : int searchInsert ( vector < int >& nums , int target ) { if ( nums . size () == 0 ) return 0 ; int l = 0 , r = nums . size (); while ( l < r ) { int m = l + ( r - l ) / 2 ; if ( nums [ m ] < target ) { l = m + 1 ; } else { r = m ; } } return l ; } }; 33. Search in Rotated Sorted Array \u00b6 How to locate the sorted half? If left half is sorted, check where the target t is like be. else if right half is sorted, check where the target t is like to be. else if mid element is equal to left or right. Remove one of them. Although no duplicate, should consider short input like [3 1], 1 will have the equal case. C++ /** t = 1 t = 3 t = 5 t = 4 t = -1 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 3 4 5 1 1 3 5 4 1 <--need check */ class Solution { public : int search ( vector < int >& A , int t ) { if ( A . empty ()) return - 1 ; int l = 0 , r = A . size () - 1 ; while ( l < r ) { int m = l + ( r - l ) / 2 ; if ( A [ m ] == t ) return m ; if ( A [ l ] < A [ m ]) { // left is sorted if ( A [ l ] <= t && t < A [ m ]) { r = m - 1 ; } else { l = m + 1 ; } } else if ( A [ m ] < A [ r ]) { // right is sorted if ( A [ m ] < t && t <= A [ r ]) { l = m + 1 ; } else { r = m - 1 ; } } else { // if equal, remove one. case: [3, 1], 1 if ( A [ l ] == A [ m ]) l ++ ; if ( A [ m ] == A [ r ]) r -- ; } } return A [ l ] == t ? l : - 1 ; } }; Python class Solution ( object ): def search ( self , nums , target ): \"\"\" :type nums: List[int] :type target: int :rtype: int \"\"\" if len ( nums ) == 0 : return - 1 left = 0 right = len ( nums ) - 1 while left < right : mid = left + ( right - left ) / 2 if nums [ mid ] == target : return mid ; if nums [ left ] < nums [ mid ]: if nums [ left ] <= target and target < nums [ mid ]: right = mid - 1 else : left = mid + 1 elif nums [ mid ] < nums [ right ]: if nums [ mid ] < target and target <= nums [ right ]: left = mid + 1 else : right = mid - 1 else : if nums [ left ] == nums [ mid ]: left += 1 if nums [ right ] == nums [ mid ]: right -= 1 if nums [ left ] == target : return left return - 1 81. Search in Rotated Sorted Array II \u00b6 How to locate the sorted half? C++ class Solution { public : bool search ( vector < int >& A , int t ) { if ( A . empty ()) return false ; int l = 0 , r = A . size () - 1 ; while ( l < r ) { int m = l + ( r - l ) / 2 ; if ( A [ m ] == t ) return true ; if ( A [ l ] < A [ m ]) { if ( A [ l ] <= t && t < A [ m ]) { r = m - 1 ; } else { l = m + 1 ; } } else if ( A [ m ] < A [ r ]) { if ( A [ m ] < t && t <= A [ r ]) { l = m + 1 ; } else { r = m - 1 ; } } else { if ( A [ l ] == A [ m ]) l ++ ; if ( A [ m ] == A [ r ]) r -- ; } } return A [ l ] == t ? true : false ; } }; Python class Solution ( object ): def search ( self , nums , target ): \"\"\" :type nums: List[int] :type target: int :rtype: int \"\"\" if len ( nums ) == 0 : return False left = 0 right = len ( nums ) - 1 while left < right : mid = left + ( right - left ) / 2 if nums [ mid ] == target : return True if nums [ left ] < nums [ mid ]: if nums [ left ] <= target and target < nums [ mid ]: right = mid - 1 else : left = mid + 1 elif nums [ mid ] < nums [ right ]: if nums [ mid ] < target and target <= nums [ right ]: left = mid + 1 else : right = mid - 1 else : if nums [ left ] == nums [ mid ]: left += 1 if nums [ right ] == nums [ mid ]: right -= 1 if nums [ left ] == target : return True return False 153. Find Minimum in Rotated Sorted Array \u00b6 Try to locate the valley which contains the min. Notice when A[0] < A[n - 1] , return A[0] . Draw a monotonic curve and then split the curve into two half, swith the order. This can help you to write the code. C++ class Solution { public : int findMin ( vector < int >& A ) { int l = 0 , r = A . size () - 1 ; while ( l < r ) { if ( A [ l ] < A [ r ]) // serve as base case. return A [ l ]; int m = l + ( r - l ) / 2 ; if ( A [ m ] > A [ r ]) { // also works. looking for not sorted half l = m + 1 ; } else if ( A [ m ] < A [ r ]) { // don't really need if statement r = m ; } } return A [ l ]; } }; Python class Solution ( object ): def findMin ( self , nums ): \"\"\" :type nums: List[int] :rtype: int \"\"\" if len ( nums ) == 0 : return - 1 left = 0 right = len ( nums ) - 1 while left < right : if nums [ left ] == nums [ right ]: return nums [ left ] mid = left + ( right - left ) / 2 if nums [ mid ] > nums [ right ]: left = mid + 1 else : right = mid return nums [ left ] 154. Find Minimum in Rotated Sorted Array II \u00b6 Locate the valley which contains the min. Since duplicates exist. we cannot use the observation A[l] == A[r] . Here we deal with duplicates using decrease by one step. C++ class Solution { public : int findMin ( vector < int >& A ) { int l = 0 , r = A . size () - 1 ; while ( l < r ) { int m = l + ( r - l ) / 2 ; if ( A [ m ] > A [ r ]) { l = m + 1 ; } else if ( A [ m ] < A [ r ]) { r = m ; } else { r -- ; } } return A [ l ]; } }; Python class Solution ( object ): def findMin ( self , nums ): \"\"\" :type nums: List[int] :rtype: int \"\"\" if len ( nums ) == 0 : return - 1 left = 0 right = len ( nums ) - 1 while left < right : mid = left + ( right - left ) / 2 if nums [ mid ] > nums [ right ]: left = mid + 1 elif nums [ mid ] < nums [ right ]: right = mid else : right -= 1 return nums [ left ] 162 Find Peak Element \u00b6 Use Binary search Use the neighboring relation to determin which side a peak value may occur then eliminate the other side. C++ class Solution { public : int findPeakElement ( vector < int >& A ) { int l = 0 , r = A . size () - 1 ; while ( l < r ) { int m = l + ( r - l ) / 2 ; if ( A [ m ] < A [ m + 1 ]) { l = m + 1 ; } else if ( A [ m ] > A [ m + 1 ]) { r = m ; } } return l ; } }; 278. First Bad Version \u00b6 Binary search Notice how this can be related to the ordering abstraction. // Forward declaration of isBadVersion API. bool isBadVersion ( int version ); class Solution { public : int firstBadVersion ( int n ) { int l = 0 , r = n ; while ( l < r ) { int m = l + ( r - l ) / 2 ; if ( ! isBadVersion ( m )) { l = m + 1 ; } else { r = m ; } } return l ; } }; 374. Guess Number Higher or Lower \u00b6 C++ binary search // Forward declaration of guess API. // @param num, your guess // @return -1 if my number is lower, 1 if my number is higher, otherwise return 0 int guess ( int num ); class Solution { public : int guessNumber ( int n ) { int start = 1 , end = n ; while ( start < end ) { int mid = start + ( end - start ) / 2 ; if ( guess ( mid ) == 0 ) return mid ; if ( guess ( mid ) == 1 ) { start = mid + 1 ; } else { end = mid ; } } return start ; } }; Python # The guess API is already defined for you. # @param num, your guess # @return -1 if my number is lower, 1 if my number is higher, otherwise return 0 # def guess(num): class Solution ( object ): def guessNumber ( self , n ): \"\"\" :type n: int :rtype: int \"\"\" begin = 0 end = n while begin != end : mid = begin + ( end - begin ) / 2 if guess ( mid ) == 0 : return mid if guess ( mid ) == 1 : begin = mid + 1 else : end = mid return begin 475. Heaters \u00b6 Sort then brute force The solution we are looking for is the max value of the smallest house-heater distance. Think through what is the distance you want to keep, min or max C++ class Solution { public : int findRadius ( vector < int >& houses , vector < int >& heaters ) { int m = houses . size (); int n = heaters . size (); sort ( houses . begin (), houses . end ()); sort ( heaters . begin (), heaters . end ()); int res = INT_MIN ; int i , j = 0 ; for ( i = 0 ; i < m ; ++ i ) { while ( j < n - 1 && abs ( heaters [ j + 1 ] - houses [ i ]) <= abs ( heaters [ j ] - houses [ i ])) { j ++ ; } res = max ( res , abs ( houses [ i ] - heaters [ j ])); } return res ; } }; Python class Solution ( object ): def findRadius ( self , houses , heaters ): \"\"\" :type houses: List[int] :type heaters: List[int] :rtype: int \"\"\" m = len ( houses ) n = len ( heaters ) houses . sort () heaters . sort () i = 0 j = 0 res = 0 for i in range ( m ): while j < n - 1 and abs ( heaters [ j + 1 ] - houses [ i ]) <= abs ( heaters [ j ] - houses [ i ]): j += 1 res = max ( res , abs ( houses [ i ] - heaters [ j ])) return res Binary search the neighboring heaters get max of min Notice we cannot sort hourses and then search each heater's position. A special cases [1, 2, 3] 2 , the result is 0 whereis it should be 1 . C++ class Solution { public : int findRadius ( vector < int >& houses , vector < int >& heaters ) { int n = heaters . size (); sort ( heaters . begin (), heaters . end ()); int res = INT_MIN ; for ( int house : houses ) { int start = 0 , end = n ; while ( start < end ) { int mid = start + ( end - start ) / 2 ; if ( heaters [ mid ] < house ) start = mid + 1 ; else end = mid ; } int dist1 = ( start == n ) ? INT_MAX : heaters [ start ] - house ; int dist2 = ( start == 0 ) ? INT_MAX : house - heaters [ start - 1 ]; res = max ( res , min ( dist1 , dist2 )); } return res ; } }; Python class Solution ( object ): def findRadius ( self , houses , heaters ): \"\"\" :type houses: List[int] :type heaters: List[int] :rtype: int \"\"\" m = len ( houses ) n = len ( heaters ) heaters . sort () i = 0 j = 0 res = float ( '-inf' ) for i in range ( m ): start = 0 end = n while start != end : mid = start + ( end - start ) / 2 if heaters [ mid ] < houses [ i ]: start = mid + 1 else : end = mid dist1 = float ( 'inf' ) dist2 = float ( 'inf' ) if start != n : dist1 = heaters [ start ] - houses [ i ] if start != 0 : dist2 = houses [ i ] - heaters [ start - 1 ] res = max ( res , min ( dist1 , dist2 )) return res 74. Search a 2D Matrix \u00b6 Binary search We can view the matrix as a big sorted array and then binary search the target. Notice test your finished routine using edge cases. (i.e. the initial value of end) C++ class Solution { public : bool searchMatrix ( vector < vector < int >>& matrix , int target ) { int m = matrix . size (); int n = m ? matrix [ 0 ]. size () : 0 ; if ( m == 0 || n == 0 ) return false ; int start = 0 , end = m * n - 1 ; while ( start < end ) { int mid = start + ( end - start ) / 2 ; int i = mid / n , j = mid % n ; if ( matrix [ i ][ j ] < target ) { start = mid + 1 ; } else { end = mid ; } } return matrix [ start / n ][ start % n ] == target ? true : false ; } }; 240. Search a 2D Matrix II \u00b6 Binary search to exclude whole column or whole row the key is you decide where to start the compare. If you start from left bottom or right top, the solution should be abvious. Notice the idea is from binary search, if ordering info available, we want to exclude as many as impossible values as we can. class Solution { public : bool searchMatrix ( vector < vector < int >>& matrix , int target ) { int m = matrix . size (); int n = m ? matrix [ 0 ]. size () : 0 ; if ( m == 0 || n == 0 ) return false ; int x = m - 1 , y = 0 ; while ( x >= 0 && y < n ) { if ( matrix [ x ][ y ] == target ) { return true ; } if ( matrix [ x ][ y ] < target ) { y ++ ; } else { x -- ; } } return false ; } }; 302. Smallest Rectangle Enclosing Black Pixels \u00b6 C++ Brute Force class Solution { public : int minArea ( vector < vector < char >>& image , int x , int y ) { int m = image . size (); int n = m ? image [ 0 ]. size () : 0 ; int top = m , bottom = 0 , left = n , right = 0 ; for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { if ( image [ i ][ j ] == '1' ) { top = min ( top , i ); bottom = max ( bottom , i + 1 ); left = min ( left , j ); right = max ( right , j + 1 ); } } } return ( right - left ) * ( bottom - top ); } }; Binary search Notice the binary search idea is related to the problem Smallest Good Base and Wood Cut. The basic idea is seach each of furthest 1 from 4 directions. First make sure you can search one boundary and the others are similar. For example, to search the first row that contains 1 , we can look at the whole column/row to see whether this col/row have 1 . Because we are searching the first row that have 1 top down, bisec based on the count of 1 on each row we can know whether we ignore upper half or lower half. C++ Binary Search class Solution { public : int minArea ( vector < vector < char >>& image , int x , int y ) { int m = image . size (); int n = m ? image [ 0 ]. size () : 0 ; int top = bsearch_byrows ( image , 0 , x , 0 , n , true ); // search top int bottom = bsearch_byrows ( image , x + 1 , m , 0 , n , false ); int left = bsearch_bycols ( image , 0 , y , top , bottom , true ); int right = bsearch_bycols ( image , y + 1 , n , top , bottom , false ); return ( bottom - top ) * ( right - left ); } int bsearch_byrows ( vector < vector < char >>& image , int x , int y , int left , int right , bool white2black ) { while ( x < y ) { int m = ( x + y ) / 2 ; int k = left ; while ( k < right && image [ m ][ k ] == '0' ) k ++ ; if ( k < right == white2black ) { // mth row have '1' y = m ; } else { x = m + 1 ; } } return x ; } int bsearch_bycols ( vector < vector < char >>& image , int x , int y , int top , int bottom , bool white2black ) { while ( x < y ) { int m = ( x + y ) / 2 ; int k = top ; while ( k < bottom && image [ k ][ m ] == '0' ) k ++ ; if ( k < bottom == white2black ) { // mth column have '1' y = m ; } else { x = m + 1 ; } } return x ; } }; Python Binary Search class Solution ( object ): def minArea ( self , image , x , y ): \"\"\" :type image: List[List[str]] :type x: int :type y: int :rtype: int \"\"\" m = len ( image ) n = 0 if m != 0 : n = len ( image [ 0 ]) top = self . bsearch_row ( image , 0 , x , 0 , n , True ) bottom = self . bsearch_row ( image , x + 1 , m , 0 , n , False ) left = self . bsearch_col ( image , 0 , y , top , bottom , True ) right = self . bsearch_col ( image , y + 1 , n , top , bottom , False ) return ( bottom - top ) * ( right - left ) def bsearch_row ( self , image , start , end , lower , upper , white2black ): while start < end : m = ( start + end ) / 2 k = lower while k < upper and image [ m ][ k ] == '0' : k += 1 if ( k < upper ) == white2black : end = m else : start = m + 1 return start def bsearch_col ( self , image , start , end , lower , upper , white2black ): while start < end : m = ( start + end ) / 2 k = lower while k < upper and image [ k ][ m ] == '0' : k += 1 if ( k < upper ) == white2black : end = m else : start = m + 1 return start BFS class Solution { public : int minArea ( vector < vector < char >>& image , int x , int y ) { int m = image . size (); int n = m ? image [ 0 ]. size () : 0 ; int top = m , bottom = 0 , left = n , right = 0 ; int xx [ 4 ] = { - 1 , 0 , 1 , 0 }; int yy [ 4 ] = { 0 , 1 , 0 , - 1 }; queue < pair < int , int >> q ; q . push ({ x , y }); image [ x ][ y ] = '0' ; while ( ! q . empty ()) { pair < int , int > t = q . front (); q . pop (); top = min ( top , t . first ); bottom = max ( bottom , t . first + 1 ); left = min ( left , t . second ); right = max ( right , t . second + 1 ); for ( int k = 0 ; k < 4 ; ++ k ) { int a = t . first + xx [ k ]; int b = t . second + yy [ k ]; if ( a >= 0 && a < m && b >= 0 && b < n && image [ a ][ b ] == '1' ) { q . push ({ a , b }); image [ a ][ b ] = '0' ; } } } return ( right - left ) * ( bottom - top ); } }; BFS from collections import deque class Solution ( object ): def minArea ( self , image , x , y ): \"\"\" :type image: List[List[str]] :type x: int :type y: int :rtype: int \"\"\" m = len ( image ) n = 0 if m != 0 : n = len ( image [ 0 ]) xx = [ - 1 , 0 , 1 , 0 ] yy = [ 0 , - 1 , 0 , 1 ] top = m bottom = 0 left = n right = 0 q = deque () q . append ([ x , y ]) image [ x ][ y ] = '0' while len ( q ) > 0 : t = q . popleft () top = min ( top , t [ 0 ]) bottom = max ( bottom , t [ 0 ] + 1 ) left = min ( left , t [ 1 ]) right = max ( right , t [ 1 ] + 1 ) for k in range ( 4 ): a = t [ 0 ] + xx [ k ] b = t [ 1 ] + yy [ k ] if a >= 0 and a < m and b >= 0 and b < n and image [ a ][ b ] == '1' : q . append ([ a , b ]) image [ a ][ b ] = '0' return ( right - left ) * ( bottom - top ) DF class Solution { private : int m , n ; int top , bottom , left , right ; public : int minArea ( vector < vector < char >>& image , int x , int y ) { m = image . size (); n = m ? image [ 0 ]. size () : 0 ; top = m , bottom = 0 , left = n , right = 0 ; dfs_helper ( image , x , y ); return ( right - left ) * ( bottom - top ); } void dfs_helper ( vector < vector < char >>& image , int x , int y ) { if ( x < 0 || x >= m || y < 0 || y >= n || image [ x ][ y ] == '0' ) { return ; } image [ x ][ y ] = '0' ; top = min ( top , x ); bottom = max ( bottom , x + 1 ); left = min ( left , y ); right = max ( right , y + 1 ); dfs_helper ( image , x - 1 , y ); dfs_helper ( image , x , y + 1 ); dfs_helper ( image , x + 1 , y ); dfs_helper ( image , x , y - 1 ); } }; 363. Max Sum of Rectangle No Larger Than K \u00b6 Iterate the wide of the matrix and using prefix sum and set lower_bound . From the problem Max Sum of Subarry No Larger Than K, we have to enumerate the width of the sub-matrix and sum up all row elements and get an array of length m , m is the number of rows of the matrix. Then apply the method. C++ presum class Solution { public : int maxSumSubmatrix ( vector < vector < int >>& matrix , int k ) { if ( matrix . empty ()) return 0 ; int m = matrix . size (); int n = m ? matrix [ 0 ]. size () : 0 ; int res = INT_MIN ; for ( int l = 0 ; l < n ; ++ l ) { vector < int > sums ( m , 0 ); for ( int r = l ; r < n ; ++ r ) { for ( int i = 0 ; i < m ; ++ i ) { sums [ i ] += matrix [ i ][ r ]; } set < int > preSumSet ; preSumSet . insert ( 0 ); int preSum = 0 , curMax = INT_MIN ; for ( int sum : sums ) { preSum += sum ; set < int >:: iterator it = preSumSet . lower_bound ( preSum - k ); if ( it != preSumSet . end ()) { curMax = max ( curMax , preSum - * it ); } preSumSet . insert ( preSum ); } res = max ( res , curMax ); } } return res ; } }; merge sort The idea is similar that solution 1. Instead of calculate preSum on the fly, we finish calculation and pass it to a mergeSort routine. The use mergeSort here is to find the A[j] - A[i] <= k efficiently, O(nlogn) . C++ Merge Sort class Solution { public : int maxSumSubmatrix ( vector < vector < int >>& matrix , int k ) { int m = matrix . size (); int n = m ? matrix [ 0 ]. size () : 0 ; int res = INT_MIN ; vector < long long > sums ( m + 1 , 0 ); for ( int l = 0 ; l < n ; ++ l ) { vector < long long > sumInRow ( m , 0 ); for ( int r = l ; r < n ; ++ r ) { for ( int i = 0 ; i < m ; ++ i ) { sumInRow [ i ] += matrix [ i ][ r ]; sums [ i + 1 ] = sums [ i ] + sumInRow [ i ]; } res = max ( res , mergeSort ( sums , 0 , m + 1 , k )); if ( res == k ) return k ; } } return res ; } int mergeSort ( vector < long long >& sums , int start , int end , int k ) { if ( end == start + 1 ) return INT_MIN ; int mid = start + ( end - start ) / 2 ; int res = mergeSort ( sums , start , mid , k ); if ( res == k ) return k ; res = max ( res , mergeSort ( sums , mid , end , k )); if ( res == k ) return k ; long long cache [ end - start ]; int j = mid , c = 0 , t = mid ; for ( int i = start ; i < mid ; ++ i ) { while ( j < end && sums [ j ] - sums [ i ] <= k ) ++ j ; // search first time sums[j] - sums[i] > k if ( j - 1 >= mid ) { // sums[j - 1] - sums[i] <= k, make sure j - 1 is still in right side res = max ( res , ( int )( sums [ j - 1 ] - sums [ i ])); if ( res == k ) return k ; } while ( t < end && sums [ t ] < sums [ i ]) { cache [ c ++ ] = sums [ t ++ ]; } cache [ c ++ ] = sums [ i ]; } for ( int i = start ; i < t ; ++ i ) { sums [ i ] = cache [ i - start ]; } return res ; } }; Category 2 Using ordering abstration \u00b6 69. Sqrt(x) \u00b6 Solution 1 using ordering abstraction definition To find a square root of a integer x using binary search. We need to first determin the range [left, right] that the target value sqrt(x) may in. The potential range we can search is [0, x/2 + 1] . Then we should clarify this binary search is the \"find the first one\" type or the \"find the last one\" type. Basically, we want to determine our ordering abstraction f(target, g(i)) that is able to produce a boolean array. The boolean array have true part and false part seperated. Here target = sqrt(x) and g(i) = i . We define f(sqrt(x), i) = true when i <= sqrt(x) and f(sqrt(x), i) = false when i > sqrt(x) . This came from the following intuition: We are looking for the \"last\" integer whose square is less than x . Why not the otherwise? Because if you change to find the \"first\" integer whose square is greater than the x from right section of the boolean array, it is hard to define our ordering abstraction f . Of cause, we can search the \"first\" integer whose square is greater than x and find the previous integer next to it as the solution, but this later solution is a bit complex and counter intuitive. We prefer the first definition of ordering abstraction. Although a workable solution following the second ordering abstraction is also given below. For example: to solve the sqrt(8) and sqrt(9) using our definition: k, i = 0 1 2 3 4 5 6 7 8 9 10 n = 11 A = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] f(sqrt(8), k) = [T T T F F] f(sqrt(9), k) = [T T T T F] The binary search routine will be: C++ class Solution { public : int mySqrt ( int x ) { int l = 0 , r = x / 2 + 1 ; while ( l < r ) { // int m = l + (r - l) / 2; // will deadloop for 4, why? int m = r - ( r - l ) / 2 ; if ( m <= x / m ) { l = m ; } else { r = m - 1 ; } } return l ; } }; Solution 2 using the alternative ordering abstraction definition Second ordering abstraction (find first value whose square is greater than x) k, i = 0 1 2 3 4 5 6 7 8 9 10 n = 11 A = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] f(sqrt(8), k) = [F F F T T] f(sqrt(9), k) = [F F F F T] C++ class Solution { public : int mySqrt ( int x ) { if ( x == 0 ) return 0 ; // should handle, but will got division by zero in line 9. int l = 0 , r = x / 2 + 2 ; // r = x / 2 + 1 will not working for x = 1, have to have the one past last; while ( l < r ) { //int m = r - (r - l) / 2; // will dead loop for 4 int m = l + ( r - l ) / 2 ; if ( m > x / m ) { r = m ; } else { l = m + 1 ; } } return l - 1 ; } }; 367. Valid Perfect Square \u00b6 Solution 1 Binary search using ordering abstraction Notice you have to run tests for cases from 1 to 5. C++ class Solution { public : bool isPerfectSquare ( int num ) { if ( num == 1 ) return true ; int begin = 1 , end = num / 2 ; while ( begin < end ) { //long long mid = begin + (end - begin) / 2; // not working, deadloop for 5 long long mid = end - ( end - begin ) / 2 ; if ( mid * mid == num ) return true ; if ( mid * mid < num ) { begin = mid ; } else { end = mid - 1 ; } } return false ; } }; Python class Solution ( object ): def isPerfectSquare ( self , num ): \"\"\" :type num: int :rtype: bool \"\"\" if num == 1 : return True lower = 1 upper = num / 2 while lower < upper : mid = upper - ( upper - lower ) / 2 if mid * mid == num : return True if mid * mid < num : lower = mid else : upper = mid - 1 return False 633. Sum of Square Numbers \u00b6 Solution 1 Binary search Once you have derived value b from a and c , you can binary search b . C++ class Solution { public : bool judgeSquareSum ( int c ) { if ( c == 0 ) return true ; for ( long long a = 0 ; a * a <= c ; ++ a ) { int b = c - ( int ) ( a * a ); int l = 0 , r = b / 2 + 1 ; while ( l < r ) { long long m = r - ( r - l ) / 2 ; if ( m * m == b ) return true ; if ( m * m < b ) { l = m ; } else { r = m - 1 ; } } } return false ; } }; Solution 2 Two pointers Notice this square sum can be found efficiently using two pointers. C++ class Solution { public : bool judgeSquareSum ( int c ) { int a = 0 , b = sqrt ( c ); while ( a <= b ){ int sum = a * a + b * b ; if ( sum < c ) a ++ ; else if ( sum > c ) b -- ; else return true ; } return false ; } }; Solution 3 Using set Keep inserting the value into a set, in the meantime also look up the other C++ class Solution { public : bool judgeSquareSum ( int c ) { set < int > s ; for ( int i = 0 ; i <= sqrt ( c ); ++ i ) { s . insert ( c - i * i ); if ( s . count ( i * i )) return true ; } return false ; } }; 658. Find K Closest Elements \u00b6 Solution 1 Binary search Compare to problem 475. Heaters Our search target is to find the starting index of the subarray of length K. C++ class Solution { public : vector < int > findClosestElements ( vector < int >& arr , int k , int x ) { int start = 0 , end = arr . size () - k ; while ( start < end ) { int mid = start + ( end - start ) / 2 ; // looking for a \"mid\" that if ( x - arr [ mid ] > arr [ mid + k ] - x ) { start = mid + 1 ; } else { end = mid ; } } return vector < int > ( arr . begin () + start , arr . begin () + start + k ); } }; Solution 2 Binary search and Two pointers We first use binary search to locate the x value then expand to left and right looking for the k closest elements Notice the i < 0 in the if condition, it is very important to be there. otherwise the array index will be out of bound. C++ class Solution { public : vector < int > findClosestElements ( vector < int >& arr , int k , int x ) { int index = lower_bound ( arr . begin (), arr . end (), x ) - arr . begin (); int i = index - 1 , j = index ; while ( k -- ) { if ( i < 0 || j < arr . size () && abs ( arr [ j ] - x ) < abs ( arr [ i ] - x )) { j ++ ; } else { i -- ; } } return vector < int > ( arr . begin () + i + 1 , arr . begin () + j ); } }; 611. Valid Triangle Number \u00b6 The main idea comes from the triangle lateral property, in which the triple should fullfil: a + b > c , a + c > b , and b + c > a . Once we sort it. We are able to gain some advantages that we don't have to check all these 3 relations. Instead, we should only take care of A[i] + A[j] > A[k] , in which i < j < k . Because we sorted the array, we can also fix the i and j , using binary search to find the k in the ragne of A[j + 1] ~ A[n - 1] . We can use our classic binary search template to achieve the goal. C++ class Solution { public : int triangleNumber ( vector < int >& nums ) { int n = nums . size (); int res = 0 ; sort ( nums . begin (), nums . end ()); for ( int i = 0 ; i < n - 2 ; ++ i ) { for ( int j = i + 1 ; j < n - 1 ; ++ j ) { int l = j + 1 , r = n ; // range of all possible k, notice l start with j + 1 int t = nums [ i ] + nums [ j ]; while ( l < r ) { int m = l + ( r - l ) / 2 ; if ( nums [ m ] < t ) { l = m + 1 ; } else { r = m ; } } res += l - j - 1 ; // notice the count start from j + 1 to l - 1. } } return res ; } }; Category 3 Using ordering abstration (counter as a gueesing guage) \u00b6 287. Find the Duplicate Number \u00b6 Solution 1 Binary search The problem asking for better than O(n^2) we could check to see whether binary search will work. If you count how many value <= the mid elements of [1, ..., n-1] , it will give you enough information to discard part of the array elements. Here you should distinguish what will be halfed and what will be searched. The answer to that is the [1, ..., n-1] sequence, not the given array. The simple proof of why it works can be put in this the following way. if the count of elements that <=mid in the array is less than mid , we can learn that the duplicate is in the higher end. If the count is greater, we can know that the duplicate element is in the lower end of the sequence [1, ..., n-1] . C++ class Solution { public : int findDuplicate ( vector < int >& nums ) { int begin = 1 , end = nums . size () - 1 ; while ( begin < end ) { int mid = begin + ( end - begin ) / 2 ; int count = 0 ; for ( int a : nums ) { if ( a <= mid ) ++ count ; } if ( count <= mid ) // \"=\" for [1,2,2] begin = mid + 1 ; else end = mid ; } return begin ; } }; Solution 2 tortoise and hare algorithm This problem is very similar to the the find circle in linked list. Generally, if you repeate A[A[i]] , the out put will show some periodic patterns. In fact you can imagine a rho shaped sequence. Image there is a function f(i) = A[i] , it mapping from 1, 2, 3, ... n to 1, 2, 3, ... n . Try to traverse A[i] , you will finally get circle through some same sequence of elements again and again, thus you obtain a rho shaped sequency like a circle in a linked list. The reason of it being a rho shape is becuase at least one element you will not come back to it if you leave it. http://www.keithschwarz.com/interesting/code/find-duplicate/FindDuplicate.python.html C++ class Solution { public : int findDuplicate ( vector < int >& nums ) { int n = nums . size (); if ( n == 0 ) return 0 ; int slow = 0 , fast = 0 , find = 0 ; while ( slow != fast || ( slow == 0 && fast == 0 )) { slow = nums [ slow ]; fast = nums [ nums [ fast ]]; } while ( slow != find ) { slow = nums [ slow ]; find = nums [ find ]; } return find ; } }; 360. Sort Transformed Array \u00b6 410. Split Array Largest Sum \u00b6 Copy books (lintcode) \u00b6 183. Wood cut (lintcode) \u00b6 Solution 1 Binary search It requires to get equal or more than k pieces of wood with same length. So you have to cut the wood to fulfill the requirement. However, you need to promise that each of the k wood is the longest that is possible. Imagine that you are given bunch of wood to cut. How would you do it? You probably want to try to make one cut and see whether you can make it or not. If not, you may want to try make two cuts, and so on. But how could you program such a solution. It is very hard. Start thinking about the length seems a good option. Suppose you know your final maximum length. You would be able to make the cut accordingly. Now given a length out of guessing, can you verify whether it going to work or not? Yes, you can! That's the core idea of this solution. C++ class Solution { public : int woodCut ( vector < int > & L , int k ) { if ( L . empty ()) return 0 ; int maxlen = * max_element ( L . begin (), L . end ()); if ( k == 0 ) return maxlen ; int start = max ( 1 , maxlen / k ), end = maxlen ; while ( start < end ) { int mid = start + ( end - start ) / 2 ; int count = 0 ; for ( int len : L ) { count += len / ( mid + 1 ); } if ( count >= k ) start = mid + 1 ; else end = mid ; } int count = 0 ; for ( int len : L ) count += len / start ; return count >= k ? start : 0 ; } }; 774. Minimize Max Distance to Gas Station \u00b6 Solution 1 Binary search It is very similar to the problem Wood cut. You just need to take care of the accuracy of the results, namely also the int/double casts. It is also the hard part of the problem. Notice the count variable is int type, you should test your solution expecially for the line `count += dist[i] / mid; class Solution { public : double minmaxGasDist ( vector < int >& stations , int K ) { int n = stations . size (); vector < int > dist ( n , 0 ); // dist[0] = null; int d = 0 ; for ( int i = 1 ; i < n ; ++ i ) { dist [ i ] = stations [ i ] - stations [ i - 1 ]; d = max ( d , dist [ i ]); } double low = 0 , high = d ; while ( low + 0.000001 < high ) { double mid = low + ( high - low ) / 2 ; int count = 0 ; for ( int i = 1 ; i < n ; ++ i ) { count += dist [ i ] / mid ; } if ( count > K ) { // mid is too small low = mid ; } else { high = mid ; } } return low ; } }; 644. Maximum Average Subarray II \u00b6 Notice the great trick you used to test whether there is a subarray of length greater than k whose average is larger than current mid . The trick is calculate the diff[i] = nums[i] - mid , and then calculate the prefix sum of the diff array, and compare to another prefix sum of the same array diff , the two prefix sum are calculated at two position at least k distant apart. We actually compare the prefix sum to the smallest prefix sum k distant apart. C++ class Solution { public : double findMaxAverage ( vector < int >& nums , int k ) { int n = nums . size (); double upper = INT_MIN , lower = INT_MAX ; for ( auto num : nums ) { upper = max ( upper , ( double ) num ); lower = min ( lower , ( double ) num ); } while ( lower + 0.00001 < upper ) { double mid = lower + ( upper - lower ) / 2 ; if ( isLarger ( nums , mid , k )) { // is average value >= mid? lower = mid ; } else { upper = mid ; } } return lower ; } /* return true if a greater average value is possible */ bool isLarger ( vector < int >& nums , double mid , int k ) { int n = nums . size (); double sums = 0 , prev = 0 , prev_min = 0 ; for ( int i = 0 ; i < k ; i ++ ) { sums += nums [ i ] - mid ; } if ( sums >= 0 ) { return true ; } /* we keep looking for whether a subarray sum of length >= k in array \"sums\" * is possible to be greater than zero. If such a subarray exist, it means * that the target average value is greater than the \"mid\" value. * we look at the front part of sums that at least k element apart from i. * If we can find the minimum of the sums[0, 1, ..., i - k] and check if * sums[i] - min(sum[0, 1, ..., i - k]) >= 0. If this is the case, it indicate * there exist a subarray of length >= k with sum greater than 0 in sums, * we can return ture, otherwise, it return false. */ for ( int i = k ; i < n ; i ++ ) { sums += nums [ i ] - mid ; prev += nums [ i - k ] - mid ; prev_min = min ( prev_min , prev ); if ( sums >= prev_min ) return true ; } return false ; } }; 778. Swim in Rising Water \u00b6 In This problem we are trying to find a path, in which the maximum element in the path among all paths is minimum. Meaning we look for a target value in the grid, such that there exist a path from grid[0][0] to grid[n-1][n-1] which includes this value and it is the maximum value in the path. C++ class Solution { int x [ 4 ] = { 0 , - 1 , 0 , 1 }; int y [ 4 ] = { - 1 , 0 , 1 , 0 }; public : int swimInWater ( vector < vector < int >>& grid ) { int n = grid . size (); int begin = grid [ 0 ][ 0 ], end = n * n - 1 ; // binary search find a path with mini elevation while ( begin < end ) { int mid = begin + ( end - begin ) / 2 ; if ( pathExist ( grid , mid )) { end = mid ; } else { begin = mid + 1 ; } } return begin ; } bool pathExist ( vector < vector < int >> & grid , int mid ) { int n = grid . size (); vector < vector < int >> visited ( n , vector < int > ( n , 0 )); return dfs_helper ( grid , visited , n , mid , 0 , 0 ); } bool dfs_helper ( vector < vector < int >> & grid , vector < vector < int >>& visited , int n , int mid , int i , int j ) { visited [ i ][ j ] = 1 ; for ( int k = 0 ; k < 4 ; ++ k ) { int a = i + x [ k ]; int b = j + y [ k ]; if ( a < 0 || a >= n || b < 0 || b >= n || visited [ a ][ b ] == 1 || grid [ a ][ b ] > mid ) continue ; if ( a == n - 1 && b == n - 1 ) return true ; if ( dfs_helper ( grid , visited , n , mid , a , b )) return true ; } return false ; } }; 483 Smallest Good Base \u00b6 Solution 1 Binary search This problem requires a bit reasoning to achieve the solution. The starting point is realy mean what's asking by the problem. Here it is asking a minimum base that represent the given number n in a representation like binary representation. For example: 13 = 3^0 + 3^1 + 3^2 so 13 can be representd as 111 (base 3). First of all, there is a special case that such a base may not exist. (precisely, we should seperate the special case when n = (n-1)^0 + (n-1)^1 ; With this special case in mind, we can use binary search to iterate through each m from largest to smallest and check whether the corresponding k is a good base of the given value n . Because when m is the largest, k is the smallest, so if the bianry search find one it must be the smallest k we are looking for. If binary search found nothing, we simpley return the special case n-1 . C++ class Solution { public : string smallestGoodBase ( string n ) { long long num = stoll ( n ); /* for each lenght of the potentional representation, * n = 1 + k + ... + k^{i-1} = (k^i-1)/(k-1), lower bound k is 2, * we have 2^i-1 = n ==> upper bound i = log2(n+1). */ for ( int i = log2 ( num + 1 ); i >= 2 ; -- i ) { /* upper bound is obtained by n = 1 + k + k^2 ... + k^(i-1) > k^(i-1), * n > k^(i-1) ==> k < n^(1/(i-1)); */ long long left = 2 , right = pow ( num , 1.0 / ( i - 1 )) + 1 ; while ( left < right ) { long long mid = left + ( right - left ) / 2 ; long long sum = 0 ; /* calculate i digits value with base \"mid\" */ for ( int j = 0 ; j < i ; ++ j ) { sum = sum * mid + 1 ; } /* binary search for the mid (good base) */ if ( sum == num ) return to_string ( mid ); if ( sum < num ) left = mid + 1 ; else right = mid ; } } return to_string ( num - 1 ); } }; 658. Find K Closest Elements \u00b6 Solution 1 Binary Search C++ class Solution { public : vector < int > findClosestElements ( vector < int >& arr , int k , int x ) { int start = 0 , end = arr . size () - k ; while ( start < end ) { int mid = start + ( end - start ) / 2 ; if ( x - arr [ mid ] > arr [ mid + k ] - x ) { start = mid + 1 ; } else { end = mid ; } } return vector < int > ( arr . begin () + start , arr . begin () + start + k ); } }; 378. Kth Smallest Element in a Sorted Matrix \u00b6 Solution 1 Binary Search The idea of using binary search for this problem my not be straightforward. But the method is very important. The idea is very similar to the problem Search in a rotated sorted array. Because the matrix is sorted row wise and column wise, there are some ordering information we can make use of. Notice we are not try to search using the matrix index, we are searching the matrix element value. Compare to the problem 287. Find the Duplicate Number. The comparison if (count < k) isn't include mid explicitly. but the count is some function f(mid) , with the current mid , the count value is unique and can be use to test a condition that decide which side we can go to shrink the range the target value is possible in. C++ class Solution { public : int kthSmallest ( vector < vector < int >>& matrix , int k ) { int m = matrix . size (); int n = m ? matrix [ 0 ]. size () : 0 ; int start = matrix [ 0 ][ 0 ], end = matrix [ m - 1 ][ n - 1 ]; while ( start < end ) { int mid = start + ( end - start ) / 2 ; int count = 0 ; for ( int i = 0 ; i < m ; ++ i ) { count += upper_bound ( matrix [ i ]. begin (), matrix [ i ]. end (), mid ) - matrix [ i ]. begin (); } if ( count < k ) { // notice no mid here, but count is a function of mid. start = mid + 1 ; } else { end = mid ; } } return start ; } }; Solution 2 Priority Queue Notice when the k <= n^2 , index j < matrix.size() will also make it work. C++ class Solution { public : int kthSmallest ( vector < vector < int >>& matrix , int k ) { priority_queue < int > pq ; for ( int i = 0 ; i < matrix . size (); ++ i ) { for ( int j = 0 ; j < matrix [ 0 ]. size (); ++ j ) { pq . push ( matrix [ i ][ j ]); if ( pq . size () > k ) pq . pop (); } } return pq . top (); } }; 668. Kth Smallest Number in Multiplication Table \u00b6 Solution 1 Binary search While this problem looks simple. But it really isn't unless you observed the following. The condition used for binary search is \"whether there are k smaller elements in the range [start, mid]\". You are looking for the smallest number that has k elements less than or equal to it. Like in the problem Kth Smallest Element in a Sorted Matrix, we will move the number not the index. We move the start or end appropriately based on this condition, if there are more than k, we shrink the range by reduce end: end = mid . If there are less than k numbers, we increase begin hopefully to make mid larger so as to have close to k numbers in the range of [1, mid] . When being == end , we've located the kth number desired. In case k > m*n , we will got begin == end < k , which is not a solution. In counting how many element less than mid, you have to be clever a bit by using the feature that this matrix is multiplicative table. That is for row i , you can at most have x/i number smaller than x , why? Follow up: Does the kth element will be in the range of [1, m*n] ? C++ class Solution { public : int findKthNumber ( int m , int n , int k ) { int begin = 1 , end = m * n ; while ( begin < end ) { int mid = begin + ( end - begin ) / 2 ; int count = 0 ; for ( int i = 1 ; i <= m ; ++ i ) { count += min ( mid / i , n ); } if ( count < k ) begin = mid + 1 ; else end = mid ; } return begin ; } }; 719. Find K-th Smallest Pair Distance \u00b6 Solution 1 Priority Queue TLE C++ class Solution { public : int smallestDistancePair ( vector < int >& nums , int k ) { priority_queue < int > pq ; for ( int i = 0 ; i < nums . size (); ++ i ) { for ( int j = i + 1 ; j < nums . size (); ++ j ) { int dist = abs ( nums [ i ] - nums [ j ]); if ( pq . size () < k ) { pq . push ( dist ); } else if ( dist < pq . top ()) { pq . push ( dist ), pq . pop (); } } } return pq . top (); } }; Solution 2 Binary search Similar to Problem 668. Kth Smallest Number in Multiplication Table. The problem is complicated at firt glass. A brute force solutoin generates all the absolute distances and then sort to find the kth smallest one. We found it is potentially a searchable senario if we sort the elements. We have range [min_distance, max_distance] . We search a distance in this range such that there are exactly k pairs distance including itself. If the count of pair distance less than k, we try to increase it buy start = mid + 1 , vice versa. When the binary search loop stops, if the result exist, start point to the distance we are searching. Since this problem guarrantee solution exist, we return start . C++ class Solution { public : int smallestDistancePair ( vector < int >& nums , int k ) { sort ( nums . begin (), nums . end ()); int start = nums [ 1 ] - nums [ 0 ]; for ( int i = 2 ; i < nums . size (); ++ i ) { start = min ( start , nums [ i ] - nums [ i - 1 ]); } int end = nums . back () - nums [ 0 ]; while ( start < end ) { int mid = start + ( end - start ) / 2 ; // count how many absolute differences that <= mid; int count = 0 ; for ( int i = 0 ; i < nums . size (); ++ i ) { int j = i ; while ( j < nums . size () && nums [ j ] - nums [ i ] <= mid ) j ++ ; count += j - i - 1 ; } if ( count < k ) { start = mid + 1 ; } else { end = mid ; } } return start ; } }; Solution 3 Using binary search to optimize the counting You can also write your own binary search routine upper_bound . class Solution { public : int smallestDistancePair ( vector < int >& nums , int k ) { sort ( nums . begin (), nums . end ()); int start = nums [ 1 ] - nums [ 0 ]; for ( int i = 2 ; i < nums . size (); ++ i ) { start = min ( start , nums [ i ] - nums [ i - 1 ]); } int end = nums . back () - nums [ 0 ]; while ( start < end ) { int mid = start + ( end - start ) / 2 ; // count how many absolute differences that <= mid; int count = 0 ; /* for (int i = 0; i < nums.size(); ++i) { int j = i; while (j < nums.size() && nums[j] - nums[i] <= mid) j++; count += j - i - 1; } */ // optimize the counting use binary search (nested binary search) for ( int i = 0 ; i < nums . size (); ++ i ) { auto iter = upper_bound ( nums . begin () + i , nums . end (), nums [ i ] + mid ); count += iter - ( nums . begin () + i ) - 1 ; } if ( count < k ) { start = mid + 1 ; } else { end = mid ; } } return start ; } }; Category 4 Binary search as an optimization routine \u00b6 300 Longest Increasing Subsequence \u00b6 Solution 1 DP The base case is single char. f[i] is the length of LIS from the begining. C++ class Solution { public : int lengthOfLIS ( vector < int >& nums ) { if ( n == nums . size ()) return 0 ; int f [ n ] = { 0 }; int res = 0 ; for ( int j = 0 ; j < n ; j ++ ) { f [ j ] = 1 ; for ( int i = 0 ; i < j ; i ++ ) { if ( nums [ i ] < nums [ j ] && f [ i ] + 1 > f [ j ]) f [ j ] = f [ i ] + 1 ; } res = max ( res , f [ j ]); } return res ; } }; Solution 2 Using binary search The DP solution is O(n^2) . Using binary search could reduce to O(nlogn) . Binary search solution analysis. For each i , we are looking for the largest f value that has smallest A value. For example, A[0] = 5 could be ignored because of its f value is same as A[1] = 1 , which is smaller. In searching for the LIS, we prefer a small ending value when the length is the same. The following solution using a vector b to record the minimum A value for each length of LIS ( f value), we use binary search to find the last value in b that smaller that current value A[i] . If we found such a value in b , we use A[i] to replace the value next to the found value in b ). i 0 1 2 3 4 5 6 7 A 5 1 3 7 6 4 2 10 f 1 1 2 3 3 3 2 4 f[1] = 1, a[1] = 1 f[6] = 2, a[6] = 2 f[5] = 3, a[5] = 4 f[7] = 4, a[7] = 10 C++ class Solution { public : int lengthOfLIS ( vector < int >& nums ) { vector < int > b ; for ( int i = 0 ; i < nums . size (); ++ i ) { int l = 0 , r = b . size (); while ( l < r ) { int m = l + ( r - l ) / 2 ; if ( b [ m ] < nums [ i ]) { // nums[i] is the target l = m + 1 ; } else { r = m ; } } if ( l == b . size ()) // nums[i] greater than all element in b b . push_back ( nums [ i ]); else // begin point to next element no less than the target nums[i]. b [ l ] = nums [ i ]; } return b . size (); } }; Alternatively, we could use lower_bound . C++ class Solution { public : int lengthOfLIS ( vector < int >& nums ) { vector < int > b ; for ( int i = 0 ; i < nums . size (); ++ i ) { int l = lower_bound ( b . begin (), b . end (), nums [ i ]) - b . begin (); if ( l == b . size ()) // nums[i] greater than all element in b b . push_back ( nums [ i ]); else // begin point to next element no less than the target nums[i]. b [ l ] = nums [ i ]; } return b . size (); } }; 354. Russian Doll Envelopes \u00b6 174. Dungeon Game \u00b6","title":"Binary Search"},{"location":"leetcode/binary-search/notes/#binary-search","text":"","title":"Binary Search"},{"location":"leetcode/binary-search/notes/#binary-search-problem-characteristics","text":"Ordered binary search. You need to find index or array element where ordering info is available, either explicitly (sorted array) or implicitly (partially sorted or other special info). monotony pattern. If the ordering info isn't available, but you can exclude \"all\" the possible cases from left or right by a condition camparing f(mid) to the target .","title":"Binary search problem characteristics"},{"location":"leetcode/binary-search/notes/#binary-search-problem-solving-techniques","text":"Clarify that you are trying to find the first one or find the last one. Clarify that you are trying to move the index or move the value (i.e. kth smallest number in multiplicative table). Use an \"ordering abstraction\" vless(target, f(mid)) . This ordering abstraction will produce a boolean array that indicate the ordering information between the target value and the f(mid) . Decide whether the left or right part the f(mid) should fall into. The principle to determine the predicate is simple: don't rule out the possible result (maintain the loop invariant). Shrink the range accordingly based on the predicate decided in step 3. Test the case that the search range is small, such as only have one or two elements.","title":"Binary search problem solving techniques"},{"location":"leetcode/binary-search/notes/#binary-search-practical-use-case","text":"Find whether the given target is in the array. Find the the position of the first value equal to the given target. Find the insertion position of the given target in the array. Find the the position of the last value equal to the given target. Find the total number of x in a sorted array. Find the last element less than the target. Find the first element greater than the target.","title":"Binary search practical use case"},{"location":"leetcode/binary-search/notes/#binary-search-in-c-stl","text":"lower_bound : return iterator point to the element no less than the target. upper_bound : return iterator point to the element greater than the target. equal_range : return a pair of iterators, first of which is lower_bound , second is upper_bound . binary_search : return true if an element equivalent to val is found, and false otherwise.","title":"Binary search in C++ STL"},{"location":"leetcode/binary-search/notes/#caveat-of-binary-search-implementation","text":"Specify the range: [start, end) or [start, end] ? C++ STL used [start, end) to denote a range, which bring in many conveniences. We will stick on this convention. Which while loop condition? start < end ? start <= end ? start != end ? start + 1 < end ? The calculation of the mid . mid = start + (end - start) / 2 or mid = (start + end) / 2 ? To proof mid is always in the range [begin, end) . The \"bisection\": start = mid + 1 , start = mid , or end = mid - 1 or end = mid ? Where is the result? start ? end ? How to make sure?","title":"Caveat of binary search implementation"},{"location":"leetcode/binary-search/notes/#a-universial-binary-search-implementation","text":"Despite the above caveas, just remember that there are two version of binary search one can write based on the range [begin, end) and [begin, end] . Iterator type in C++ using the former, it have many benefits in reduce the code complexity. Among all the binary search implementation you have seen, the following one is the most powerful version and it equivalent to C++ STL lower_bound algorithm. /** * return index to an element no less than x. Be more specifically, if there is * an element in the given array equal to x, it returns the index of first such * element; if there is no element that is equal to x, it returns the index * where x can be inserted into the position without changing the ordering of * the elements. * * All possible return value for calling this function with array.size() == n is * [0, 1, ..., n - 1, n] * */ size_t binary_search ( int x , vector < int >& array , size_t n ) { size_t begin = 0 , end = n ; while ( begin != end ) { size_t mid = begin + ( end - begin ) / 2 ; if ( array [ mid ] < x ) { begin = mid + 1 ; } else { end = mid ; } } return begin ; }","title":"A \"universial\" binary search implementation"},{"location":"leetcode/binary-search/notes/#important-observations-about-this-implementation","text":"mid cannot less than begin , they can be equal. This will ensure begin = mid + 1 in the if statement at least reduce the size of [begin, end] by 1. Informal proof: if (array[mid] < x) , it indicate x can only possible be in array[mid + 1, mid + 2, ... n - 1]. mid + 1 is at least 1 greater than begin. mid and end never equal inside the while loop, mid < end is always hold. This will ensure end = mid in the else statement at least reduce the size of [begin, end] by 1. Informal proof: we have begain < end , so begin + end < 2 * end , thus (begin + end) / 2 < end , because integer divisioin truncate down, mid = (begin + end) / 2 always less than end. begin and end never cross. Informal proof: Inside the while loop, at the begining, we have begin < end . If the current iteration executes the if condition, begin = mid + 1 at most advance begin to end but not exceed end . If it execute the else condition, end = mid would at worst case change end point to the minimum value of mid , because we have begin <= mid . Thus, we can conclude that executing the statement end = mid will not change end less than begin , at worst equal to begin .","title":"Important observations about this implementation"},{"location":"leetcode/binary-search/notes/#claims-regarding-this-binary-search-routine","text":"The range [begin, end) is used, which comply to the convention used in C++ iterator. It is impossible that mid == end . If they are equal, array[n] is invalid memory access. We use the loop condition while (begin != end) to indicate that once the loop terminates, we have begin == end . By checking whether begin is a valid index to the array or not, we can know whether x is greater than all the elements in the array or not. If we want to check whether x is found in the array, we simply check array[begin] == x . However, this condition is based on the assumption that begin < end initially. Considering that, using while (begin < end) is better if you cannot ensure begin < end before the loop. Setting begin = mid + 1 reduces the size of the remaining interested sub-array and maintains the invariant, which is if x in the array, x is in [begin, end) . Setting end = mid reduces the size of the remaining interested sub-array (mid never equal to end) and maintains the invariant, which is if x in the array, x is in [begin, end) . This claim is a little hard to absorb. On way to understand is like the following: ~~Because we need keep searching x in the range [begin, mid] if we get in the else statement. In the else case there are two possibilities: 1) array[mid] > x . 2) array[mid] = x . For 1) it indicates x is in [begin, mid) , setting end = mid maintains the loop invariant correctly, which is that x is in the shrinked range. For the 2) it is a little complex. If array[mid] is the only element equal to x, setting end = mid appears violate the loop invariant by exclude x from the range [begin, end) . however, remember array[mid] is the only element equal to x, after the while loop, begin = end , we have the x found by begin even though theoretically [begin, end) is already an empty range since begin = end and array[begin] = array[end] = x . If there are more values are equal to x before and after the element array[mid] the loop will always end up finding the first x value in the array. If we use end = mid + 1 . Try test case [1, 3, 5, 7] , with x = 0 . deadloop will accur. i.e. begin = 0, mid = 1, end = 2 .","title":"Claims regarding this binary search routine"},{"location":"leetcode/binary-search/notes/#category-1-binary-search-basics-and-binary-search-on-special-array-ie-rotated-sorted","text":"To solve this type of binary search problem. You should focus on the following: Come up test cases to verify your solution. Be able to find which side to drop for each iteration. Be extremly careful \"off by 1\" bugs. (1. reasoning: is mid value possible to be the solution or not. 2. exercise test cases: especially short ones)","title":"Category 1 Binary search basics and binary search on special array (i.e. rotated sorted)"},{"location":"leetcode/binary-search/notes/#34-search-for-a-range","text":"C++ Use STL function class Solution { public : vector < int > searchRange ( vector < int >& nums , int target ) { vector < int > res ( 2 , - 1 ); int low = lower_bound ( nums . begin (), nums . end (), target ) - nums . begin (); int high = upper_bound ( nums . begin (), nums . end (), target ) - nums . begin (); if ( low == high ) return res ; return { low , hight - 1 }; } }; C++ Implementation of binary search class Solution { public : vector < int > searchRange ( vector < int >& nums , int target ) { vector < int > res ( 2 , - 1 ); int low = lower_bound ( nums , target ); //int high = lower_bound(nums, target + 1); // also works. int high = upper_bound ( nums , target ); if ( low == high ) { return res ; } return { low , high - 1 }; } int lower_bound ( vector < int >& nums , int target ) { if ( nums . size () == 0 ) return 0 ; int l = 0 , r = nums . size (); while ( l < r ) { int m = l + ( r - l ) / 2 ; if ( nums [ m ] < target ) { l = m + 1 ; } else { r = m ; } } return l ; } int upper_bound ( vector < int >& nums , int target ) { if ( nums . size () == 0 ) return 0 ; int l = 0 , r = nums . size (); while ( l < r ) { int m = l + ( r - l ) / 2 ; if ( nums [ m ] <= target ) { l = m + 1 ; } else { r = m ; } } return l ; } }; Python solution class Solution ( object ): def searchRange ( self , nums , target ): \"\"\" :type nums: List[int] :type target: int :rtype: List[int] \"\"\" if len ( nums ) == 0 : return [ - 1 , - 1 ] begin = 0 end = len ( nums ) while begin != end : mid = begin + ( end - begin ) / 2 if nums [ mid ] < target : begin = mid + 1 else : end = mid if begin == len ( nums ): return [ - 1 , - 1 ] if nums [ begin ] == target : lower = begin else : lower = - 1 begin = 0 end = len ( nums ) while begin != end : mid = begin + ( end - begin ) / 2 if nums [ mid ] <= target : begin = mid + 1 else : end = mid if nums [ begin - 1 ] == target : upper = begin - 1 else : upper = - 1 return [ lower , upper ]","title":"34. Search for a Range"},{"location":"leetcode/binary-search/notes/#35-search-insert-position","text":"C++ solution lower_bound class Solution { public : int searchInsert ( vector < int >& nums , int target ) { if ( nums . size () == 0 ) return 0 ; int l = 0 , r = nums . size (); while ( l < r ) { int m = l + ( r - l ) / 2 ; if ( nums [ m ] < target ) { l = m + 1 ; } else { r = m ; } } return l ; } };","title":"35. Search Insert Position"},{"location":"leetcode/binary-search/notes/#33-search-in-rotated-sorted-array","text":"How to locate the sorted half? If left half is sorted, check where the target t is like be. else if right half is sorted, check where the target t is like to be. else if mid element is equal to left or right. Remove one of them. Although no duplicate, should consider short input like [3 1], 1 will have the equal case. C++ /** t = 1 t = 3 t = 5 t = 4 t = -1 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 3 4 5 1 1 3 5 4 1 <--need check */ class Solution { public : int search ( vector < int >& A , int t ) { if ( A . empty ()) return - 1 ; int l = 0 , r = A . size () - 1 ; while ( l < r ) { int m = l + ( r - l ) / 2 ; if ( A [ m ] == t ) return m ; if ( A [ l ] < A [ m ]) { // left is sorted if ( A [ l ] <= t && t < A [ m ]) { r = m - 1 ; } else { l = m + 1 ; } } else if ( A [ m ] < A [ r ]) { // right is sorted if ( A [ m ] < t && t <= A [ r ]) { l = m + 1 ; } else { r = m - 1 ; } } else { // if equal, remove one. case: [3, 1], 1 if ( A [ l ] == A [ m ]) l ++ ; if ( A [ m ] == A [ r ]) r -- ; } } return A [ l ] == t ? l : - 1 ; } }; Python class Solution ( object ): def search ( self , nums , target ): \"\"\" :type nums: List[int] :type target: int :rtype: int \"\"\" if len ( nums ) == 0 : return - 1 left = 0 right = len ( nums ) - 1 while left < right : mid = left + ( right - left ) / 2 if nums [ mid ] == target : return mid ; if nums [ left ] < nums [ mid ]: if nums [ left ] <= target and target < nums [ mid ]: right = mid - 1 else : left = mid + 1 elif nums [ mid ] < nums [ right ]: if nums [ mid ] < target and target <= nums [ right ]: left = mid + 1 else : right = mid - 1 else : if nums [ left ] == nums [ mid ]: left += 1 if nums [ right ] == nums [ mid ]: right -= 1 if nums [ left ] == target : return left return - 1","title":"33. Search in Rotated Sorted Array"},{"location":"leetcode/binary-search/notes/#81-search-in-rotated-sorted-array-ii","text":"How to locate the sorted half? C++ class Solution { public : bool search ( vector < int >& A , int t ) { if ( A . empty ()) return false ; int l = 0 , r = A . size () - 1 ; while ( l < r ) { int m = l + ( r - l ) / 2 ; if ( A [ m ] == t ) return true ; if ( A [ l ] < A [ m ]) { if ( A [ l ] <= t && t < A [ m ]) { r = m - 1 ; } else { l = m + 1 ; } } else if ( A [ m ] < A [ r ]) { if ( A [ m ] < t && t <= A [ r ]) { l = m + 1 ; } else { r = m - 1 ; } } else { if ( A [ l ] == A [ m ]) l ++ ; if ( A [ m ] == A [ r ]) r -- ; } } return A [ l ] == t ? true : false ; } }; Python class Solution ( object ): def search ( self , nums , target ): \"\"\" :type nums: List[int] :type target: int :rtype: int \"\"\" if len ( nums ) == 0 : return False left = 0 right = len ( nums ) - 1 while left < right : mid = left + ( right - left ) / 2 if nums [ mid ] == target : return True if nums [ left ] < nums [ mid ]: if nums [ left ] <= target and target < nums [ mid ]: right = mid - 1 else : left = mid + 1 elif nums [ mid ] < nums [ right ]: if nums [ mid ] < target and target <= nums [ right ]: left = mid + 1 else : right = mid - 1 else : if nums [ left ] == nums [ mid ]: left += 1 if nums [ right ] == nums [ mid ]: right -= 1 if nums [ left ] == target : return True return False","title":"81. Search in Rotated Sorted Array II"},{"location":"leetcode/binary-search/notes/#153-find-minimum-in-rotated-sorted-array","text":"Try to locate the valley which contains the min. Notice when A[0] < A[n - 1] , return A[0] . Draw a monotonic curve and then split the curve into two half, swith the order. This can help you to write the code. C++ class Solution { public : int findMin ( vector < int >& A ) { int l = 0 , r = A . size () - 1 ; while ( l < r ) { if ( A [ l ] < A [ r ]) // serve as base case. return A [ l ]; int m = l + ( r - l ) / 2 ; if ( A [ m ] > A [ r ]) { // also works. looking for not sorted half l = m + 1 ; } else if ( A [ m ] < A [ r ]) { // don't really need if statement r = m ; } } return A [ l ]; } }; Python class Solution ( object ): def findMin ( self , nums ): \"\"\" :type nums: List[int] :rtype: int \"\"\" if len ( nums ) == 0 : return - 1 left = 0 right = len ( nums ) - 1 while left < right : if nums [ left ] == nums [ right ]: return nums [ left ] mid = left + ( right - left ) / 2 if nums [ mid ] > nums [ right ]: left = mid + 1 else : right = mid return nums [ left ]","title":"153. Find Minimum in Rotated Sorted Array"},{"location":"leetcode/binary-search/notes/#154-find-minimum-in-rotated-sorted-array-ii","text":"Locate the valley which contains the min. Since duplicates exist. we cannot use the observation A[l] == A[r] . Here we deal with duplicates using decrease by one step. C++ class Solution { public : int findMin ( vector < int >& A ) { int l = 0 , r = A . size () - 1 ; while ( l < r ) { int m = l + ( r - l ) / 2 ; if ( A [ m ] > A [ r ]) { l = m + 1 ; } else if ( A [ m ] < A [ r ]) { r = m ; } else { r -- ; } } return A [ l ]; } }; Python class Solution ( object ): def findMin ( self , nums ): \"\"\" :type nums: List[int] :rtype: int \"\"\" if len ( nums ) == 0 : return - 1 left = 0 right = len ( nums ) - 1 while left < right : mid = left + ( right - left ) / 2 if nums [ mid ] > nums [ right ]: left = mid + 1 elif nums [ mid ] < nums [ right ]: right = mid else : right -= 1 return nums [ left ]","title":"154. Find Minimum in Rotated Sorted Array II"},{"location":"leetcode/binary-search/notes/#162-find-peak-element","text":"Use Binary search Use the neighboring relation to determin which side a peak value may occur then eliminate the other side. C++ class Solution { public : int findPeakElement ( vector < int >& A ) { int l = 0 , r = A . size () - 1 ; while ( l < r ) { int m = l + ( r - l ) / 2 ; if ( A [ m ] < A [ m + 1 ]) { l = m + 1 ; } else if ( A [ m ] > A [ m + 1 ]) { r = m ; } } return l ; } };","title":"162 Find Peak Element"},{"location":"leetcode/binary-search/notes/#278-first-bad-version","text":"Binary search Notice how this can be related to the ordering abstraction. // Forward declaration of isBadVersion API. bool isBadVersion ( int version ); class Solution { public : int firstBadVersion ( int n ) { int l = 0 , r = n ; while ( l < r ) { int m = l + ( r - l ) / 2 ; if ( ! isBadVersion ( m )) { l = m + 1 ; } else { r = m ; } } return l ; } };","title":"278. First Bad Version"},{"location":"leetcode/binary-search/notes/#374-guess-number-higher-or-lower","text":"C++ binary search // Forward declaration of guess API. // @param num, your guess // @return -1 if my number is lower, 1 if my number is higher, otherwise return 0 int guess ( int num ); class Solution { public : int guessNumber ( int n ) { int start = 1 , end = n ; while ( start < end ) { int mid = start + ( end - start ) / 2 ; if ( guess ( mid ) == 0 ) return mid ; if ( guess ( mid ) == 1 ) { start = mid + 1 ; } else { end = mid ; } } return start ; } }; Python # The guess API is already defined for you. # @param num, your guess # @return -1 if my number is lower, 1 if my number is higher, otherwise return 0 # def guess(num): class Solution ( object ): def guessNumber ( self , n ): \"\"\" :type n: int :rtype: int \"\"\" begin = 0 end = n while begin != end : mid = begin + ( end - begin ) / 2 if guess ( mid ) == 0 : return mid if guess ( mid ) == 1 : begin = mid + 1 else : end = mid return begin","title":"374. Guess Number Higher or Lower"},{"location":"leetcode/binary-search/notes/#475-heaters","text":"Sort then brute force The solution we are looking for is the max value of the smallest house-heater distance. Think through what is the distance you want to keep, min or max C++ class Solution { public : int findRadius ( vector < int >& houses , vector < int >& heaters ) { int m = houses . size (); int n = heaters . size (); sort ( houses . begin (), houses . end ()); sort ( heaters . begin (), heaters . end ()); int res = INT_MIN ; int i , j = 0 ; for ( i = 0 ; i < m ; ++ i ) { while ( j < n - 1 && abs ( heaters [ j + 1 ] - houses [ i ]) <= abs ( heaters [ j ] - houses [ i ])) { j ++ ; } res = max ( res , abs ( houses [ i ] - heaters [ j ])); } return res ; } }; Python class Solution ( object ): def findRadius ( self , houses , heaters ): \"\"\" :type houses: List[int] :type heaters: List[int] :rtype: int \"\"\" m = len ( houses ) n = len ( heaters ) houses . sort () heaters . sort () i = 0 j = 0 res = 0 for i in range ( m ): while j < n - 1 and abs ( heaters [ j + 1 ] - houses [ i ]) <= abs ( heaters [ j ] - houses [ i ]): j += 1 res = max ( res , abs ( houses [ i ] - heaters [ j ])) return res Binary search the neighboring heaters get max of min Notice we cannot sort hourses and then search each heater's position. A special cases [1, 2, 3] 2 , the result is 0 whereis it should be 1 . C++ class Solution { public : int findRadius ( vector < int >& houses , vector < int >& heaters ) { int n = heaters . size (); sort ( heaters . begin (), heaters . end ()); int res = INT_MIN ; for ( int house : houses ) { int start = 0 , end = n ; while ( start < end ) { int mid = start + ( end - start ) / 2 ; if ( heaters [ mid ] < house ) start = mid + 1 ; else end = mid ; } int dist1 = ( start == n ) ? INT_MAX : heaters [ start ] - house ; int dist2 = ( start == 0 ) ? INT_MAX : house - heaters [ start - 1 ]; res = max ( res , min ( dist1 , dist2 )); } return res ; } }; Python class Solution ( object ): def findRadius ( self , houses , heaters ): \"\"\" :type houses: List[int] :type heaters: List[int] :rtype: int \"\"\" m = len ( houses ) n = len ( heaters ) heaters . sort () i = 0 j = 0 res = float ( '-inf' ) for i in range ( m ): start = 0 end = n while start != end : mid = start + ( end - start ) / 2 if heaters [ mid ] < houses [ i ]: start = mid + 1 else : end = mid dist1 = float ( 'inf' ) dist2 = float ( 'inf' ) if start != n : dist1 = heaters [ start ] - houses [ i ] if start != 0 : dist2 = houses [ i ] - heaters [ start - 1 ] res = max ( res , min ( dist1 , dist2 )) return res","title":"475. Heaters"},{"location":"leetcode/binary-search/notes/#74-search-a-2d-matrix","text":"Binary search We can view the matrix as a big sorted array and then binary search the target. Notice test your finished routine using edge cases. (i.e. the initial value of end) C++ class Solution { public : bool searchMatrix ( vector < vector < int >>& matrix , int target ) { int m = matrix . size (); int n = m ? matrix [ 0 ]. size () : 0 ; if ( m == 0 || n == 0 ) return false ; int start = 0 , end = m * n - 1 ; while ( start < end ) { int mid = start + ( end - start ) / 2 ; int i = mid / n , j = mid % n ; if ( matrix [ i ][ j ] < target ) { start = mid + 1 ; } else { end = mid ; } } return matrix [ start / n ][ start % n ] == target ? true : false ; } };","title":"74. Search a 2D Matrix"},{"location":"leetcode/binary-search/notes/#240-search-a-2d-matrix-ii","text":"Binary search to exclude whole column or whole row the key is you decide where to start the compare. If you start from left bottom or right top, the solution should be abvious. Notice the idea is from binary search, if ordering info available, we want to exclude as many as impossible values as we can. class Solution { public : bool searchMatrix ( vector < vector < int >>& matrix , int target ) { int m = matrix . size (); int n = m ? matrix [ 0 ]. size () : 0 ; if ( m == 0 || n == 0 ) return false ; int x = m - 1 , y = 0 ; while ( x >= 0 && y < n ) { if ( matrix [ x ][ y ] == target ) { return true ; } if ( matrix [ x ][ y ] < target ) { y ++ ; } else { x -- ; } } return false ; } };","title":"240. Search a 2D Matrix II"},{"location":"leetcode/binary-search/notes/#302-smallest-rectangle-enclosing-black-pixels","text":"C++ Brute Force class Solution { public : int minArea ( vector < vector < char >>& image , int x , int y ) { int m = image . size (); int n = m ? image [ 0 ]. size () : 0 ; int top = m , bottom = 0 , left = n , right = 0 ; for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { if ( image [ i ][ j ] == '1' ) { top = min ( top , i ); bottom = max ( bottom , i + 1 ); left = min ( left , j ); right = max ( right , j + 1 ); } } } return ( right - left ) * ( bottom - top ); } }; Binary search Notice the binary search idea is related to the problem Smallest Good Base and Wood Cut. The basic idea is seach each of furthest 1 from 4 directions. First make sure you can search one boundary and the others are similar. For example, to search the first row that contains 1 , we can look at the whole column/row to see whether this col/row have 1 . Because we are searching the first row that have 1 top down, bisec based on the count of 1 on each row we can know whether we ignore upper half or lower half. C++ Binary Search class Solution { public : int minArea ( vector < vector < char >>& image , int x , int y ) { int m = image . size (); int n = m ? image [ 0 ]. size () : 0 ; int top = bsearch_byrows ( image , 0 , x , 0 , n , true ); // search top int bottom = bsearch_byrows ( image , x + 1 , m , 0 , n , false ); int left = bsearch_bycols ( image , 0 , y , top , bottom , true ); int right = bsearch_bycols ( image , y + 1 , n , top , bottom , false ); return ( bottom - top ) * ( right - left ); } int bsearch_byrows ( vector < vector < char >>& image , int x , int y , int left , int right , bool white2black ) { while ( x < y ) { int m = ( x + y ) / 2 ; int k = left ; while ( k < right && image [ m ][ k ] == '0' ) k ++ ; if ( k < right == white2black ) { // mth row have '1' y = m ; } else { x = m + 1 ; } } return x ; } int bsearch_bycols ( vector < vector < char >>& image , int x , int y , int top , int bottom , bool white2black ) { while ( x < y ) { int m = ( x + y ) / 2 ; int k = top ; while ( k < bottom && image [ k ][ m ] == '0' ) k ++ ; if ( k < bottom == white2black ) { // mth column have '1' y = m ; } else { x = m + 1 ; } } return x ; } }; Python Binary Search class Solution ( object ): def minArea ( self , image , x , y ): \"\"\" :type image: List[List[str]] :type x: int :type y: int :rtype: int \"\"\" m = len ( image ) n = 0 if m != 0 : n = len ( image [ 0 ]) top = self . bsearch_row ( image , 0 , x , 0 , n , True ) bottom = self . bsearch_row ( image , x + 1 , m , 0 , n , False ) left = self . bsearch_col ( image , 0 , y , top , bottom , True ) right = self . bsearch_col ( image , y + 1 , n , top , bottom , False ) return ( bottom - top ) * ( right - left ) def bsearch_row ( self , image , start , end , lower , upper , white2black ): while start < end : m = ( start + end ) / 2 k = lower while k < upper and image [ m ][ k ] == '0' : k += 1 if ( k < upper ) == white2black : end = m else : start = m + 1 return start def bsearch_col ( self , image , start , end , lower , upper , white2black ): while start < end : m = ( start + end ) / 2 k = lower while k < upper and image [ k ][ m ] == '0' : k += 1 if ( k < upper ) == white2black : end = m else : start = m + 1 return start BFS class Solution { public : int minArea ( vector < vector < char >>& image , int x , int y ) { int m = image . size (); int n = m ? image [ 0 ]. size () : 0 ; int top = m , bottom = 0 , left = n , right = 0 ; int xx [ 4 ] = { - 1 , 0 , 1 , 0 }; int yy [ 4 ] = { 0 , 1 , 0 , - 1 }; queue < pair < int , int >> q ; q . push ({ x , y }); image [ x ][ y ] = '0' ; while ( ! q . empty ()) { pair < int , int > t = q . front (); q . pop (); top = min ( top , t . first ); bottom = max ( bottom , t . first + 1 ); left = min ( left , t . second ); right = max ( right , t . second + 1 ); for ( int k = 0 ; k < 4 ; ++ k ) { int a = t . first + xx [ k ]; int b = t . second + yy [ k ]; if ( a >= 0 && a < m && b >= 0 && b < n && image [ a ][ b ] == '1' ) { q . push ({ a , b }); image [ a ][ b ] = '0' ; } } } return ( right - left ) * ( bottom - top ); } }; BFS from collections import deque class Solution ( object ): def minArea ( self , image , x , y ): \"\"\" :type image: List[List[str]] :type x: int :type y: int :rtype: int \"\"\" m = len ( image ) n = 0 if m != 0 : n = len ( image [ 0 ]) xx = [ - 1 , 0 , 1 , 0 ] yy = [ 0 , - 1 , 0 , 1 ] top = m bottom = 0 left = n right = 0 q = deque () q . append ([ x , y ]) image [ x ][ y ] = '0' while len ( q ) > 0 : t = q . popleft () top = min ( top , t [ 0 ]) bottom = max ( bottom , t [ 0 ] + 1 ) left = min ( left , t [ 1 ]) right = max ( right , t [ 1 ] + 1 ) for k in range ( 4 ): a = t [ 0 ] + xx [ k ] b = t [ 1 ] + yy [ k ] if a >= 0 and a < m and b >= 0 and b < n and image [ a ][ b ] == '1' : q . append ([ a , b ]) image [ a ][ b ] = '0' return ( right - left ) * ( bottom - top ) DF class Solution { private : int m , n ; int top , bottom , left , right ; public : int minArea ( vector < vector < char >>& image , int x , int y ) { m = image . size (); n = m ? image [ 0 ]. size () : 0 ; top = m , bottom = 0 , left = n , right = 0 ; dfs_helper ( image , x , y ); return ( right - left ) * ( bottom - top ); } void dfs_helper ( vector < vector < char >>& image , int x , int y ) { if ( x < 0 || x >= m || y < 0 || y >= n || image [ x ][ y ] == '0' ) { return ; } image [ x ][ y ] = '0' ; top = min ( top , x ); bottom = max ( bottom , x + 1 ); left = min ( left , y ); right = max ( right , y + 1 ); dfs_helper ( image , x - 1 , y ); dfs_helper ( image , x , y + 1 ); dfs_helper ( image , x + 1 , y ); dfs_helper ( image , x , y - 1 ); } };","title":"302. Smallest Rectangle Enclosing Black Pixels"},{"location":"leetcode/binary-search/notes/#363-max-sum-of-rectangle-no-larger-than-k","text":"Iterate the wide of the matrix and using prefix sum and set lower_bound . From the problem Max Sum of Subarry No Larger Than K, we have to enumerate the width of the sub-matrix and sum up all row elements and get an array of length m , m is the number of rows of the matrix. Then apply the method. C++ presum class Solution { public : int maxSumSubmatrix ( vector < vector < int >>& matrix , int k ) { if ( matrix . empty ()) return 0 ; int m = matrix . size (); int n = m ? matrix [ 0 ]. size () : 0 ; int res = INT_MIN ; for ( int l = 0 ; l < n ; ++ l ) { vector < int > sums ( m , 0 ); for ( int r = l ; r < n ; ++ r ) { for ( int i = 0 ; i < m ; ++ i ) { sums [ i ] += matrix [ i ][ r ]; } set < int > preSumSet ; preSumSet . insert ( 0 ); int preSum = 0 , curMax = INT_MIN ; for ( int sum : sums ) { preSum += sum ; set < int >:: iterator it = preSumSet . lower_bound ( preSum - k ); if ( it != preSumSet . end ()) { curMax = max ( curMax , preSum - * it ); } preSumSet . insert ( preSum ); } res = max ( res , curMax ); } } return res ; } }; merge sort The idea is similar that solution 1. Instead of calculate preSum on the fly, we finish calculation and pass it to a mergeSort routine. The use mergeSort here is to find the A[j] - A[i] <= k efficiently, O(nlogn) . C++ Merge Sort class Solution { public : int maxSumSubmatrix ( vector < vector < int >>& matrix , int k ) { int m = matrix . size (); int n = m ? matrix [ 0 ]. size () : 0 ; int res = INT_MIN ; vector < long long > sums ( m + 1 , 0 ); for ( int l = 0 ; l < n ; ++ l ) { vector < long long > sumInRow ( m , 0 ); for ( int r = l ; r < n ; ++ r ) { for ( int i = 0 ; i < m ; ++ i ) { sumInRow [ i ] += matrix [ i ][ r ]; sums [ i + 1 ] = sums [ i ] + sumInRow [ i ]; } res = max ( res , mergeSort ( sums , 0 , m + 1 , k )); if ( res == k ) return k ; } } return res ; } int mergeSort ( vector < long long >& sums , int start , int end , int k ) { if ( end == start + 1 ) return INT_MIN ; int mid = start + ( end - start ) / 2 ; int res = mergeSort ( sums , start , mid , k ); if ( res == k ) return k ; res = max ( res , mergeSort ( sums , mid , end , k )); if ( res == k ) return k ; long long cache [ end - start ]; int j = mid , c = 0 , t = mid ; for ( int i = start ; i < mid ; ++ i ) { while ( j < end && sums [ j ] - sums [ i ] <= k ) ++ j ; // search first time sums[j] - sums[i] > k if ( j - 1 >= mid ) { // sums[j - 1] - sums[i] <= k, make sure j - 1 is still in right side res = max ( res , ( int )( sums [ j - 1 ] - sums [ i ])); if ( res == k ) return k ; } while ( t < end && sums [ t ] < sums [ i ]) { cache [ c ++ ] = sums [ t ++ ]; } cache [ c ++ ] = sums [ i ]; } for ( int i = start ; i < t ; ++ i ) { sums [ i ] = cache [ i - start ]; } return res ; } };","title":"363. Max Sum of Rectangle No Larger Than K"},{"location":"leetcode/binary-search/notes/#category-2-using-ordering-abstration","text":"","title":"Category 2 Using ordering abstration"},{"location":"leetcode/binary-search/notes/#69-sqrtx","text":"Solution 1 using ordering abstraction definition To find a square root of a integer x using binary search. We need to first determin the range [left, right] that the target value sqrt(x) may in. The potential range we can search is [0, x/2 + 1] . Then we should clarify this binary search is the \"find the first one\" type or the \"find the last one\" type. Basically, we want to determine our ordering abstraction f(target, g(i)) that is able to produce a boolean array. The boolean array have true part and false part seperated. Here target = sqrt(x) and g(i) = i . We define f(sqrt(x), i) = true when i <= sqrt(x) and f(sqrt(x), i) = false when i > sqrt(x) . This came from the following intuition: We are looking for the \"last\" integer whose square is less than x . Why not the otherwise? Because if you change to find the \"first\" integer whose square is greater than the x from right section of the boolean array, it is hard to define our ordering abstraction f . Of cause, we can search the \"first\" integer whose square is greater than x and find the previous integer next to it as the solution, but this later solution is a bit complex and counter intuitive. We prefer the first definition of ordering abstraction. Although a workable solution following the second ordering abstraction is also given below. For example: to solve the sqrt(8) and sqrt(9) using our definition: k, i = 0 1 2 3 4 5 6 7 8 9 10 n = 11 A = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] f(sqrt(8), k) = [T T T F F] f(sqrt(9), k) = [T T T T F] The binary search routine will be: C++ class Solution { public : int mySqrt ( int x ) { int l = 0 , r = x / 2 + 1 ; while ( l < r ) { // int m = l + (r - l) / 2; // will deadloop for 4, why? int m = r - ( r - l ) / 2 ; if ( m <= x / m ) { l = m ; } else { r = m - 1 ; } } return l ; } }; Solution 2 using the alternative ordering abstraction definition Second ordering abstraction (find first value whose square is greater than x) k, i = 0 1 2 3 4 5 6 7 8 9 10 n = 11 A = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] f(sqrt(8), k) = [F F F T T] f(sqrt(9), k) = [F F F F T] C++ class Solution { public : int mySqrt ( int x ) { if ( x == 0 ) return 0 ; // should handle, but will got division by zero in line 9. int l = 0 , r = x / 2 + 2 ; // r = x / 2 + 1 will not working for x = 1, have to have the one past last; while ( l < r ) { //int m = r - (r - l) / 2; // will dead loop for 4 int m = l + ( r - l ) / 2 ; if ( m > x / m ) { r = m ; } else { l = m + 1 ; } } return l - 1 ; } };","title":"69. Sqrt(x)"},{"location":"leetcode/binary-search/notes/#367-valid-perfect-square","text":"Solution 1 Binary search using ordering abstraction Notice you have to run tests for cases from 1 to 5. C++ class Solution { public : bool isPerfectSquare ( int num ) { if ( num == 1 ) return true ; int begin = 1 , end = num / 2 ; while ( begin < end ) { //long long mid = begin + (end - begin) / 2; // not working, deadloop for 5 long long mid = end - ( end - begin ) / 2 ; if ( mid * mid == num ) return true ; if ( mid * mid < num ) { begin = mid ; } else { end = mid - 1 ; } } return false ; } }; Python class Solution ( object ): def isPerfectSquare ( self , num ): \"\"\" :type num: int :rtype: bool \"\"\" if num == 1 : return True lower = 1 upper = num / 2 while lower < upper : mid = upper - ( upper - lower ) / 2 if mid * mid == num : return True if mid * mid < num : lower = mid else : upper = mid - 1 return False","title":"367. Valid Perfect Square"},{"location":"leetcode/binary-search/notes/#633-sum-of-square-numbers","text":"Solution 1 Binary search Once you have derived value b from a and c , you can binary search b . C++ class Solution { public : bool judgeSquareSum ( int c ) { if ( c == 0 ) return true ; for ( long long a = 0 ; a * a <= c ; ++ a ) { int b = c - ( int ) ( a * a ); int l = 0 , r = b / 2 + 1 ; while ( l < r ) { long long m = r - ( r - l ) / 2 ; if ( m * m == b ) return true ; if ( m * m < b ) { l = m ; } else { r = m - 1 ; } } } return false ; } }; Solution 2 Two pointers Notice this square sum can be found efficiently using two pointers. C++ class Solution { public : bool judgeSquareSum ( int c ) { int a = 0 , b = sqrt ( c ); while ( a <= b ){ int sum = a * a + b * b ; if ( sum < c ) a ++ ; else if ( sum > c ) b -- ; else return true ; } return false ; } }; Solution 3 Using set Keep inserting the value into a set, in the meantime also look up the other C++ class Solution { public : bool judgeSquareSum ( int c ) { set < int > s ; for ( int i = 0 ; i <= sqrt ( c ); ++ i ) { s . insert ( c - i * i ); if ( s . count ( i * i )) return true ; } return false ; } };","title":"633. Sum of Square Numbers"},{"location":"leetcode/binary-search/notes/#658-find-k-closest-elements","text":"Solution 1 Binary search Compare to problem 475. Heaters Our search target is to find the starting index of the subarray of length K. C++ class Solution { public : vector < int > findClosestElements ( vector < int >& arr , int k , int x ) { int start = 0 , end = arr . size () - k ; while ( start < end ) { int mid = start + ( end - start ) / 2 ; // looking for a \"mid\" that if ( x - arr [ mid ] > arr [ mid + k ] - x ) { start = mid + 1 ; } else { end = mid ; } } return vector < int > ( arr . begin () + start , arr . begin () + start + k ); } }; Solution 2 Binary search and Two pointers We first use binary search to locate the x value then expand to left and right looking for the k closest elements Notice the i < 0 in the if condition, it is very important to be there. otherwise the array index will be out of bound. C++ class Solution { public : vector < int > findClosestElements ( vector < int >& arr , int k , int x ) { int index = lower_bound ( arr . begin (), arr . end (), x ) - arr . begin (); int i = index - 1 , j = index ; while ( k -- ) { if ( i < 0 || j < arr . size () && abs ( arr [ j ] - x ) < abs ( arr [ i ] - x )) { j ++ ; } else { i -- ; } } return vector < int > ( arr . begin () + i + 1 , arr . begin () + j ); } };","title":"658. Find K Closest Elements"},{"location":"leetcode/binary-search/notes/#611-valid-triangle-number","text":"The main idea comes from the triangle lateral property, in which the triple should fullfil: a + b > c , a + c > b , and b + c > a . Once we sort it. We are able to gain some advantages that we don't have to check all these 3 relations. Instead, we should only take care of A[i] + A[j] > A[k] , in which i < j < k . Because we sorted the array, we can also fix the i and j , using binary search to find the k in the ragne of A[j + 1] ~ A[n - 1] . We can use our classic binary search template to achieve the goal. C++ class Solution { public : int triangleNumber ( vector < int >& nums ) { int n = nums . size (); int res = 0 ; sort ( nums . begin (), nums . end ()); for ( int i = 0 ; i < n - 2 ; ++ i ) { for ( int j = i + 1 ; j < n - 1 ; ++ j ) { int l = j + 1 , r = n ; // range of all possible k, notice l start with j + 1 int t = nums [ i ] + nums [ j ]; while ( l < r ) { int m = l + ( r - l ) / 2 ; if ( nums [ m ] < t ) { l = m + 1 ; } else { r = m ; } } res += l - j - 1 ; // notice the count start from j + 1 to l - 1. } } return res ; } };","title":"611. Valid Triangle Number"},{"location":"leetcode/binary-search/notes/#category-3-using-ordering-abstration-counter-as-a-gueesing-guage","text":"","title":"Category 3 Using ordering abstration (counter as a gueesing guage)"},{"location":"leetcode/binary-search/notes/#287-find-the-duplicate-number","text":"Solution 1 Binary search The problem asking for better than O(n^2) we could check to see whether binary search will work. If you count how many value <= the mid elements of [1, ..., n-1] , it will give you enough information to discard part of the array elements. Here you should distinguish what will be halfed and what will be searched. The answer to that is the [1, ..., n-1] sequence, not the given array. The simple proof of why it works can be put in this the following way. if the count of elements that <=mid in the array is less than mid , we can learn that the duplicate is in the higher end. If the count is greater, we can know that the duplicate element is in the lower end of the sequence [1, ..., n-1] . C++ class Solution { public : int findDuplicate ( vector < int >& nums ) { int begin = 1 , end = nums . size () - 1 ; while ( begin < end ) { int mid = begin + ( end - begin ) / 2 ; int count = 0 ; for ( int a : nums ) { if ( a <= mid ) ++ count ; } if ( count <= mid ) // \"=\" for [1,2,2] begin = mid + 1 ; else end = mid ; } return begin ; } }; Solution 2 tortoise and hare algorithm This problem is very similar to the the find circle in linked list. Generally, if you repeate A[A[i]] , the out put will show some periodic patterns. In fact you can imagine a rho shaped sequence. Image there is a function f(i) = A[i] , it mapping from 1, 2, 3, ... n to 1, 2, 3, ... n . Try to traverse A[i] , you will finally get circle through some same sequence of elements again and again, thus you obtain a rho shaped sequency like a circle in a linked list. The reason of it being a rho shape is becuase at least one element you will not come back to it if you leave it. http://www.keithschwarz.com/interesting/code/find-duplicate/FindDuplicate.python.html C++ class Solution { public : int findDuplicate ( vector < int >& nums ) { int n = nums . size (); if ( n == 0 ) return 0 ; int slow = 0 , fast = 0 , find = 0 ; while ( slow != fast || ( slow == 0 && fast == 0 )) { slow = nums [ slow ]; fast = nums [ nums [ fast ]]; } while ( slow != find ) { slow = nums [ slow ]; find = nums [ find ]; } return find ; } };","title":"287. Find the Duplicate Number"},{"location":"leetcode/binary-search/notes/#360-sort-transformed-array","text":"","title":"360. Sort Transformed Array"},{"location":"leetcode/binary-search/notes/#410-split-array-largest-sum","text":"","title":"410. Split Array Largest Sum"},{"location":"leetcode/binary-search/notes/#copy-books-lintcode","text":"","title":"Copy books (lintcode)"},{"location":"leetcode/binary-search/notes/#183-wood-cut-lintcode","text":"Solution 1 Binary search It requires to get equal or more than k pieces of wood with same length. So you have to cut the wood to fulfill the requirement. However, you need to promise that each of the k wood is the longest that is possible. Imagine that you are given bunch of wood to cut. How would you do it? You probably want to try to make one cut and see whether you can make it or not. If not, you may want to try make two cuts, and so on. But how could you program such a solution. It is very hard. Start thinking about the length seems a good option. Suppose you know your final maximum length. You would be able to make the cut accordingly. Now given a length out of guessing, can you verify whether it going to work or not? Yes, you can! That's the core idea of this solution. C++ class Solution { public : int woodCut ( vector < int > & L , int k ) { if ( L . empty ()) return 0 ; int maxlen = * max_element ( L . begin (), L . end ()); if ( k == 0 ) return maxlen ; int start = max ( 1 , maxlen / k ), end = maxlen ; while ( start < end ) { int mid = start + ( end - start ) / 2 ; int count = 0 ; for ( int len : L ) { count += len / ( mid + 1 ); } if ( count >= k ) start = mid + 1 ; else end = mid ; } int count = 0 ; for ( int len : L ) count += len / start ; return count >= k ? start : 0 ; } };","title":"183. Wood cut (lintcode)"},{"location":"leetcode/binary-search/notes/#774-minimize-max-distance-to-gas-station","text":"Solution 1 Binary search It is very similar to the problem Wood cut. You just need to take care of the accuracy of the results, namely also the int/double casts. It is also the hard part of the problem. Notice the count variable is int type, you should test your solution expecially for the line `count += dist[i] / mid; class Solution { public : double minmaxGasDist ( vector < int >& stations , int K ) { int n = stations . size (); vector < int > dist ( n , 0 ); // dist[0] = null; int d = 0 ; for ( int i = 1 ; i < n ; ++ i ) { dist [ i ] = stations [ i ] - stations [ i - 1 ]; d = max ( d , dist [ i ]); } double low = 0 , high = d ; while ( low + 0.000001 < high ) { double mid = low + ( high - low ) / 2 ; int count = 0 ; for ( int i = 1 ; i < n ; ++ i ) { count += dist [ i ] / mid ; } if ( count > K ) { // mid is too small low = mid ; } else { high = mid ; } } return low ; } };","title":"774. Minimize Max Distance to Gas Station"},{"location":"leetcode/binary-search/notes/#644-maximum-average-subarray-ii","text":"Notice the great trick you used to test whether there is a subarray of length greater than k whose average is larger than current mid . The trick is calculate the diff[i] = nums[i] - mid , and then calculate the prefix sum of the diff array, and compare to another prefix sum of the same array diff , the two prefix sum are calculated at two position at least k distant apart. We actually compare the prefix sum to the smallest prefix sum k distant apart. C++ class Solution { public : double findMaxAverage ( vector < int >& nums , int k ) { int n = nums . size (); double upper = INT_MIN , lower = INT_MAX ; for ( auto num : nums ) { upper = max ( upper , ( double ) num ); lower = min ( lower , ( double ) num ); } while ( lower + 0.00001 < upper ) { double mid = lower + ( upper - lower ) / 2 ; if ( isLarger ( nums , mid , k )) { // is average value >= mid? lower = mid ; } else { upper = mid ; } } return lower ; } /* return true if a greater average value is possible */ bool isLarger ( vector < int >& nums , double mid , int k ) { int n = nums . size (); double sums = 0 , prev = 0 , prev_min = 0 ; for ( int i = 0 ; i < k ; i ++ ) { sums += nums [ i ] - mid ; } if ( sums >= 0 ) { return true ; } /* we keep looking for whether a subarray sum of length >= k in array \"sums\" * is possible to be greater than zero. If such a subarray exist, it means * that the target average value is greater than the \"mid\" value. * we look at the front part of sums that at least k element apart from i. * If we can find the minimum of the sums[0, 1, ..., i - k] and check if * sums[i] - min(sum[0, 1, ..., i - k]) >= 0. If this is the case, it indicate * there exist a subarray of length >= k with sum greater than 0 in sums, * we can return ture, otherwise, it return false. */ for ( int i = k ; i < n ; i ++ ) { sums += nums [ i ] - mid ; prev += nums [ i - k ] - mid ; prev_min = min ( prev_min , prev ); if ( sums >= prev_min ) return true ; } return false ; } };","title":"644. Maximum Average Subarray II"},{"location":"leetcode/binary-search/notes/#778-swim-in-rising-water","text":"In This problem we are trying to find a path, in which the maximum element in the path among all paths is minimum. Meaning we look for a target value in the grid, such that there exist a path from grid[0][0] to grid[n-1][n-1] which includes this value and it is the maximum value in the path. C++ class Solution { int x [ 4 ] = { 0 , - 1 , 0 , 1 }; int y [ 4 ] = { - 1 , 0 , 1 , 0 }; public : int swimInWater ( vector < vector < int >>& grid ) { int n = grid . size (); int begin = grid [ 0 ][ 0 ], end = n * n - 1 ; // binary search find a path with mini elevation while ( begin < end ) { int mid = begin + ( end - begin ) / 2 ; if ( pathExist ( grid , mid )) { end = mid ; } else { begin = mid + 1 ; } } return begin ; } bool pathExist ( vector < vector < int >> & grid , int mid ) { int n = grid . size (); vector < vector < int >> visited ( n , vector < int > ( n , 0 )); return dfs_helper ( grid , visited , n , mid , 0 , 0 ); } bool dfs_helper ( vector < vector < int >> & grid , vector < vector < int >>& visited , int n , int mid , int i , int j ) { visited [ i ][ j ] = 1 ; for ( int k = 0 ; k < 4 ; ++ k ) { int a = i + x [ k ]; int b = j + y [ k ]; if ( a < 0 || a >= n || b < 0 || b >= n || visited [ a ][ b ] == 1 || grid [ a ][ b ] > mid ) continue ; if ( a == n - 1 && b == n - 1 ) return true ; if ( dfs_helper ( grid , visited , n , mid , a , b )) return true ; } return false ; } };","title":"778. Swim in Rising Water"},{"location":"leetcode/binary-search/notes/#483-smallest-good-base","text":"Solution 1 Binary search This problem requires a bit reasoning to achieve the solution. The starting point is realy mean what's asking by the problem. Here it is asking a minimum base that represent the given number n in a representation like binary representation. For example: 13 = 3^0 + 3^1 + 3^2 so 13 can be representd as 111 (base 3). First of all, there is a special case that such a base may not exist. (precisely, we should seperate the special case when n = (n-1)^0 + (n-1)^1 ; With this special case in mind, we can use binary search to iterate through each m from largest to smallest and check whether the corresponding k is a good base of the given value n . Because when m is the largest, k is the smallest, so if the bianry search find one it must be the smallest k we are looking for. If binary search found nothing, we simpley return the special case n-1 . C++ class Solution { public : string smallestGoodBase ( string n ) { long long num = stoll ( n ); /* for each lenght of the potentional representation, * n = 1 + k + ... + k^{i-1} = (k^i-1)/(k-1), lower bound k is 2, * we have 2^i-1 = n ==> upper bound i = log2(n+1). */ for ( int i = log2 ( num + 1 ); i >= 2 ; -- i ) { /* upper bound is obtained by n = 1 + k + k^2 ... + k^(i-1) > k^(i-1), * n > k^(i-1) ==> k < n^(1/(i-1)); */ long long left = 2 , right = pow ( num , 1.0 / ( i - 1 )) + 1 ; while ( left < right ) { long long mid = left + ( right - left ) / 2 ; long long sum = 0 ; /* calculate i digits value with base \"mid\" */ for ( int j = 0 ; j < i ; ++ j ) { sum = sum * mid + 1 ; } /* binary search for the mid (good base) */ if ( sum == num ) return to_string ( mid ); if ( sum < num ) left = mid + 1 ; else right = mid ; } } return to_string ( num - 1 ); } };","title":"483 Smallest Good Base"},{"location":"leetcode/binary-search/notes/#658-find-k-closest-elements_1","text":"Solution 1 Binary Search C++ class Solution { public : vector < int > findClosestElements ( vector < int >& arr , int k , int x ) { int start = 0 , end = arr . size () - k ; while ( start < end ) { int mid = start + ( end - start ) / 2 ; if ( x - arr [ mid ] > arr [ mid + k ] - x ) { start = mid + 1 ; } else { end = mid ; } } return vector < int > ( arr . begin () + start , arr . begin () + start + k ); } };","title":"658. Find K Closest Elements"},{"location":"leetcode/binary-search/notes/#378-kth-smallest-element-in-a-sorted-matrix","text":"Solution 1 Binary Search The idea of using binary search for this problem my not be straightforward. But the method is very important. The idea is very similar to the problem Search in a rotated sorted array. Because the matrix is sorted row wise and column wise, there are some ordering information we can make use of. Notice we are not try to search using the matrix index, we are searching the matrix element value. Compare to the problem 287. Find the Duplicate Number. The comparison if (count < k) isn't include mid explicitly. but the count is some function f(mid) , with the current mid , the count value is unique and can be use to test a condition that decide which side we can go to shrink the range the target value is possible in. C++ class Solution { public : int kthSmallest ( vector < vector < int >>& matrix , int k ) { int m = matrix . size (); int n = m ? matrix [ 0 ]. size () : 0 ; int start = matrix [ 0 ][ 0 ], end = matrix [ m - 1 ][ n - 1 ]; while ( start < end ) { int mid = start + ( end - start ) / 2 ; int count = 0 ; for ( int i = 0 ; i < m ; ++ i ) { count += upper_bound ( matrix [ i ]. begin (), matrix [ i ]. end (), mid ) - matrix [ i ]. begin (); } if ( count < k ) { // notice no mid here, but count is a function of mid. start = mid + 1 ; } else { end = mid ; } } return start ; } }; Solution 2 Priority Queue Notice when the k <= n^2 , index j < matrix.size() will also make it work. C++ class Solution { public : int kthSmallest ( vector < vector < int >>& matrix , int k ) { priority_queue < int > pq ; for ( int i = 0 ; i < matrix . size (); ++ i ) { for ( int j = 0 ; j < matrix [ 0 ]. size (); ++ j ) { pq . push ( matrix [ i ][ j ]); if ( pq . size () > k ) pq . pop (); } } return pq . top (); } };","title":"378. Kth Smallest Element in a Sorted Matrix"},{"location":"leetcode/binary-search/notes/#668-kth-smallest-number-in-multiplication-table","text":"Solution 1 Binary search While this problem looks simple. But it really isn't unless you observed the following. The condition used for binary search is \"whether there are k smaller elements in the range [start, mid]\". You are looking for the smallest number that has k elements less than or equal to it. Like in the problem Kth Smallest Element in a Sorted Matrix, we will move the number not the index. We move the start or end appropriately based on this condition, if there are more than k, we shrink the range by reduce end: end = mid . If there are less than k numbers, we increase begin hopefully to make mid larger so as to have close to k numbers in the range of [1, mid] . When being == end , we've located the kth number desired. In case k > m*n , we will got begin == end < k , which is not a solution. In counting how many element less than mid, you have to be clever a bit by using the feature that this matrix is multiplicative table. That is for row i , you can at most have x/i number smaller than x , why? Follow up: Does the kth element will be in the range of [1, m*n] ? C++ class Solution { public : int findKthNumber ( int m , int n , int k ) { int begin = 1 , end = m * n ; while ( begin < end ) { int mid = begin + ( end - begin ) / 2 ; int count = 0 ; for ( int i = 1 ; i <= m ; ++ i ) { count += min ( mid / i , n ); } if ( count < k ) begin = mid + 1 ; else end = mid ; } return begin ; } };","title":"668. Kth Smallest Number in Multiplication Table"},{"location":"leetcode/binary-search/notes/#719-find-k-th-smallest-pair-distance","text":"Solution 1 Priority Queue TLE C++ class Solution { public : int smallestDistancePair ( vector < int >& nums , int k ) { priority_queue < int > pq ; for ( int i = 0 ; i < nums . size (); ++ i ) { for ( int j = i + 1 ; j < nums . size (); ++ j ) { int dist = abs ( nums [ i ] - nums [ j ]); if ( pq . size () < k ) { pq . push ( dist ); } else if ( dist < pq . top ()) { pq . push ( dist ), pq . pop (); } } } return pq . top (); } }; Solution 2 Binary search Similar to Problem 668. Kth Smallest Number in Multiplication Table. The problem is complicated at firt glass. A brute force solutoin generates all the absolute distances and then sort to find the kth smallest one. We found it is potentially a searchable senario if we sort the elements. We have range [min_distance, max_distance] . We search a distance in this range such that there are exactly k pairs distance including itself. If the count of pair distance less than k, we try to increase it buy start = mid + 1 , vice versa. When the binary search loop stops, if the result exist, start point to the distance we are searching. Since this problem guarrantee solution exist, we return start . C++ class Solution { public : int smallestDistancePair ( vector < int >& nums , int k ) { sort ( nums . begin (), nums . end ()); int start = nums [ 1 ] - nums [ 0 ]; for ( int i = 2 ; i < nums . size (); ++ i ) { start = min ( start , nums [ i ] - nums [ i - 1 ]); } int end = nums . back () - nums [ 0 ]; while ( start < end ) { int mid = start + ( end - start ) / 2 ; // count how many absolute differences that <= mid; int count = 0 ; for ( int i = 0 ; i < nums . size (); ++ i ) { int j = i ; while ( j < nums . size () && nums [ j ] - nums [ i ] <= mid ) j ++ ; count += j - i - 1 ; } if ( count < k ) { start = mid + 1 ; } else { end = mid ; } } return start ; } }; Solution 3 Using binary search to optimize the counting You can also write your own binary search routine upper_bound . class Solution { public : int smallestDistancePair ( vector < int >& nums , int k ) { sort ( nums . begin (), nums . end ()); int start = nums [ 1 ] - nums [ 0 ]; for ( int i = 2 ; i < nums . size (); ++ i ) { start = min ( start , nums [ i ] - nums [ i - 1 ]); } int end = nums . back () - nums [ 0 ]; while ( start < end ) { int mid = start + ( end - start ) / 2 ; // count how many absolute differences that <= mid; int count = 0 ; /* for (int i = 0; i < nums.size(); ++i) { int j = i; while (j < nums.size() && nums[j] - nums[i] <= mid) j++; count += j - i - 1; } */ // optimize the counting use binary search (nested binary search) for ( int i = 0 ; i < nums . size (); ++ i ) { auto iter = upper_bound ( nums . begin () + i , nums . end (), nums [ i ] + mid ); count += iter - ( nums . begin () + i ) - 1 ; } if ( count < k ) { start = mid + 1 ; } else { end = mid ; } } return start ; } };","title":"719. Find K-th Smallest Pair Distance"},{"location":"leetcode/binary-search/notes/#category-4-binary-search-as-an-optimization-routine","text":"","title":"Category 4 Binary search as an optimization routine"},{"location":"leetcode/binary-search/notes/#300-longest-increasing-subsequence","text":"Solution 1 DP The base case is single char. f[i] is the length of LIS from the begining. C++ class Solution { public : int lengthOfLIS ( vector < int >& nums ) { if ( n == nums . size ()) return 0 ; int f [ n ] = { 0 }; int res = 0 ; for ( int j = 0 ; j < n ; j ++ ) { f [ j ] = 1 ; for ( int i = 0 ; i < j ; i ++ ) { if ( nums [ i ] < nums [ j ] && f [ i ] + 1 > f [ j ]) f [ j ] = f [ i ] + 1 ; } res = max ( res , f [ j ]); } return res ; } }; Solution 2 Using binary search The DP solution is O(n^2) . Using binary search could reduce to O(nlogn) . Binary search solution analysis. For each i , we are looking for the largest f value that has smallest A value. For example, A[0] = 5 could be ignored because of its f value is same as A[1] = 1 , which is smaller. In searching for the LIS, we prefer a small ending value when the length is the same. The following solution using a vector b to record the minimum A value for each length of LIS ( f value), we use binary search to find the last value in b that smaller that current value A[i] . If we found such a value in b , we use A[i] to replace the value next to the found value in b ). i 0 1 2 3 4 5 6 7 A 5 1 3 7 6 4 2 10 f 1 1 2 3 3 3 2 4 f[1] = 1, a[1] = 1 f[6] = 2, a[6] = 2 f[5] = 3, a[5] = 4 f[7] = 4, a[7] = 10 C++ class Solution { public : int lengthOfLIS ( vector < int >& nums ) { vector < int > b ; for ( int i = 0 ; i < nums . size (); ++ i ) { int l = 0 , r = b . size (); while ( l < r ) { int m = l + ( r - l ) / 2 ; if ( b [ m ] < nums [ i ]) { // nums[i] is the target l = m + 1 ; } else { r = m ; } } if ( l == b . size ()) // nums[i] greater than all element in b b . push_back ( nums [ i ]); else // begin point to next element no less than the target nums[i]. b [ l ] = nums [ i ]; } return b . size (); } }; Alternatively, we could use lower_bound . C++ class Solution { public : int lengthOfLIS ( vector < int >& nums ) { vector < int > b ; for ( int i = 0 ; i < nums . size (); ++ i ) { int l = lower_bound ( b . begin (), b . end (), nums [ i ]) - b . begin (); if ( l == b . size ()) // nums[i] greater than all element in b b . push_back ( nums [ i ]); else // begin point to next element no less than the target nums[i]. b [ l ] = nums [ i ]; } return b . size (); } };","title":"300 Longest Increasing Subsequence"},{"location":"leetcode/binary-search/notes/#354-russian-doll-envelopes","text":"","title":"354. Russian Doll Envelopes"},{"location":"leetcode/binary-search/notes/#174-dungeon-game","text":"","title":"174. Dungeon Game"},{"location":"leetcode/dynamic-programming/notes/","text":"Dynamic Programming \u00b6 \u96be\u70b9 \u00b6 \u5982\u4f55\u5728\u5206\u6790\u6700\u540e\u4e00\u6b65\u65f6\u9009\u62e9\u6b63\u786e\u89d2\u5ea6\u5e76\u8003\u8651\u6240\u6709\u53ef\u80fd\u60c5\u51b5\uff0c\u8fdb\u800c\u5199\u51fa\u72b6\u6001\u8f6c\u5316\u65b9\u7a0b. i.e. Buy and Shell stock, Edit distance, Longest Increasing Subsequence. Race Car \u00b6 waymo \u5750\u6807\u578b \u00b6 Triangle \u00b6 Unique Paths \u00b6 Unique Paths II \u00b6 Minimum Path Sum \u00b6 Bomb Enemy \u00b6 Dungeon Game \u00b6 \u5e8f\u5217\u578b \u00b6 Perfect Squares \u00b6 Longest Increasing Subsequence \u00b6 Number of Longest Increasing Subsequence \u00b6 Longest Increasing Continuous Subsequence \u00b6 \u5e8f\u5217\u578b + \u72b6\u6001 \u00b6 Paint House \u00b6 Paint House II \u00b6 House Robber \u00b6 House Robber II \u00b6 Best Time to Buy and Sell Stock \u00b6 Best Time to Buy and Sell Stock II \u00b6 Best Time to Buy and Sell Stock III \u00b6 Best Time to Buy and Sell Stock IV \u00b6 \u5212\u5206\u578b \u00b6 Word Break \u00b6 Solution 1 use set dictionary O(n^2), iterate forwards class Solution { public : bool wordBreak ( string s , vector < string >& wordDict ) { int m = s . length (); unordered_set < string > wordSet ( wordDict . begin (), wordDict . end ()); bool f [ m + 1 ] = { 0 }; f [ 0 ] = true ; for ( int i = 1 ; i <= m ; i ++ ) { for ( int j = 0 ; j <= i ; j ++ ) { if ( f [ j ] && wordSet . count ( s . substr ( j , i - j )) != 0 ) { f [ i ] = true ; break ; } } } return f [ m ]; } }; Solution 2 use set dictionary O(n^2), iterate backwards class Solution { public : bool wordBreak ( string s , vector < string >& wordDict ) { int n = s . length (); unordered_set < string > dict ( wordDict . begin (), wordDict . end ()); int f [ n + 1 ] = { 0 }; f [ 0 ] = 1 ; for ( int i = 0 ; i <= n ; ++ i ) { for ( int j = n - 1 ; j >= 0 ; -- j ) { if ( f [ j ] && dict . count ( s . substr ( j , i - j ))) { f [ i ] = 1 ; break ; } } } return f [ n ]; } }; Solution 3 (best solution) use vector, iterate through string Do not use word set to check exist or not, use each word as the last step. class Solution { public : bool wordBreak ( string s , vector < string >& wordDict ) { int n = s . length (); int f [ n + 1 ] = { 0 }; f [ 0 ] = 1 ; for ( int i = 0 ; i <= n ; ++ i ) { for ( string word : wordDict ) { if ( word . length () <= i && f [ i - word . length ()]) { if ( s . substr ( i - word . length (), word . length ()) == word ) { f [ i ] = 1 ; break ; } } } } return f [ n ]; } }; Maximum Vacation Days \u00b6 Solution 1 class Solution { public : int maxVacationDays ( vector < vector < int >>& flights , vector < vector < int >>& days ) { int n = flights . size (); int k = days [ 0 ]. size (); vector < vector < int >> f ( n , vector < int > ( k , - 1 )); f [ 0 ][ 0 ] = days [ 0 ][ 0 ]; for ( int i = 1 ; i < n ; ++ i ) { if ( flights [ 0 ][ i ]) { f [ i ][ 0 ] = days [ i ][ 0 ]; } } for ( int d = 1 ; d < k ; ++ d ) { for ( int i = 0 ; i < n ; ++ i ) { for ( int j = 0 ; j < n ; ++ j ) { if (( flights [ j ][ i ] || i == j ) && f [ j ][ d - 1 ] != 1 ) { f [ i ][ d ] = max ( f [ i ][ d ], f [ j ][ d - 1 ] + days [ i ][ d ]); } } } } int result = 0 ; for ( int i = 0 ; i < n ; ++ i ) { result = max ( result , f [ i ][ k - 1 ]); } return result ; } }; Solution 2 DFS with Cache class Solution { vector < vector < int >> memo ; public : int maxVacationDays ( vector < vector < int >>& flights , vector < vector < int >>& days ) { vector < vector < int >> memo ( flights . size (), vector < int > ( days [ 0 ]. size (), INT_MIN )); return dfs ( flights , days , 0 , 0 , memo ); } int dfs ( vector < vector < int >>& flights , vector < vector < int >>& days , int start , int day , vector < vector < int >>& memo ) { if ( day == days [ 0 ]. size ()) { return 0 ; } if ( memo [ start ][ day ] != INT_MIN ) { return memo [ start ][ day ]; } int maxVal = 0 ; for ( int i = 0 ; i < flights . size (); ++ i ) { if ( flights [ start ][ i ] || start == i ) { maxVal = max ( maxVal , days [ i ][ day ] + dfs ( flights , days , i , day + 1 , memo )); } } memo [ start ][ day ] = maxVal ; return maxVal ; } }; Solution 3 (greedy, wrong) class Solution { public : int maxVacationDays ( vector < vector < int >>& flights , vector < vector < int >>& days ) { int n = flights . size (); int k = days [ 0 ]. size (); int start = 0 ; int result = 0 ; set < pair < int , int > , std :: greater < pair < int , int >>> s ; for ( int col = 0 ; col < k ; ++ col ) { for ( int row = 0 ; row < n ; ++ row ) { s . insert ({ days [ row ][ col ], row }); } int stay = 1 ; for ( auto p : s ) { if ( flights [ start ][ p . second ]) { result += p . first ; cout << p . first << \" \" << p . second << endl ; start = p . second ; stay = 0 ; break ; } } if ( stay ) { result += days [ start ][ col ]; } s . clear (); } return result ; } }; Decode Ways \u00b6 Decode Ways II \u00b6 Solution 1 class Solution { public : int numDecodings ( string s ) { const int MOD = 1000000007 ; int n = s . length (); long long f [ n + 1 ] = { 0 }; if ( n == 0 ) return 0 ; f [ 0 ] = 1 ; for ( int i = 1 ; i <= n ; ++ i ) { if ( s [ i - 1 ] == '*' ) { // not include '0' according to the problem statement f [ i ] = ( f [ i ] + f [ i - 1 ] * 9 ) % MOD ; // number of ways only decode one digit if ( i > 1 ) { // number of ways decode two digits if ( s [ i - 2 ] == '*' ) { f [ i ] = ( f [ i ] + f [ i - 2 ] * 15 ) % MOD ; // 11 - 26 } else if ( s [ i - 2 ] == '1' ) { f [ i ] = ( f [ i ] + f [ i - 2 ] * 9 ) % MOD ; // 11 - 19 } else if ( s [ i - 2 ] == '2' ) { f [ i ] = ( f [ i ] + f [ i - 2 ] * 6 ) % MOD ; // 21 - 26 } } } else { // s[i - 1] != '*', have to consider the case '0' if ( s [ i - 1 ] != '0' ) { // number of ways only decode one digit f [ i ] = ( f [ i ] + f [ i - 1 ]) % MOD ; } if ( i > 1 ) { // number of ways decode two digits if ( s [ i - 2 ] == '*' ) { if ( s [ i - 1 ] <= '6' ) { // '*' can represent 1, 2 f [ i ] = ( f [ i ] + f [ i - 2 ] * 2 ) % MOD ; } else { // '*' can only represent 1 f [ i ] = ( f [ i ] + f [ i - 2 ]) % MOD ; } } else { // no '*' case int t = ( s [ i - 2 ] - '0' ) * 10 + s [ i - 1 ] - '0' ; if ( t >= 10 && t <= 26 ) { f [ i ] = ( f [ i ] + f [ i - 2 ]) % MOD ; } } } } } return f [ n ] % MOD ; } }; \u53cc\u5e8f\u5217\u578b \u00b6 Longest Common Subsequence \u00b6 Solution 1 consider the last step, the case is NOT A[n-1] == B[l-1] and A[n-1] != B[m-1] but, three cases: A[n-1] is included, B[m-1] is included, both A[n-1] and B[m-1] are included, (A[n-1] == B[m-1]) . Interleaving String \u00b6 Solution 1 class Solution { public : bool isInterleave ( string s1 , string s2 , string s3 ) { // f[i][j] = OR(f[i][j - 1]|B[j - 1] in C, f[i - 1][j]|A[i - 1] in C); int m = s1 . length (); int n = s2 . length (); int l = s3 . length (); if ( m + n != l ) return false ; int f [ m + 1 ][ n + 1 ] = { 0 }; f [ 0 ][ 0 ] = 1 ; for ( int i = 0 ; i <= m ; ++ i ) { for ( int j = 0 ; j <= n ; ++ j ) { if ( i > 0 && s1 [ i - 1 ] == s3 [ i + j - 1 ]) { f [ i ][ j ] |= f [ i - 1 ][ j ]; } if ( j > 0 && s2 [ j - 1 ] == s3 [ i + j - 1 ]) { f [ i ][ j ] |= f [ i ][ j - 1 ]; } } } return f [ m ][ n ]; } }; Edit Distance \u00b6 Solution 1 state: f[i][j] is min edit distance to make the first i chars of A the same as first j chars of B, as we consider the last operation (insert, delete, replace). One key idea of this problem is after the operation, the length of the A and B are not necessarily i and j . The focus here is the \"min edit distance\", not the length of A or B. For example, if the last step is insert, A's length will become i + 1 , B's length will maintain j . You should also note that the A, B concept is imaginary, B is actually changed from A by editing once. As a result, we know i + 1 == j after the insertion operation. Don't iterpret the state f[i][j] as min edit distance of first i chars in A and first j chars in B and after the editing, A's length is i and B's length is j . You should seperate the two intepretations. class Solution { public : int minDistance ( string word1 , string word2 ) { int m = word1 . length (); int n = word2 . length (); int f [ m + 1 ][ n + 1 ]; int i , j ; for ( i = 0 ; i <= m ; i ++ ) { for ( j = 0 ; j <= n ; j ++ ) { if ( i == 0 ) { f [ i ][ j ] = j ; continue ; } if ( j == 0 ) { f [ i ][ j ] = i ; continue ; } // delete insert f [ i ][ j ] = min ( f [ i - 1 ][ j ] + 1 , f [ i ][ j - 1 ] + 1 ); // replace f [ i ][ j ] = min ( f [ i ][ j ], f [ i - 1 ][ j - 1 ] + 1 ); if ( word1 [ i - 1 ] == word2 [ j - 1 ]) { f [ i ][ j ] = min ( f [ i ][ j ], f [ i - 1 ][ j - 1 ]); } } } return f [ m ][ n ]; } }; Solution 2 Space optimized class Solution { public : int minDistance ( string word1 , string word2 ) { int m = word1 . length (); int n = word2 . length (); int f [ 2 ][ n + 1 ]; int prev = 0 , curr = 0 ; for ( int i = 0 ; i <= m ; i ++ ) { prev = curr ; curr = 1 - curr ; for ( int j = 0 ; j <= n ; j ++ ) { if ( i == 0 ) { f [ curr ][ j ] = j ; continue ; } if ( j == 0 ) { f [ curr ][ j ] = i ; continue ; } // delete insert f [ curr ][ j ] = min ( f [ prev ][ j ] + 1 , f [ curr ][ j - 1 ] + 1 ); // replace f [ curr ][ j ] = min ( f [ curr ][ j ], f [ prev ][ j - 1 ] + 1 ); if ( word1 [ i - 1 ] == word2 [ j - 1 ]) { f [ curr ][ j ] = min ( f [ curr ][ j ], f [ prev ][ j - 1 ]); } } } return f [ curr ][ n ]; } }; Memoization \u00b6 Sentence Screen Fitting \u00b6 Solution 1 You can view the sentence as a space seperated English sentence and then use each row to \"frame\" the sentence. If the end of the frame overlap a space, we continue move to the next frame if available. If the end of the frame overlap a character, we should move the right the frame to the end of the prev word and move on to the next frame. class Solution { public : int wordsTyping ( vector < string >& sentence , int rows , int cols ) { int n = sentence . size (); string s ; for ( string w : sentence ) { s += string ( w + \" \" ); } int start = 0 , len = s . length (); //s with an extra trailing space. for ( int i = 0 ; i < rows ; i ++ ) { start += cols ; if ( s [ start % len ] == ' ' ) { ++ start ; } else { while ( start > 0 && s [( start - 1 ) % len ] != ' ' ) { -- start ; } } } return start / len ; } }; Solution 2 One insight of this solution is that every word in the sentence is possible to start a new row, but not necessary. For example, a long row can contain multiple of the setence, but the last word cannot fit in to this sigle row, it start a new row. In this case only the first word in the sentence and the last word can start a new row. We use this insight in the following way, for each word, we assume it could start a new row, then greedyly use the following words in the sentence to fix the row, we record the total number of words can fit this row in dp[i], i is the index of the starting word in the sentence. after the calculation, we could count sum(dp[i]), i is starting word's index class Solution { public : int wordsTyping ( vector < string >& sentence , int rows , int cols ) { int n = sentence . size (); int f [ n ] = { 0 }; for ( int i = 0 ; i < n ; ++ i ) { int len = 0 , count = 0 , idx = i ; while ( len + sentence [ idx % n ]. length () <= cols ) { len += ( sentence [ idx % n ]. length () + 1 ); idx ++ ; count ++ ; } f [ i ] = count ; } int total = 0 ; for ( int i = 0 , idx = 0 ; i < rows ; ++ i ) { total += f [ idx ]; idx = ( f [ idx ] + idx ) % n ; } return total / n ; } }; TODO: This solution may be further improved because for some of the count, we never used. Can we calculate the final result by only one loop, or in a nested loop.","title":"Dynamic Programming"},{"location":"leetcode/dynamic-programming/notes/#dynamic-programming","text":"","title":"Dynamic Programming"},{"location":"leetcode/dynamic-programming/notes/#_1","text":"\u5982\u4f55\u5728\u5206\u6790\u6700\u540e\u4e00\u6b65\u65f6\u9009\u62e9\u6b63\u786e\u89d2\u5ea6\u5e76\u8003\u8651\u6240\u6709\u53ef\u80fd\u60c5\u51b5\uff0c\u8fdb\u800c\u5199\u51fa\u72b6\u6001\u8f6c\u5316\u65b9\u7a0b. i.e. Buy and Shell stock, Edit distance, Longest Increasing Subsequence.","title":"\u96be\u70b9"},{"location":"leetcode/dynamic-programming/notes/#race-car","text":"waymo","title":"Race Car"},{"location":"leetcode/dynamic-programming/notes/#_2","text":"","title":"\u5750\u6807\u578b"},{"location":"leetcode/dynamic-programming/notes/#triangle","text":"","title":"Triangle"},{"location":"leetcode/dynamic-programming/notes/#unique-paths","text":"","title":"Unique Paths"},{"location":"leetcode/dynamic-programming/notes/#unique-paths-ii","text":"","title":"Unique Paths II"},{"location":"leetcode/dynamic-programming/notes/#minimum-path-sum","text":"","title":"Minimum Path Sum"},{"location":"leetcode/dynamic-programming/notes/#bomb-enemy","text":"","title":"Bomb Enemy"},{"location":"leetcode/dynamic-programming/notes/#dungeon-game","text":"","title":"Dungeon Game"},{"location":"leetcode/dynamic-programming/notes/#_3","text":"","title":"\u5e8f\u5217\u578b"},{"location":"leetcode/dynamic-programming/notes/#perfect-squares","text":"","title":"Perfect Squares"},{"location":"leetcode/dynamic-programming/notes/#longest-increasing-subsequence","text":"","title":"Longest Increasing Subsequence"},{"location":"leetcode/dynamic-programming/notes/#number-of-longest-increasing-subsequence","text":"","title":"Number of Longest Increasing Subsequence"},{"location":"leetcode/dynamic-programming/notes/#longest-increasing-continuous-subsequence","text":"","title":"Longest Increasing Continuous Subsequence"},{"location":"leetcode/dynamic-programming/notes/#_4","text":"","title":"\u5e8f\u5217\u578b + \u72b6\u6001"},{"location":"leetcode/dynamic-programming/notes/#paint-house","text":"","title":"Paint House"},{"location":"leetcode/dynamic-programming/notes/#paint-house-ii","text":"","title":"Paint House II"},{"location":"leetcode/dynamic-programming/notes/#house-robber","text":"","title":"House Robber"},{"location":"leetcode/dynamic-programming/notes/#house-robber-ii","text":"","title":"House Robber II"},{"location":"leetcode/dynamic-programming/notes/#best-time-to-buy-and-sell-stock","text":"","title":"Best Time to Buy and Sell Stock"},{"location":"leetcode/dynamic-programming/notes/#best-time-to-buy-and-sell-stock-ii","text":"","title":"Best Time to Buy and Sell Stock II"},{"location":"leetcode/dynamic-programming/notes/#best-time-to-buy-and-sell-stock-iii","text":"","title":"Best Time to Buy and Sell Stock III"},{"location":"leetcode/dynamic-programming/notes/#best-time-to-buy-and-sell-stock-iv","text":"","title":"Best Time to Buy and Sell Stock IV"},{"location":"leetcode/dynamic-programming/notes/#_5","text":"","title":"\u5212\u5206\u578b"},{"location":"leetcode/dynamic-programming/notes/#word-break","text":"Solution 1 use set dictionary O(n^2), iterate forwards class Solution { public : bool wordBreak ( string s , vector < string >& wordDict ) { int m = s . length (); unordered_set < string > wordSet ( wordDict . begin (), wordDict . end ()); bool f [ m + 1 ] = { 0 }; f [ 0 ] = true ; for ( int i = 1 ; i <= m ; i ++ ) { for ( int j = 0 ; j <= i ; j ++ ) { if ( f [ j ] && wordSet . count ( s . substr ( j , i - j )) != 0 ) { f [ i ] = true ; break ; } } } return f [ m ]; } }; Solution 2 use set dictionary O(n^2), iterate backwards class Solution { public : bool wordBreak ( string s , vector < string >& wordDict ) { int n = s . length (); unordered_set < string > dict ( wordDict . begin (), wordDict . end ()); int f [ n + 1 ] = { 0 }; f [ 0 ] = 1 ; for ( int i = 0 ; i <= n ; ++ i ) { for ( int j = n - 1 ; j >= 0 ; -- j ) { if ( f [ j ] && dict . count ( s . substr ( j , i - j ))) { f [ i ] = 1 ; break ; } } } return f [ n ]; } }; Solution 3 (best solution) use vector, iterate through string Do not use word set to check exist or not, use each word as the last step. class Solution { public : bool wordBreak ( string s , vector < string >& wordDict ) { int n = s . length (); int f [ n + 1 ] = { 0 }; f [ 0 ] = 1 ; for ( int i = 0 ; i <= n ; ++ i ) { for ( string word : wordDict ) { if ( word . length () <= i && f [ i - word . length ()]) { if ( s . substr ( i - word . length (), word . length ()) == word ) { f [ i ] = 1 ; break ; } } } } return f [ n ]; } };","title":"Word Break"},{"location":"leetcode/dynamic-programming/notes/#maximum-vacation-days","text":"Solution 1 class Solution { public : int maxVacationDays ( vector < vector < int >>& flights , vector < vector < int >>& days ) { int n = flights . size (); int k = days [ 0 ]. size (); vector < vector < int >> f ( n , vector < int > ( k , - 1 )); f [ 0 ][ 0 ] = days [ 0 ][ 0 ]; for ( int i = 1 ; i < n ; ++ i ) { if ( flights [ 0 ][ i ]) { f [ i ][ 0 ] = days [ i ][ 0 ]; } } for ( int d = 1 ; d < k ; ++ d ) { for ( int i = 0 ; i < n ; ++ i ) { for ( int j = 0 ; j < n ; ++ j ) { if (( flights [ j ][ i ] || i == j ) && f [ j ][ d - 1 ] != 1 ) { f [ i ][ d ] = max ( f [ i ][ d ], f [ j ][ d - 1 ] + days [ i ][ d ]); } } } } int result = 0 ; for ( int i = 0 ; i < n ; ++ i ) { result = max ( result , f [ i ][ k - 1 ]); } return result ; } }; Solution 2 DFS with Cache class Solution { vector < vector < int >> memo ; public : int maxVacationDays ( vector < vector < int >>& flights , vector < vector < int >>& days ) { vector < vector < int >> memo ( flights . size (), vector < int > ( days [ 0 ]. size (), INT_MIN )); return dfs ( flights , days , 0 , 0 , memo ); } int dfs ( vector < vector < int >>& flights , vector < vector < int >>& days , int start , int day , vector < vector < int >>& memo ) { if ( day == days [ 0 ]. size ()) { return 0 ; } if ( memo [ start ][ day ] != INT_MIN ) { return memo [ start ][ day ]; } int maxVal = 0 ; for ( int i = 0 ; i < flights . size (); ++ i ) { if ( flights [ start ][ i ] || start == i ) { maxVal = max ( maxVal , days [ i ][ day ] + dfs ( flights , days , i , day + 1 , memo )); } } memo [ start ][ day ] = maxVal ; return maxVal ; } }; Solution 3 (greedy, wrong) class Solution { public : int maxVacationDays ( vector < vector < int >>& flights , vector < vector < int >>& days ) { int n = flights . size (); int k = days [ 0 ]. size (); int start = 0 ; int result = 0 ; set < pair < int , int > , std :: greater < pair < int , int >>> s ; for ( int col = 0 ; col < k ; ++ col ) { for ( int row = 0 ; row < n ; ++ row ) { s . insert ({ days [ row ][ col ], row }); } int stay = 1 ; for ( auto p : s ) { if ( flights [ start ][ p . second ]) { result += p . first ; cout << p . first << \" \" << p . second << endl ; start = p . second ; stay = 0 ; break ; } } if ( stay ) { result += days [ start ][ col ]; } s . clear (); } return result ; } };","title":"Maximum Vacation Days"},{"location":"leetcode/dynamic-programming/notes/#decode-ways","text":"","title":"Decode Ways"},{"location":"leetcode/dynamic-programming/notes/#decode-ways-ii","text":"Solution 1 class Solution { public : int numDecodings ( string s ) { const int MOD = 1000000007 ; int n = s . length (); long long f [ n + 1 ] = { 0 }; if ( n == 0 ) return 0 ; f [ 0 ] = 1 ; for ( int i = 1 ; i <= n ; ++ i ) { if ( s [ i - 1 ] == '*' ) { // not include '0' according to the problem statement f [ i ] = ( f [ i ] + f [ i - 1 ] * 9 ) % MOD ; // number of ways only decode one digit if ( i > 1 ) { // number of ways decode two digits if ( s [ i - 2 ] == '*' ) { f [ i ] = ( f [ i ] + f [ i - 2 ] * 15 ) % MOD ; // 11 - 26 } else if ( s [ i - 2 ] == '1' ) { f [ i ] = ( f [ i ] + f [ i - 2 ] * 9 ) % MOD ; // 11 - 19 } else if ( s [ i - 2 ] == '2' ) { f [ i ] = ( f [ i ] + f [ i - 2 ] * 6 ) % MOD ; // 21 - 26 } } } else { // s[i - 1] != '*', have to consider the case '0' if ( s [ i - 1 ] != '0' ) { // number of ways only decode one digit f [ i ] = ( f [ i ] + f [ i - 1 ]) % MOD ; } if ( i > 1 ) { // number of ways decode two digits if ( s [ i - 2 ] == '*' ) { if ( s [ i - 1 ] <= '6' ) { // '*' can represent 1, 2 f [ i ] = ( f [ i ] + f [ i - 2 ] * 2 ) % MOD ; } else { // '*' can only represent 1 f [ i ] = ( f [ i ] + f [ i - 2 ]) % MOD ; } } else { // no '*' case int t = ( s [ i - 2 ] - '0' ) * 10 + s [ i - 1 ] - '0' ; if ( t >= 10 && t <= 26 ) { f [ i ] = ( f [ i ] + f [ i - 2 ]) % MOD ; } } } } } return f [ n ] % MOD ; } };","title":"Decode Ways II"},{"location":"leetcode/dynamic-programming/notes/#_6","text":"","title":"\u53cc\u5e8f\u5217\u578b"},{"location":"leetcode/dynamic-programming/notes/#longest-common-subsequence","text":"Solution 1 consider the last step, the case is NOT A[n-1] == B[l-1] and A[n-1] != B[m-1] but, three cases: A[n-1] is included, B[m-1] is included, both A[n-1] and B[m-1] are included, (A[n-1] == B[m-1]) .","title":"Longest Common Subsequence"},{"location":"leetcode/dynamic-programming/notes/#interleaving-string","text":"Solution 1 class Solution { public : bool isInterleave ( string s1 , string s2 , string s3 ) { // f[i][j] = OR(f[i][j - 1]|B[j - 1] in C, f[i - 1][j]|A[i - 1] in C); int m = s1 . length (); int n = s2 . length (); int l = s3 . length (); if ( m + n != l ) return false ; int f [ m + 1 ][ n + 1 ] = { 0 }; f [ 0 ][ 0 ] = 1 ; for ( int i = 0 ; i <= m ; ++ i ) { for ( int j = 0 ; j <= n ; ++ j ) { if ( i > 0 && s1 [ i - 1 ] == s3 [ i + j - 1 ]) { f [ i ][ j ] |= f [ i - 1 ][ j ]; } if ( j > 0 && s2 [ j - 1 ] == s3 [ i + j - 1 ]) { f [ i ][ j ] |= f [ i ][ j - 1 ]; } } } return f [ m ][ n ]; } };","title":"Interleaving String"},{"location":"leetcode/dynamic-programming/notes/#edit-distance","text":"Solution 1 state: f[i][j] is min edit distance to make the first i chars of A the same as first j chars of B, as we consider the last operation (insert, delete, replace). One key idea of this problem is after the operation, the length of the A and B are not necessarily i and j . The focus here is the \"min edit distance\", not the length of A or B. For example, if the last step is insert, A's length will become i + 1 , B's length will maintain j . You should also note that the A, B concept is imaginary, B is actually changed from A by editing once. As a result, we know i + 1 == j after the insertion operation. Don't iterpret the state f[i][j] as min edit distance of first i chars in A and first j chars in B and after the editing, A's length is i and B's length is j . You should seperate the two intepretations. class Solution { public : int minDistance ( string word1 , string word2 ) { int m = word1 . length (); int n = word2 . length (); int f [ m + 1 ][ n + 1 ]; int i , j ; for ( i = 0 ; i <= m ; i ++ ) { for ( j = 0 ; j <= n ; j ++ ) { if ( i == 0 ) { f [ i ][ j ] = j ; continue ; } if ( j == 0 ) { f [ i ][ j ] = i ; continue ; } // delete insert f [ i ][ j ] = min ( f [ i - 1 ][ j ] + 1 , f [ i ][ j - 1 ] + 1 ); // replace f [ i ][ j ] = min ( f [ i ][ j ], f [ i - 1 ][ j - 1 ] + 1 ); if ( word1 [ i - 1 ] == word2 [ j - 1 ]) { f [ i ][ j ] = min ( f [ i ][ j ], f [ i - 1 ][ j - 1 ]); } } } return f [ m ][ n ]; } }; Solution 2 Space optimized class Solution { public : int minDistance ( string word1 , string word2 ) { int m = word1 . length (); int n = word2 . length (); int f [ 2 ][ n + 1 ]; int prev = 0 , curr = 0 ; for ( int i = 0 ; i <= m ; i ++ ) { prev = curr ; curr = 1 - curr ; for ( int j = 0 ; j <= n ; j ++ ) { if ( i == 0 ) { f [ curr ][ j ] = j ; continue ; } if ( j == 0 ) { f [ curr ][ j ] = i ; continue ; } // delete insert f [ curr ][ j ] = min ( f [ prev ][ j ] + 1 , f [ curr ][ j - 1 ] + 1 ); // replace f [ curr ][ j ] = min ( f [ curr ][ j ], f [ prev ][ j - 1 ] + 1 ); if ( word1 [ i - 1 ] == word2 [ j - 1 ]) { f [ curr ][ j ] = min ( f [ curr ][ j ], f [ prev ][ j - 1 ]); } } } return f [ curr ][ n ]; } };","title":"Edit Distance"},{"location":"leetcode/dynamic-programming/notes/#memoization","text":"","title":"Memoization"},{"location":"leetcode/dynamic-programming/notes/#sentence-screen-fitting","text":"Solution 1 You can view the sentence as a space seperated English sentence and then use each row to \"frame\" the sentence. If the end of the frame overlap a space, we continue move to the next frame if available. If the end of the frame overlap a character, we should move the right the frame to the end of the prev word and move on to the next frame. class Solution { public : int wordsTyping ( vector < string >& sentence , int rows , int cols ) { int n = sentence . size (); string s ; for ( string w : sentence ) { s += string ( w + \" \" ); } int start = 0 , len = s . length (); //s with an extra trailing space. for ( int i = 0 ; i < rows ; i ++ ) { start += cols ; if ( s [ start % len ] == ' ' ) { ++ start ; } else { while ( start > 0 && s [( start - 1 ) % len ] != ' ' ) { -- start ; } } } return start / len ; } }; Solution 2 One insight of this solution is that every word in the sentence is possible to start a new row, but not necessary. For example, a long row can contain multiple of the setence, but the last word cannot fit in to this sigle row, it start a new row. In this case only the first word in the sentence and the last word can start a new row. We use this insight in the following way, for each word, we assume it could start a new row, then greedyly use the following words in the sentence to fix the row, we record the total number of words can fit this row in dp[i], i is the index of the starting word in the sentence. after the calculation, we could count sum(dp[i]), i is starting word's index class Solution { public : int wordsTyping ( vector < string >& sentence , int rows , int cols ) { int n = sentence . size (); int f [ n ] = { 0 }; for ( int i = 0 ; i < n ; ++ i ) { int len = 0 , count = 0 , idx = i ; while ( len + sentence [ idx % n ]. length () <= cols ) { len += ( sentence [ idx % n ]. length () + 1 ); idx ++ ; count ++ ; } f [ i ] = count ; } int total = 0 ; for ( int i = 0 , idx = 0 ; i < rows ; ++ i ) { total += f [ idx ]; idx = ( f [ idx ] + idx ) % n ; } return total / n ; } }; TODO: This solution may be further improved because for some of the count, we never used. Can we calculate the final result by only one loop, or in a nested loop.","title":"Sentence Screen Fitting"},{"location":"leetcode/heap/notes/","text":"Heap \u00b6 Binary heap properties \u00b6 Usually implemented using an array of elements but visualized as a complete binary tree (not a full binary tree). From the array index, we can compute the parent index from child's index and vice versa. The element values are partially ordered, briefly, they are ordered level by level. There is no ordering information of element values in the same level. The minimum number of elements in a binary heap with height h h is 2^h 2^h , the maximum elements in it is 2^{h + 1} - 1 2^{h + 1} - 1 The height of a binary heap is the floor of \\log n \\log n . n n is the total number of elements. The minimum element of a maximum heap is in the leaf nodes. With the array representation for storing an n-element heap, the leaves are the nodes indexed by \\lfloor n/2 \\rfloor + 1, \\lfloor n/2 \\rfloor + 2, \\dots, n \\lfloor n/2 \\rfloor + 1, \\lfloor n/2 \\rfloor + 2, \\dots, n . Heap sort \u00b6 Heap implementation using array \u00b6 Heap implementation using HeapNode \u00b6 Priority queue operations \u00b6 Supported operations \u00b6 # Operation complexity comment 1 buildMaxHeap(A) O(n) O(n) Build a priority queue routine iteratively call maxHeapify() n n times. The complexity is O(n) O(n) instead of O(n \\log n) O(n \\log n) 2 maxHeapify(A, i) O(\\log n) O(\\log n) Maintains the heap properties for subtree rooted at i . Assumes the subtrees of i are max heap before calling this routine. 3 insert() O(\\log n) O(\\log n) Insert a element to the heap. 4 peekMaximum() O(1) O(1) Return the max element without remove it from the max-heap. 5 popMaximum() O(\\log n) O(\\log n) Remove the max element from the max-heap. 6 increaseKey() O(\\log n) O(\\log n) Update the element's key. This essentially changed the priority of an element. 7 delete() Not supported by the binary heap implementation, but can be achieved by introduce more complex data structures. C++ STL priority queue \u00b6 Notice if the elements in the queue is some complex container or object, you have to declare priority_queue with its underline container type, and the binary predicate std::greater if necessary. Here is example in the solution of the problem Minimum Unique Word Abbreviation priority_queue < pair < int , string > , vector < pair < int , string >> , greater < pair < int , string >>> pq ; This line of code declared a priority queue pq , whose element is piar<int, string> , with the minimum element on the top. But minimum what? How the priority queue sort a pair<int, string> element? In this case it implicitly sort the pair<int, string> element base on the first int value. How can we sort the priority queue element based on the second value of the pair<int, string> . To achieve that, we cannot use the building function object greater we have write a customized function object and pass to the declaration of the pq like this, class mycomparison { public : mycomparison () {} bool operator () ( const pair < int , string > & a , const pair < int , string > & b ) const { return a . second . compare ( b . second ); } }; ... priority_queue < pair < int , string > , vector < pair < int , string >> , mycomparison > pq ; ... We may need to write customized binary predicate function in order to implement the min-heap or max-heap . The binary predication function is depends on the underline container and also the type of the container element. Priority Queue \u00b6 Merge k Sorted Lists \u00b6 Solution 1 Priority queue Use a min priority queue to store all the list, iteratively pop the \"min list\", take the head node to the final list and push back to the priority queue if the \"min list\" still have node. Priority queue solution is O(N \\log k) O(N \\log k) /** * Definition for singly-linked list. * struct ListNode { * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) {} * }; */ class listComparison { public : listComparison () {} bool operator ( const ListNode * a , const ListNode * b ) const { return a -> val > b -> val ; } } class Solution { public : ListNode * mergeKLists ( vector < ListNode *>& lists ) { priority_queue < ListNode * , vector < ListNode *> , listComparison > q ; for ( auto list : lists ) { if ( list ) { q . push ( list ); } } ListNode dummy ( INT_MIN ); ListNode * curr = & dummy ; while ( ! q . empty ()) { curr -> next = q . top (); q . pop (); curr = curr -> next ; if ( curr -> next ) { q . push ( curr -> next ); } } return dummy . next ; } } Solution 2 Divide and conquer merge Use the divide and conquer idea to reduce the problem to a smaller problem and then solve the smallest problem, combined together with the solution of small problems, we will arrived at the final solution. Compare the following solution to merge sort algorithm. The complexity is O(N \\log k) O(N \\log k) . Because merge two list will take O(N) O(N) , and we will do O(\\log k) O(\\log k) time of merging. class Solution { public : ListNode * mergeKLists ( vector < ListNode *>& lists ) { return mergeKListsHelper ( lists , 0 , lists . size () - 1 ); } ListNode * mergeKListsHelper ( vector < ListNode *>& lists , int s , int e ) { if ( s > e ) return nullptr ; if ( s == e ) return lists [ s ]; if ( s < e ) { int m = s + ( e - s ) / 2 ; ListNode * l1 = mergeKListsHelper ( lists , s , m ); ListNode * l2 = mergeKListsHelper ( lists , m + 1 , e ); return merge ( l1 , l2 ); } } ListNode * merge ( ListNode * l1 , ListNode * l2 ) { ListNode dummy ( INT_MIN ); ListNode * curr = & dummy ; while ( l1 && l2 ) { if ( l1 -> val < l2 -> val ) { curr -> next = l1 ; l1 = l1 -> next ; } else { curr -> next = l2 ; l2 = l2 -> next ; } curr = curr -> next ; } curr -> next = l1 ? l1 : l2 ; return dummy . next ; } }; Minimum Unique Word Abbreviation \u00b6 Smallest Range Covering Elements from K Lists \u00b6 Solution 1 Priority queue Notice this solution pushed some meta data to the priority queue. class Solution { public : struct mycompare { bool operator () ( pair < int , int >& a , pair < int , int >& b ) { return a . first > b . first ; } }; vector < int > smallestRange ( vector < vector < int >>& nums ) { int n = nums . size (); priority_queue < pair < int , int > , vector < pair < int , int >> , mycompare > pq ; int end = INT_MIN ; for ( int i = 0 ; i < n ; i ++ ) { end = max ( end , nums [ i ][ 0 ]); // push the first element of kth vector and k to the pq pq . push ({ nums [ i ][ 0 ], i }); } vector < int > idx ( n , 0 ); // keep tracking the index to the k arrays. vector < int > res = { - 100000 , 100000 }; // the range is given. while ( pq . size () == n ) { int start = pq . top (). first ; int k = pq . top (). second ; pq . pop (); if ( end - start < res [ 1 ] - res [ 0 ]) { res [ 0 ] = start ; res [ 1 ] = end ; } // if nums[k] have more element, we push next to the queue. if ( ++ idx [ k ] < nums [ k ]. size ()) { pq . push ({ nums [ k ][ idx [ k ]], k }); end = max ( end , nums [ k ][ idx [ k ]]); } } return res ; } }; Kth Largest Element in an Array \u00b6 Solution 1 priority queue Build a heap takes O(n) . max-heap O(n) + O(klogk) . class Solution { public : int findKthLargest ( vector < int >& nums , int k ) { priority_queue < int > pq ( nums . begin (), nums . end ()); k -- ; while ( k -- > 0 ) { pq . pop (); } return pq . top (); } }; Solution 2 priority queue min-heap O(n*logk) . class Solution { public : int findKthLargest ( vector < int >& nums , int k ) { priority_queue < int , vector < int > , greater < int >> pq ; for ( int n : nums ) { pq . push ( n ); if ( pq . size () > k ) { pq . pop (); } } return pq . top (); } }; Solution 3 quickselect algorithm class Solution { public : int findKthLargest ( vector < int >& nums , int k ) { int n = nums . size (); if ( n < k ) return 0 ; return orderStats ( nums , 0 , n - 1 , k ); } int orderStats ( vector < int >& nums , int start , int end , int k ) { if ( start == end ) return nums [ start ]; int p = partition ( nums , start , end ); int order = p - start + 1 ; if ( order == k ) { return nums [ p ]; } else if ( order > k ) { return orderStats ( nums , start , p - 1 , k ); } else { return orderStats ( nums , p + 1 , end , k - order ); } } int partition ( vector < int >& nums , int start , int end ) { if ( start == end ) return 0 ; int p = start + floor ( rand () % ( end - start + 1 )); swap ( nums [ p ], nums [ end ]); int pivot = nums [ end ]; int i = start - 1 , j ; for ( j = start ; j < end ; ++ j ) { if ( nums [ j ] >= pivot ) { swap ( nums [ ++ i ], nums [ j ]); } } swap ( nums [ end ], nums [ i + 1 ]); return i + 1 ; } }; The Skyline Problem \u00b6 Solution 1 priority queue maximum heap class Solution { public : vector < pair < int , int >> getSkyline ( vector < vector < int >>& buildings ) { vector < pair < int , int >> res ; int i = 0 , x = 0 , h = 0 , len = buildings . size (); priority_queue < pair < int , int >> q ; // max heap store <height, end> while ( i < len || ! q . empty ()) { // building i start before the end of current tallest building if ( q . empty () || i < len && buildings [ i ][ 0 ] <= q . top (). second ) { x = buildings [ i ][ 0 ]; // current start // overlapped start of multiple buildings (compare to if) while ( i < len && buildings [ i ][ 0 ] == x ) { q . push ({ buildings [ i ][ 2 ], buildings [ i ][ 1 ]}); i ++ ; } } else { // building i start after the end of current tallest building x = q . top (). second ; // current end // pop all buildings that end <= currently tallest buildings while ( ! q . empty () && q . top (). second <= x ) q . pop (); } h = q . empty () ? 0 : q . top (). first ; if ( res . empty () || res . back (). second != h ) { res . push_back ({ x , h }); } } return res ; } }; Solution 2 multiset + scanline solution class Solution { private : static bool cmp ( pair < int , int > p1 , pair < int , int > p2 ){ if ( p1 . first != p2 . first ) return p1 . first < p2 . first ; return p1 . second > p2 . second ; // if x duplicate, tallest building first } public : vector < pair < int , int >> getSkyline ( vector < vector < int >>& buildings ) { vector < pair < int , int >> h ; // store the start and end. vector < pair < int , int >> res ; multiset < int , greater < int >> s ; int prev = 0 , curr = 0 ; // scanline technique to obtain the starting point and end point for ( auto & a : buildings ) { h . push_back ({ a [ 0 ], a [ 2 ]}); h . push_back ({ a [ 1 ], - a [ 2 ]}); // use \"-\" mark the end point } sort ( h . begin (), h . end (), cmp ); s . insert ( 0 ); // edge case, consider a.second == 0 for ( auto & a : h ) { if ( a . second > 0 ) s . insert ( a . second ); // if it is a start, insert to the set else s . erase ( s . find ( - a . second )); // if it is a end, erase the inserted start curr = * s . begin (); // take the maximum from the set if ( curr != prev ) { // remove duplicate res . push_back ({ a . first , curr }); prev = curr ; } } return res ; } }; Sliding Window Median \u00b6 Solution 1 heap + hash (simulated) class Solution { public : vector < double > medianSlidingWindow ( vector < int >& nums , int k ) { int n = nums . size (); vector < double > res ; priority_queue < int > lo ; priority_queue < int , vector < int > , greater < int >> hi ; unordered_map < int , int > hash_heap ; int i = 0 ; while ( i < k ) lo . push ( nums [ i ++ ]); for ( int j = 0 ; j < k / 2 ; j ++ ) { hi . push ( lo . top ()); lo . pop (); } while ( true ) { res . push_back ( k & 1 ? lo . top () : (( double ) lo . top () + ( double ) hi . top ()) * 0.5 ); if ( i >= nums . size ()) break ; int out_num = nums [ i - k ]; // first element in the window int in_num = nums [ i ++ ]; // new element entering window int balance = 0 ; // -1, when out_num in lo, +1 when out_num in hi. // balance is the count of element diff in lo and hi. // lo.size() == hi.size() -> balance = 0 // lo.size() > hi.size() --> balance > 0 // lo.size() < hi.size() --> balance < 0 balance += ( out_num <= lo . top () ? - 1 : 1 ); hash_heap [ out_num ] ++ ; // count of the invalid element, will remove when it on the top. if ( ! lo . empty () && in_num <= lo . top ()) { balance ++ ; lo . push ( in_num ); } else { balance -- ; hi . push ( in_num ); } if ( balance < 0 ) { // lo lack of element. lo . push ( hi . top ()); hi . pop (); balance ++ ; } else if ( balance > 0 ) { // hi lack of element hi . push ( lo . top ()); lo . pop (); balance -- ; } // lazy remove of elements previous removed, mimic hashheap of java. while ( ! lo . empty () && hash_heap [ lo . top ()]) { hash_heap [ lo . top ()] -- ; lo . pop (); } // lazy remove of elements previous removed, mimic hashheap of java. while ( ! hi . empty () && hash_heap [ hi . top ()]) { hash_heap [ hi . top ()] -- ; hi . pop (); } } return res ; } }; Use priority queue to rearange tasks (characters, string, etc.) \u00b6 Rearrange String k Distance Apart \u00b6 Solution 1 Greedy + Priority Queue class Solution { public : string rearrangeString ( string s , int k ) { if ( k <= 0 ) return s ; unordered_map < char , int > mp ; for ( auto & c : s ) mp [ c ] ++ ; priority_queue < pair < int , char > , vector < pair < int , char >> , std :: less < pair < int , char >>> pq ; // max heap; for ( auto & p : mp ) { pq . push ({ p . second , p . first }); } string res ; while ( ! pq . empty ()) { vector < pair < int , char >> used ; int i = 0 ; while ( i < k && ! pq . empty ()) { auto t = pq . top (); pq . pop (); res . push_back ( t . second ); i ++ ; if ( -- t . first > 0 ) { used . push_back ( t ); } } if ( i != k && used . size ()) return \"\" ; for ( auto & p : used ) { pq . push ( p ); } } return res ; } }; Task Scheduler \u00b6 Solution 1 Greedy + Priority Queue class Solution { public : class cmp { public : cmp () {} bool operator ()( const pair < int , char >& a , const pair < int , char >& b ){ return a . first < b . first ; } }; int leastInterval ( vector < char >& tasks , int n ) { int chmap [ 26 ] = { 0 }; priority_queue < pair < int , char > , vector < pair < int , char >> , cmp > pq ; for ( char c : tasks ) { chmap [ c - 'A' ] ++ ; } for ( int i = 0 ; i < 26 ; ++ i ) { if ( chmap [ i ]) { pq . push ({ chmap [ i ], i + 'A' }); } } int cycle = n + 1 ; int res = 0 ; while ( ! pq . empty ()) { vector < pair < int , char >> used ; for ( int i = 0 ; i < cycle ; ++ i ) { // pick total of n + 1 in freq order if ( ! pq . empty ()) { used . push_back ( pq . top ()); pq . pop (); } } for ( auto p : used ) { if ( -- p . first ) { pq . push ( p ); // put back used chars if extra char exist after the useage. } } // handled partial cycle and full cycle res += ! pq . empty () ? cycle : used . size (); } return res ; } }; Reorganize String \u00b6 Solution 1 Greedy + Priority Queue class Solution { public : class cmp { public : cmp () {} bool operator ()( const pair < int , char >& a , const pair < int , char >& b ){ return a . first < b . first ; } }; string reorganizeString ( string S ) { int n = S . length (); int chmap [ 26 ] = { 0 }; // max heap priority_queue < pair < int , char > , vector < pair < int , char >> , cmp > pq ; for ( auto & c : S ) { //store to map chmap [ c - 'a' ] ++ ; } // push to priority queue for ( int i = 0 ; i < 26 ; ++ i ) { if ( chmap [ i ]) { pq . push ({ chmap [ i ], i + 'a' }); } } if ( pq . top (). first > ( n + 1 ) / 2 ) return \"\" ; string res = \"\" ; while ( ! pq . empty ()) { pair < int , char > p = pq . top (); pq . pop (); if ( res . empty () || res . back () != p . second ) { res . push_back ( p . second ); if ( -- p . first > 0 ) { // use one char pq . push ( p ); } } else { // cannot use duplicate char, take another pair < int , char > q = pq . top (); pq . pop (); res . push_back ( q . second ); if ( -- q . first > 0 ) { // use another char pq . push ( q ); } // remember put p back to the pq pq . push ( p ); } } return res ; } }; Topological sorting \u00b6 Course Schedule \u00b6 Solution 1 DFS class Solution { public : bool canFinish ( int numCourses , vector < pair < int , int >>& prerequisites ) { vector < vector < int >> graph ( numCourses , vector < int > ( 0 )); vector < int > visit ( numCourses , 0 ); /* construct the graph in adjacency list */ for ( auto a : prerequisites ) { graph [ a . second ]. push_back ( a . first ); } for ( int i = 0 ; i < numCourses ; ++ i ) { if ( ! canFinishDFS ( graph , visit , i )) return false ; } return true ; } bool canFinishDFS ( vector < vector < int >>& graph , vector < int >& visit , int i ) { if ( visit [ i ] == - 1 ) return false ; // visiting if ( visit [ i ] == 1 ) return true ; visit [ i ] = - 1 ; for ( auto a : graph [ i ]) { if ( ! canFinishDFS ( graph , visit , a )) return false ; } visit [ i ] = 1 ; return true ; } }; Solution 2 BFS class Solution { public : bool canFinish ( int numCourses , vector < pair < int , int >>& prerequisites ) { vector < vector < int >> graph ( numCourses , vector < int > ( 0 )); vector < int > in ( numCourses , 0 ); for ( auto a : prerequisites ) { graph [ a . second ]. push_back ( a . first ); ++ in [ a . first ]; /* indegree of nodes */ } queue < int > q ; /* locate the \"start\" of the directed acyclic graph */ for ( int i = 0 ; i < numCourses ; ++ i ) { if ( in [ i ] == 0 ) q . push ( i ); } while ( ! q . empty ()) { int t = q . front (); q . pop (); for ( auto a : graph [ t ]) { -- in [ a ]; /* visit the edge t->a */ if ( in [ a ] == 0 ) q . push ( a ); } } /* if there are cycle, (with node indegree > 0) */ for ( int i = 0 ; i < numCourses ; ++ i ) { if ( in [ i ] != 0 ) return false ; } return true ; } }; Course Schedule II \u00b6 Solution 1 DFS class Solution { public : vector < int > findOrder ( int numCourses , vector < pair < int , int >>& prerequisites ) { int n = prerequisites . size (); vector < vector < int >> graph ( numCourses , vector < int > ( 0 )); vector < int > visit ( numCourses , 0 ); /* 0: not visited, 1: visiting, -1: visited */ vector < int > res ; for ( auto a : prerequisites ) { graph [ a . second ]. push_back ( a . first ); } for ( int i = 0 ; i < numCourses ; ++ i ) { if ( ! visit [ i ] && ! findOrderDFS ( graph , visit , i , res )) { return {}; } } reverse ( res . begin (), res . end ()); return res ; } bool findOrderDFS ( vector < vector < int >>& graph , vector < int >& visit , int i , vector < int >& res ) { if ( visit [ i ] == 1 ) return false ; /* visiting */ if ( visit [ i ] == - 1 ) return true ; visit [ i ] = 1 ; for ( auto a : graph [ i ]) { if ( ! findOrderDFS ( graph , visit , a , res )) { return false ; } } visit [ i ] = - 1 ; res . push_back ( i ); return true ; } }; Solution 2 BFS class Solution { public : vector < int > findOrder ( int numCourses , vector < pair < int , int >>& prerequisites ) { vector < vector < int >> graph ( numCourses , vector < int > ( 0 )); vector < int > in ( numCourses , 0 ); vector < int > res ; // identify the sink edge by indegree for ( auto a : prerequisites ) { graph [ a . second ]. push_back ( a . first ); ++ in [ a . first ]; } queue < int > q ; for ( int i = 0 ; i < numCourses ; ++ i ) { //push the \"sink\" vetex to the queue if ( in [ i ] == 0 ) q . push ( i ); } while ( ! q . empty ()) { int t = q . front (); q . pop (); res . push_back ( t ); for ( auto a : graph [ t ]) { -- in [ a ]; if ( in [ a ] == 0 ) q . push ( a ); // add a new sink vertex } } // circle detection for ( int i = 0 ; i < numCourses ; ++ i ) { if ( in [ i ] != 0 ) return {}; } return res ; } }; Alien Dictionary \u00b6 Solution 1 Topological Sort class Solution { public : string alienOrder ( vector < string >& words ) { set < pair < char , char >> g ; // adj list graph, second -> first, unordered_set < char > set ; // all unique characters set vector < int > in ( 256 , 0 ); // indegree for each chars string res = \"\" ; for ( auto a : words ) // store all unique char of the dict words set . insert ( a . begin (), a . end ()); for ( int i = 0 ; i < words . size () - 1 ; i ++ ) { int min_len = min ( words [ i ]. size (), words [ i + 1 ]. size ()), j ; for ( j = 0 ; j < min_len ; j ++ ) { if ( words [ i ][ j ] != words [ i + 1 ][ j ]) { // build graph from the dictionary g . insert ({ words [ i ][ j ], words [ i + 1 ][ j ]}); // word[0][0] is the sink vertex break ; // take only one } } } // calculate indegree of nodes for ( auto c : g ) ++ in [ c . second ]; // push sink node to the queue queue < int > q ; for ( char c : set ) { if ( in [ c ] == 0 ) { q . push ( c ); res += c ; // sink vertex added to result } } while ( ! q . empty ()) { // BFS to output the sorted order char a = q . front (); q . pop (); for ( auto c : g ) { if ( c . first == a ) { // for each incomming edge of sink node a. -- in [ c . second ]; // here we treat the topological order c.second -> c.first if ( in [ c . second ] == 0 ) { q . push ( c . second ); res += c . second ; } } } } return res . size () == set . size () ? res : \"\" ; } }; Sequence Reconstruction \u00b6 Solution 1 Topological Sort class Solution { public : bool sequenceReconstruction ( vector < int >& org , vector < vector < int >>& seqs ) { int n = org . size (); int m = seqs . size (); vector < vector < int >> graph ( n + 1 , vector < int > ( 0 )); vector < int > in ( n + 1 , 0 ); bool empty = true ; for ( auto seq : seqs ) { if ( seq . empty ()) continue ; if ( seq . size () < 2 && ( seq [ 0 ] < 1 || seq [ 0 ] > n )) return false ; empty = false ; for ( int i = 0 ; i < seq . size () - 1 ; ++ i ) { int u = seq [ i ]; int v = seq [ i + 1 ]; if ( u < 1 || u > n || v < 1 || v > n ) return false ; graph [ u ]. push_back ( v ); // build the graph in [ v ] ++ ; // compute the indegree } } if ( empty ) return false ; queue < int > q ; for ( int i = 1 ; i <= n ; ++ i ) { if ( in [ i ] == 0 ) { q . push ( i ); } } int k = 0 ; while ( ! q . empty ()) { // BFS to compute the topological order // inorder to get a unique sequence, the q.size() should always be 1, // this means that the topological order is unique, consider the first example if ( q . size () > 1 ) return false ; int t = q . front (); q . pop (); if ( t != org [ k ++ ]) return false ; for ( auto a : graph [ t ]) { in [ a ] -- ; if ( in [ a ] == 0 ) q . push ( a ); } } return k == n ; } };","title":"Heap"},{"location":"leetcode/heap/notes/#heap","text":"","title":"Heap"},{"location":"leetcode/heap/notes/#binary-heap-properties","text":"Usually implemented using an array of elements but visualized as a complete binary tree (not a full binary tree). From the array index, we can compute the parent index from child's index and vice versa. The element values are partially ordered, briefly, they are ordered level by level. There is no ordering information of element values in the same level. The minimum number of elements in a binary heap with height h h is 2^h 2^h , the maximum elements in it is 2^{h + 1} - 1 2^{h + 1} - 1 The height of a binary heap is the floor of \\log n \\log n . n n is the total number of elements. The minimum element of a maximum heap is in the leaf nodes. With the array representation for storing an n-element heap, the leaves are the nodes indexed by \\lfloor n/2 \\rfloor + 1, \\lfloor n/2 \\rfloor + 2, \\dots, n \\lfloor n/2 \\rfloor + 1, \\lfloor n/2 \\rfloor + 2, \\dots, n .","title":"Binary heap properties"},{"location":"leetcode/heap/notes/#heap-sort","text":"","title":"Heap sort"},{"location":"leetcode/heap/notes/#heap-implementation-using-array","text":"","title":"Heap implementation using array"},{"location":"leetcode/heap/notes/#heap-implementation-using-heapnode","text":"","title":"Heap implementation using HeapNode"},{"location":"leetcode/heap/notes/#priority-queue-operations","text":"","title":"Priority queue operations"},{"location":"leetcode/heap/notes/#supported-operations","text":"# Operation complexity comment 1 buildMaxHeap(A) O(n) O(n) Build a priority queue routine iteratively call maxHeapify() n n times. The complexity is O(n) O(n) instead of O(n \\log n) O(n \\log n) 2 maxHeapify(A, i) O(\\log n) O(\\log n) Maintains the heap properties for subtree rooted at i . Assumes the subtrees of i are max heap before calling this routine. 3 insert() O(\\log n) O(\\log n) Insert a element to the heap. 4 peekMaximum() O(1) O(1) Return the max element without remove it from the max-heap. 5 popMaximum() O(\\log n) O(\\log n) Remove the max element from the max-heap. 6 increaseKey() O(\\log n) O(\\log n) Update the element's key. This essentially changed the priority of an element. 7 delete() Not supported by the binary heap implementation, but can be achieved by introduce more complex data structures.","title":"Supported operations"},{"location":"leetcode/heap/notes/#c-stl-priority-queue","text":"Notice if the elements in the queue is some complex container or object, you have to declare priority_queue with its underline container type, and the binary predicate std::greater if necessary. Here is example in the solution of the problem Minimum Unique Word Abbreviation priority_queue < pair < int , string > , vector < pair < int , string >> , greater < pair < int , string >>> pq ; This line of code declared a priority queue pq , whose element is piar<int, string> , with the minimum element on the top. But minimum what? How the priority queue sort a pair<int, string> element? In this case it implicitly sort the pair<int, string> element base on the first int value. How can we sort the priority queue element based on the second value of the pair<int, string> . To achieve that, we cannot use the building function object greater we have write a customized function object and pass to the declaration of the pq like this, class mycomparison { public : mycomparison () {} bool operator () ( const pair < int , string > & a , const pair < int , string > & b ) const { return a . second . compare ( b . second ); } }; ... priority_queue < pair < int , string > , vector < pair < int , string >> , mycomparison > pq ; ... We may need to write customized binary predicate function in order to implement the min-heap or max-heap . The binary predication function is depends on the underline container and also the type of the container element.","title":"C++ STL priority queue"},{"location":"leetcode/heap/notes/#priority-queue","text":"","title":"Priority Queue"},{"location":"leetcode/heap/notes/#merge-k-sorted-lists","text":"Solution 1 Priority queue Use a min priority queue to store all the list, iteratively pop the \"min list\", take the head node to the final list and push back to the priority queue if the \"min list\" still have node. Priority queue solution is O(N \\log k) O(N \\log k) /** * Definition for singly-linked list. * struct ListNode { * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) {} * }; */ class listComparison { public : listComparison () {} bool operator ( const ListNode * a , const ListNode * b ) const { return a -> val > b -> val ; } } class Solution { public : ListNode * mergeKLists ( vector < ListNode *>& lists ) { priority_queue < ListNode * , vector < ListNode *> , listComparison > q ; for ( auto list : lists ) { if ( list ) { q . push ( list ); } } ListNode dummy ( INT_MIN ); ListNode * curr = & dummy ; while ( ! q . empty ()) { curr -> next = q . top (); q . pop (); curr = curr -> next ; if ( curr -> next ) { q . push ( curr -> next ); } } return dummy . next ; } } Solution 2 Divide and conquer merge Use the divide and conquer idea to reduce the problem to a smaller problem and then solve the smallest problem, combined together with the solution of small problems, we will arrived at the final solution. Compare the following solution to merge sort algorithm. The complexity is O(N \\log k) O(N \\log k) . Because merge two list will take O(N) O(N) , and we will do O(\\log k) O(\\log k) time of merging. class Solution { public : ListNode * mergeKLists ( vector < ListNode *>& lists ) { return mergeKListsHelper ( lists , 0 , lists . size () - 1 ); } ListNode * mergeKListsHelper ( vector < ListNode *>& lists , int s , int e ) { if ( s > e ) return nullptr ; if ( s == e ) return lists [ s ]; if ( s < e ) { int m = s + ( e - s ) / 2 ; ListNode * l1 = mergeKListsHelper ( lists , s , m ); ListNode * l2 = mergeKListsHelper ( lists , m + 1 , e ); return merge ( l1 , l2 ); } } ListNode * merge ( ListNode * l1 , ListNode * l2 ) { ListNode dummy ( INT_MIN ); ListNode * curr = & dummy ; while ( l1 && l2 ) { if ( l1 -> val < l2 -> val ) { curr -> next = l1 ; l1 = l1 -> next ; } else { curr -> next = l2 ; l2 = l2 -> next ; } curr = curr -> next ; } curr -> next = l1 ? l1 : l2 ; return dummy . next ; } };","title":"Merge k Sorted Lists"},{"location":"leetcode/heap/notes/#minimum-unique-word-abbreviation","text":"","title":"Minimum Unique Word Abbreviation"},{"location":"leetcode/heap/notes/#smallest-range-covering-elements-from-k-lists","text":"Solution 1 Priority queue Notice this solution pushed some meta data to the priority queue. class Solution { public : struct mycompare { bool operator () ( pair < int , int >& a , pair < int , int >& b ) { return a . first > b . first ; } }; vector < int > smallestRange ( vector < vector < int >>& nums ) { int n = nums . size (); priority_queue < pair < int , int > , vector < pair < int , int >> , mycompare > pq ; int end = INT_MIN ; for ( int i = 0 ; i < n ; i ++ ) { end = max ( end , nums [ i ][ 0 ]); // push the first element of kth vector and k to the pq pq . push ({ nums [ i ][ 0 ], i }); } vector < int > idx ( n , 0 ); // keep tracking the index to the k arrays. vector < int > res = { - 100000 , 100000 }; // the range is given. while ( pq . size () == n ) { int start = pq . top (). first ; int k = pq . top (). second ; pq . pop (); if ( end - start < res [ 1 ] - res [ 0 ]) { res [ 0 ] = start ; res [ 1 ] = end ; } // if nums[k] have more element, we push next to the queue. if ( ++ idx [ k ] < nums [ k ]. size ()) { pq . push ({ nums [ k ][ idx [ k ]], k }); end = max ( end , nums [ k ][ idx [ k ]]); } } return res ; } };","title":"Smallest Range Covering Elements from K Lists"},{"location":"leetcode/heap/notes/#kth-largest-element-in-an-array","text":"Solution 1 priority queue Build a heap takes O(n) . max-heap O(n) + O(klogk) . class Solution { public : int findKthLargest ( vector < int >& nums , int k ) { priority_queue < int > pq ( nums . begin (), nums . end ()); k -- ; while ( k -- > 0 ) { pq . pop (); } return pq . top (); } }; Solution 2 priority queue min-heap O(n*logk) . class Solution { public : int findKthLargest ( vector < int >& nums , int k ) { priority_queue < int , vector < int > , greater < int >> pq ; for ( int n : nums ) { pq . push ( n ); if ( pq . size () > k ) { pq . pop (); } } return pq . top (); } }; Solution 3 quickselect algorithm class Solution { public : int findKthLargest ( vector < int >& nums , int k ) { int n = nums . size (); if ( n < k ) return 0 ; return orderStats ( nums , 0 , n - 1 , k ); } int orderStats ( vector < int >& nums , int start , int end , int k ) { if ( start == end ) return nums [ start ]; int p = partition ( nums , start , end ); int order = p - start + 1 ; if ( order == k ) { return nums [ p ]; } else if ( order > k ) { return orderStats ( nums , start , p - 1 , k ); } else { return orderStats ( nums , p + 1 , end , k - order ); } } int partition ( vector < int >& nums , int start , int end ) { if ( start == end ) return 0 ; int p = start + floor ( rand () % ( end - start + 1 )); swap ( nums [ p ], nums [ end ]); int pivot = nums [ end ]; int i = start - 1 , j ; for ( j = start ; j < end ; ++ j ) { if ( nums [ j ] >= pivot ) { swap ( nums [ ++ i ], nums [ j ]); } } swap ( nums [ end ], nums [ i + 1 ]); return i + 1 ; } };","title":"Kth Largest Element in an Array"},{"location":"leetcode/heap/notes/#the-skyline-problem","text":"Solution 1 priority queue maximum heap class Solution { public : vector < pair < int , int >> getSkyline ( vector < vector < int >>& buildings ) { vector < pair < int , int >> res ; int i = 0 , x = 0 , h = 0 , len = buildings . size (); priority_queue < pair < int , int >> q ; // max heap store <height, end> while ( i < len || ! q . empty ()) { // building i start before the end of current tallest building if ( q . empty () || i < len && buildings [ i ][ 0 ] <= q . top (). second ) { x = buildings [ i ][ 0 ]; // current start // overlapped start of multiple buildings (compare to if) while ( i < len && buildings [ i ][ 0 ] == x ) { q . push ({ buildings [ i ][ 2 ], buildings [ i ][ 1 ]}); i ++ ; } } else { // building i start after the end of current tallest building x = q . top (). second ; // current end // pop all buildings that end <= currently tallest buildings while ( ! q . empty () && q . top (). second <= x ) q . pop (); } h = q . empty () ? 0 : q . top (). first ; if ( res . empty () || res . back (). second != h ) { res . push_back ({ x , h }); } } return res ; } }; Solution 2 multiset + scanline solution class Solution { private : static bool cmp ( pair < int , int > p1 , pair < int , int > p2 ){ if ( p1 . first != p2 . first ) return p1 . first < p2 . first ; return p1 . second > p2 . second ; // if x duplicate, tallest building first } public : vector < pair < int , int >> getSkyline ( vector < vector < int >>& buildings ) { vector < pair < int , int >> h ; // store the start and end. vector < pair < int , int >> res ; multiset < int , greater < int >> s ; int prev = 0 , curr = 0 ; // scanline technique to obtain the starting point and end point for ( auto & a : buildings ) { h . push_back ({ a [ 0 ], a [ 2 ]}); h . push_back ({ a [ 1 ], - a [ 2 ]}); // use \"-\" mark the end point } sort ( h . begin (), h . end (), cmp ); s . insert ( 0 ); // edge case, consider a.second == 0 for ( auto & a : h ) { if ( a . second > 0 ) s . insert ( a . second ); // if it is a start, insert to the set else s . erase ( s . find ( - a . second )); // if it is a end, erase the inserted start curr = * s . begin (); // take the maximum from the set if ( curr != prev ) { // remove duplicate res . push_back ({ a . first , curr }); prev = curr ; } } return res ; } };","title":"The Skyline Problem"},{"location":"leetcode/heap/notes/#sliding-window-median","text":"Solution 1 heap + hash (simulated) class Solution { public : vector < double > medianSlidingWindow ( vector < int >& nums , int k ) { int n = nums . size (); vector < double > res ; priority_queue < int > lo ; priority_queue < int , vector < int > , greater < int >> hi ; unordered_map < int , int > hash_heap ; int i = 0 ; while ( i < k ) lo . push ( nums [ i ++ ]); for ( int j = 0 ; j < k / 2 ; j ++ ) { hi . push ( lo . top ()); lo . pop (); } while ( true ) { res . push_back ( k & 1 ? lo . top () : (( double ) lo . top () + ( double ) hi . top ()) * 0.5 ); if ( i >= nums . size ()) break ; int out_num = nums [ i - k ]; // first element in the window int in_num = nums [ i ++ ]; // new element entering window int balance = 0 ; // -1, when out_num in lo, +1 when out_num in hi. // balance is the count of element diff in lo and hi. // lo.size() == hi.size() -> balance = 0 // lo.size() > hi.size() --> balance > 0 // lo.size() < hi.size() --> balance < 0 balance += ( out_num <= lo . top () ? - 1 : 1 ); hash_heap [ out_num ] ++ ; // count of the invalid element, will remove when it on the top. if ( ! lo . empty () && in_num <= lo . top ()) { balance ++ ; lo . push ( in_num ); } else { balance -- ; hi . push ( in_num ); } if ( balance < 0 ) { // lo lack of element. lo . push ( hi . top ()); hi . pop (); balance ++ ; } else if ( balance > 0 ) { // hi lack of element hi . push ( lo . top ()); lo . pop (); balance -- ; } // lazy remove of elements previous removed, mimic hashheap of java. while ( ! lo . empty () && hash_heap [ lo . top ()]) { hash_heap [ lo . top ()] -- ; lo . pop (); } // lazy remove of elements previous removed, mimic hashheap of java. while ( ! hi . empty () && hash_heap [ hi . top ()]) { hash_heap [ hi . top ()] -- ; hi . pop (); } } return res ; } };","title":"Sliding Window Median"},{"location":"leetcode/heap/notes/#use-priority-queue-to-rearange-tasks-characters-string-etc","text":"","title":"Use priority queue to rearange tasks (characters, string, etc.)"},{"location":"leetcode/heap/notes/#rearrange-string-k-distance-apart","text":"Solution 1 Greedy + Priority Queue class Solution { public : string rearrangeString ( string s , int k ) { if ( k <= 0 ) return s ; unordered_map < char , int > mp ; for ( auto & c : s ) mp [ c ] ++ ; priority_queue < pair < int , char > , vector < pair < int , char >> , std :: less < pair < int , char >>> pq ; // max heap; for ( auto & p : mp ) { pq . push ({ p . second , p . first }); } string res ; while ( ! pq . empty ()) { vector < pair < int , char >> used ; int i = 0 ; while ( i < k && ! pq . empty ()) { auto t = pq . top (); pq . pop (); res . push_back ( t . second ); i ++ ; if ( -- t . first > 0 ) { used . push_back ( t ); } } if ( i != k && used . size ()) return \"\" ; for ( auto & p : used ) { pq . push ( p ); } } return res ; } };","title":"Rearrange String k Distance Apart"},{"location":"leetcode/heap/notes/#task-scheduler","text":"Solution 1 Greedy + Priority Queue class Solution { public : class cmp { public : cmp () {} bool operator ()( const pair < int , char >& a , const pair < int , char >& b ){ return a . first < b . first ; } }; int leastInterval ( vector < char >& tasks , int n ) { int chmap [ 26 ] = { 0 }; priority_queue < pair < int , char > , vector < pair < int , char >> , cmp > pq ; for ( char c : tasks ) { chmap [ c - 'A' ] ++ ; } for ( int i = 0 ; i < 26 ; ++ i ) { if ( chmap [ i ]) { pq . push ({ chmap [ i ], i + 'A' }); } } int cycle = n + 1 ; int res = 0 ; while ( ! pq . empty ()) { vector < pair < int , char >> used ; for ( int i = 0 ; i < cycle ; ++ i ) { // pick total of n + 1 in freq order if ( ! pq . empty ()) { used . push_back ( pq . top ()); pq . pop (); } } for ( auto p : used ) { if ( -- p . first ) { pq . push ( p ); // put back used chars if extra char exist after the useage. } } // handled partial cycle and full cycle res += ! pq . empty () ? cycle : used . size (); } return res ; } };","title":"Task Scheduler"},{"location":"leetcode/heap/notes/#reorganize-string","text":"Solution 1 Greedy + Priority Queue class Solution { public : class cmp { public : cmp () {} bool operator ()( const pair < int , char >& a , const pair < int , char >& b ){ return a . first < b . first ; } }; string reorganizeString ( string S ) { int n = S . length (); int chmap [ 26 ] = { 0 }; // max heap priority_queue < pair < int , char > , vector < pair < int , char >> , cmp > pq ; for ( auto & c : S ) { //store to map chmap [ c - 'a' ] ++ ; } // push to priority queue for ( int i = 0 ; i < 26 ; ++ i ) { if ( chmap [ i ]) { pq . push ({ chmap [ i ], i + 'a' }); } } if ( pq . top (). first > ( n + 1 ) / 2 ) return \"\" ; string res = \"\" ; while ( ! pq . empty ()) { pair < int , char > p = pq . top (); pq . pop (); if ( res . empty () || res . back () != p . second ) { res . push_back ( p . second ); if ( -- p . first > 0 ) { // use one char pq . push ( p ); } } else { // cannot use duplicate char, take another pair < int , char > q = pq . top (); pq . pop (); res . push_back ( q . second ); if ( -- q . first > 0 ) { // use another char pq . push ( q ); } // remember put p back to the pq pq . push ( p ); } } return res ; } };","title":"Reorganize String"},{"location":"leetcode/heap/notes/#topological-sorting","text":"","title":"Topological sorting"},{"location":"leetcode/heap/notes/#course-schedule","text":"Solution 1 DFS class Solution { public : bool canFinish ( int numCourses , vector < pair < int , int >>& prerequisites ) { vector < vector < int >> graph ( numCourses , vector < int > ( 0 )); vector < int > visit ( numCourses , 0 ); /* construct the graph in adjacency list */ for ( auto a : prerequisites ) { graph [ a . second ]. push_back ( a . first ); } for ( int i = 0 ; i < numCourses ; ++ i ) { if ( ! canFinishDFS ( graph , visit , i )) return false ; } return true ; } bool canFinishDFS ( vector < vector < int >>& graph , vector < int >& visit , int i ) { if ( visit [ i ] == - 1 ) return false ; // visiting if ( visit [ i ] == 1 ) return true ; visit [ i ] = - 1 ; for ( auto a : graph [ i ]) { if ( ! canFinishDFS ( graph , visit , a )) return false ; } visit [ i ] = 1 ; return true ; } }; Solution 2 BFS class Solution { public : bool canFinish ( int numCourses , vector < pair < int , int >>& prerequisites ) { vector < vector < int >> graph ( numCourses , vector < int > ( 0 )); vector < int > in ( numCourses , 0 ); for ( auto a : prerequisites ) { graph [ a . second ]. push_back ( a . first ); ++ in [ a . first ]; /* indegree of nodes */ } queue < int > q ; /* locate the \"start\" of the directed acyclic graph */ for ( int i = 0 ; i < numCourses ; ++ i ) { if ( in [ i ] == 0 ) q . push ( i ); } while ( ! q . empty ()) { int t = q . front (); q . pop (); for ( auto a : graph [ t ]) { -- in [ a ]; /* visit the edge t->a */ if ( in [ a ] == 0 ) q . push ( a ); } } /* if there are cycle, (with node indegree > 0) */ for ( int i = 0 ; i < numCourses ; ++ i ) { if ( in [ i ] != 0 ) return false ; } return true ; } };","title":"Course Schedule"},{"location":"leetcode/heap/notes/#course-schedule-ii","text":"Solution 1 DFS class Solution { public : vector < int > findOrder ( int numCourses , vector < pair < int , int >>& prerequisites ) { int n = prerequisites . size (); vector < vector < int >> graph ( numCourses , vector < int > ( 0 )); vector < int > visit ( numCourses , 0 ); /* 0: not visited, 1: visiting, -1: visited */ vector < int > res ; for ( auto a : prerequisites ) { graph [ a . second ]. push_back ( a . first ); } for ( int i = 0 ; i < numCourses ; ++ i ) { if ( ! visit [ i ] && ! findOrderDFS ( graph , visit , i , res )) { return {}; } } reverse ( res . begin (), res . end ()); return res ; } bool findOrderDFS ( vector < vector < int >>& graph , vector < int >& visit , int i , vector < int >& res ) { if ( visit [ i ] == 1 ) return false ; /* visiting */ if ( visit [ i ] == - 1 ) return true ; visit [ i ] = 1 ; for ( auto a : graph [ i ]) { if ( ! findOrderDFS ( graph , visit , a , res )) { return false ; } } visit [ i ] = - 1 ; res . push_back ( i ); return true ; } }; Solution 2 BFS class Solution { public : vector < int > findOrder ( int numCourses , vector < pair < int , int >>& prerequisites ) { vector < vector < int >> graph ( numCourses , vector < int > ( 0 )); vector < int > in ( numCourses , 0 ); vector < int > res ; // identify the sink edge by indegree for ( auto a : prerequisites ) { graph [ a . second ]. push_back ( a . first ); ++ in [ a . first ]; } queue < int > q ; for ( int i = 0 ; i < numCourses ; ++ i ) { //push the \"sink\" vetex to the queue if ( in [ i ] == 0 ) q . push ( i ); } while ( ! q . empty ()) { int t = q . front (); q . pop (); res . push_back ( t ); for ( auto a : graph [ t ]) { -- in [ a ]; if ( in [ a ] == 0 ) q . push ( a ); // add a new sink vertex } } // circle detection for ( int i = 0 ; i < numCourses ; ++ i ) { if ( in [ i ] != 0 ) return {}; } return res ; } };","title":"Course Schedule II"},{"location":"leetcode/heap/notes/#alien-dictionary","text":"Solution 1 Topological Sort class Solution { public : string alienOrder ( vector < string >& words ) { set < pair < char , char >> g ; // adj list graph, second -> first, unordered_set < char > set ; // all unique characters set vector < int > in ( 256 , 0 ); // indegree for each chars string res = \"\" ; for ( auto a : words ) // store all unique char of the dict words set . insert ( a . begin (), a . end ()); for ( int i = 0 ; i < words . size () - 1 ; i ++ ) { int min_len = min ( words [ i ]. size (), words [ i + 1 ]. size ()), j ; for ( j = 0 ; j < min_len ; j ++ ) { if ( words [ i ][ j ] != words [ i + 1 ][ j ]) { // build graph from the dictionary g . insert ({ words [ i ][ j ], words [ i + 1 ][ j ]}); // word[0][0] is the sink vertex break ; // take only one } } } // calculate indegree of nodes for ( auto c : g ) ++ in [ c . second ]; // push sink node to the queue queue < int > q ; for ( char c : set ) { if ( in [ c ] == 0 ) { q . push ( c ); res += c ; // sink vertex added to result } } while ( ! q . empty ()) { // BFS to output the sorted order char a = q . front (); q . pop (); for ( auto c : g ) { if ( c . first == a ) { // for each incomming edge of sink node a. -- in [ c . second ]; // here we treat the topological order c.second -> c.first if ( in [ c . second ] == 0 ) { q . push ( c . second ); res += c . second ; } } } } return res . size () == set . size () ? res : \"\" ; } };","title":"Alien Dictionary"},{"location":"leetcode/heap/notes/#sequence-reconstruction","text":"Solution 1 Topological Sort class Solution { public : bool sequenceReconstruction ( vector < int >& org , vector < vector < int >>& seqs ) { int n = org . size (); int m = seqs . size (); vector < vector < int >> graph ( n + 1 , vector < int > ( 0 )); vector < int > in ( n + 1 , 0 ); bool empty = true ; for ( auto seq : seqs ) { if ( seq . empty ()) continue ; if ( seq . size () < 2 && ( seq [ 0 ] < 1 || seq [ 0 ] > n )) return false ; empty = false ; for ( int i = 0 ; i < seq . size () - 1 ; ++ i ) { int u = seq [ i ]; int v = seq [ i + 1 ]; if ( u < 1 || u > n || v < 1 || v > n ) return false ; graph [ u ]. push_back ( v ); // build the graph in [ v ] ++ ; // compute the indegree } } if ( empty ) return false ; queue < int > q ; for ( int i = 1 ; i <= n ; ++ i ) { if ( in [ i ] == 0 ) { q . push ( i ); } } int k = 0 ; while ( ! q . empty ()) { // BFS to compute the topological order // inorder to get a unique sequence, the q.size() should always be 1, // this means that the topological order is unique, consider the first example if ( q . size () > 1 ) return false ; int t = q . front (); q . pop (); if ( t != org [ k ++ ]) return false ; for ( auto a : graph [ t ]) { in [ a ] -- ; if ( in [ a ] == 0 ) q . push ( a ); } } return k == n ; } };","title":"Sequence Reconstruction"},{"location":"leetcode/string/notes/","text":"String Problems \u00b6 Understand the problem by listing all the test cases first One of the most important principle to solve a string proble during an interview is to first make sure you have all the possible test cases laid out. Loop invariance \u00b6 To keep loop invariance, you have to be really clear about the different type of chars in the string that may affect the complexity of the loop. The following problems can be tricky to implement when you keep the wrong loop invariance or have identified wrong if condition or while conditions. Decode String \u00b6 Iterative solution v.s. Recursive solution Make sure you have all the test cases first Consider the special cases. i.e. nested brackets 3[a]2[a3[bc]] , and unbracketed chars 3[a]abcd and 3[a]2[a3[bc]aa] This solution requires a lot of attention to details. You should draw the variable and exercise it using example. Use loop invariance and only process one char at a time. We can also use the divide and conquer idea and using recursive helper function to solve the problem. The key to implement this solution is to keep the loop invariance in mind. Iterative solution class Solution { public : string decodeString ( string s ) { int n = s . size (); stack < int > stk_cnt ; stack < string > stk_str ; int cnt = 0 ; string tmp_str = \"\" ; for ( int i = 0 ; i < n ; i ++ ) { if ( isdigit ( s [ i ])) { cnt = cnt * 10 + s [ i ] - '0' ; } else if ( s [ i ] == '[' ) { stk_cnt . push ( cnt ); stk_str . push ( tmp_str ); cnt = 0 ; tmp_str . clear (); } else if ( s [ i ] == ']' ) { // output, or modify stacks int c = stk_cnt . top (); stk_cnt . pop (); for ( int i = 0 ; i < c ; i ++ ) { stk_str . top () += tmp_str ; } tmp_str = stk_str . top (); stk_str . pop (); } else { tmp_str += s [ i ]; } } return stk_str . empty () ? tmp_str : stk_str . top (); } }; Recursive solution class Solution { public : string decodeString ( string s ) { int i = 0 ; return decode_helper ( s , i ); } string decode_helper ( string s , int & i ) { int n = s . size (); string res = \"\" ; while ( i < n && s [ i ] != ']' ) { if ( s [ i ] < '0' || s [ i ] > '9' ) { //if (isalpha(s[i])) { res += s [ i ++ ]; } else { int cnt = 0 ; //while (i < n && isdigit(s[i])) { while ( i < n && s [ i ] >= '0' && s [ i ] <= '9' ) { cnt = cnt * 10 + s [ i ++ ] - '0' ; } ++ i ; // '[' string t = decode_helper ( s , i ); ++ i ; // ']' while ( cnt -- > 0 ) { res += t ; } } } return res ; } }; Longest Absolute File Path \u00b6 Maximum Score After Splitting a String \u00b6 The steps to solve this type of problem is enumerate all the possible test cases write a naive solution to cover all test cases refactor to improve the complexity Two pass class Solution { public : int maxScore ( string s ) { int len = s . size (); if ( len == 0 ) { return 0 ; } int ones = 0 ; int zeros = 0 ; int max_score = INT_MIN ; for ( int i = 0 ; i < len ; i ++ ) { if ( s [ i ] - '0' == 1 ) ones ++ ; } for ( int i = 0 ; i < len - 1 ; i ++ ) { if ( s [ i ] - '0' == 0 ) zeros ++ ; else ones -- ; max_score = max ( max_score , zeros + ones ); } return max_score ; } }; One pass class Solution { public : int maxScore ( string s ) { int len = s . size (); if ( len == 0 ) { return 0 ; } int ones = 0 ; int zeros = 0 ; int max_score = INT_MIN ; for ( int i = 0 ; i < len ; i ++ ) { if ( s [ i ] - '0' == 0 ) zeros ++ ; else ones ++ ; if ( i < len - 1 ) max_score = max ( max_score , zeros - ones ); } return max_score + ones ; } };","title":"String"},{"location":"leetcode/string/notes/#string-problems","text":"Understand the problem by listing all the test cases first One of the most important principle to solve a string proble during an interview is to first make sure you have all the possible test cases laid out.","title":"String Problems"},{"location":"leetcode/string/notes/#loop-invariance","text":"To keep loop invariance, you have to be really clear about the different type of chars in the string that may affect the complexity of the loop. The following problems can be tricky to implement when you keep the wrong loop invariance or have identified wrong if condition or while conditions.","title":"Loop invariance"},{"location":"leetcode/string/notes/#decode-string","text":"Iterative solution v.s. Recursive solution Make sure you have all the test cases first Consider the special cases. i.e. nested brackets 3[a]2[a3[bc]] , and unbracketed chars 3[a]abcd and 3[a]2[a3[bc]aa] This solution requires a lot of attention to details. You should draw the variable and exercise it using example. Use loop invariance and only process one char at a time. We can also use the divide and conquer idea and using recursive helper function to solve the problem. The key to implement this solution is to keep the loop invariance in mind. Iterative solution class Solution { public : string decodeString ( string s ) { int n = s . size (); stack < int > stk_cnt ; stack < string > stk_str ; int cnt = 0 ; string tmp_str = \"\" ; for ( int i = 0 ; i < n ; i ++ ) { if ( isdigit ( s [ i ])) { cnt = cnt * 10 + s [ i ] - '0' ; } else if ( s [ i ] == '[' ) { stk_cnt . push ( cnt ); stk_str . push ( tmp_str ); cnt = 0 ; tmp_str . clear (); } else if ( s [ i ] == ']' ) { // output, or modify stacks int c = stk_cnt . top (); stk_cnt . pop (); for ( int i = 0 ; i < c ; i ++ ) { stk_str . top () += tmp_str ; } tmp_str = stk_str . top (); stk_str . pop (); } else { tmp_str += s [ i ]; } } return stk_str . empty () ? tmp_str : stk_str . top (); } }; Recursive solution class Solution { public : string decodeString ( string s ) { int i = 0 ; return decode_helper ( s , i ); } string decode_helper ( string s , int & i ) { int n = s . size (); string res = \"\" ; while ( i < n && s [ i ] != ']' ) { if ( s [ i ] < '0' || s [ i ] > '9' ) { //if (isalpha(s[i])) { res += s [ i ++ ]; } else { int cnt = 0 ; //while (i < n && isdigit(s[i])) { while ( i < n && s [ i ] >= '0' && s [ i ] <= '9' ) { cnt = cnt * 10 + s [ i ++ ] - '0' ; } ++ i ; // '[' string t = decode_helper ( s , i ); ++ i ; // ']' while ( cnt -- > 0 ) { res += t ; } } } return res ; } };","title":"Decode String"},{"location":"leetcode/string/notes/#longest-absolute-file-path","text":"","title":"Longest Absolute File Path"},{"location":"leetcode/string/notes/#maximum-score-after-splitting-a-string","text":"The steps to solve this type of problem is enumerate all the possible test cases write a naive solution to cover all test cases refactor to improve the complexity Two pass class Solution { public : int maxScore ( string s ) { int len = s . size (); if ( len == 0 ) { return 0 ; } int ones = 0 ; int zeros = 0 ; int max_score = INT_MIN ; for ( int i = 0 ; i < len ; i ++ ) { if ( s [ i ] - '0' == 1 ) ones ++ ; } for ( int i = 0 ; i < len - 1 ; i ++ ) { if ( s [ i ] - '0' == 0 ) zeros ++ ; else ones -- ; max_score = max ( max_score , zeros + ones ); } return max_score ; } }; One pass class Solution { public : int maxScore ( string s ) { int len = s . size (); if ( len == 0 ) { return 0 ; } int ones = 0 ; int zeros = 0 ; int max_score = INT_MIN ; for ( int i = 0 ; i < len ; i ++ ) { if ( s [ i ] - '0' == 0 ) zeros ++ ; else ones ++ ; if ( i < len - 1 ) max_score = max ( max_score , zeros - ones ); } return max_score + ones ; } };","title":"Maximum Score After Splitting a String"},{"location":"leetcode/union-find/notes/","text":"Union Find \u00b6 Introduction \u00b6 Union Find problem can be used to model the dynamic connectivity problem. Briefly, given a set of N N objects, implement a union command to connect two objects, and a find (or connected ) command to query whether exist a path connecting the two objects. To model the connections, we assume the \u201cis connected to\u201d is reflective, symmetric, and transitive. Moreover, we define the connected components as a maximal set of objects that are mutually connected. We now implement the following operations. Find query, Check if two objects are in the same components. Union command. Replace components containing two objects with their union. Quick find \u00b6 Data structure \u00b6 integer array id[N] interpretation: p and q are connected iff they have the same id. i = 0 1 2 3 4 5 6 7 8 9 id=[0, 1, 1, 8, 8, 0, 0, 1, 8, 8] Find. Check if p and q have the same id. Union. Change the id of nodes have same id as p to the id of q . Implementation \u00b6 public class QuickFindUF { private int [] id ; public QuickFindUF ( int N ) { id = new int [ N ] ; for ( int i = 0 ; i < N ; i ++ ) id [ i ] = i ; } public boolean connected ( int p , int q ) { return id [ p ] == id [ q ] ; } public void union ( int p , int q ) { int pid = id [ p ] ; int qid = id [ q ] ; for ( int i = 0 ; i < id . length ; i ++ ) if ( id [ i ] == pid ) id [ i ] = qid ; } } Complexity \u00b6 Algorithm Initialization Find Union Quick Find N N 1 N N Quick find defect \u00b6 Union is too expensive ( O(N) O(N) ) Trees are flat, but too expensive to keep them flat. Quick union \u00b6 Data structure \u00b6 Integer array id[N] Interpretation: id[i] is parent of i . Root of i is id[id[id[...id[i]...]]] . (the root\u2019s parent is itself). i = 0 1 2 3 4 5 6 7 8 9 id=[0, 1, 9, 4, 9, 6, 6, 7, 8, 9] Find. Check if p and q have the same root. Union. set the id of p \u2019s root to the id of q \u2019s root. Implementation \u00b6 public class QucikUnionUF { private int [] id ; public QuickUnionUF ( int N ) { id = new int [ N ] ; for ( int i = 0 ; i < N ; i ++ ) id [ i ] = i ; } private int root ( int i ) { while ( i != id [ i ] ) i = id [ i ] ; return i ; } public boolean connected ( int p , int q ) { return root ( p ) == root ( q ); } public void union ( int p , int q ) { int i = root ( p ); int j = root ( q ); id [ i ] = j ; } } Complexity \u00b6 Algorithm Initialization Find Union Quick Union N N N N N N Quick union defect \u00b6 Trees can get tall Find is too expensive O(N) O(N) Improvement by weighting \u00b6 Principles \u00b6 Modify quick-union to avoid tall trees. Keep track of the size of each tree. Balance by linking root of the smaller tree to root of the large tree. Data structure \u00b6 Same as the quick union, but maintain extra array sz[i] to count the number of objects in the tree rooted at i. Find. Identical to the Quick Union Union. Link root of the smaller tree to root of the larger tree. Implementation \u00b6 Java /* weighted quick union */ public void union ( int p , int q ) { int i = root ( p ); int j = root ( q ); if ( i == j ) return ; if ( sz [ i ] < sz [ j ] ) { id [ i ] = j ; sz [ j ] += sz [ i ] ; } else { id [ j ] = i ; sz [ i ] += sz [ j ] ; } } Weighted quick-union complexity \u00b6 Find. takes time proportional to the depth of p and q . Union. Take constant time, given root. Algorithm Initialization Find Union Quick Find N N 1 N N Quick Union N N N N N N Weighted Quick Union N N lg(N) lg(N) lg(N) lg(N) Analysis \u00b6 Proposition. Depth of any node x x is at most lg(N) lg(N) Proof. When does depth of x x increse? The depth of x x in tree T_1 T_1 will increase 1 when it merged with a larger tree. How many merges it could possibly happen? Because each merge will double the size of the tree containing x x , it can double the tree at most lg(N) lg(N) times. We can prove that it can merge at most lg(N) lg(N) times. Quick union with path compression \u00b6 Principles \u00b6 After computing the root of the p , set the id of each examined node to point to that root. Implementation \u00b6 /* one-pass solution */ private int root ( in i ) { while ( i != id [ i ] ) { id [ i ] = id [ id [ i ]] ; /* object i's grad parent becomes its parent. */ i = id [ i ] ; } } Weighted quick-union with path compression \u00b6 Amortized analysis \u00b6 Proposition. Starting from an empty data structure, any sequence of M M union-find ops on N N objects makes \\leq c(N + Mlg^*N) \\leq c(N + Mlg^*N) array access. Analysis can be implemented to N + M \\alpha(M, N) N + M \\alpha(M, N) . The proof is too complex to be discussed here. Here is a notation of lg^* lg^* , which is the iterate log function. it growing very slow. In theory, the Weighted Quick Union Path Compression algorithm cost within a constant factor of reading in the data. But not truly linear. In practice, WQUPC is linear. Summary \u00b6 M M union-find operations on a set of N N objects Algorithm worst-case time quick find MN MN quick union MN MN weighted quick union N+MlgN N+MlgN quick union with path compression N+MlgN N+MlgN weighted quick union with path compression N+Mlg^*N N+Mlg^*N","title":"Union Find"},{"location":"leetcode/union-find/notes/#union-find","text":"","title":"Union Find"},{"location":"leetcode/union-find/notes/#introduction","text":"Union Find problem can be used to model the dynamic connectivity problem. Briefly, given a set of N N objects, implement a union command to connect two objects, and a find (or connected ) command to query whether exist a path connecting the two objects. To model the connections, we assume the \u201cis connected to\u201d is reflective, symmetric, and transitive. Moreover, we define the connected components as a maximal set of objects that are mutually connected. We now implement the following operations. Find query, Check if two objects are in the same components. Union command. Replace components containing two objects with their union.","title":"Introduction"},{"location":"leetcode/union-find/notes/#quick-find","text":"","title":"Quick find"},{"location":"leetcode/union-find/notes/#data-structure","text":"integer array id[N] interpretation: p and q are connected iff they have the same id. i = 0 1 2 3 4 5 6 7 8 9 id=[0, 1, 1, 8, 8, 0, 0, 1, 8, 8] Find. Check if p and q have the same id. Union. Change the id of nodes have same id as p to the id of q .","title":"Data structure"},{"location":"leetcode/union-find/notes/#implementation","text":"public class QuickFindUF { private int [] id ; public QuickFindUF ( int N ) { id = new int [ N ] ; for ( int i = 0 ; i < N ; i ++ ) id [ i ] = i ; } public boolean connected ( int p , int q ) { return id [ p ] == id [ q ] ; } public void union ( int p , int q ) { int pid = id [ p ] ; int qid = id [ q ] ; for ( int i = 0 ; i < id . length ; i ++ ) if ( id [ i ] == pid ) id [ i ] = qid ; } }","title":"Implementation"},{"location":"leetcode/union-find/notes/#complexity","text":"Algorithm Initialization Find Union Quick Find N N 1 N N","title":"Complexity"},{"location":"leetcode/union-find/notes/#quick-find-defect","text":"Union is too expensive ( O(N) O(N) ) Trees are flat, but too expensive to keep them flat.","title":"Quick find defect"},{"location":"leetcode/union-find/notes/#quick-union","text":"","title":"Quick union"},{"location":"leetcode/union-find/notes/#data-structure_1","text":"Integer array id[N] Interpretation: id[i] is parent of i . Root of i is id[id[id[...id[i]...]]] . (the root\u2019s parent is itself). i = 0 1 2 3 4 5 6 7 8 9 id=[0, 1, 9, 4, 9, 6, 6, 7, 8, 9] Find. Check if p and q have the same root. Union. set the id of p \u2019s root to the id of q \u2019s root.","title":"Data structure"},{"location":"leetcode/union-find/notes/#implementation_1","text":"public class QucikUnionUF { private int [] id ; public QuickUnionUF ( int N ) { id = new int [ N ] ; for ( int i = 0 ; i < N ; i ++ ) id [ i ] = i ; } private int root ( int i ) { while ( i != id [ i ] ) i = id [ i ] ; return i ; } public boolean connected ( int p , int q ) { return root ( p ) == root ( q ); } public void union ( int p , int q ) { int i = root ( p ); int j = root ( q ); id [ i ] = j ; } }","title":"Implementation"},{"location":"leetcode/union-find/notes/#complexity_1","text":"Algorithm Initialization Find Union Quick Union N N N N N N","title":"Complexity"},{"location":"leetcode/union-find/notes/#quick-union-defect","text":"Trees can get tall Find is too expensive O(N) O(N)","title":"Quick union defect"},{"location":"leetcode/union-find/notes/#improvement-by-weighting","text":"","title":"Improvement by weighting"},{"location":"leetcode/union-find/notes/#principles","text":"Modify quick-union to avoid tall trees. Keep track of the size of each tree. Balance by linking root of the smaller tree to root of the large tree.","title":"Principles"},{"location":"leetcode/union-find/notes/#data-structure_2","text":"Same as the quick union, but maintain extra array sz[i] to count the number of objects in the tree rooted at i. Find. Identical to the Quick Union Union. Link root of the smaller tree to root of the larger tree.","title":"Data structure"},{"location":"leetcode/union-find/notes/#implementation_2","text":"Java /* weighted quick union */ public void union ( int p , int q ) { int i = root ( p ); int j = root ( q ); if ( i == j ) return ; if ( sz [ i ] < sz [ j ] ) { id [ i ] = j ; sz [ j ] += sz [ i ] ; } else { id [ j ] = i ; sz [ i ] += sz [ j ] ; } }","title":"Implementation"},{"location":"leetcode/union-find/notes/#weighted-quick-union-complexity","text":"Find. takes time proportional to the depth of p and q . Union. Take constant time, given root. Algorithm Initialization Find Union Quick Find N N 1 N N Quick Union N N N N N N Weighted Quick Union N N lg(N) lg(N) lg(N) lg(N)","title":"Weighted quick-union complexity"},{"location":"leetcode/union-find/notes/#analysis","text":"Proposition. Depth of any node x x is at most lg(N) lg(N) Proof. When does depth of x x increse? The depth of x x in tree T_1 T_1 will increase 1 when it merged with a larger tree. How many merges it could possibly happen? Because each merge will double the size of the tree containing x x , it can double the tree at most lg(N) lg(N) times. We can prove that it can merge at most lg(N) lg(N) times.","title":"Analysis"},{"location":"leetcode/union-find/notes/#quick-union-with-path-compression","text":"","title":"Quick union with path compression"},{"location":"leetcode/union-find/notes/#principles_1","text":"After computing the root of the p , set the id of each examined node to point to that root.","title":"Principles"},{"location":"leetcode/union-find/notes/#implementation_3","text":"/* one-pass solution */ private int root ( in i ) { while ( i != id [ i ] ) { id [ i ] = id [ id [ i ]] ; /* object i's grad parent becomes its parent. */ i = id [ i ] ; } }","title":"Implementation"},{"location":"leetcode/union-find/notes/#weighted-quick-union-with-path-compression","text":"","title":"Weighted quick-union with path compression"},{"location":"leetcode/union-find/notes/#amortized-analysis","text":"Proposition. Starting from an empty data structure, any sequence of M M union-find ops on N N objects makes \\leq c(N + Mlg^*N) \\leq c(N + Mlg^*N) array access. Analysis can be implemented to N + M \\alpha(M, N) N + M \\alpha(M, N) . The proof is too complex to be discussed here. Here is a notation of lg^* lg^* , which is the iterate log function. it growing very slow. In theory, the Weighted Quick Union Path Compression algorithm cost within a constant factor of reading in the data. But not truly linear. In practice, WQUPC is linear.","title":"Amortized analysis"},{"location":"leetcode/union-find/notes/#summary","text":"M M union-find operations on a set of N N objects Algorithm worst-case time quick find MN MN quick union MN MN weighted quick union N+MlgN N+MlgN quick union with path compression N+MlgN N+MlgN weighted quick union with path compression N+Mlg^*N N+Mlg^*N","title":"Summary"},{"location":"research/coalition-game/notes/","text":"Coalition Game Theory Notes \u00b6 https://wiki.rui-han.com/index.php/Private:Research/Coalition_Game","title":"Coalition Game"},{"location":"research/coalition-game/notes/#coalition-game-theory-notes","text":"https://wiki.rui-han.com/index.php/Private:Research/Coalition_Game","title":"Coalition Game Theory Notes"},{"location":"research/contextual-bandit/notes/","text":"Contextual Multi-Armed Bandit \u00b6 https://wiki.rui-han.com/index.php/Private:Research/Multi-Armed_Bandit Multi-armed bandit problems \u00b6 NB: action is equivalent to arm, action i i means play the i i -th arm T T : total rounds K K : number of arms G_t(k) G_t(k) : reward (gain) obtained by play the k k -th arm at the t t -th round R_t(k) R_t(k) : regret (loss) incurr by play the k k -th arm at the t t -th round \\mu^* \\mu^* : the optimal expected payoff for the best arm (action) Brute force algorithm \u00b6 This doesn't apply to the real setting, because in the real bandit problem you can only play one arm at each round. But it doesn't heart that we evaluate this scenario as a benchmark for other algorithms. In this algorithm, we basically evaluate all arms in every round, pretended we are in god's angle and know the underline distribution of each arm. Optimistic initialization \u00b6 This is what we do in the supervised training, Random selection algorithm \u00b6 Each step, you select one classifier uniform randomly and calculate the reward. Greedy ( \\epsilon \\epsilon -Greedy) algorithm \u00b6 In each round, switch to a random arm with probability \\epsilon \\epsilon , otherwise stick on the current optimal arm. The first few rounds are important. For example, if the optimal arm is selected in the beginning, this algorithm will achieve better performance; If the worst arm is selected in the beginning, it will stuck in getting the least reward. Upper Confidence bound \u00b6 Confidence bound is usually misunderstood by it's name. It is NOT the probability of certain outcome fall between the lower bound and upper bound (confidence interval). Confidence bound also called confidence interval, both the lower bound and upper bound are random variables. It describe the property of a method to come up a confidence bound in individual trails. It can be iterpreted as the probability that a realized confidence bound include the ture estimate. Notice UCB method is a classic statistic method. The expected reward \\mu_k \\mu_k is treated as a fixed value, not a random variable. In Bayesian bandit, the expected reward is measured as random variable. For arm a a , \\hat Q_t(a) \\hat Q_t(a) is the sample mean, \\hat U_t(a) \\hat U_t(a) is the upper confidence bound. Q_t(a) Q_t(a) is the true mean, then we have Q_t(a) \\le \\hat Q_t(a) + \\hat U_t(a) Q_t(a) \\le \\hat Q_t(a) + \\hat U_t(a) . \\hat U_t(a) \\hat U_t(a) is inverse proportional to N_t(a) N_t(a) , the total number of selection of arm a a , because the more we played the less uncertain we have for \\hat Q_t(a) \\hat Q_t(a) , thus the upper bound shrink as N_t(a) N_t(a) increases. In UCB method, we select the arm with highest upper confidence bound, this is because it is not played sufficient enough and we believe this arm have the high potential to be the optimal arm. Formally, UCB is equivalent to the following optimization problem: a_t = \\operatorname*{arg\\,max}_{a \\in A} \\hat Q_t(a) + \\hat U_t(a) a_t = \\operatorname*{arg\\,max}_{a \\in A} \\hat Q_t(a) + \\hat U_t(a) UCB1 \u00b6 Bayesian bandit (Thompson Sampling) \u00b6 Exploitation versus exploration \u00b6 Contextual multi-armed bandit \u00b6 Reference \u00b6","title":"Contextual Multi-Armed Bandit"},{"location":"research/contextual-bandit/notes/#contextual-multi-armed-bandit","text":"https://wiki.rui-han.com/index.php/Private:Research/Multi-Armed_Bandit","title":"Contextual Multi-Armed Bandit"},{"location":"research/contextual-bandit/notes/#multi-armed-bandit-problems","text":"NB: action is equivalent to arm, action i i means play the i i -th arm T T : total rounds K K : number of arms G_t(k) G_t(k) : reward (gain) obtained by play the k k -th arm at the t t -th round R_t(k) R_t(k) : regret (loss) incurr by play the k k -th arm at the t t -th round \\mu^* \\mu^* : the optimal expected payoff for the best arm (action)","title":"Multi-armed bandit problems"},{"location":"research/contextual-bandit/notes/#brute-force-algorithm","text":"This doesn't apply to the real setting, because in the real bandit problem you can only play one arm at each round. But it doesn't heart that we evaluate this scenario as a benchmark for other algorithms. In this algorithm, we basically evaluate all arms in every round, pretended we are in god's angle and know the underline distribution of each arm.","title":"Brute force algorithm"},{"location":"research/contextual-bandit/notes/#optimistic-initialization","text":"This is what we do in the supervised training,","title":"Optimistic initialization"},{"location":"research/contextual-bandit/notes/#random-selection-algorithm","text":"Each step, you select one classifier uniform randomly and calculate the reward.","title":"Random selection algorithm"},{"location":"research/contextual-bandit/notes/#greedy-epsilonepsilon-greedy-algorithm","text":"In each round, switch to a random arm with probability \\epsilon \\epsilon , otherwise stick on the current optimal arm. The first few rounds are important. For example, if the optimal arm is selected in the beginning, this algorithm will achieve better performance; If the worst arm is selected in the beginning, it will stuck in getting the least reward.","title":"Greedy (\\epsilon\\epsilon-Greedy) algorithm"},{"location":"research/contextual-bandit/notes/#upper-confidence-bound","text":"Confidence bound is usually misunderstood by it's name. It is NOT the probability of certain outcome fall between the lower bound and upper bound (confidence interval). Confidence bound also called confidence interval, both the lower bound and upper bound are random variables. It describe the property of a method to come up a confidence bound in individual trails. It can be iterpreted as the probability that a realized confidence bound include the ture estimate. Notice UCB method is a classic statistic method. The expected reward \\mu_k \\mu_k is treated as a fixed value, not a random variable. In Bayesian bandit, the expected reward is measured as random variable. For arm a a , \\hat Q_t(a) \\hat Q_t(a) is the sample mean, \\hat U_t(a) \\hat U_t(a) is the upper confidence bound. Q_t(a) Q_t(a) is the true mean, then we have Q_t(a) \\le \\hat Q_t(a) + \\hat U_t(a) Q_t(a) \\le \\hat Q_t(a) + \\hat U_t(a) . \\hat U_t(a) \\hat U_t(a) is inverse proportional to N_t(a) N_t(a) , the total number of selection of arm a a , because the more we played the less uncertain we have for \\hat Q_t(a) \\hat Q_t(a) , thus the upper bound shrink as N_t(a) N_t(a) increases. In UCB method, we select the arm with highest upper confidence bound, this is because it is not played sufficient enough and we believe this arm have the high potential to be the optimal arm. Formally, UCB is equivalent to the following optimization problem: a_t = \\operatorname*{arg\\,max}_{a \\in A} \\hat Q_t(a) + \\hat U_t(a) a_t = \\operatorname*{arg\\,max}_{a \\in A} \\hat Q_t(a) + \\hat U_t(a)","title":"Upper Confidence bound"},{"location":"research/contextual-bandit/notes/#ucb1","text":"","title":"UCB1"},{"location":"research/contextual-bandit/notes/#bayesian-bandit-thompson-sampling","text":"","title":"Bayesian bandit (Thompson Sampling)"},{"location":"research/contextual-bandit/notes/#exploitation-versus-exploration","text":"","title":"Exploitation versus exploration"},{"location":"research/contextual-bandit/notes/#contextual-multi-armed-bandit_1","text":"","title":"Contextual multi-armed bandit"},{"location":"research/contextual-bandit/notes/#reference","text":"","title":"Reference"},{"location":"research/tfidf-score/notes/","text":"TF-IDF and Scoring for Information Retrieval \u00b6 Youtube Lecture by Dr. Manning Book chapter tfidf slides term document incidence matrices \u00b6 not practical because of the sparsity. The inverted index \u00b6 token to document list mapping word -> [doc1, doc2, ...] UA -> [2020-04-01 13:03:23, 2020-04-01 13:03:23] Query processing \u00b6 If we want to query which document have the phrash \"information retrival\", we'd like to take the invert index postings list for \"information\" and \"retrival\", then find the document that have both word. (by run a merge algorithm on the two postings list) Boolean query \u00b6 Mainly leverage the boolean operation on the list merging algorithm. Phrase queries \u00b6 no longer suffice to store <term, doc> entries. First attempt, biword indexes. treat the two words phrase as a single phrase. For longer phrase queries, we can broken into the boolean query on biword. false positive, two words not adjacent index blowup due to big dictionary positional indexes, store the positions of each token occurs at a document. use a merge algorithm to recursively at the document level. Ranked retriveval \u00b6 \"How can we rank-order the documents in the collection with respect to a query?\" assign a score to each document - say in [0, 1] - to each document. This score measures how well document and query match. We need a way to assigning a score to a query/document pair. The more frequent a term appears in a document, the higher the score for the document. Scoring with the Jaccard Coefficient \u00b6 doesn't consider term frequency. wee need a better way of normalizing for length. (compare with cosine similarity) Term frequency weighting \u00b6 binary vector \\in {0, 1} \\in {0, 1} count vector bag of words model Tip We want to use TF when computing query-document match scores, but the relevance does not increase proportionally with term frequency Log-frequency weighting \u00b6 w_{t, d} = \\begin{cases} 1 + \\log_{10} (tf_{t,d}), & tf_{t,d} \\ge 0 \\\\ 0, & \\textrm{otherwise} \\end{cases} w_{t, d} = \\begin{cases} 1 + \\log_{10} (tf_{t,d}), & tf_{t,d} \\ge 0 \\\\ 0, & \\textrm{otherwise} \\end{cases} Score for a document-query pair: sum over terms in both q q and d d : \\textrm{score} = \\sum_{t\\in q \\cap d} (1 + \\log_{10} (tf_{t, d})) \\textrm{score} = \\sum_{t\\in q \\cap d} (1 + \\log_{10} (tf_{t, d})) The score is 0 0 if none of the terms is presented in the document. (Inverted) Document frequency weighting \u00b6 idea: Rare terms are more informative than frequent terms. df_t df_t is an inverse measure of informativeness of term t t . There is one idf value for each term t t in a collection. idf_t = \\log_{10}(N/df_t) idf_t = \\log_{10}(N/df_t) Question Does idf have an effect on ranking for one-term queries, like \"iPhone\"? The answer is no, but why? collection frequency vs document frequency \u00b6 TF-IDF weighting \u00b6 \\textrm{score}(q, d) = \\sum_{t\\in q \\cap d} tf_{t, d} \\times idf_{t} \\textrm{score}(q, d) = \\sum_{t\\in q \\cap d} tf_{t, d} \\times idf_{t} binary vector -> count vector -> weight matrix Vector Space Model \u00b6 Euclidean distance, not a good option. because different vector length can have large distance application for https request embedding \u00b6 If we can build a dataset, given a \"query\", it can efficiently retrive how many relevant \"document\" are in the past 2 hours? We will be able to tell wether the \"query\" is mallicious or not. in anti-abuse, we can use the current request as the query, all the reprevious request data that are indexed as the \"documents\". This will give us a score about the current request. This score can tell us the malliciousness of the request. problem \u00b6 Current distribution score cannot identify scraping that routates headers. How to up weight bad requests with certain patterns? Currently, no good way to block signals with low false positives. How to use score instead of text pattern matching to reduce false positives? Solution \u00b6 calculate tf-idf for individual header fields \u00b6 This solution models a collection of requests during a window as a document. term frequency = count of each UA, HC, CC, and RF in a time window document frequency = count of how many windows each UA, HC, CC, and RF occurs the window can be a time window or fixed size window. In the fixed size window, instead of divided by hour, we divide by fixed number of requests and treat each collection as a document. This model can also be used for the tokenized solution. tokenize all request facets \u00b6 This solution is to break down the header fields into tokens and use a set of tokens as the query. Compare this to a phrase retrival task. term frequency = count of each token Question Can we use the consine similarity to compare two queries? Because the similarirty measure described in the lectures is for query-document pair. Can we compare query-query pair? Whether the TF-IDF embedding can capture enough similarity info and used for clustering? Intuitions \u00b6 Using TF-IDF, we are able to measure the speed of demage from request possess the similar attributes (belongs to certain attacks). Specifically, given a request in realtime, it can check the current TF, and DF, and calculate the TF-IDF. All these three values are useful for us. TF - If TF is greater than a threshold setting. We can it is may be a high qps scraping. If TF is moderate or very small, we can check DF. DF - If DF is very large, the \"term\" maybe common to all sample in the space. If DF streak is not long enough in a series of previous documents. there might be a spike of increase for the term in some window before. If DF is very small, and the TF is large, we detected a spike. # TF DF result 1 large small bad v 2 large large bad/good x 3 small large bad/good x 4 small small good x TF and IDF used separately can help to observe bad signals, but they are not enough. We will need to use TF-IDF to help us to enhance our believe about the badness of a request. TF-IDF Practical consideration \u00b6 How to calculation TF-IDF in sliding windows? title: HTTP requests embedding to score and search bot traffic.","title":"TF-IDF for Information Retrieval"},{"location":"research/tfidf-score/notes/#tf-idf-and-scoring-for-information-retrieval","text":"Youtube Lecture by Dr. Manning Book chapter tfidf slides","title":"TF-IDF and Scoring for Information Retrieval"},{"location":"research/tfidf-score/notes/#term-document-incidence-matrices","text":"not practical because of the sparsity.","title":"term document incidence matrices"},{"location":"research/tfidf-score/notes/#the-inverted-index","text":"token to document list mapping word -> [doc1, doc2, ...] UA -> [2020-04-01 13:03:23, 2020-04-01 13:03:23]","title":"The inverted index"},{"location":"research/tfidf-score/notes/#query-processing","text":"If we want to query which document have the phrash \"information retrival\", we'd like to take the invert index postings list for \"information\" and \"retrival\", then find the document that have both word. (by run a merge algorithm on the two postings list)","title":"Query processing"},{"location":"research/tfidf-score/notes/#boolean-query","text":"Mainly leverage the boolean operation on the list merging algorithm.","title":"Boolean query"},{"location":"research/tfidf-score/notes/#phrase-queries","text":"no longer suffice to store <term, doc> entries. First attempt, biword indexes. treat the two words phrase as a single phrase. For longer phrase queries, we can broken into the boolean query on biword. false positive, two words not adjacent index blowup due to big dictionary positional indexes, store the positions of each token occurs at a document. use a merge algorithm to recursively at the document level.","title":"Phrase queries"},{"location":"research/tfidf-score/notes/#ranked-retriveval","text":"\"How can we rank-order the documents in the collection with respect to a query?\" assign a score to each document - say in [0, 1] - to each document. This score measures how well document and query match. We need a way to assigning a score to a query/document pair. The more frequent a term appears in a document, the higher the score for the document.","title":"Ranked retriveval"},{"location":"research/tfidf-score/notes/#scoring-with-the-jaccard-coefficient","text":"doesn't consider term frequency. wee need a better way of normalizing for length. (compare with cosine similarity)","title":"Scoring with the Jaccard Coefficient"},{"location":"research/tfidf-score/notes/#term-frequency-weighting","text":"binary vector \\in {0, 1} \\in {0, 1} count vector bag of words model Tip We want to use TF when computing query-document match scores, but the relevance does not increase proportionally with term frequency","title":"Term frequency weighting"},{"location":"research/tfidf-score/notes/#log-frequency-weighting","text":"w_{t, d} = \\begin{cases} 1 + \\log_{10} (tf_{t,d}), & tf_{t,d} \\ge 0 \\\\ 0, & \\textrm{otherwise} \\end{cases} w_{t, d} = \\begin{cases} 1 + \\log_{10} (tf_{t,d}), & tf_{t,d} \\ge 0 \\\\ 0, & \\textrm{otherwise} \\end{cases} Score for a document-query pair: sum over terms in both q q and d d : \\textrm{score} = \\sum_{t\\in q \\cap d} (1 + \\log_{10} (tf_{t, d})) \\textrm{score} = \\sum_{t\\in q \\cap d} (1 + \\log_{10} (tf_{t, d})) The score is 0 0 if none of the terms is presented in the document.","title":"Log-frequency weighting"},{"location":"research/tfidf-score/notes/#inverted-document-frequency-weighting","text":"idea: Rare terms are more informative than frequent terms. df_t df_t is an inverse measure of informativeness of term t t . There is one idf value for each term t t in a collection. idf_t = \\log_{10}(N/df_t) idf_t = \\log_{10}(N/df_t) Question Does idf have an effect on ranking for one-term queries, like \"iPhone\"? The answer is no, but why?","title":"(Inverted) Document frequency weighting"},{"location":"research/tfidf-score/notes/#collection-frequency-vs-document-frequency","text":"","title":"collection frequency vs document frequency"},{"location":"research/tfidf-score/notes/#tf-idf-weighting","text":"\\textrm{score}(q, d) = \\sum_{t\\in q \\cap d} tf_{t, d} \\times idf_{t} \\textrm{score}(q, d) = \\sum_{t\\in q \\cap d} tf_{t, d} \\times idf_{t} binary vector -> count vector -> weight matrix","title":"TF-IDF weighting"},{"location":"research/tfidf-score/notes/#vector-space-model","text":"Euclidean distance, not a good option. because different vector length can have large distance","title":"Vector Space Model"},{"location":"research/tfidf-score/notes/#application-for-https-request-embedding","text":"If we can build a dataset, given a \"query\", it can efficiently retrive how many relevant \"document\" are in the past 2 hours? We will be able to tell wether the \"query\" is mallicious or not. in anti-abuse, we can use the current request as the query, all the reprevious request data that are indexed as the \"documents\". This will give us a score about the current request. This score can tell us the malliciousness of the request.","title":"application for https request embedding"},{"location":"research/tfidf-score/notes/#problem","text":"Current distribution score cannot identify scraping that routates headers. How to up weight bad requests with certain patterns? Currently, no good way to block signals with low false positives. How to use score instead of text pattern matching to reduce false positives?","title":"problem"},{"location":"research/tfidf-score/notes/#solution","text":"","title":"Solution"},{"location":"research/tfidf-score/notes/#calculate-tf-idf-for-individual-header-fields","text":"This solution models a collection of requests during a window as a document. term frequency = count of each UA, HC, CC, and RF in a time window document frequency = count of how many windows each UA, HC, CC, and RF occurs the window can be a time window or fixed size window. In the fixed size window, instead of divided by hour, we divide by fixed number of requests and treat each collection as a document. This model can also be used for the tokenized solution.","title":"calculate tf-idf for individual header fields"},{"location":"research/tfidf-score/notes/#tokenize-all-request-facets","text":"This solution is to break down the header fields into tokens and use a set of tokens as the query. Compare this to a phrase retrival task. term frequency = count of each token Question Can we use the consine similarity to compare two queries? Because the similarirty measure described in the lectures is for query-document pair. Can we compare query-query pair? Whether the TF-IDF embedding can capture enough similarity info and used for clustering?","title":"tokenize all request facets"},{"location":"research/tfidf-score/notes/#intuitions","text":"Using TF-IDF, we are able to measure the speed of demage from request possess the similar attributes (belongs to certain attacks). Specifically, given a request in realtime, it can check the current TF, and DF, and calculate the TF-IDF. All these three values are useful for us. TF - If TF is greater than a threshold setting. We can it is may be a high qps scraping. If TF is moderate or very small, we can check DF. DF - If DF is very large, the \"term\" maybe common to all sample in the space. If DF streak is not long enough in a series of previous documents. there might be a spike of increase for the term in some window before. If DF is very small, and the TF is large, we detected a spike. # TF DF result 1 large small bad v 2 large large bad/good x 3 small large bad/good x 4 small small good x TF and IDF used separately can help to observe bad signals, but they are not enough. We will need to use TF-IDF to help us to enhance our believe about the badness of a request. TF-IDF","title":"Intuitions"},{"location":"research/tfidf-score/notes/#practical-consideration","text":"How to calculation TF-IDF in sliding windows? title: HTTP requests embedding to score and search bot traffic.","title":"Practical consideration"},{"location":"seedlabs/public-key-cryptography-and-pki/notes/","text":"Public-Key Cryptography and PKI \u00b6 Questions to answer \u00b6 What is a Certificate Authority (CA), and how to become one? How to create a certificate for a customer? (i.e. www.ruihan.org) How PKI is used for web site authentication and encryption? What are the necessary files of such an infrastructure? 0x1 Become a Certificate Authority (CA) \u00b6 CA is a trusted entity to issue digital certificates. The digital certificate certifies the ownership of a public key by the named subject of the certificate. Root CAs are a number of trusted commercial CAs. (VeriSign is one of the largest) However, we are not obligate to the commercial world, we are free to become a root CA, and then use this root CA to issue certificates to others. Remember that the root CAs are self-signed. root CA's certificates are pre-loaded into most OS, browsers, and other software that using PKI. Root CA's certificates are unconditionally trusted. To create a root CA, we use the following openssl command to generate a self-signed certificate for the CA. seed@ubuntu:~/SEED-Labs/PKI-Lab$ openssl req -new -x509 -keyout ca.key -out ca.crt -config openssl.cnf Generating a 1024 bit RSA private key ..........................++++++ ..++++++ unable to write 'random state' writing new private key to 'ca.key' Enter PEM pass phrase: Verifying - Enter PEM pass phrase: ----- You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter '.', the field will be left blank. ----- Country Name (2 letter code) [AU]:US State or Province Name (full name) [Some-State]:CA Locality Name (eg, city) []:San Jose Organization Name (eg, company) [Internet Widgits Pty Ltd]:PKILabServer Organizational Unit Name (eg, section) []: Common Name (e.g. server FQDN or YOUR name) []:PKILabServer.com Email Address []:rui@PKILabServer.com [11/17/2016 11:14] seed@ubuntu:~/SEED-Labs/PKI-Lab$ The Configure File openssl.conf is used by three OpenSSL command: ca , req , and x509 . To get a glimpse of it's purporse, here is partial of the openssl.conf (from [CA_Default} section). dir = ./demoCA # Where everything is kept certs = $dir/certs # Where the issued certs are kept crl_dir = $dir/crl # Where the issued crl are kept new_certs_dir = $dir/newcerts # default place for new certs database = $dir/index.txt # database index file serial = $dir/serial # The current serial number 0x2 Create a Certificate for PKILabServer.com \u00b6 When one becomes a certificate autority(CA), he/she is ready to sign digital certificates for his/her customers. PKILabServer.com is a company that tries to get a digital certificate from a CA. He needs to do the following steps: Generate RSA key pairs Generate a certificate signing request (CSR) Generate a certificate 0x21 Generate RSA Key Paris \u00b6 This step could be done using OpenSSL command line tool like this: seed@ubuntu:~/SEED-Labs/PKI-Lab$ openssl genrsa -aes128 -out server.key 1024 Generating RSA private key, 1024 bit long modulus ....++++++ .............++++++ unable to write 'random state' e is 65537 (0x10001) Enter pass phrase for server.key: Verifying - Enter pass phrase for server.key: [11/17/2016 11:16] seed@ubuntu:~/SEED-Labs/PKI-Lab$ The generated keys will be stored in the file server.key . This file is encrypted as indicated by the option -ase128 , which means using AES-128 encrytpion algorithm. The effect of this is you have to enter a password when executing the above command. To see the plan text content of the server.key file, you can run the following command: seed@ubuntu:~/SEED-Labs/PKI-Lab$ openssl rsa -in server.key -text Enter pass phrase for server.key: Private-Key: (1024 bit) modulus: 00:e1:23:f9:75:2d:2c:d9:21:36:6f:62:d6:28:6a: 2c:c6:1a:3f:d0:77:c2:8c:e0:49:1a:95:b8:99:6d: 7f:15:cb:db:9c:42:6e:a9:c7:fd:af:cd:94:3e:b5: d3:48:1a:14:11:ca:a2:63:d6:27:32:bc:db:75:d7: 5e:05:9a:8c:2b:7d:0d:fc:6f:61:77:69:c3:37:6b: 94:a3:d0:5b:70:0d:80:9a:89:ea:10:04:cd:6f:4a: 72:b9:e7:d6:1b:fa:ff:25:a1:10:0e:05:d5:14:c7: 34:97:78:55:0b:c7:ed:c4:16:7c:0b:fa:df:46:dd: 7f:35:00:7f:72:21:02:38:51 publicExponent: 65537 (0x10001) privateExponent: 00:bb:06:91:72:1f:c7:03:d0:ad:51:b6:7f:45:2f: 7e:66:4a:e3:fc:1f:a0:84:3a:c5:3f:ca:64:81:40: 0e:b4:62:96:da:a5:ed:1c:29:40:ba:cc:42:7c:1d: bb:98:47:e1:a8:2d:f6:dc:8c:c8:1f:43:f7:e3:5a: b7:86:5c:2e:57:5a:ea:fa:8b:48:28:6a:4c:35:6b: fe:80:83:1f:9b:44:80:73:98:62:e2:bc:7e:e1:f0: 15:dd:74:dc:05:e0:5d:a5:59:a4:71:5f:96:13:e5: 8f:a6:4c:c0:31:50:ca:b1:b9:e4:92:de:e1:e9:2a: 2b:3a:2e:07:1c:fb:dc:ac:01 prime1: 00:f9:bf:6b:f0:28:15:b5:ba:2e:5f:c1:62:63:d7: 53:b0:ab:9c:3f:53:e1:9b:76:1c:12:fb:b0:61:c4: 34:fa:28:d7:94:d2:d3:d9:fe:e5:84:57:60:77:23: af:82:75:50:d6:0c:2e:2e:23:55:82:8c:6f:5c:b4: 5a:2f:77:82:b1 prime2: 00:e6:c6:d9:bf:57:3b:a4:1e:b4:62:c9:fd:5f:e9: d8:cc:d7:49:9e:13:db:99:2c:99:2e:7e:78:0a:c2: b1:b4:c4:2f:08:fc:76:89:e2:60:12:7f:b7:47:bb: d2:98:1e:03:99:42:d7:ec:ab:0d:55:52:95:5e:b8: 77:c5:55:37:a1 exponent1: 63:09:e2:fe:f0:96:73:63:6a:a2:74:68:d5:18:fd: ca:30:b3:9c:75:62:21:29:3c:46:d6:e2:82:52:b1: 83:86:90:bf:26:bf:f6:51:db:a8:98:91:db:8d:1e: 3b:88:d4:4e:9d:b3:ee:7d:fa:99:f3:a0:f1:cd:5f: 7a:35:55:71 exponent2: 00:b9:05:4e:48:80:a8:b9:71:40:90:3f:7f:5b:a9: 81:7e:e1:50:0d:63:c0:58:f3:0f:b0:de:06:62:22: 2c:15:50:80:a1:44:bf:c8:d6:6c:ce:08:05:2a:86: e4:55:bf:22:85:7a:b8:e0:ef:56:d6:44:4a:ab:51: 5b:fd:22:d0:61 coefficient: 3d:ec:20:c8:23:3e:d9:f3:88:8f:03:9d:1b:57:c8: 9d:87:14:83:a2:a8:2c:bc:cc:e7:dd:2d:c4:ef:74: 2a:21:2b:5a:ef:fb:79:49:f0:bc:6f:4e:d3:0e:f7: d2:48:af:b1:12:ae:43:e6:1b:03:bb:f2:18:f1:61: 8b:7a:1e:8f writing RSA key -----BEGIN RSA PRIVATE KEY----- MIICXQIBAAKBgQDhI/l1LSzZITZvYtYoaizGGj/Qd8KM4EkalbiZbX8Vy9ucQm6p x/2vzZQ+tdNIGhQRyqJj1icyvNt1114FmowrfQ38b2F3acM3a5Sj0FtwDYCaieoQ BM1vSnK559Yb+v8loRAOBdUUxzSXeFULx+3EFnwL+t9G3X81AH9yIQI4UQIDAQAB AoGBALsGkXIfxwPQrVG2f0UvfmZK4/wfoIQ6xT/KZIFADrRiltql7RwpQLrMQnwd u5hH4agt9tyMyB9D9+Nat4ZcLlda6vqLSChqTDVr/oCDH5tEgHOYYuK8fuHwFd10 3AXgXaVZpHFflhPlj6ZMwDFQyrG55JLe4ekqKzouBxz73KwBAkEA+b9r8CgVtbou X8FiY9dTsKucP1Phm3YcEvuwYcQ0+ijXlNLT2f7lhFdgdyOvgnVQ1gwuLiNVgoxv XLRaL3eCsQJBAObG2b9XO6QetGLJ/V/p2MzXSZ4T25ksmS5+eArCsbTELwj8doni YBJ/t0e70pgeA5lC1+yrDVVSlV64d8VVN6ECQGMJ4v7wlnNjaqJ0aNUY/cows5x1 YiEpPEbW4oJSsYOGkL8mv/ZR26iYkduNHjuI1E6ds+59+pnzoPHNX3o1VXECQQC5 BU5IgKi5cUCQP39bqYF+4VANY8BY8w+w3gZiIiwVUIChRL/I1mzOCAUqhuRVvyKF erjg71bWREqrUVv9ItBhAkA97CDIIz7Z84iPA50bV8idhxSDoqgsvMzn3S3E73Qq ISta7/t5SfC8b07TDvfSSK+xEq5D5hsDu/IY8WGLeh6P -----END RSA PRIVATE KEY----- [11/17/2016 11:57] seed@ubuntu:~/SEED-Labs/PKI-Lab$ The password you entered works like a key to decrypt the ciphertext in the file. 0x22 Generate a certificate signing request (CSR) \u00b6 To request a certificate, the company need to include its public key in a certificate signing request (CSR) and send the CSR to the certificate authority. The command to generate the CSR using OpenSSL is: seed@ubuntu:~/SEED-Labs/PKI-Lab$ openssl req -new -key server.key -out server.csr -config openssl.cnf Enter pass phrase for server.key: You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter '.', the field will be left blank. ----- Country Name (2 letter code) [AU]:US State or Province Name (full name) [Some-State]:CA Locality Name (eg, city) []:San Jose Organization Name (eg, company) [Internet Widgits Pty Ltd]:PKILabServer Organizational Unit Name (eg, section) []: Common Name (e.g. server FQDN or YOUR name) []:PKILabServer.com Email Address []:admin@PKILabServer.com Please enter the following 'extra' attributes to be sent with your certificate request A challenge password []:server.csr An optional company name []: [11/17/2016 11:19] seed@ubuntu:~/SEED-Labs/PKI-Lab$ Note that this command is quite similar to the one we used in creating a self-signed certificate for the certificate authority previously. The only difference is the -x509 option. 0x23 Generate a certificate \u00b6 The CSR file needs to have the CA's signature to form a certificate. When received the CSR file, CA will first verify the identity information in the CSR. In reality, CA might contact the company through physical channel to do the verification. For example, through traditional mails or letters. For this experiment, we can assume we are the CA and will generate the certificate for the company. The following command turns the certificate signing request ( server.csr ) into an X509 certificate ( server.crt ), using the CA's ca.crt and ca.key : seed@ubuntu:~/SEED-Labs/PKI-Lab$ openssl ca -in server.csr -out server.crt -cert ca.crt -keyfile ca.key -config openssl.cnf Using configuration from openssl.cnf Enter pass phrase for ca.key: Check that the request matches the signature Signature ok Certificate Details: Serial Number: 4096 (0x1000) Validity Not Before: Nov 17 19:21:33 2016 GMT Not After : Nov 17 19:21:33 2017 GMT Subject: countryName = US stateOrProvinceName = CA localityName = San Jose organizationName = PKILabServer commonName = PKILabServer.com emailAddress = admin@PKILabServer.com X509v3 extensions: X509v3 Basic Constraints: CA:FALSE Netscape Comment: OpenSSL Generated Certificate X509v3 Subject Key Identifier: B4:F1:39:2A:56:C6:1F:1F:9C:49:AF:0B:45:FC:FD:79:66:90:F2:07 X509v3 Authority Key Identifier: keyid:1D:11:39:21:1C:E2:0D:8B:14:34:B8:36:C4:F5:34:27:09:9D:E8:7A Certificate is to be certified until Nov 17 19:21:33 2017 GMT (365 days) Sign the certificate? [y/n]:y 1 out of 1 certificate requests certified, commit? [y/n]y Write out database with 1 new entries Data Base Updated unable to write 'random state' [11/17/2016 11:22] seed@ubuntu:~/SEED-Labs/PKI-Lab$ If OpenSSL refuses to generate certificates, it is very likely that the names in your requests do not match with those of CA. (yes, think about you are creating a root CA for your company, and use it to sign certificates for different departments.) The matching rules are specified in the configuration file (at the [policy match] section). You can change the names of your requests to comply with the policy, or you can change the policy. The configuration file also includes another policy (called policy_anything ), which is less restrictive. You can choose that policy by changing the following line: \"policy = policy_match\" change to \"policy = policy_anything\". 0x3 Use PKI for Web Sites \u00b6 Following the lab manual didn't work out. While adding the ca.crt to Firefox, I run into the following error that the format isn't supported by Firefox. Failed to decode the file. Either it is not in PKCS #12 format, has been corrupted, or the password you entered was incorrect. Find this stackoverflow post . We should use the following command to combine the server.crt with the ca.crt . openssl pkcs12 -export -in server.crt -inkey server.key -out PKILabServerFirefox.pfx -certfile ca.crt Finally, we can import PKILabServerFirefox.pfx to Firefox and right click the certificate and select \"Edit Trust->this certificate can identify websites\" 0x4 Test the certificate \u00b6 start a openssl simple webserver: seed@ubuntu:~/SEED-Labs/PKI-Lab$ cp server.key server.pem seed@ubuntu:~/SEED-Labs/PKI-Lab$ cat server.crt >> server.pem seed@ubuntu:~/SEED-Labs/PKI-Lab$ openssl s_server -cert server.pem -www -accpet 6666 Now navigate to https://www.pkilabserver.com:6666 , the security verification is passed and show the openssl web server default page. s_server -cert server.pem -www -accept 3633 Ciphers supported in s_server binary TLSv1/SSLv3:ECDHE-RSA-AES256-GCM-SHA384TLSv1/SSLv3:ECDHE-ECDSA-AES256-GCM-SHA384 TLSv1/SSLv3:ECDHE-RSA-AES256-SHA384 TLSv1/SSLv3:ECDHE-ECDSA-AES256-SHA384 TLSv1/SSLv3:ECDHE-RSA-AES256-SHA TLSv1/SSLv3:ECDHE-ECDSA-AES256-SHA TLSv1/SSLv3:SRP-DSS-AES-256-CBC-SHA TLSv1/SSLv3:SRP-RSA-AES-256-CBC-SHA TLSv1/SSLv3:SRP-AES-256-CBC-SHA TLSv1/SSLv3:DHE-DSS-AES256-GCM-SHA384 TLSv1/SSLv3:DHE-RSA-AES256-GCM-SHA384TLSv1/SSLv3:DHE-RSA-AES256-SHA256 TLSv1/SSLv3:DHE-DSS-AES256-SHA256 TLSv1/SSLv3:DHE-RSA-AES256-SHA TLSv1/SSLv3:DHE-DSS-AES256-SHA TLSv1/SSLv3:DHE-RSA-CAMELLIA256-SHA TLSv1/SSLv3:DHE-DSS-CAMELLIA256-SHA TLSv1/SSLv3:ECDH-RSA-AES256-GCM-SHA384 TLSv1/SSLv3:ECDH-ECDSA-AES256-GCM-SHA384TLSv1/SSLv3:ECDH-RSA-AES256-SHA384 TLSv1/SSLv3:ECDH-ECDSA-AES256-SHA384 TLSv1/SSLv3:ECDH-RSA-AES256-SHA TLSv1/SSLv3:ECDH-ECDSA-AES256-SHA TLSv1/SSLv3:AES256-GCM-SHA384 TLSv1/SSLv3:AES256-SHA256 TLSv1/SSLv3:AES256-SHA TLSv1/SSLv3:CAMELLIA256-SHA TLSv1/SSLv3:PSK-AES256-CBC-SHA TLSv1/SSLv3:ECDHE-RSA-DES-CBC3-SHA TLSv1/SSLv3:ECDHE-ECDSA-DES-CBC3-SHA TLSv1/SSLv3:SRP-DSS-3DES-EDE-CBC-SHA TLSv1/SSLv3:SRP-RSA-3DES-EDE-CBC-SHA TLSv1/SSLv3:SRP-3DES-EDE-CBC-SHA TLSv1/SSLv3:EDH-RSA-DES-CBC3-SHA TLSv1/SSLv3:EDH-DSS-DES-CBC3-SHA TLSv1/SSLv3:ECDH-RSA-DES-CBC3-SHA TLSv1/SSLv3:ECDH-ECDSA-DES-CBC3-SHA TLSv1/SSLv3:DES-CBC3-SHA TLSv1/SSLv3:PSK-3DES-EDE-CBC-SHA TLSv1/SSLv3:ECDHE-RSA-AES128-GCM-SHA256 TLSv1/SSLv3:ECDHE-ECDSA-AES128-GCM-SHA256TLSv1/SSLv3:ECDHE-RSA-AES128-SHA256 TLSv1/SSLv3:ECDHE-ECDSA-AES128-SHA256TLSv1/SSLv3:ECDHE-RSA-AES128-SHA TLSv1/SSLv3:ECDHE-ECDSA-AES128-SHA TLSv1/SSLv3:SRP-DSS-AES-128-CBC-SHA TLSv1/SSLv3:SRP-RSA-AES-128-CBC-SHA TLSv1/SSLv3:SRP-AES-128-CBC-SHA TLSv1/SSLv3:DHE-DSS-AES128-GCM-SHA256TLSv1/SSLv3:DHE-RSA-AES128-GCM-SHA256 TLSv1/SSLv3:DHE-RSA-AES128-SHA256 TLSv1/SSLv3:DHE-DSS-AES128-SHA256 TLSv1/SSLv3:DHE-RSA-AES128-SHA TLSv1/SSLv3:DHE-DSS-AES128-SHA TLSv1/SSLv3:DHE-RSA-SEED-SHA TLSv1/SSLv3:DHE-DSS-SEED-SHA TLSv1/SSLv3:DHE-RSA-CAMELLIA128-SHA TLSv1/SSLv3:DHE-DSS-CAMELLIA128-SHA TLSv1/SSLv3:ECDH-RSA-AES128-GCM-SHA256TLSv1/SSLv3:ECDH-ECDSA-AES128-GCM-SHA256 TLSv1/SSLv3:ECDH-RSA-AES128-SHA256 TLSv1/SSLv3:ECDH-ECDSA-AES128-SHA256 TLSv1/SSLv3:ECDH-RSA-AES128-SHA TLSv1/SSLv3:ECDH-ECDSA-AES128-SHA TLSv1/SSLv3:AES128-GCM-SHA256 TLSv1/SSLv3:AES128-SHA256 TLSv1/SSLv3:AES128-SHA TLSv1/SSLv3:SEED-SHA TLSv1/SSLv3:CAMELLIA128-SHA TLSv1/SSLv3:PSK-AES128-CBC-SHA TLSv1/SSLv3:ECDHE-RSA-RC4-SHA TLSv1/SSLv3:ECDHE-ECDSA-RC4-SHA TLSv1/SSLv3:ECDH-RSA-RC4-SHA TLSv1/SSLv3:ECDH-ECDSA-RC4-SHA TLSv1/SSLv3:RC4-SHA TLSv1/SSLv3:RC4-MD5 TLSv1/SSLv3:PSK-RC4-SHA TLSv1/SSLv3:EDH-RSA-DES-CBC-SHA TLSv1/SSLv3:EDH-DSS-DES-CBC-SHA TLSv1/SSLv3:DES-CBC-SHA --- Reused, TLSv1/SSLv3, Cipher is ECDHE-RSA-AES256-SHA SSL-Session: Protocol : TLSv1 Cipher : ECDHE-RSA-AES256-SHA Session-ID: 124BFE9E98B4DF8E5DC0CC87DEB16DDE615FEC0B63A5FD687CD7033012639DD1 Session-ID-ctx: 01000000 Master-Key: A23B112D5A56B98709D5C140EEC5876155C6023CF8D5DCD81858B1EFF3B4E9CE8723E2D9703C217D8C517A762820B32C Key-Arg : None PSK identity: None PSK identity hint: None SRP username: None Start Time: 1479414975 Timeout : 300 (sec) Verify return code: 0 (ok) --- 0 items in the session cache 0 client connects (SSL_connect()) 0 client renegotiates (SSL_connect()) 0 client connects that finished 16 server accepts (SSL_accept()) 0 server renegotiates (SSL_accept()) 16 server accepts that finished 3 session cache hits 0 session cache misses 0 session cache timeouts 0 callback cache hits 0 cache full overflows (128 allowed) --- no client certificate available 0x5 RSA and AES encryption performace \u00b6 We use the following commands to evaluate RSA encryption and decryption: # generate private key openssl genrsa -out private-rsa.pem 1024 # generate public key openssl genrsa -in private-rsa.pem -pubout -out public-rsa.pem # encrypt using public key openssl rsautl -encrypt -pubin -inkey public-rsa.pem -in message.txt -out message_enc.txt # decrypt using private key openssl rsautl -decrypt -inkey public-rsa.pem -in message_enc.txt -out message_dec.txt Commands for AES encrytption and decryption TODO","title":"Public Key Cryptography and PKI"},{"location":"seedlabs/public-key-cryptography-and-pki/notes/#public-key-cryptography-and-pki","text":"","title":"Public-Key Cryptography and PKI"},{"location":"seedlabs/public-key-cryptography-and-pki/notes/#questions-to-answer","text":"What is a Certificate Authority (CA), and how to become one? How to create a certificate for a customer? (i.e. www.ruihan.org) How PKI is used for web site authentication and encryption? What are the necessary files of such an infrastructure?","title":"Questions to answer"},{"location":"seedlabs/public-key-cryptography-and-pki/notes/#0x1-become-a-certificate-authority-ca","text":"CA is a trusted entity to issue digital certificates. The digital certificate certifies the ownership of a public key by the named subject of the certificate. Root CAs are a number of trusted commercial CAs. (VeriSign is one of the largest) However, we are not obligate to the commercial world, we are free to become a root CA, and then use this root CA to issue certificates to others. Remember that the root CAs are self-signed. root CA's certificates are pre-loaded into most OS, browsers, and other software that using PKI. Root CA's certificates are unconditionally trusted. To create a root CA, we use the following openssl command to generate a self-signed certificate for the CA. seed@ubuntu:~/SEED-Labs/PKI-Lab$ openssl req -new -x509 -keyout ca.key -out ca.crt -config openssl.cnf Generating a 1024 bit RSA private key ..........................++++++ ..++++++ unable to write 'random state' writing new private key to 'ca.key' Enter PEM pass phrase: Verifying - Enter PEM pass phrase: ----- You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter '.', the field will be left blank. ----- Country Name (2 letter code) [AU]:US State or Province Name (full name) [Some-State]:CA Locality Name (eg, city) []:San Jose Organization Name (eg, company) [Internet Widgits Pty Ltd]:PKILabServer Organizational Unit Name (eg, section) []: Common Name (e.g. server FQDN or YOUR name) []:PKILabServer.com Email Address []:rui@PKILabServer.com [11/17/2016 11:14] seed@ubuntu:~/SEED-Labs/PKI-Lab$ The Configure File openssl.conf is used by three OpenSSL command: ca , req , and x509 . To get a glimpse of it's purporse, here is partial of the openssl.conf (from [CA_Default} section). dir = ./demoCA # Where everything is kept certs = $dir/certs # Where the issued certs are kept crl_dir = $dir/crl # Where the issued crl are kept new_certs_dir = $dir/newcerts # default place for new certs database = $dir/index.txt # database index file serial = $dir/serial # The current serial number","title":"0x1 Become a Certificate Authority (CA)"},{"location":"seedlabs/public-key-cryptography-and-pki/notes/#0x2-create-a-certificate-for-pkilabservercom","text":"When one becomes a certificate autority(CA), he/she is ready to sign digital certificates for his/her customers. PKILabServer.com is a company that tries to get a digital certificate from a CA. He needs to do the following steps: Generate RSA key pairs Generate a certificate signing request (CSR) Generate a certificate","title":"0x2 Create a Certificate for PKILabServer.com"},{"location":"seedlabs/public-key-cryptography-and-pki/notes/#0x21-generate-rsa-key-paris","text":"This step could be done using OpenSSL command line tool like this: seed@ubuntu:~/SEED-Labs/PKI-Lab$ openssl genrsa -aes128 -out server.key 1024 Generating RSA private key, 1024 bit long modulus ....++++++ .............++++++ unable to write 'random state' e is 65537 (0x10001) Enter pass phrase for server.key: Verifying - Enter pass phrase for server.key: [11/17/2016 11:16] seed@ubuntu:~/SEED-Labs/PKI-Lab$ The generated keys will be stored in the file server.key . This file is encrypted as indicated by the option -ase128 , which means using AES-128 encrytpion algorithm. The effect of this is you have to enter a password when executing the above command. To see the plan text content of the server.key file, you can run the following command: seed@ubuntu:~/SEED-Labs/PKI-Lab$ openssl rsa -in server.key -text Enter pass phrase for server.key: Private-Key: (1024 bit) modulus: 00:e1:23:f9:75:2d:2c:d9:21:36:6f:62:d6:28:6a: 2c:c6:1a:3f:d0:77:c2:8c:e0:49:1a:95:b8:99:6d: 7f:15:cb:db:9c:42:6e:a9:c7:fd:af:cd:94:3e:b5: d3:48:1a:14:11:ca:a2:63:d6:27:32:bc:db:75:d7: 5e:05:9a:8c:2b:7d:0d:fc:6f:61:77:69:c3:37:6b: 94:a3:d0:5b:70:0d:80:9a:89:ea:10:04:cd:6f:4a: 72:b9:e7:d6:1b:fa:ff:25:a1:10:0e:05:d5:14:c7: 34:97:78:55:0b:c7:ed:c4:16:7c:0b:fa:df:46:dd: 7f:35:00:7f:72:21:02:38:51 publicExponent: 65537 (0x10001) privateExponent: 00:bb:06:91:72:1f:c7:03:d0:ad:51:b6:7f:45:2f: 7e:66:4a:e3:fc:1f:a0:84:3a:c5:3f:ca:64:81:40: 0e:b4:62:96:da:a5:ed:1c:29:40:ba:cc:42:7c:1d: bb:98:47:e1:a8:2d:f6:dc:8c:c8:1f:43:f7:e3:5a: b7:86:5c:2e:57:5a:ea:fa:8b:48:28:6a:4c:35:6b: fe:80:83:1f:9b:44:80:73:98:62:e2:bc:7e:e1:f0: 15:dd:74:dc:05:e0:5d:a5:59:a4:71:5f:96:13:e5: 8f:a6:4c:c0:31:50:ca:b1:b9:e4:92:de:e1:e9:2a: 2b:3a:2e:07:1c:fb:dc:ac:01 prime1: 00:f9:bf:6b:f0:28:15:b5:ba:2e:5f:c1:62:63:d7: 53:b0:ab:9c:3f:53:e1:9b:76:1c:12:fb:b0:61:c4: 34:fa:28:d7:94:d2:d3:d9:fe:e5:84:57:60:77:23: af:82:75:50:d6:0c:2e:2e:23:55:82:8c:6f:5c:b4: 5a:2f:77:82:b1 prime2: 00:e6:c6:d9:bf:57:3b:a4:1e:b4:62:c9:fd:5f:e9: d8:cc:d7:49:9e:13:db:99:2c:99:2e:7e:78:0a:c2: b1:b4:c4:2f:08:fc:76:89:e2:60:12:7f:b7:47:bb: d2:98:1e:03:99:42:d7:ec:ab:0d:55:52:95:5e:b8: 77:c5:55:37:a1 exponent1: 63:09:e2:fe:f0:96:73:63:6a:a2:74:68:d5:18:fd: ca:30:b3:9c:75:62:21:29:3c:46:d6:e2:82:52:b1: 83:86:90:bf:26:bf:f6:51:db:a8:98:91:db:8d:1e: 3b:88:d4:4e:9d:b3:ee:7d:fa:99:f3:a0:f1:cd:5f: 7a:35:55:71 exponent2: 00:b9:05:4e:48:80:a8:b9:71:40:90:3f:7f:5b:a9: 81:7e:e1:50:0d:63:c0:58:f3:0f:b0:de:06:62:22: 2c:15:50:80:a1:44:bf:c8:d6:6c:ce:08:05:2a:86: e4:55:bf:22:85:7a:b8:e0:ef:56:d6:44:4a:ab:51: 5b:fd:22:d0:61 coefficient: 3d:ec:20:c8:23:3e:d9:f3:88:8f:03:9d:1b:57:c8: 9d:87:14:83:a2:a8:2c:bc:cc:e7:dd:2d:c4:ef:74: 2a:21:2b:5a:ef:fb:79:49:f0:bc:6f:4e:d3:0e:f7: d2:48:af:b1:12:ae:43:e6:1b:03:bb:f2:18:f1:61: 8b:7a:1e:8f writing RSA key -----BEGIN RSA PRIVATE KEY----- MIICXQIBAAKBgQDhI/l1LSzZITZvYtYoaizGGj/Qd8KM4EkalbiZbX8Vy9ucQm6p x/2vzZQ+tdNIGhQRyqJj1icyvNt1114FmowrfQ38b2F3acM3a5Sj0FtwDYCaieoQ BM1vSnK559Yb+v8loRAOBdUUxzSXeFULx+3EFnwL+t9G3X81AH9yIQI4UQIDAQAB AoGBALsGkXIfxwPQrVG2f0UvfmZK4/wfoIQ6xT/KZIFADrRiltql7RwpQLrMQnwd u5hH4agt9tyMyB9D9+Nat4ZcLlda6vqLSChqTDVr/oCDH5tEgHOYYuK8fuHwFd10 3AXgXaVZpHFflhPlj6ZMwDFQyrG55JLe4ekqKzouBxz73KwBAkEA+b9r8CgVtbou X8FiY9dTsKucP1Phm3YcEvuwYcQ0+ijXlNLT2f7lhFdgdyOvgnVQ1gwuLiNVgoxv XLRaL3eCsQJBAObG2b9XO6QetGLJ/V/p2MzXSZ4T25ksmS5+eArCsbTELwj8doni YBJ/t0e70pgeA5lC1+yrDVVSlV64d8VVN6ECQGMJ4v7wlnNjaqJ0aNUY/cows5x1 YiEpPEbW4oJSsYOGkL8mv/ZR26iYkduNHjuI1E6ds+59+pnzoPHNX3o1VXECQQC5 BU5IgKi5cUCQP39bqYF+4VANY8BY8w+w3gZiIiwVUIChRL/I1mzOCAUqhuRVvyKF erjg71bWREqrUVv9ItBhAkA97CDIIz7Z84iPA50bV8idhxSDoqgsvMzn3S3E73Qq ISta7/t5SfC8b07TDvfSSK+xEq5D5hsDu/IY8WGLeh6P -----END RSA PRIVATE KEY----- [11/17/2016 11:57] seed@ubuntu:~/SEED-Labs/PKI-Lab$ The password you entered works like a key to decrypt the ciphertext in the file.","title":"0x21 Generate RSA Key Paris"},{"location":"seedlabs/public-key-cryptography-and-pki/notes/#0x22-generate-a-certificate-signing-request-csr","text":"To request a certificate, the company need to include its public key in a certificate signing request (CSR) and send the CSR to the certificate authority. The command to generate the CSR using OpenSSL is: seed@ubuntu:~/SEED-Labs/PKI-Lab$ openssl req -new -key server.key -out server.csr -config openssl.cnf Enter pass phrase for server.key: You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter '.', the field will be left blank. ----- Country Name (2 letter code) [AU]:US State or Province Name (full name) [Some-State]:CA Locality Name (eg, city) []:San Jose Organization Name (eg, company) [Internet Widgits Pty Ltd]:PKILabServer Organizational Unit Name (eg, section) []: Common Name (e.g. server FQDN or YOUR name) []:PKILabServer.com Email Address []:admin@PKILabServer.com Please enter the following 'extra' attributes to be sent with your certificate request A challenge password []:server.csr An optional company name []: [11/17/2016 11:19] seed@ubuntu:~/SEED-Labs/PKI-Lab$ Note that this command is quite similar to the one we used in creating a self-signed certificate for the certificate authority previously. The only difference is the -x509 option.","title":"0x22 Generate a certificate signing request (CSR)"},{"location":"seedlabs/public-key-cryptography-and-pki/notes/#0x23-generate-a-certificate","text":"The CSR file needs to have the CA's signature to form a certificate. When received the CSR file, CA will first verify the identity information in the CSR. In reality, CA might contact the company through physical channel to do the verification. For example, through traditional mails or letters. For this experiment, we can assume we are the CA and will generate the certificate for the company. The following command turns the certificate signing request ( server.csr ) into an X509 certificate ( server.crt ), using the CA's ca.crt and ca.key : seed@ubuntu:~/SEED-Labs/PKI-Lab$ openssl ca -in server.csr -out server.crt -cert ca.crt -keyfile ca.key -config openssl.cnf Using configuration from openssl.cnf Enter pass phrase for ca.key: Check that the request matches the signature Signature ok Certificate Details: Serial Number: 4096 (0x1000) Validity Not Before: Nov 17 19:21:33 2016 GMT Not After : Nov 17 19:21:33 2017 GMT Subject: countryName = US stateOrProvinceName = CA localityName = San Jose organizationName = PKILabServer commonName = PKILabServer.com emailAddress = admin@PKILabServer.com X509v3 extensions: X509v3 Basic Constraints: CA:FALSE Netscape Comment: OpenSSL Generated Certificate X509v3 Subject Key Identifier: B4:F1:39:2A:56:C6:1F:1F:9C:49:AF:0B:45:FC:FD:79:66:90:F2:07 X509v3 Authority Key Identifier: keyid:1D:11:39:21:1C:E2:0D:8B:14:34:B8:36:C4:F5:34:27:09:9D:E8:7A Certificate is to be certified until Nov 17 19:21:33 2017 GMT (365 days) Sign the certificate? [y/n]:y 1 out of 1 certificate requests certified, commit? [y/n]y Write out database with 1 new entries Data Base Updated unable to write 'random state' [11/17/2016 11:22] seed@ubuntu:~/SEED-Labs/PKI-Lab$ If OpenSSL refuses to generate certificates, it is very likely that the names in your requests do not match with those of CA. (yes, think about you are creating a root CA for your company, and use it to sign certificates for different departments.) The matching rules are specified in the configuration file (at the [policy match] section). You can change the names of your requests to comply with the policy, or you can change the policy. The configuration file also includes another policy (called policy_anything ), which is less restrictive. You can choose that policy by changing the following line: \"policy = policy_match\" change to \"policy = policy_anything\".","title":"0x23 Generate a certificate"},{"location":"seedlabs/public-key-cryptography-and-pki/notes/#0x3-use-pki-for-web-sites","text":"Following the lab manual didn't work out. While adding the ca.crt to Firefox, I run into the following error that the format isn't supported by Firefox. Failed to decode the file. Either it is not in PKCS #12 format, has been corrupted, or the password you entered was incorrect. Find this stackoverflow post . We should use the following command to combine the server.crt with the ca.crt . openssl pkcs12 -export -in server.crt -inkey server.key -out PKILabServerFirefox.pfx -certfile ca.crt Finally, we can import PKILabServerFirefox.pfx to Firefox and right click the certificate and select \"Edit Trust->this certificate can identify websites\"","title":"0x3 Use PKI for Web Sites"},{"location":"seedlabs/public-key-cryptography-and-pki/notes/#0x4-test-the-certificate","text":"start a openssl simple webserver: seed@ubuntu:~/SEED-Labs/PKI-Lab$ cp server.key server.pem seed@ubuntu:~/SEED-Labs/PKI-Lab$ cat server.crt >> server.pem seed@ubuntu:~/SEED-Labs/PKI-Lab$ openssl s_server -cert server.pem -www -accpet 6666 Now navigate to https://www.pkilabserver.com:6666 , the security verification is passed and show the openssl web server default page. s_server -cert server.pem -www -accept 3633 Ciphers supported in s_server binary TLSv1/SSLv3:ECDHE-RSA-AES256-GCM-SHA384TLSv1/SSLv3:ECDHE-ECDSA-AES256-GCM-SHA384 TLSv1/SSLv3:ECDHE-RSA-AES256-SHA384 TLSv1/SSLv3:ECDHE-ECDSA-AES256-SHA384 TLSv1/SSLv3:ECDHE-RSA-AES256-SHA TLSv1/SSLv3:ECDHE-ECDSA-AES256-SHA TLSv1/SSLv3:SRP-DSS-AES-256-CBC-SHA TLSv1/SSLv3:SRP-RSA-AES-256-CBC-SHA TLSv1/SSLv3:SRP-AES-256-CBC-SHA TLSv1/SSLv3:DHE-DSS-AES256-GCM-SHA384 TLSv1/SSLv3:DHE-RSA-AES256-GCM-SHA384TLSv1/SSLv3:DHE-RSA-AES256-SHA256 TLSv1/SSLv3:DHE-DSS-AES256-SHA256 TLSv1/SSLv3:DHE-RSA-AES256-SHA TLSv1/SSLv3:DHE-DSS-AES256-SHA TLSv1/SSLv3:DHE-RSA-CAMELLIA256-SHA TLSv1/SSLv3:DHE-DSS-CAMELLIA256-SHA TLSv1/SSLv3:ECDH-RSA-AES256-GCM-SHA384 TLSv1/SSLv3:ECDH-ECDSA-AES256-GCM-SHA384TLSv1/SSLv3:ECDH-RSA-AES256-SHA384 TLSv1/SSLv3:ECDH-ECDSA-AES256-SHA384 TLSv1/SSLv3:ECDH-RSA-AES256-SHA TLSv1/SSLv3:ECDH-ECDSA-AES256-SHA TLSv1/SSLv3:AES256-GCM-SHA384 TLSv1/SSLv3:AES256-SHA256 TLSv1/SSLv3:AES256-SHA TLSv1/SSLv3:CAMELLIA256-SHA TLSv1/SSLv3:PSK-AES256-CBC-SHA TLSv1/SSLv3:ECDHE-RSA-DES-CBC3-SHA TLSv1/SSLv3:ECDHE-ECDSA-DES-CBC3-SHA TLSv1/SSLv3:SRP-DSS-3DES-EDE-CBC-SHA TLSv1/SSLv3:SRP-RSA-3DES-EDE-CBC-SHA TLSv1/SSLv3:SRP-3DES-EDE-CBC-SHA TLSv1/SSLv3:EDH-RSA-DES-CBC3-SHA TLSv1/SSLv3:EDH-DSS-DES-CBC3-SHA TLSv1/SSLv3:ECDH-RSA-DES-CBC3-SHA TLSv1/SSLv3:ECDH-ECDSA-DES-CBC3-SHA TLSv1/SSLv3:DES-CBC3-SHA TLSv1/SSLv3:PSK-3DES-EDE-CBC-SHA TLSv1/SSLv3:ECDHE-RSA-AES128-GCM-SHA256 TLSv1/SSLv3:ECDHE-ECDSA-AES128-GCM-SHA256TLSv1/SSLv3:ECDHE-RSA-AES128-SHA256 TLSv1/SSLv3:ECDHE-ECDSA-AES128-SHA256TLSv1/SSLv3:ECDHE-RSA-AES128-SHA TLSv1/SSLv3:ECDHE-ECDSA-AES128-SHA TLSv1/SSLv3:SRP-DSS-AES-128-CBC-SHA TLSv1/SSLv3:SRP-RSA-AES-128-CBC-SHA TLSv1/SSLv3:SRP-AES-128-CBC-SHA TLSv1/SSLv3:DHE-DSS-AES128-GCM-SHA256TLSv1/SSLv3:DHE-RSA-AES128-GCM-SHA256 TLSv1/SSLv3:DHE-RSA-AES128-SHA256 TLSv1/SSLv3:DHE-DSS-AES128-SHA256 TLSv1/SSLv3:DHE-RSA-AES128-SHA TLSv1/SSLv3:DHE-DSS-AES128-SHA TLSv1/SSLv3:DHE-RSA-SEED-SHA TLSv1/SSLv3:DHE-DSS-SEED-SHA TLSv1/SSLv3:DHE-RSA-CAMELLIA128-SHA TLSv1/SSLv3:DHE-DSS-CAMELLIA128-SHA TLSv1/SSLv3:ECDH-RSA-AES128-GCM-SHA256TLSv1/SSLv3:ECDH-ECDSA-AES128-GCM-SHA256 TLSv1/SSLv3:ECDH-RSA-AES128-SHA256 TLSv1/SSLv3:ECDH-ECDSA-AES128-SHA256 TLSv1/SSLv3:ECDH-RSA-AES128-SHA TLSv1/SSLv3:ECDH-ECDSA-AES128-SHA TLSv1/SSLv3:AES128-GCM-SHA256 TLSv1/SSLv3:AES128-SHA256 TLSv1/SSLv3:AES128-SHA TLSv1/SSLv3:SEED-SHA TLSv1/SSLv3:CAMELLIA128-SHA TLSv1/SSLv3:PSK-AES128-CBC-SHA TLSv1/SSLv3:ECDHE-RSA-RC4-SHA TLSv1/SSLv3:ECDHE-ECDSA-RC4-SHA TLSv1/SSLv3:ECDH-RSA-RC4-SHA TLSv1/SSLv3:ECDH-ECDSA-RC4-SHA TLSv1/SSLv3:RC4-SHA TLSv1/SSLv3:RC4-MD5 TLSv1/SSLv3:PSK-RC4-SHA TLSv1/SSLv3:EDH-RSA-DES-CBC-SHA TLSv1/SSLv3:EDH-DSS-DES-CBC-SHA TLSv1/SSLv3:DES-CBC-SHA --- Reused, TLSv1/SSLv3, Cipher is ECDHE-RSA-AES256-SHA SSL-Session: Protocol : TLSv1 Cipher : ECDHE-RSA-AES256-SHA Session-ID: 124BFE9E98B4DF8E5DC0CC87DEB16DDE615FEC0B63A5FD687CD7033012639DD1 Session-ID-ctx: 01000000 Master-Key: A23B112D5A56B98709D5C140EEC5876155C6023CF8D5DCD81858B1EFF3B4E9CE8723E2D9703C217D8C517A762820B32C Key-Arg : None PSK identity: None PSK identity hint: None SRP username: None Start Time: 1479414975 Timeout : 300 (sec) Verify return code: 0 (ok) --- 0 items in the session cache 0 client connects (SSL_connect()) 0 client renegotiates (SSL_connect()) 0 client connects that finished 16 server accepts (SSL_accept()) 0 server renegotiates (SSL_accept()) 16 server accepts that finished 3 session cache hits 0 session cache misses 0 session cache timeouts 0 callback cache hits 0 cache full overflows (128 allowed) --- no client certificate available","title":"0x4 Test the certificate"},{"location":"seedlabs/public-key-cryptography-and-pki/notes/#0x5-rsa-and-aes-encryption-performace","text":"We use the following commands to evaluate RSA encryption and decryption: # generate private key openssl genrsa -out private-rsa.pem 1024 # generate public key openssl genrsa -in private-rsa.pem -pubout -out public-rsa.pem # encrypt using public key openssl rsautl -encrypt -pubin -inkey public-rsa.pem -in message.txt -out message_enc.txt # decrypt using private key openssl rsautl -decrypt -inkey public-rsa.pem -in message_enc.txt -out message_dec.txt Commands for AES encrytption and decryption TODO","title":"0x5 RSA and AES encryption performace"}]}