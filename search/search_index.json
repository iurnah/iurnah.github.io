{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs \u00b6 For full documentation visit mkdocs.org . Commands \u00b6 mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout \u00b6 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files. import tensorflow as tf \"\"\" Bubble sort \"\"\" def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] > items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ] Bash #!/bin/bash echo \"Hello world!\" C #include <stdio.h> int main ( void ) { printf ( \"Hello world! \\n \" ); } C++ #include <iostream> int main () { std :: cout << \"Hello world!\" << std :: endl ; return 0 ; } C# using System ; class Program { static void Main ( string [] args ) { Console . WriteLine ( \"Hello world!\" ); } }","title":"Welcome to MkDocs"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files. import tensorflow as tf \"\"\" Bubble sort \"\"\" def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] > items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ] Bash #!/bin/bash echo \"Hello world!\" C #include <stdio.h> int main ( void ) { printf ( \"Hello world! \\n \" ); } C++ #include <iostream> int main () { std :: cout << \"Hello world!\" << std :: endl ; return 0 ; } C# using System ; class Program { static void Main ( string [] args ) { Console . WriteLine ( \"Hello world!\" ); } }","title":"Project layout"},{"location":"courses/6.431-probability/notes/","text":"6.041/6.431 Probability - The Science of Uncertainty and Data \u00b6 Lecture 1 (September 10, 2018) \u00b6 Probability Models \u00b6 To specify such a model, it includes 2 steps: Sample space: describe possible outcomes of an experiment Probability laws: describe beliefs about the likelihood of outcomes (or assign a probability to outcomes) Sample space \u00b6 list of possible outcomes must be mutually exclusive, collectively exhaustive, and at the right granularity. Probability laws \u00b6 Probability axioms \u00b6 Motivation: Assign individual probability to a single dot is a challenge (in the dart example, each dot has zero probability), so we assign a probability to the subset of the sample space. Nonnegativity: {\\bf P}(A) \\geq 0 {\\bf P}(A) \\geq 0 Normalization: {\\bf P}(\\Omega) = 1 {\\bf P}(\\Omega) = 1 (Finite) Additivity: if A \\cap B = \\emptyset A \\cap B = \\emptyset then {\\bf P}(A \\cup B) = {\\bf P}(A) + {\\bf P}(B) {\\bf P}(A \\cup B) = {\\bf P}(A) + {\\bf P}(B) (need to be refined in future) Consequences of axioms (properties of probability axioms) \u00b6 {\\bf P}(A) \\leq 1 {\\bf P}(A) \\leq 1 {\\bf P}(\\emptyset) = 0 {\\bf P}(\\emptyset) = 0 {\\bf P}(A) + {\\bf P}(A^c) = 1 {\\bf P}(A) + {\\bf P}(A^c) = 1 A, B, \\text{and } C A, B, \\text{and } C are disjoint: {\\bf P}(A \\cup B \\cup C) = {\\bf P}(A) + {\\bf P}(B) + {\\bf P}(C) {\\bf P}(A \\cup B \\cup C) = {\\bf P}(A) + {\\bf P}(B) + {\\bf P}(C) A_1, \\cdots A_k, A_1, \\cdots A_k, are disjoint: {\\bf P}(A_1 \\cup \\cdots, \\cup A_k) = \\sum_{i - 1}^k{\\bf P}(A_i) {\\bf P}(A_1 \\cup \\cdots, \\cup A_k) = \\sum_{i - 1}^k{\\bf P}(A_i) If A \\subset B A \\subset B , then {\\bf P}(A) \\leq {\\bf P}(B) {\\bf P}(A) \\leq {\\bf P}(B) {\\bf P}(A \\cup B) = {\\bf P}(A) + {\\bf P}(B) - {\\bf P}(A \\cap B) {\\bf P}(A \\cup B) = {\\bf P}(A) + {\\bf P}(B) - {\\bf P}(A \\cap B) {\\bf P}(A \\cup B \\cup C) = {\\bf P}(A) + {\\bf P}(A^c \\cap B) + {\\bf P}(A^c \\cap B^c \\cap C) {\\bf P}(A \\cup B \\cup C) = {\\bf P}(A) + {\\bf P}(A^c \\cap B) + {\\bf P}(A^c \\cap B^c \\cap C) Union Bound: {\\bf P}(A \\cup B) \\leq {\\bf P}(A) + {\\bf P}(B) {\\bf P}(A \\cup B) \\leq {\\bf P}(A) + {\\bf P}(B) Probability calculation steps \u00b6 Specify the sample space Specify a probability law Identify an event of interest Calculate Countable additivity \u00b6 The 3rd probability axiom previously mentioned can be extended to countable set (all integers), which means that the element in the set can be arranged in a sequence. This is contrary to the concept of uncountable set (i.e. a dart board, 2D plane). So you need to distinguish the following concepts: discrete set (countable set) continuous set (uncountable set, impossible to arrange in a sequence) finite set infinite set (possible to be arranged in a sequence) discrete (countable) finite set (i.e. {1, 2, 3}) discrete (countable) infinite set (i.e. all integers, all even integers, all old integers) continuous (uncountable) infinite set (i.e. \\{x | 0 \\leq x \\leq 1\\} \\{x | 0 \\leq x \\leq 1\\} ) Example: Sample space \\{1, 2, 3, \\cdots\\} \\{1, 2, 3, \\cdots\\} , given {\\bf P}(n) = \\frac{1}{2^n}, n = 1, 2, \\cdots {\\bf P}(n) = \\frac{1}{2^n}, n = 1, 2, \\cdots . Check the against the probability axioms that \\sum_{n = 1}^{\\infty}{\\bf P}(n) = 1 \\sum_{n = 1}^{\\infty}{\\bf P}(n) = 1 . What about the probability {\\bf P}(\\text{outcome is even}) {\\bf P}(\\text{outcome is even}) ? Using countable additivity axiom. Countable Additivity Axiom (refined probability axiom) \u00b6 Note if A_1, A_2, A_3, \\cdot A_1, A_2, A_3, \\cdot is an infinite sequence of disjoint events, then {\\bf P}(A_1 \\cup A_2 \\cup A_3 \\cdots) = {\\bf P}(A_1) + {\\bf P}(A_2) + {\\bf P}(A_3) + \\cdots {\\bf P}(A_1 \\cup A_2 \\cup A_3 \\cdots) = {\\bf P}(A_1) + {\\bf P}(A_2) + {\\bf P}(A_3) + \\cdots To refine the 3rd probability axiom, additivity holds only for \"countable\" sequences of events. That means the additivity axiom must be a sequence , with finite or infinite elements. Note the following contradiction, the additivity axiom can not be applied to continuous sample space. A_1, A_2, A_3, \\cdots A_1, A_2, A_3, \\cdots are real coordinates, if you apply the additivity axiom, you will get: \\begin{align*} 1={\\bf P}(\\Omega ) &={\\bf P}\\big (\\{ A_1\\} \\cup \\{ A_2\\} \\cup \\{ A_3\\} \\cdots \\big )\\\\ &={\\bf P}(\\{ A_1\\} )+{\\bf P}(\\{ A_2\\} )+{\\bf P}(\\{ A_3\\} )+\\cdots \\\\ &= 0+0+0+\\cdots =0, \\end{align*} \\begin{align*} 1={\\bf P}(\\Omega ) &={\\bf P}\\big (\\{ A_1\\} \\cup \\{ A_2\\} \\cup \\{ A_3\\} \\cdots \\big )\\\\ &={\\bf P}(\\{ A_1\\} )+{\\bf P}(\\{ A_2\\} )+{\\bf P}(\\{ A_3\\} )+\\cdots \\\\ &= 0+0+0+\\cdots =0, \\end{align*} which is contradicts. This is because \"unit square\" is not a countable set. Interpretations of probability theory \u00b6 frequency of events A? What is P(the president will be reelected)? Probability is often interpreted as: Description of beliefs Betting preference Lecture 2 Conditioning and Baye's rule \u00b6 Conditional probability \u00b6 use new information to review a model Definition {\\bf P}(A|B) = {\\bf P}(A|B) = \"probability of A given that B occurred\" {\\bf P}(A|B) = \\frac{{\\bf P}(A \\cap B)}{{\\bf P}(B)} {\\bf P}(A|B) = \\frac{{\\bf P}(A \\cap B)}{{\\bf P}(B)} , defined only when {\\bf P}(B) > 0 {\\bf P}(B) > 0 Conditional probability share properties of ordinary probabilities (finite and infinite countable additivity) If A \\cap C = \\emptyset A \\cap C = \\emptyset , then {\\bf P}(A \\cup C|B) = P(A|B) + {\\bf P}(C|B) {\\bf P}(A \\cup C|B) = P(A|B) + {\\bf P}(C|B) Multiplication rule \u00b6 {\\bf P}(A \\cap B) = {\\bf P}(A){\\bf P}(B|A) = {\\bf P}(B){\\bf P}(A|B) {\\bf P}(A \\cap B) = {\\bf P}(A){\\bf P}(B|A) = {\\bf P}(B){\\bf P}(A|B) {\\bf P}(A \\cap B \\cap C) = {\\bf P}(A){\\bf P}(B|A){\\bf P}(C|A \\cap B) {\\bf P}(A \\cap B \\cap C) = {\\bf P}(A){\\bf P}(B|A){\\bf P}(C|A \\cap B) It also applies to n events. Total probability theorem (divide and conquer) \u00b6 The settings: Partition of sample space into A_1, A_2, A_3 A_1, A_2, A_3 , have {\\bf P}(A_i) {\\bf P}(A_i) for every i i have {\\bf P}(B|A_i) {\\bf P}(B|A_i) for every i i {\\bf P}(B) = \\sum_{i} {\\bf P}(A_i){\\bf P}(B|A_i) {\\bf P}(B) = \\sum_{i} {\\bf P}(A_i){\\bf P}(B|A_i) It also applies to infinite countable sets according to the countable additivity axiom. Baye's rule \u00b6 The setting is the same as the total probability theorem. Partition of sample space into A_1, A_2, A_3 A_1, A_2, A_3 , have {\\bf P}(A_i) {\\bf P}(A_i) for every i i (initial believes) have {\\bf P}(B|A_i) {\\bf P}(B|A_i) for every i i revise \"believe\" given that B occurred: {\\bf P}(A_i|B) = \\frac{{\\bf P}(A_i \\cap B)}{{\\bf P}(B)} {\\bf P}(A_i|B) = \\frac{{\\bf P}(A_i \\cap B)}{{\\bf P}(B)} It turns out that we can use multiplication rule to calculate the nominator and use the total probability rule to calculate the denominator, thusly {\\bf P}(A_i|B) = \\frac{{\\bf P}(A_i){\\bf P}(B|A_i)}{\\sum_j{\\bf P}(A_j){\\bf P}(B|A_i)} {\\bf P}(A_i|B) = \\frac{{\\bf P}(A_i){\\bf P}(B|A_i)}{\\sum_j{\\bf P}(A_j){\\bf P}(B|A_i)} Lecture 3 Independence \u00b6 Independence of two events \u00b6 Motivation coin toss example: First toss is head or tail doesn't affect the probability of second toss is head. Occurance of A A privides no new information about B B : {\\bf P}(B|A) = {\\bf P}(B) {\\bf P}(B|A) = {\\bf P}(B) . Definition of independence \u00b6 Formal definition {\\bf P}(A \\cap B) = {\\bf P}(A) \\cdot {\\bf P}(B) {\\bf P}(A \\cap B) = {\\bf P}(A) \\cdot {\\bf P}(B) symmetric with respect to events. implies {\\bf P}(A|B) = {\\bf P}(A) {\\bf P}(A|B) = {\\bf P}(A) . Applies even if {\\bf P}(A) = 0 {\\bf P}(A) = 0 . Distinction between disjoin and independence Disjoin usually means dependent, because if one event happens we know the other event will not happen. Independence of event complements \u00b6 If A A and B B are independent, the A A and B^c B^c are independent. If A A and B B are independent, the A^c A^c and B^c B^c are independent. Conditioning independence \u00b6 Ordinary independence properties also apply to conditional independence. But independence doesn't imply conditional independence. Independence of a collection of events \u00b6 Ituition \"definition\": Information on some of the events doesn't change probability related to the remaining events. Formal definition pairwise independence \u00b6 independence V.S. pairwise independence \u00b6 Used two independence fair coin tosses as an example to show that pariwise independence isn't enough to show independence of collection of events. Reliability \u00b6 The king's sibling puzzle Monty Hall problem \u00b6 Lecture 4 \u00b6 Lecture 5 \u00b6 Definition of random variables \u00b6 A random variable (\"r.v.\") associates a value (a number) to every possible outcome. Mathematically, A function from sample space \\Omega \\Omega to the real numbers (discrete or continuous). We can have several random variables defined on the same sample space. (In Lecture 18, X_1, X_2, \\cdots X_1, X_2, \\cdots are independent random variables from the same distribution) A function of one or several random variables is also another random variable. A concrete example to understand the concept of a random variable is take a class of students, selecting students and measuring their body mass or height will give us the random variable W W and H H . The \"Body mass index\" is another random variable that is a function of the random variable W W and H H . Bernoulli and indicator random variables \u00b6 Bernoulli random variable X X : models a trial that results in success/failure, heads/tails, etc. indicator random variables of event A A : I_A = 1 I_A = 1 if A A occurs. connection between events and random variables. P(A) = P(I_A = 1) = P_{I_A}(1) P(A) = P(I_A = 1) = P_{I_A}(1) , P(A) P(A) : probability of event A happens P(I_A = 1) P(I_A = 1) : probablistic notition of indication R.V. when I_A I_A equal to 1 1 P_{I_A}(1) P_{I_A}(1) : PMF notation. Lecture 6 \u00b6 Lecture 7 \u00b6 Independence, variances, and binomial variance \u00b6 Lecture 8 \u00b6 Exponential random variables \u00b6 PDF: f_{X}(x) = \\begin{cases} \\lambda e^{-\\lambda x}, & x \\ge 0 \\\\ 0, & x < 0 \\end{cases} f_{X}(x) = \\begin{cases} \\lambda e^{-\\lambda x}, & x \\ge 0 \\\\ 0, & x < 0 \\end{cases} Probability of greater than a a , \\begin{align*} P(X \\ge a) &= \\displaystyle \\int_a^{+\\infty} \\lambda e^{-\\lambda x}dx \\\\ &= - \\displaystyle \\int_a^{+\\infty} de^{-\\lambda x} \\\\ &= -e^{-\\lambda x}\\Big|_a^{+\\infty} = -e^{-\\lambda \\cdot +\\infty} + e^{-\\lambda a} = e^{-\\lambda a} \\end{align*} \\begin{align*} P(X \\ge a) &= \\displaystyle \\int_a^{+\\infty} \\lambda e^{-\\lambda x}dx \\\\ &= - \\displaystyle \\int_a^{+\\infty} de^{-\\lambda x} \\\\ &= -e^{-\\lambda x}\\Big|_a^{+\\infty} = -e^{-\\lambda \\cdot +\\infty} + e^{-\\lambda a} = e^{-\\lambda a} \\end{align*} Expectation of exponential random variable {\\bf E}\\big[X\\big] = \\displaystyle \\int_0^{\\infty} x \\cdot \\lambda e^{-\\lambda x} dx = 1/\\lambda {\\bf E}\\big[X\\big] = \\displaystyle \\int_0^{\\infty} x \\cdot \\lambda e^{-\\lambda x} dx = 1/\\lambda Second moment of an exponential random variable {\\bf E}\\big[X^2 \\big] = \\displaystyle \\int_0^{\\infty} x^2 \\cdot \\lambda e^{-\\lambda x} dx = 2/\\lambda^2 {\\bf E}\\big[X^2 \\big] = \\displaystyle \\int_0^{\\infty} x^2 \\cdot \\lambda e^{-\\lambda x} dx = 2/\\lambda^2 Variance of an exponential random variable \\textbf{Var}(X) = {\\bf E}\\big[X^2 \\big] - \\Big({\\bf E}\\big[X\\big]\\Big)^2 = 1/\\lambda^2 \\textbf{Var}(X) = {\\bf E}\\big[X^2 \\big] - \\Big({\\bf E}\\big[X\\big]\\Big)^2 = 1/\\lambda^2 Lecture 9 \u00b6 Conditioning a continuous random variable on an event \u00b6 Memorylessness of the exponential PDF \u00b6 Total probability and expectation theorems \u00b6 Lecture 10 \u00b6 Total probability and total expectation theorems \u00b6 Follows from the discrete cases: Rules Discrete continous Total probability p_X(x) = \\displaystyle \\sum\\limits_{y} p_Y(y)p_{X \\mid Y}(x \\mid y) p_X(x) = \\displaystyle \\sum\\limits_{y} p_Y(y)p_{X \\mid Y}(x \\mid y) f_X(x) = \\displaystyle \\int_{-\\infty}^{+\\infty} f_Y(y) f_{X \\mid Y}(x \\mid y) dy f_X(x) = \\displaystyle \\int_{-\\infty}^{+\\infty} f_Y(y) f_{X \\mid Y}(x \\mid y) dy Conditional expectation {\\bf E}\\big[X \\mid Y = y \\big] = \\displaystyle \\sum\\limits_{x} x p_{X \\mid Y}(x \\mid y) {\\bf E}\\big[X \\mid Y = y \\big] = \\displaystyle \\sum\\limits_{x} x p_{X \\mid Y}(x \\mid y) {\\bf E}\\big[X \\mid Y = y \\big] = \\displaystyle {\\int_{-\\infty}^{+\\infty}} x f_{X \\mid Y}(x \\mid y) dx {\\bf E}\\big[X \\mid Y = y \\big] = \\displaystyle {\\int_{-\\infty}^{+\\infty}} x f_{X \\mid Y}(x \\mid y) dx Total expectation {\\bf E}\\big[X \\big] = \\displaystyle \\sum\\limits_{y} p_{Y}(y){\\bf E} \\big[X \\mid Y = y \\big] {\\bf E}\\big[X \\big] = \\displaystyle \\sum\\limits_{y} p_{Y}(y){\\bf E} \\big[X \\mid Y = y \\big] derivation hint: replace with the conditional expectatioin {\\bf E}\\big[X \\big] = \\displaystyle {\\int_{-\\infty}^{+\\infty}} f_{Y}(y){\\bf E}\\big[X \\mid Y = y \\big] dy {\\bf E}\\big[X \\big] = \\displaystyle {\\int_{-\\infty}^{+\\infty}} f_{Y}(y){\\bf E}\\big[X \\mid Y = y \\big] dy derivation hint: replace with the conditional expectatioin Expected value rule {\\bf E}\\big[ g(X) \\mid Y = y \\big] = \\displaystyle {\\sum\\limits_{x}} g(x) p_{X \\mid Y}(x \\mid y) {\\bf E}\\big[ g(X) \\mid Y = y \\big] = \\displaystyle {\\sum\\limits_{x}} g(x) p_{X \\mid Y}(x \\mid y) {\\bf E}\\big[ g(X) \\mid Y = y \\big] = \\displaystyle {\\int_{-\\infty}^{+\\infty}} g(x) f_{X \\mid Y}(x \\mid y) dx {\\bf E}\\big[ g(X) \\mid Y = y \\big] = \\displaystyle {\\int_{-\\infty}^{+\\infty}} g(x) f_{X \\mid Y}(x \\mid y) dx Solved problems (Lecture 8 - 10) \u00b6 10 Buffon's needle and Monte Carlo Simulation \u00b6 This problem is discussed in the text page 161, Example 3.11. Lecture 11 \u00b6 Lecture 12 \u00b6 Covariance \u00b6 Definition: \\textbf{cov}(X, Y) = {\\bf E}\\big[(X - {\\bf E}[X]) (Y - {\\bf E}[Y])\\big] \\textbf{cov}(X, Y) = {\\bf E}\\big[(X - {\\bf E}[X]) (Y - {\\bf E}[Y])\\big] Covariance tell use whether two r.v.s tend to move together, both to low or both to high Two random variables X X and Y Y are independent indicates: \\textbf{cov}(X, Y) = 0 \\textbf{cov}(X, Y) = 0 , but the converse it not true. Covariance properties \u00b6 Lecture 13 \u00b6 The conditional expectation as a random variable {\\bf E} \\big[X \\mid Y\\big] {\\bf E} \\big[X \\mid Y\\big] \u00b6 Given a function h(x) = x^2 h(x) = x^2 for all x x , and a random variable X X . what is h(X) h(X) ? h(X) h(X) is a function of a random variable X X . h(X) h(X) itself is a random variable that take value x^2 x^2 if X X happens to take the value x x . (you should really understand this statement, it is crucial to understand the concept of conditional expectatioin) For discrete case: {\\bf E}\\big[X \\mid Y = y \\big] = \\displaystyle \\sum\\limits_{x} x p_{X \\mid Y}(x \\mid y) {\\bf E}\\big[X \\mid Y = y \\big] = \\displaystyle \\sum\\limits_{x} x p_{X \\mid Y}(x \\mid y) , in which different vaule of y y will give us the different value of conditional expectation of X X . Then we can really treat {\\bf E}\\big[X \\mid Y = y \\big] {\\bf E}\\big[X \\mid Y = y \\big] as a function of y y , noted as g(y) g(y) . From the above reasoning, because Y Y is a random variable, g(Y) g(Y) is also a reandom variable, that takes the value of g(y) = {\\bf E}\\big[X \\mid Y = y \\big] g(y) = {\\bf E}\\big[X \\mid Y = y \\big] if Y Y happens to take the value y y . g(Y) = {\\bf E}\\big[X \\mid Y\\big] g(Y) = {\\bf E}\\big[X \\mid Y\\big] , is a function of Y Y a random variable has a distribution, mean, variance, etc. The law of iterated expectations \u00b6 By calculate {\\bf E}\\Big[{\\bf E}\\big[X \\mid Y\\big]\\Big] {\\bf E}\\Big[{\\bf E}\\big[X \\mid Y\\big]\\Big] , we could abtain that it is equal to {\\bf E}\\big[X\\big] {\\bf E}\\big[X\\big] The conditional variance as a random variable \\textbf{Var}(X \\mid Y = y) \\textbf{Var}(X \\mid Y = y) \u00b6 definition: \\textbf{Var}(X \\mid Y = y) = {\\bf E}\\Big[\\big(X - {\\bf E}\\big[X \\mid Y = y \\big]\\big)^2\\mid Y = y\\Big] \\textbf{Var}(X \\mid Y = y) = {\\bf E}\\Big[\\big(X - {\\bf E}\\big[X \\mid Y = y \\big]\\big)^2\\mid Y = y\\Big] \\textbf{Var}(X\\mid Y) \\textbf{Var}(X\\mid Y) is also a random variable, it takes the value \\textbf{Var}(X\\mid y) \\textbf{Var}(X\\mid y) when Y Y happens to take the value of y y . Total variance rule: \\textbf{Var}(X) = {\\bf E}\\Big[\\textbf{Var}(X \\mid Y)\\Big] + \\textbf{Var}\\Big({\\bf E}\\big[X\\mid Y\\big]\\Big) \\textbf{Var}(X) = {\\bf E}\\Big[\\textbf{Var}(X \\mid Y)\\Big] + \\textbf{Var}\\Big({\\bf E}\\big[X\\mid Y\\big]\\Big) Lecture 14 Introduction to Bayesian Inference \u00b6 The Bayesian inference framework \u00b6 Treat the unknown \\Theta \\Theta as a random variable. with prior p_{\\Theta} p_{\\Theta} or f_{\\Theta} f_{\\Theta} . With the observations X X , using Bayes rule to obtain the posterior probability of \\Theta \\Theta , P_{\\Theta|X}(\\cdot|X=x) P_{\\Theta|X}(\\cdot|X=x) . Once the posterior is available, the estimator can be obtained via maximize a posterioral probability (MAP) rule or leaset mean squares (LMS). The performance can be measured by \"probability of error\" or \"mean squared error\". Conditional probability of error and total probability of error \u00b6 The conditional probability of other estimators that made a mistake. One of the properties of MAP rule is the it guarantee smallest probability of error. The taotal probability of error is an abstract notion of error for MAP rule to estimator. It can be caculated using total probability theory. Discrete unknowns, discrete observation example (8. Exercise: Discrete unknowns) \u00b6 Let \\Theta_1 \\Theta_1 and \\Theta_2 \\Theta_2 be some unobserved Bernoulli random variables and let be an observation. Conditional on X = x X = x , the posterior joint PMF of \\Theta_1 \\Theta_1 and \\Theta_2 \\Theta_2 is given by p_{\\Theta _1,\\Theta _2\\mid X}(\\theta _1,\\theta _2\\mid x) = \\begin{cases} 0.26, & \\mbox{if } \\theta _1=0, \\theta _2=0, \\\\ 0.26, & \\mbox{if } \\theta _1=0, \\theta _2=1, \\\\ 0.21, & \\mbox{if } \\theta _1=1, \\theta _2=0, \\\\ 0.27, & \\mbox{if } \\theta _1=1, \\theta _2=1, \\\\ 0, & \\mbox{otherwise.} \\end{cases} p_{\\Theta _1,\\Theta _2\\mid X}(\\theta _1,\\theta _2\\mid x) = \\begin{cases} 0.26, & \\mbox{if } \\theta _1=0, \\theta _2=0, \\\\ 0.26, & \\mbox{if } \\theta _1=0, \\theta _2=1, \\\\ 0.21, & \\mbox{if } \\theta _1=1, \\theta _2=0, \\\\ 0.27, & \\mbox{if } \\theta _1=1, \\theta _2=1, \\\\ 0, & \\mbox{otherwise.} \\end{cases} What is the estimate of (\\Theta_1, \\Theta_2) (\\Theta_1, \\Theta_2) provided by MAP rule? (1, 1), because the p_{\\Theta _1,\\Theta _2\\mid X}(1, 1\\mid x) p_{\\Theta _1,\\Theta _2\\mid X}(1, 1\\mid x) is the maximum of all. What is the MAP estimate of \\Theta_1 \\Theta_1 based on X X , that is, the one that maximizes p_{\\Theta_1 \\mid X}(\\theta_1 \\mid x) p_{\\Theta_1 \\mid X}(\\theta_1 \\mid x) ? 0, from the marginal PMF, you can see that p_{\\Theta_1 \\mid X}(0 \\mid x) p_{\\Theta_1 \\mid X}(0 \\mid x) is maximized The moral of this problem is that an estimate of $ \\Theta_1 $ obtained by identifying the maximum of the join PMF of all unknown random variables can be different from the MAP estimate of \\Theta_1 \\Theta_1 from the marginal PMF. Discrete parameter, continuous observation \u00b6 Digital signal transmission continous parameter, continuous observation \u00b6 Analog signal transmission Lecture 15 Linear models with normal noise \u00b6 recognizing normal PDFs \u00b6 f_X(x) = c \\cdot e^{-(\\alpha x^2 + \\beta x + \\gamma)} f_X(x) = c \\cdot e^{-(\\alpha x^2 + \\beta x + \\gamma)} is a normal random variable with \\mu = -\\frac{\\beta}{2\\alpha} \\mu = -\\frac{\\beta}{2\\alpha} and variance \\frac{1}{2\\alpha} \\frac{1}{2\\alpha} The mean squared error \u00b6 The most important take away from this section is that for normal unknown and normal noise signal, X = \\Theta + W X = \\Theta + W , no matter which x_i x_i we observed, the mean squared error estimates {\\hat \\theta} {\\hat \\theta} are the same (the variance of \\Theta \\Theta ). In other words, the remaining uncertainty about \\theta \\theta after an observation is the same no matter what the observation is. The observation only determined the estimated mean of the random variable \\theta \\theta . (See the drawing at the end of the lecture) Measurement, estimate, and learning \u00b6 How to measure the gravitational attraction constant Lecture 16 Least mean square (LMS) estimation \u00b6 LMS estimation without any observations \u00b6 Given the prior p_{\\Theta}(\\theta) p_{\\Theta}(\\theta) unknow random variable \\Theta \\Theta , what's you realy want to estimate? You may interested in a point estimate. You may interested in find the estimator. With no observation, to minimize the mean squared error (MSE), {\\bf E}\\Big[\\big(\\Theta - \\hat \\theta \\big)^2\\Big] {\\bf E}\\Big[\\big(\\Theta - \\hat \\theta \\big)^2\\Big] , the value \\hat \\theta = {\\bf E}\\big[X\\big] \\hat \\theta = {\\bf E}\\big[X\\big] minimize the error. and the minimum error equal to \\textbf{Var}(\\Theta) \\textbf{Var}(\\Theta) Optimal mean squared error: {\\bf E}\\Big[\\big(\\Theta - {\\bf E}\\big[\\Theta \\big] \\big)^2\\Big] = \\textbf{Var}(\\Theta) {\\bf E}\\Big[\\big(\\Theta - {\\bf E}\\big[\\Theta \\big] \\big)^2\\Big] = \\textbf{Var}(\\Theta) . Because {\\bf E}\\big[\\Theta \\big] {\\bf E}\\big[\\Theta \\big] is the optimal value that minimize the MSE, if we replace \\hat \\theta \\hat \\theta with {\\bf E}\\big[\\Theta \\big] {\\bf E}\\big[\\Theta \\big] in the expression MSE, it coincidently match the definition of variance. You can alse use the {\\bf E}\\Big[\\big(\\Theta - \\hat \\theta \\big)^2\\Big] = \\textbf{Var}(\\Theta - \\hat \\theta) + \\Big({\\bf E}\\big[\\Theta - \\hat \\theta \\big]\\Big)^2 {\\bf E}\\Big[\\big(\\Theta - \\hat \\theta \\big)^2\\Big] = \\textbf{Var}(\\Theta - \\hat \\theta) + \\Big({\\bf E}\\big[\\Theta - \\hat \\theta \\big]\\Big)^2 to derive \\hat \\theta = {\\bf E}\\big[X\\big] \\hat \\theta = {\\bf E}\\big[X\\big] is the optimal value. LMS estimation; single unknown and observation \u00b6 Goal: interested in a point estimate \\hat \\theta \\hat \\theta of unknow random variable \\Theta \\Theta , with prior p_{\\Theta}(\\theta) p_{\\Theta}(\\theta) . (Given observation X X ; model p_{X\\mid \\Theta}(x\\mid \\theta) p_{X\\mid \\Theta}(x\\mid \\theta) ) We want to minimize the MSE, because this time we have the particular observation X = x X = x , we now live in a conditional universe, we need to minimize the conditional MSE, {\\bf E}\\Big[\\big(\\Theta - {\\bf E}\\big[\\Theta \\big] \\big)^2 \\mid X = x \\Big] {\\bf E}\\Big[\\big(\\Theta - {\\bf E}\\big[\\Theta \\big] \\big)^2 \\mid X = x \\Big] , the optimal value is \\hat \\theta = {\\bf E}\\big[\\Theta \\mid X = x\\big] \\hat \\theta = {\\bf E}\\big[\\Theta \\mid X = x\\big] By the \"happens to take\" reasoning and the iterated expection rule, we can achieve the conclusion: that \\hat \\Theta_{LMS} = {\\bf E}\\big[\\Theta \\mid X\\big] \\hat \\Theta_{LMS} = {\\bf E}\\big[\\Theta \\mid X\\big] minimize the MSE {\\bf E}\\Big[\\big(\\Theta - g(X) \\big)^2 \\Big] {\\bf E}\\Big[\\big(\\Theta - g(X) \\big)^2 \\Big] over all estimators \\hat \\Theta = g(X) \\hat \\Theta = g(X) . LMS performance evaluation \u00b6 MSE: {\\bf E}\\Big[\\big(\\Theta - {\\bf E}\\big[\\Theta \\mid X = x\\big] \\big)^2 \\mid X = x \\Big] = \\textbf{Var}(\\Theta \\mid X = x) {\\bf E}\\Big[\\big(\\Theta - {\\bf E}\\big[\\Theta \\mid X = x\\big] \\big)^2 \\mid X = x \\Big] = \\textbf{Var}(\\Theta \\mid X = x) Expected performance of the design: {\\bf E}\\Big[\\big(\\Theta - {\\bf E}\\big[\\Theta \\mid X\\big] \\big)^2 \\Big] = {\\bf E}\\Big[\\textbf{Var}(\\Theta \\mid X)\\Big] {\\bf E}\\Big[\\big(\\Theta - {\\bf E}\\big[\\Theta \\mid X\\big] \\big)^2 \\Big] = {\\bf E}\\Big[\\textbf{Var}(\\Theta \\mid X)\\Big] LMS relavant to estimation (not hypothesis testing) Same as MAP if the posterior is unimodal and symmetric around the mean. e.g. when the posterior is normal (the case in \"linear-normal\" models) The multidimensional case \u00b6 Lecture 17 Linear least mean squares (LLMS) estimation \u00b6 Lecture 18 Inequalities, convergence, and the Weak Law of Large Numbers \u00b6 The Weak Law of Large Numbers \u00b6 X_1, X_2, \\cdots, X_n X_1, X_2, \\cdots, X_n , i.i.d.; finite mean \\mu \\mu and variance \\sigma^2 \\sigma^2 Sample mean M_n M_n : it is not a constant, but a function of multiple random variables. You can understand it this way. In a experiment which you draw the exam score of student one by one, once you finish the first draw X_1 X_1 , the first sample is a fixed real number. But before your first draw, X_1 X_1 can be any score and it is a random variable. After you draw a total of n n scores, you have all the fixed value of X_1, X_2, \\cdots, X_n X_1, X_2, \\cdots, X_n , the sample mean is determined for this draw. However, abstractly, sample mean is a random variable, if you draw another n n example scores to get the sample mean, you get a different set of X_1, X_2, \\cdots, X_n X_1, X_2, \\cdots, X_n . The randomness comes from the randomness of different experiments, in each of the experiment, the sample X_i X_i is not random. {\\bf E}\\big[M_n\\big] =\\mu {\\bf E}\\big[M_n\\big] =\\mu . Two level of averaging. M_n M_n itself is averaging over n samples. Since these n samples are drawn randomly, taking the expectation of M_n M_n is averaging all possible sample means obtained through many (infinity) experiments. \\textbf{Var}(M_n) = \\frac{1}{n^2} \\cdot n \\cdot \\textbf{Var}(X_1) = \\frac{\\sigma^2}{n} \\textbf{Var}(M_n) = \\frac{1}{n^2} \\cdot n \\cdot \\textbf{Var}(X_1) = \\frac{\\sigma^2}{n} . Apply Chebyshev inequality: \\textbf{P}(|M_n - \\mu| \\ge \\epsilon) \\le \\frac{\\textbf{var}(M_n)}{\\epsilon^2} = \\frac{\\sigma^2}{n\\epsilon^2} \\rightarrow 0 \\textbf{P}(|M_n - \\mu| \\ge \\epsilon) \\le \\frac{\\textbf{var}(M_n)}{\\epsilon^2} = \\frac{\\sigma^2}{n\\epsilon^2} \\rightarrow 0 , as n \\rightarrow \\infty n \\rightarrow \\infty Weak Law of Large Numbers For \\epsilon > 0 \\epsilon > 0 , \\textbf{P}(|M_n - \\mu| \\ge \\epsilon) = \\textbf{P}\\Big(\\Big|\\frac{X_1 + X_2 + \\cdots + X_n}{n} - \\mu\\Big| \\ge \\epsilon \\Big) \\rightarrow 0 \\textbf{P}(|M_n - \\mu| \\ge \\epsilon) = \\textbf{P}\\Big(\\Big|\\frac{X_1 + X_2 + \\cdots + X_n}{n} - \\mu\\Big| \\ge \\epsilon \\Big) \\rightarrow 0 , as n \\rightarrow \\infty n \\rightarrow \\infty . Interpreting the WLLN \u00b6 One experiment many measurements X_i = \\mu + W_i X_i = \\mu + W_i , where W_i W_i is noise, \\textbf{E}[W_i] = 0 \\textbf{E}[W_i] = 0 , independent W_i W_i . Sample mean M_n M_n is unlikely to be far off from true mean \\mu \\mu Many independent repetitions of the same experiments event A A , with p = \\textbf{P}(A) p = \\textbf{P}(A) X_i X_i : indicator of event A A The sampel mean M_n M_n is the empirical frequency of event A A Application of WLLN - polling \u00b6 The probability of error greater than \\epsilon \\epsilon is smaller than a certain probability. You can use Chebyshev inequality and WLLN to estimate how many samples you need to fulfill a specific error probability requirement. Convergence in probability \u00b6 Definition: A sequence Y_n Y_n converges in probability to a number a a if For any \\epsilon > 0 \\epsilon > 0 , \\displaystyle \\lim_{n\\rightarrow\\infty}\\textbf{P}(|Y_n - a| \\ge \\epsilon) = 0 \\displaystyle \\lim_{n\\rightarrow\\infty}\\textbf{P}(|Y_n - a| \\ge \\epsilon) = 0 * Comparison between ordinary convergence and convergence in probability: Convergence in probability examples \u00b6 convergence in probability doesn't imply convergence of the expectations. How to find what value it converges to? Make an educated conjecture about the limit \\tau \\tau , write {\\bf P}(|Y_n - \\tau| \\ge \\epsilon) {\\bf P}(|Y_n - \\tau| \\ge \\epsilon) , and derive the value of it to where you can observe from the expression that the probability \\to 0 \\to 0 , when n \\to \\infty n \\to \\infty . Related topics \u00b6 Better bounds/approximations on tail probabilities Markov and Chebyshev inequalities Chernoff bound Central limit theorem Different types of convergence Convergence in probability Convergence \"with probability 1\" Strong law of large numbers Convergence of a sequence of distributions (CDFs) to a limiting CDF. Lecture 19 The Central Limit Theorem (CLT) \u00b6 Considering the sum of random variable S_n = X_1 + X_2, + \\cdots + X_n S_n = X_1 + X_2, + \\cdots + X_n , ( X_1, X_2, \\cdots, X_n X_1, X_2, \\cdots, X_n are i.i.d. with finite mean \\mu \\mu and variance \\sigma^2 \\sigma^2 ) Z_n = \\frac{S_n - n\\mu}{\\sqrt{n}\\sigma} Z_n = \\frac{S_n - n\\mu}{\\sqrt{n}\\sigma} , then we have the central limit Theorem, Central Limit Theorem: For every z z : \\displaystyle \\lim_{n \\to \\infty}\\textbf{P}(Z_n \\le z) = \\textbf{P}(Z \\le z) \\displaystyle \\lim_{n \\to \\infty}\\textbf{P}(Z_n \\le z) = \\textbf{P}(Z \\le z) What exactly does the CLT say? - Practice \u00b6 The practice of normal approximations: We have linear expression between S_n S_n and Z_n Z_n : S_n = \\sqrt n \\sigma Z_n + n\\mu S_n = \\sqrt n \\sigma Z_n + n\\mu , since Z_n Z_n can be treated as if it were normal, S_n S_n can be treated as if normal: N(n\\mu, n\\sigma^2) N(n\\mu, n\\sigma^2) Can we use the CLT when n is \"moderate\"? i.e. n = 30? Usually, yes. When the distribution of X X has common features with a normal distribution. symmetry and unimodality help Central Limit Theory examples \u00b6 \\textbf{P}(S_{\\color{red}n} \\le {\\color{red}a}) \\approx {\\color{red}b} \\textbf{P}(S_{\\color{red}n} \\le {\\color{red}a}) \\approx {\\color{red}b} Given two parameters, find the third. Package weight X_i X_i , i.i.d. exponential, \\lambda = 1/2 \\lambda = 1/2 , \\mu = \\sigma = 2 \\mu = \\sigma = 2 Load container with n = 100 n = 100 packages, what's the probability that the overall weight is heavier than 210? \\textbf{P}(S_n \\ge 210) = ? \\textbf{P}(S_n \\ge 210) = ? Load container with n = 100 n = 100 packages. Choose the \"capacity\" a so that \\textbf{P}(S_n \\ge a) \\approx 0.05 \\textbf{P}(S_n \\ge a) \\approx 0.05 Fix the capacity at 210, how large can n n be, so that \\textbf{P}(S_n \\ge 210) \\approx 0.05 \\textbf{P}(S_n \\ge 210) \\approx 0.05 Load container until wight exceeds 210, N N is the number of packages loaded, find \\textbf{P}(N > 100) \\textbf{P}(N > 100) Airline booking \u00b6 For any given flight, an airline tries to sell as many tickets as possible. Suppose that on average, of ticket holders fail to show up, all independent of one another. Knowing this, an airline will sell more tickets than there are seats available (i.e., overbook the flight) and hope that there is a sufficient number of ticket holders who do not show up, to compensate for its overbooking. Using the Central Limit Theorem, determine, the maximum number of tickets an airline can sell on a flight with 400 seats so that it can be approximately confident that all ticket holders who do show up will be able to board the plane. Use the de Moivre-Laplace -correction in your calculations. Hint: You may have to solve numerically a quadratic equation. Solution: Each ticket can either be used or not used by the the passager. It can be modeled with the random variable: X \\sim \\operatorname{Bern} \\left({p}\\right) X \\sim \\operatorname{Bern} \\left({p}\\right) , p=0.8 p=0.8 , then the total passergers presented at airport is S_n = X_1 + X_2 + \\cdots + X_n S_n = X_1 + X_2 + \\cdots + X_n , which is a Binomial: S_n \\sim \\operatorname{B} \\left({n, p}\\right) S_n \\sim \\operatorname{B} \\left({n, p}\\right) . For binomial random variable, \\mu = np = 0.8n, \\sigma^2 = np(1-p) = 0.16n \\mu = np = 0.8n, \\sigma^2 = np(1-p) = 0.16n . Our requirements is \\textbf{P}(S_n \\le 400) \\approx 0.99 \\textbf{P}(S_n \\le 400) \\approx 0.99 . Normalize S_n S_n in the probability and treat the normalized random variable Z_n Z_n as a normal distribution, refer to the normal table, and solve the number n n . \\begin{align*} \\textbf{P}(S_n \\le 400) &\\approx 0.99 \\\\ \\textbf{P}\\Big(\\frac{S_n - 0.8n}{0.4\\sqrt{n}} \\le \\frac{400.5 - 0.8n}{0.4\\sqrt{n}}\\Big) &\\approx 0.99 \\\\ \\textbf{P}\\Big(Z_n \\le \\frac{400.5 - 0.8n}{0.4\\sqrt{n}}\\Big) &\\approx 0.99 \\\\ \\Phi\\Big(\\frac{400.5 - 0.8n}{0.4\\sqrt{n}}\\Big) &\\approx 0.99 \\\\ \\frac{400.5 - 0.8n}{0.4\\sqrt{n}} &= 2.33 \\end{align*} \\begin{align*} \\textbf{P}(S_n \\le 400) &\\approx 0.99 \\\\ \\textbf{P}\\Big(\\frac{S_n - 0.8n}{0.4\\sqrt{n}} \\le \\frac{400.5 - 0.8n}{0.4\\sqrt{n}}\\Big) &\\approx 0.99 \\\\ \\textbf{P}\\Big(Z_n \\le \\frac{400.5 - 0.8n}{0.4\\sqrt{n}}\\Big) &\\approx 0.99 \\\\ \\Phi\\Big(\\frac{400.5 - 0.8n}{0.4\\sqrt{n}}\\Big) &\\approx 0.99 \\\\ \\frac{400.5 - 0.8n}{0.4\\sqrt{n}} &= 2.33 \\end{align*} Solve n n to obtain n = 475 n = 475 Normal approximation to the binomial \u00b6 Take X_i X_i as independent Bernuolli random variables: X \\sim \\operatorname{Bern} \\left({p}\\right) X \\sim \\operatorname{Bern} \\left({p}\\right) , 0 < p < 1 0 < p < 1 , then S_n = X_1 + X_2 + \\cdots + X_n S_n = X_1 + X_2 + \\cdots + X_n is Binomial: X \\sim \\operatorname{B} \\left({n, p}\\right) X \\sim \\operatorname{B} \\left({n, p}\\right) . Binomial random variable S_n S_n have \\mu = np \\mu = np , \\sigma^2=np(1-p) \\sigma^2=np(1-p) . According CLT, The normalized random variable \\frac{S_n-np}{\\sqrt{np(1-p)}} \\frac{S_n-np}{\\sqrt{np(1-p)}} is a standard normal. In order to find the \\textbf{P}(S_n \\le 21) \\textbf{P}(S_n \\le 21) (given n = 36 n = 36 , p = 0.5 p = 0.5 ), we use the equvilent of events to transform the probability \\textbf{P}(S_n \\le 21) \\textbf{P}(S_n \\le 21) into another probability about a normal random vairable \\textbf{P}\\Big(\\frac{S_n - 18}{3} \\le \\frac{21-28}{3}\\Big) = \\textbf{P}(Z_n \\le 1) = \\Phi(1) = 0.8413 \\textbf{P}\\Big(\\frac{S_n - 18}{3} \\le \\frac{21-28}{3}\\Big) = \\textbf{P}(Z_n \\le 1) = \\Phi(1) = 0.8413 . The 1/2 correction for integer random variables. To get a improved estimate of binomial from normal, we can take the middle point of two integers. Instead using \\textbf{P}(S_n \\le 21) \\textbf{P}(S_n \\le 21) or \\textbf{P}(S_n \\lt 22) \\textbf{P}(S_n \\lt 22) , we use \\textbf{P}(S_n \\lt 21.5) \\textbf{P}(S_n \\lt 21.5) which is very accurate De Moivre-Laplace CLT to the binomial. To estimate \\textbf{P}(S_n=19) \\textbf{P}(S_n=19) , we take \\textbf{P}(18.5 \\le S_n \\le 19.5) \\textbf{P}(18.5 \\le S_n \\le 19.5) and get an accurate estimate. Lecture 20 Introduction to classical statistics \u00b6 Overview of the classical statistical framework \u00b6 attributes Bayesian classical unkonwn \\Theta \\Theta is r.v. \\theta \\theta is a fixed value known p_\\Theta p_\\Theta prior distribution and samples X_i X_i only sample X_i X_i model p_{X \\mid \\Theta} p_{X \\mid \\Theta} , where observation is generated p_X(x; \\theta) p_X(x; \\theta) , \\theta \\theta is a real-valued parameter of the model, method use bayes rule to obtain p_{\\Theta \\mid X} p_{\\Theta \\mid X} , \\big(p_X = {\\int_{-\\infty}^{+\\infty}} p_\\Theta \\cdot p_{X \\mid \\Theta}d\\theta\\big) \\big(p_X = {\\int_{-\\infty}^{+\\infty}} p_\\Theta \\cdot p_{X \\mid \\Theta}d\\theta\\big) , then determind the estimate Design a estimator \\hat\\Theta \\hat\\Theta , to keep estimate error \\hat\\Theta - \\theta \\hat\\Theta - \\theta small estimator MAP, LMS ML Confidence intervals interpretation \u00b6 Every day, I try to estimate an unknown parameter using a fresh data set. I look at the data and then I use some formulas to calculate a 70% confidence interval, [\\hat\\Theta^-, \\hat\\Theta^+] [\\hat\\Theta^-, \\hat\\Theta^+] , based on the day's data. Classical statsistic interpretation \u00b6 If today I got the confidence interval [0.41, 0.47] [0.41, 0.47] , that doesn't mean there is 70\\% 70\\% probability that the true value will be inside [0.41, 0.47] [0.41, 0.47] . What the confidence interval is that 70% of those confidence intervals you obtained in all those days will include the true value. The \"confidence\" (or probability) can be think of \\frac{\\text{All CIs that include the true value}}{\\text{All CIs that include the true value + CIs that exclude the true value}} \\frac{\\text{All CIs that include the true value}}{\\text{All CIs that include the true value + CIs that exclude the true value}} . You cannot speak about a particular interval, and once you do that, there is no randomness anymore, the true value is either inside the interval or not, there are no other probabilities so to speak. Bayes' interpretation (Bayesian's Confidence Interval) \u00b6 Alternatively, I decided to use a Bayesian approach, by viewing the unknown parameter, denoted by \\Theta \\Theta , as a continuous random variable and assuming a prior PDF for \\Theta \\Theta . I observe a specific value x x , calculate the posterior f_{\\Theta}{X}(\\theta|x) f_{\\Theta}{X}(\\theta|x) , and find out that \\begin{align*} \\int _{0.41}^{0.47} f_{\\Theta |X}(\\theta \\, |\\, x)\\, d\\theta =0.70. \\end{align*} \\begin{align*} \\int _{0.41}^{0.47} f_{\\Theta |X}(\\theta \\, |\\, x)\\, d\\theta =0.70. \\end{align*} This time, I can say that there is a probability 70\\% 70\\% that the unknown parameter is inside the (Bayesian) confidence interval [0.41, 0.47] [0.41, 0.47] . Confidence intervals for the estimation of the mean \u00b6 I asked you to estimate the mean of i.i.d variables X_1, X_2, \\cdots, X_n X_1, X_2, \\cdots, X_n with true mean \\theta \\theta , and variance \\sigma^2 \\sigma^2 , and the estimate should achieve 95\\% 95\\% confidence interval. How you proceed with it? By Central Limit Theory, your estimate values \\hat\\Theta \\hat\\Theta is an normal distribution. By standardizing it, you get the standard normal Z_n = \\frac{\\hat\\Theta - \\theta}{\\sigma/\\sqrt{n}} Z_n = \\frac{\\hat\\Theta - \\theta}{\\sigma/\\sqrt{n}} . 95\\% 95\\% confidence interverl means standard normal is between symetric 95\\% 95\\% intervals [-b, b] [-b, b] . By looking up the normal table, b = 1.69 b = 1.69 corresponds to the probability 97.5\\% 97.5\\% , so \\textbf{P}(-1.69 \\le Z_n \\le 1.69) = 95\\% \\textbf{P}(-1.69 \\le Z_n \\le 1.69) = 95\\% thus \\textbf{P}\\big(\\frac{|\\hat\\Theta - \\theta|}{\\sigma/\\sqrt{n}} \\le 1.69\\big) = 95\\% \\textbf{P}\\big(\\frac{|\\hat\\Theta - \\theta|}{\\sigma/\\sqrt{n}} \\le 1.69\\big) = 95\\% , which can be rewrite as \\begin{align*} \\textbf{P}\\Big(\\hat\\Theta -\\frac{1.96\\sigma}{\\sqrt n} \\le \\theta \\le \\hat\\Theta +\\frac{1.96\\sigma}{\\sqrt n}\\Big) = 95\\%. \\end{align*} \\begin{align*} \\textbf{P}\\Big(\\hat\\Theta -\\frac{1.96\\sigma}{\\sqrt n} \\le \\theta \\le \\hat\\Theta +\\frac{1.96\\sigma}{\\sqrt n}\\Big) = 95\\%. \\end{align*} This is the way to construct the confidence interval. If you have the \\sigma \\sigma and n n , you have a concrete 95\\% 95\\% confidence interval to report. However, the \\sigma \\sigma is usually unknown, what you can do is to use the estimated \\hat\\Theta \\hat\\Theta to estimate the \\sigma \\sigma . Lecture 21 \u00b6 Lecture 22 \u00b6 Lecture 23 \u00b6 Lecture 24 \u00b6 Lecture 25 \u00b6 Lecture 26 \u00b6","title":"6.431 Probability"},{"location":"courses/6.431-probability/notes/#60416431-probability-the-science-of-uncertainty-and-data","text":"","title":"6.041/6.431 Probability - The Science of Uncertainty and Data"},{"location":"courses/6.431-probability/notes/#lecture-1-september-10-2018","text":"","title":"Lecture 1 (September 10, 2018)"},{"location":"courses/6.431-probability/notes/#probability-models","text":"To specify such a model, it includes 2 steps: Sample space: describe possible outcomes of an experiment Probability laws: describe beliefs about the likelihood of outcomes (or assign a probability to outcomes)","title":"Probability Models"},{"location":"courses/6.431-probability/notes/#sample-space","text":"list of possible outcomes must be mutually exclusive, collectively exhaustive, and at the right granularity.","title":"Sample space"},{"location":"courses/6.431-probability/notes/#probability-laws","text":"","title":"Probability laws"},{"location":"courses/6.431-probability/notes/#probability-axioms","text":"Motivation: Assign individual probability to a single dot is a challenge (in the dart example, each dot has zero probability), so we assign a probability to the subset of the sample space. Nonnegativity: {\\bf P}(A) \\geq 0 {\\bf P}(A) \\geq 0 Normalization: {\\bf P}(\\Omega) = 1 {\\bf P}(\\Omega) = 1 (Finite) Additivity: if A \\cap B = \\emptyset A \\cap B = \\emptyset then {\\bf P}(A \\cup B) = {\\bf P}(A) + {\\bf P}(B) {\\bf P}(A \\cup B) = {\\bf P}(A) + {\\bf P}(B) (need to be refined in future)","title":"Probability axioms"},{"location":"courses/6.431-probability/notes/#consequences-of-axioms-properties-of-probability-axioms","text":"{\\bf P}(A) \\leq 1 {\\bf P}(A) \\leq 1 {\\bf P}(\\emptyset) = 0 {\\bf P}(\\emptyset) = 0 {\\bf P}(A) + {\\bf P}(A^c) = 1 {\\bf P}(A) + {\\bf P}(A^c) = 1 A, B, \\text{and } C A, B, \\text{and } C are disjoint: {\\bf P}(A \\cup B \\cup C) = {\\bf P}(A) + {\\bf P}(B) + {\\bf P}(C) {\\bf P}(A \\cup B \\cup C) = {\\bf P}(A) + {\\bf P}(B) + {\\bf P}(C) A_1, \\cdots A_k, A_1, \\cdots A_k, are disjoint: {\\bf P}(A_1 \\cup \\cdots, \\cup A_k) = \\sum_{i - 1}^k{\\bf P}(A_i) {\\bf P}(A_1 \\cup \\cdots, \\cup A_k) = \\sum_{i - 1}^k{\\bf P}(A_i) If A \\subset B A \\subset B , then {\\bf P}(A) \\leq {\\bf P}(B) {\\bf P}(A) \\leq {\\bf P}(B) {\\bf P}(A \\cup B) = {\\bf P}(A) + {\\bf P}(B) - {\\bf P}(A \\cap B) {\\bf P}(A \\cup B) = {\\bf P}(A) + {\\bf P}(B) - {\\bf P}(A \\cap B) {\\bf P}(A \\cup B \\cup C) = {\\bf P}(A) + {\\bf P}(A^c \\cap B) + {\\bf P}(A^c \\cap B^c \\cap C) {\\bf P}(A \\cup B \\cup C) = {\\bf P}(A) + {\\bf P}(A^c \\cap B) + {\\bf P}(A^c \\cap B^c \\cap C) Union Bound: {\\bf P}(A \\cup B) \\leq {\\bf P}(A) + {\\bf P}(B) {\\bf P}(A \\cup B) \\leq {\\bf P}(A) + {\\bf P}(B)","title":"Consequences of axioms (properties of probability axioms)"},{"location":"courses/6.431-probability/notes/#probability-calculation-steps","text":"Specify the sample space Specify a probability law Identify an event of interest Calculate","title":"Probability calculation steps"},{"location":"courses/6.431-probability/notes/#countable-additivity","text":"The 3rd probability axiom previously mentioned can be extended to countable set (all integers), which means that the element in the set can be arranged in a sequence. This is contrary to the concept of uncountable set (i.e. a dart board, 2D plane). So you need to distinguish the following concepts: discrete set (countable set) continuous set (uncountable set, impossible to arrange in a sequence) finite set infinite set (possible to be arranged in a sequence) discrete (countable) finite set (i.e. {1, 2, 3}) discrete (countable) infinite set (i.e. all integers, all even integers, all old integers) continuous (uncountable) infinite set (i.e. \\{x | 0 \\leq x \\leq 1\\} \\{x | 0 \\leq x \\leq 1\\} ) Example: Sample space \\{1, 2, 3, \\cdots\\} \\{1, 2, 3, \\cdots\\} , given {\\bf P}(n) = \\frac{1}{2^n}, n = 1, 2, \\cdots {\\bf P}(n) = \\frac{1}{2^n}, n = 1, 2, \\cdots . Check the against the probability axioms that \\sum_{n = 1}^{\\infty}{\\bf P}(n) = 1 \\sum_{n = 1}^{\\infty}{\\bf P}(n) = 1 . What about the probability {\\bf P}(\\text{outcome is even}) {\\bf P}(\\text{outcome is even}) ? Using countable additivity axiom.","title":"Countable additivity"},{"location":"courses/6.431-probability/notes/#countable-additivity-axiom-refined-probability-axiom","text":"Note if A_1, A_2, A_3, \\cdot A_1, A_2, A_3, \\cdot is an infinite sequence of disjoint events, then {\\bf P}(A_1 \\cup A_2 \\cup A_3 \\cdots) = {\\bf P}(A_1) + {\\bf P}(A_2) + {\\bf P}(A_3) + \\cdots {\\bf P}(A_1 \\cup A_2 \\cup A_3 \\cdots) = {\\bf P}(A_1) + {\\bf P}(A_2) + {\\bf P}(A_3) + \\cdots To refine the 3rd probability axiom, additivity holds only for \"countable\" sequences of events. That means the additivity axiom must be a sequence , with finite or infinite elements. Note the following contradiction, the additivity axiom can not be applied to continuous sample space. A_1, A_2, A_3, \\cdots A_1, A_2, A_3, \\cdots are real coordinates, if you apply the additivity axiom, you will get: \\begin{align*} 1={\\bf P}(\\Omega ) &={\\bf P}\\big (\\{ A_1\\} \\cup \\{ A_2\\} \\cup \\{ A_3\\} \\cdots \\big )\\\\ &={\\bf P}(\\{ A_1\\} )+{\\bf P}(\\{ A_2\\} )+{\\bf P}(\\{ A_3\\} )+\\cdots \\\\ &= 0+0+0+\\cdots =0, \\end{align*} \\begin{align*} 1={\\bf P}(\\Omega ) &={\\bf P}\\big (\\{ A_1\\} \\cup \\{ A_2\\} \\cup \\{ A_3\\} \\cdots \\big )\\\\ &={\\bf P}(\\{ A_1\\} )+{\\bf P}(\\{ A_2\\} )+{\\bf P}(\\{ A_3\\} )+\\cdots \\\\ &= 0+0+0+\\cdots =0, \\end{align*} which is contradicts. This is because \"unit square\" is not a countable set.","title":"Countable Additivity Axiom (refined probability axiom)"},{"location":"courses/6.431-probability/notes/#interpretations-of-probability-theory","text":"frequency of events A? What is P(the president will be reelected)? Probability is often interpreted as: Description of beliefs Betting preference","title":"Interpretations of probability theory"},{"location":"courses/6.431-probability/notes/#lecture-2-conditioning-and-bayes-rule","text":"","title":"Lecture 2 Conditioning and Baye's rule"},{"location":"courses/6.431-probability/notes/#conditional-probability","text":"use new information to review a model Definition {\\bf P}(A|B) = {\\bf P}(A|B) = \"probability of A given that B occurred\" {\\bf P}(A|B) = \\frac{{\\bf P}(A \\cap B)}{{\\bf P}(B)} {\\bf P}(A|B) = \\frac{{\\bf P}(A \\cap B)}{{\\bf P}(B)} , defined only when {\\bf P}(B) > 0 {\\bf P}(B) > 0 Conditional probability share properties of ordinary probabilities (finite and infinite countable additivity) If A \\cap C = \\emptyset A \\cap C = \\emptyset , then {\\bf P}(A \\cup C|B) = P(A|B) + {\\bf P}(C|B) {\\bf P}(A \\cup C|B) = P(A|B) + {\\bf P}(C|B)","title":"Conditional probability"},{"location":"courses/6.431-probability/notes/#multiplication-rule","text":"{\\bf P}(A \\cap B) = {\\bf P}(A){\\bf P}(B|A) = {\\bf P}(B){\\bf P}(A|B) {\\bf P}(A \\cap B) = {\\bf P}(A){\\bf P}(B|A) = {\\bf P}(B){\\bf P}(A|B) {\\bf P}(A \\cap B \\cap C) = {\\bf P}(A){\\bf P}(B|A){\\bf P}(C|A \\cap B) {\\bf P}(A \\cap B \\cap C) = {\\bf P}(A){\\bf P}(B|A){\\bf P}(C|A \\cap B) It also applies to n events.","title":"Multiplication rule"},{"location":"courses/6.431-probability/notes/#total-probability-theorem-divide-and-conquer","text":"The settings: Partition of sample space into A_1, A_2, A_3 A_1, A_2, A_3 , have {\\bf P}(A_i) {\\bf P}(A_i) for every i i have {\\bf P}(B|A_i) {\\bf P}(B|A_i) for every i i {\\bf P}(B) = \\sum_{i} {\\bf P}(A_i){\\bf P}(B|A_i) {\\bf P}(B) = \\sum_{i} {\\bf P}(A_i){\\bf P}(B|A_i) It also applies to infinite countable sets according to the countable additivity axiom.","title":"Total probability theorem (divide and conquer)"},{"location":"courses/6.431-probability/notes/#bayes-rule","text":"The setting is the same as the total probability theorem. Partition of sample space into A_1, A_2, A_3 A_1, A_2, A_3 , have {\\bf P}(A_i) {\\bf P}(A_i) for every i i (initial believes) have {\\bf P}(B|A_i) {\\bf P}(B|A_i) for every i i revise \"believe\" given that B occurred: {\\bf P}(A_i|B) = \\frac{{\\bf P}(A_i \\cap B)}{{\\bf P}(B)} {\\bf P}(A_i|B) = \\frac{{\\bf P}(A_i \\cap B)}{{\\bf P}(B)} It turns out that we can use multiplication rule to calculate the nominator and use the total probability rule to calculate the denominator, thusly {\\bf P}(A_i|B) = \\frac{{\\bf P}(A_i){\\bf P}(B|A_i)}{\\sum_j{\\bf P}(A_j){\\bf P}(B|A_i)} {\\bf P}(A_i|B) = \\frac{{\\bf P}(A_i){\\bf P}(B|A_i)}{\\sum_j{\\bf P}(A_j){\\bf P}(B|A_i)}","title":"Baye's rule"},{"location":"courses/6.431-probability/notes/#lecture-3-independence","text":"","title":"Lecture 3 Independence"},{"location":"courses/6.431-probability/notes/#independence-of-two-events","text":"Motivation coin toss example: First toss is head or tail doesn't affect the probability of second toss is head. Occurance of A A privides no new information about B B : {\\bf P}(B|A) = {\\bf P}(B) {\\bf P}(B|A) = {\\bf P}(B) .","title":"Independence of two events"},{"location":"courses/6.431-probability/notes/#definition-of-independence","text":"Formal definition {\\bf P}(A \\cap B) = {\\bf P}(A) \\cdot {\\bf P}(B) {\\bf P}(A \\cap B) = {\\bf P}(A) \\cdot {\\bf P}(B) symmetric with respect to events. implies {\\bf P}(A|B) = {\\bf P}(A) {\\bf P}(A|B) = {\\bf P}(A) . Applies even if {\\bf P}(A) = 0 {\\bf P}(A) = 0 . Distinction between disjoin and independence Disjoin usually means dependent, because if one event happens we know the other event will not happen.","title":"Definition of independence"},{"location":"courses/6.431-probability/notes/#independence-of-event-complements","text":"If A A and B B are independent, the A A and B^c B^c are independent. If A A and B B are independent, the A^c A^c and B^c B^c are independent.","title":"Independence of event complements"},{"location":"courses/6.431-probability/notes/#conditioning-independence","text":"Ordinary independence properties also apply to conditional independence. But independence doesn't imply conditional independence.","title":"Conditioning independence"},{"location":"courses/6.431-probability/notes/#independence-of-a-collection-of-events","text":"Ituition \"definition\": Information on some of the events doesn't change probability related to the remaining events. Formal definition","title":"Independence of a collection of events"},{"location":"courses/6.431-probability/notes/#pairwise-independence","text":"","title":"pairwise independence"},{"location":"courses/6.431-probability/notes/#independence-vs-pairwise-independence","text":"Used two independence fair coin tosses as an example to show that pariwise independence isn't enough to show independence of collection of events.","title":"independence V.S. pairwise independence"},{"location":"courses/6.431-probability/notes/#reliability","text":"The king's sibling puzzle","title":"Reliability"},{"location":"courses/6.431-probability/notes/#monty-hall-problem","text":"","title":"Monty Hall problem"},{"location":"courses/6.431-probability/notes/#lecture-4","text":"","title":"Lecture 4"},{"location":"courses/6.431-probability/notes/#lecture-5","text":"","title":"Lecture 5"},{"location":"courses/6.431-probability/notes/#definition-of-random-variables","text":"A random variable (\"r.v.\") associates a value (a number) to every possible outcome. Mathematically, A function from sample space \\Omega \\Omega to the real numbers (discrete or continuous). We can have several random variables defined on the same sample space. (In Lecture 18, X_1, X_2, \\cdots X_1, X_2, \\cdots are independent random variables from the same distribution) A function of one or several random variables is also another random variable. A concrete example to understand the concept of a random variable is take a class of students, selecting students and measuring their body mass or height will give us the random variable W W and H H . The \"Body mass index\" is another random variable that is a function of the random variable W W and H H .","title":"Definition of random variables"},{"location":"courses/6.431-probability/notes/#bernoulli-and-indicator-random-variables","text":"Bernoulli random variable X X : models a trial that results in success/failure, heads/tails, etc. indicator random variables of event A A : I_A = 1 I_A = 1 if A A occurs. connection between events and random variables. P(A) = P(I_A = 1) = P_{I_A}(1) P(A) = P(I_A = 1) = P_{I_A}(1) , P(A) P(A) : probability of event A happens P(I_A = 1) P(I_A = 1) : probablistic notition of indication R.V. when I_A I_A equal to 1 1 P_{I_A}(1) P_{I_A}(1) : PMF notation.","title":"Bernoulli and indicator random variables"},{"location":"courses/6.431-probability/notes/#lecture-6","text":"","title":"Lecture 6"},{"location":"courses/6.431-probability/notes/#lecture-7","text":"","title":"Lecture 7"},{"location":"courses/6.431-probability/notes/#independence-variances-and-binomial-variance","text":"","title":"Independence, variances, and binomial variance"},{"location":"courses/6.431-probability/notes/#lecture-8","text":"","title":"Lecture 8"},{"location":"courses/6.431-probability/notes/#exponential-random-variables","text":"PDF: f_{X}(x) = \\begin{cases} \\lambda e^{-\\lambda x}, & x \\ge 0 \\\\ 0, & x < 0 \\end{cases} f_{X}(x) = \\begin{cases} \\lambda e^{-\\lambda x}, & x \\ge 0 \\\\ 0, & x < 0 \\end{cases} Probability of greater than a a , \\begin{align*} P(X \\ge a) &= \\displaystyle \\int_a^{+\\infty} \\lambda e^{-\\lambda x}dx \\\\ &= - \\displaystyle \\int_a^{+\\infty} de^{-\\lambda x} \\\\ &= -e^{-\\lambda x}\\Big|_a^{+\\infty} = -e^{-\\lambda \\cdot +\\infty} + e^{-\\lambda a} = e^{-\\lambda a} \\end{align*} \\begin{align*} P(X \\ge a) &= \\displaystyle \\int_a^{+\\infty} \\lambda e^{-\\lambda x}dx \\\\ &= - \\displaystyle \\int_a^{+\\infty} de^{-\\lambda x} \\\\ &= -e^{-\\lambda x}\\Big|_a^{+\\infty} = -e^{-\\lambda \\cdot +\\infty} + e^{-\\lambda a} = e^{-\\lambda a} \\end{align*} Expectation of exponential random variable {\\bf E}\\big[X\\big] = \\displaystyle \\int_0^{\\infty} x \\cdot \\lambda e^{-\\lambda x} dx = 1/\\lambda {\\bf E}\\big[X\\big] = \\displaystyle \\int_0^{\\infty} x \\cdot \\lambda e^{-\\lambda x} dx = 1/\\lambda Second moment of an exponential random variable {\\bf E}\\big[X^2 \\big] = \\displaystyle \\int_0^{\\infty} x^2 \\cdot \\lambda e^{-\\lambda x} dx = 2/\\lambda^2 {\\bf E}\\big[X^2 \\big] = \\displaystyle \\int_0^{\\infty} x^2 \\cdot \\lambda e^{-\\lambda x} dx = 2/\\lambda^2 Variance of an exponential random variable \\textbf{Var}(X) = {\\bf E}\\big[X^2 \\big] - \\Big({\\bf E}\\big[X\\big]\\Big)^2 = 1/\\lambda^2 \\textbf{Var}(X) = {\\bf E}\\big[X^2 \\big] - \\Big({\\bf E}\\big[X\\big]\\Big)^2 = 1/\\lambda^2","title":"Exponential random variables"},{"location":"courses/6.431-probability/notes/#lecture-9","text":"","title":"Lecture 9"},{"location":"courses/6.431-probability/notes/#conditioning-a-continuous-random-variable-on-an-event","text":"","title":"Conditioning a continuous random variable on an event"},{"location":"courses/6.431-probability/notes/#memorylessness-of-the-exponential-pdf","text":"","title":"Memorylessness of the exponential PDF"},{"location":"courses/6.431-probability/notes/#total-probability-and-expectation-theorems","text":"","title":"Total probability and expectation theorems"},{"location":"courses/6.431-probability/notes/#lecture-10","text":"","title":"Lecture 10"},{"location":"courses/6.431-probability/notes/#total-probability-and-total-expectation-theorems","text":"Follows from the discrete cases: Rules Discrete continous Total probability p_X(x) = \\displaystyle \\sum\\limits_{y} p_Y(y)p_{X \\mid Y}(x \\mid y) p_X(x) = \\displaystyle \\sum\\limits_{y} p_Y(y)p_{X \\mid Y}(x \\mid y) f_X(x) = \\displaystyle \\int_{-\\infty}^{+\\infty} f_Y(y) f_{X \\mid Y}(x \\mid y) dy f_X(x) = \\displaystyle \\int_{-\\infty}^{+\\infty} f_Y(y) f_{X \\mid Y}(x \\mid y) dy Conditional expectation {\\bf E}\\big[X \\mid Y = y \\big] = \\displaystyle \\sum\\limits_{x} x p_{X \\mid Y}(x \\mid y) {\\bf E}\\big[X \\mid Y = y \\big] = \\displaystyle \\sum\\limits_{x} x p_{X \\mid Y}(x \\mid y) {\\bf E}\\big[X \\mid Y = y \\big] = \\displaystyle {\\int_{-\\infty}^{+\\infty}} x f_{X \\mid Y}(x \\mid y) dx {\\bf E}\\big[X \\mid Y = y \\big] = \\displaystyle {\\int_{-\\infty}^{+\\infty}} x f_{X \\mid Y}(x \\mid y) dx Total expectation {\\bf E}\\big[X \\big] = \\displaystyle \\sum\\limits_{y} p_{Y}(y){\\bf E} \\big[X \\mid Y = y \\big] {\\bf E}\\big[X \\big] = \\displaystyle \\sum\\limits_{y} p_{Y}(y){\\bf E} \\big[X \\mid Y = y \\big] derivation hint: replace with the conditional expectatioin {\\bf E}\\big[X \\big] = \\displaystyle {\\int_{-\\infty}^{+\\infty}} f_{Y}(y){\\bf E}\\big[X \\mid Y = y \\big] dy {\\bf E}\\big[X \\big] = \\displaystyle {\\int_{-\\infty}^{+\\infty}} f_{Y}(y){\\bf E}\\big[X \\mid Y = y \\big] dy derivation hint: replace with the conditional expectatioin Expected value rule {\\bf E}\\big[ g(X) \\mid Y = y \\big] = \\displaystyle {\\sum\\limits_{x}} g(x) p_{X \\mid Y}(x \\mid y) {\\bf E}\\big[ g(X) \\mid Y = y \\big] = \\displaystyle {\\sum\\limits_{x}} g(x) p_{X \\mid Y}(x \\mid y) {\\bf E}\\big[ g(X) \\mid Y = y \\big] = \\displaystyle {\\int_{-\\infty}^{+\\infty}} g(x) f_{X \\mid Y}(x \\mid y) dx {\\bf E}\\big[ g(X) \\mid Y = y \\big] = \\displaystyle {\\int_{-\\infty}^{+\\infty}} g(x) f_{X \\mid Y}(x \\mid y) dx","title":"Total probability and total expectation theorems"},{"location":"courses/6.431-probability/notes/#solved-problems-lecture-8-10","text":"","title":"Solved problems (Lecture 8 - 10)"},{"location":"courses/6.431-probability/notes/#10-buffons-needle-and-monte-carlo-simulation","text":"This problem is discussed in the text page 161, Example 3.11.","title":"10 Buffon's needle and Monte Carlo Simulation"},{"location":"courses/6.431-probability/notes/#lecture-11","text":"","title":"Lecture 11"},{"location":"courses/6.431-probability/notes/#lecture-12","text":"","title":"Lecture 12"},{"location":"courses/6.431-probability/notes/#covariance","text":"Definition: \\textbf{cov}(X, Y) = {\\bf E}\\big[(X - {\\bf E}[X]) (Y - {\\bf E}[Y])\\big] \\textbf{cov}(X, Y) = {\\bf E}\\big[(X - {\\bf E}[X]) (Y - {\\bf E}[Y])\\big] Covariance tell use whether two r.v.s tend to move together, both to low or both to high Two random variables X X and Y Y are independent indicates: \\textbf{cov}(X, Y) = 0 \\textbf{cov}(X, Y) = 0 , but the converse it not true.","title":"Covariance"},{"location":"courses/6.431-probability/notes/#covariance-properties","text":"","title":"Covariance properties"},{"location":"courses/6.431-probability/notes/#lecture-13","text":"","title":"Lecture 13"},{"location":"courses/6.431-probability/notes/#the-conditional-expectation-as-a-random-variable-bf-e-bigx-mid-ybigbf-e-bigx-mid-ybig","text":"Given a function h(x) = x^2 h(x) = x^2 for all x x , and a random variable X X . what is h(X) h(X) ? h(X) h(X) is a function of a random variable X X . h(X) h(X) itself is a random variable that take value x^2 x^2 if X X happens to take the value x x . (you should really understand this statement, it is crucial to understand the concept of conditional expectatioin) For discrete case: {\\bf E}\\big[X \\mid Y = y \\big] = \\displaystyle \\sum\\limits_{x} x p_{X \\mid Y}(x \\mid y) {\\bf E}\\big[X \\mid Y = y \\big] = \\displaystyle \\sum\\limits_{x} x p_{X \\mid Y}(x \\mid y) , in which different vaule of y y will give us the different value of conditional expectation of X X . Then we can really treat {\\bf E}\\big[X \\mid Y = y \\big] {\\bf E}\\big[X \\mid Y = y \\big] as a function of y y , noted as g(y) g(y) . From the above reasoning, because Y Y is a random variable, g(Y) g(Y) is also a reandom variable, that takes the value of g(y) = {\\bf E}\\big[X \\mid Y = y \\big] g(y) = {\\bf E}\\big[X \\mid Y = y \\big] if Y Y happens to take the value y y . g(Y) = {\\bf E}\\big[X \\mid Y\\big] g(Y) = {\\bf E}\\big[X \\mid Y\\big] , is a function of Y Y a random variable has a distribution, mean, variance, etc.","title":"The conditional expectation as a random variable {\\bf E} \\big[X \\mid Y\\big]{\\bf E} \\big[X \\mid Y\\big]"},{"location":"courses/6.431-probability/notes/#the-law-of-iterated-expectations","text":"By calculate {\\bf E}\\Big[{\\bf E}\\big[X \\mid Y\\big]\\Big] {\\bf E}\\Big[{\\bf E}\\big[X \\mid Y\\big]\\Big] , we could abtain that it is equal to {\\bf E}\\big[X\\big] {\\bf E}\\big[X\\big]","title":"The law of iterated expectations"},{"location":"courses/6.431-probability/notes/#the-conditional-variance-as-a-random-variable-textbfvarx-mid-y-ytextbfvarx-mid-y-y","text":"definition: \\textbf{Var}(X \\mid Y = y) = {\\bf E}\\Big[\\big(X - {\\bf E}\\big[X \\mid Y = y \\big]\\big)^2\\mid Y = y\\Big] \\textbf{Var}(X \\mid Y = y) = {\\bf E}\\Big[\\big(X - {\\bf E}\\big[X \\mid Y = y \\big]\\big)^2\\mid Y = y\\Big] \\textbf{Var}(X\\mid Y) \\textbf{Var}(X\\mid Y) is also a random variable, it takes the value \\textbf{Var}(X\\mid y) \\textbf{Var}(X\\mid y) when Y Y happens to take the value of y y . Total variance rule: \\textbf{Var}(X) = {\\bf E}\\Big[\\textbf{Var}(X \\mid Y)\\Big] + \\textbf{Var}\\Big({\\bf E}\\big[X\\mid Y\\big]\\Big) \\textbf{Var}(X) = {\\bf E}\\Big[\\textbf{Var}(X \\mid Y)\\Big] + \\textbf{Var}\\Big({\\bf E}\\big[X\\mid Y\\big]\\Big)","title":"The conditional variance as a random variable \\textbf{Var}(X \\mid Y = y)\\textbf{Var}(X \\mid Y = y)"},{"location":"courses/6.431-probability/notes/#lecture-14-introduction-to-bayesian-inference","text":"","title":"Lecture 14 Introduction to Bayesian Inference"},{"location":"courses/6.431-probability/notes/#the-bayesian-inference-framework","text":"Treat the unknown \\Theta \\Theta as a random variable. with prior p_{\\Theta} p_{\\Theta} or f_{\\Theta} f_{\\Theta} . With the observations X X , using Bayes rule to obtain the posterior probability of \\Theta \\Theta , P_{\\Theta|X}(\\cdot|X=x) P_{\\Theta|X}(\\cdot|X=x) . Once the posterior is available, the estimator can be obtained via maximize a posterioral probability (MAP) rule or leaset mean squares (LMS). The performance can be measured by \"probability of error\" or \"mean squared error\".","title":"The Bayesian inference framework"},{"location":"courses/6.431-probability/notes/#conditional-probability-of-error-and-total-probability-of-error","text":"The conditional probability of other estimators that made a mistake. One of the properties of MAP rule is the it guarantee smallest probability of error. The taotal probability of error is an abstract notion of error for MAP rule to estimator. It can be caculated using total probability theory.","title":"Conditional probability of error and total probability of error"},{"location":"courses/6.431-probability/notes/#discrete-unknowns-discrete-observation-example-8-exercise-discrete-unknowns","text":"Let \\Theta_1 \\Theta_1 and \\Theta_2 \\Theta_2 be some unobserved Bernoulli random variables and let be an observation. Conditional on X = x X = x , the posterior joint PMF of \\Theta_1 \\Theta_1 and \\Theta_2 \\Theta_2 is given by p_{\\Theta _1,\\Theta _2\\mid X}(\\theta _1,\\theta _2\\mid x) = \\begin{cases} 0.26, & \\mbox{if } \\theta _1=0, \\theta _2=0, \\\\ 0.26, & \\mbox{if } \\theta _1=0, \\theta _2=1, \\\\ 0.21, & \\mbox{if } \\theta _1=1, \\theta _2=0, \\\\ 0.27, & \\mbox{if } \\theta _1=1, \\theta _2=1, \\\\ 0, & \\mbox{otherwise.} \\end{cases} p_{\\Theta _1,\\Theta _2\\mid X}(\\theta _1,\\theta _2\\mid x) = \\begin{cases} 0.26, & \\mbox{if } \\theta _1=0, \\theta _2=0, \\\\ 0.26, & \\mbox{if } \\theta _1=0, \\theta _2=1, \\\\ 0.21, & \\mbox{if } \\theta _1=1, \\theta _2=0, \\\\ 0.27, & \\mbox{if } \\theta _1=1, \\theta _2=1, \\\\ 0, & \\mbox{otherwise.} \\end{cases} What is the estimate of (\\Theta_1, \\Theta_2) (\\Theta_1, \\Theta_2) provided by MAP rule? (1, 1), because the p_{\\Theta _1,\\Theta _2\\mid X}(1, 1\\mid x) p_{\\Theta _1,\\Theta _2\\mid X}(1, 1\\mid x) is the maximum of all. What is the MAP estimate of \\Theta_1 \\Theta_1 based on X X , that is, the one that maximizes p_{\\Theta_1 \\mid X}(\\theta_1 \\mid x) p_{\\Theta_1 \\mid X}(\\theta_1 \\mid x) ? 0, from the marginal PMF, you can see that p_{\\Theta_1 \\mid X}(0 \\mid x) p_{\\Theta_1 \\mid X}(0 \\mid x) is maximized The moral of this problem is that an estimate of $ \\Theta_1 $ obtained by identifying the maximum of the join PMF of all unknown random variables can be different from the MAP estimate of \\Theta_1 \\Theta_1 from the marginal PMF.","title":"Discrete unknowns, discrete observation example (8. Exercise: Discrete unknowns)"},{"location":"courses/6.431-probability/notes/#discrete-parameter-continuous-observation","text":"Digital signal transmission","title":"Discrete parameter, continuous observation"},{"location":"courses/6.431-probability/notes/#continous-parameter-continuous-observation","text":"Analog signal transmission","title":"continous parameter, continuous observation"},{"location":"courses/6.431-probability/notes/#lecture-15-linear-models-with-normal-noise","text":"","title":"Lecture 15 Linear models with normal noise"},{"location":"courses/6.431-probability/notes/#recognizing-normal-pdfs","text":"f_X(x) = c \\cdot e^{-(\\alpha x^2 + \\beta x + \\gamma)} f_X(x) = c \\cdot e^{-(\\alpha x^2 + \\beta x + \\gamma)} is a normal random variable with \\mu = -\\frac{\\beta}{2\\alpha} \\mu = -\\frac{\\beta}{2\\alpha} and variance \\frac{1}{2\\alpha} \\frac{1}{2\\alpha}","title":"recognizing normal PDFs"},{"location":"courses/6.431-probability/notes/#the-mean-squared-error","text":"The most important take away from this section is that for normal unknown and normal noise signal, X = \\Theta + W X = \\Theta + W , no matter which x_i x_i we observed, the mean squared error estimates {\\hat \\theta} {\\hat \\theta} are the same (the variance of \\Theta \\Theta ). In other words, the remaining uncertainty about \\theta \\theta after an observation is the same no matter what the observation is. The observation only determined the estimated mean of the random variable \\theta \\theta . (See the drawing at the end of the lecture)","title":"The mean squared error"},{"location":"courses/6.431-probability/notes/#measurement-estimate-and-learning","text":"How to measure the gravitational attraction constant","title":"Measurement, estimate, and learning"},{"location":"courses/6.431-probability/notes/#lecture-16-least-mean-square-lms-estimation","text":"","title":"Lecture 16 Least mean square (LMS) estimation"},{"location":"courses/6.431-probability/notes/#lms-estimation-without-any-observations","text":"Given the prior p_{\\Theta}(\\theta) p_{\\Theta}(\\theta) unknow random variable \\Theta \\Theta , what's you realy want to estimate? You may interested in a point estimate. You may interested in find the estimator. With no observation, to minimize the mean squared error (MSE), {\\bf E}\\Big[\\big(\\Theta - \\hat \\theta \\big)^2\\Big] {\\bf E}\\Big[\\big(\\Theta - \\hat \\theta \\big)^2\\Big] , the value \\hat \\theta = {\\bf E}\\big[X\\big] \\hat \\theta = {\\bf E}\\big[X\\big] minimize the error. and the minimum error equal to \\textbf{Var}(\\Theta) \\textbf{Var}(\\Theta) Optimal mean squared error: {\\bf E}\\Big[\\big(\\Theta - {\\bf E}\\big[\\Theta \\big] \\big)^2\\Big] = \\textbf{Var}(\\Theta) {\\bf E}\\Big[\\big(\\Theta - {\\bf E}\\big[\\Theta \\big] \\big)^2\\Big] = \\textbf{Var}(\\Theta) . Because {\\bf E}\\big[\\Theta \\big] {\\bf E}\\big[\\Theta \\big] is the optimal value that minimize the MSE, if we replace \\hat \\theta \\hat \\theta with {\\bf E}\\big[\\Theta \\big] {\\bf E}\\big[\\Theta \\big] in the expression MSE, it coincidently match the definition of variance. You can alse use the {\\bf E}\\Big[\\big(\\Theta - \\hat \\theta \\big)^2\\Big] = \\textbf{Var}(\\Theta - \\hat \\theta) + \\Big({\\bf E}\\big[\\Theta - \\hat \\theta \\big]\\Big)^2 {\\bf E}\\Big[\\big(\\Theta - \\hat \\theta \\big)^2\\Big] = \\textbf{Var}(\\Theta - \\hat \\theta) + \\Big({\\bf E}\\big[\\Theta - \\hat \\theta \\big]\\Big)^2 to derive \\hat \\theta = {\\bf E}\\big[X\\big] \\hat \\theta = {\\bf E}\\big[X\\big] is the optimal value.","title":"LMS estimation without any observations"},{"location":"courses/6.431-probability/notes/#lms-estimation-single-unknown-and-observation","text":"Goal: interested in a point estimate \\hat \\theta \\hat \\theta of unknow random variable \\Theta \\Theta , with prior p_{\\Theta}(\\theta) p_{\\Theta}(\\theta) . (Given observation X X ; model p_{X\\mid \\Theta}(x\\mid \\theta) p_{X\\mid \\Theta}(x\\mid \\theta) ) We want to minimize the MSE, because this time we have the particular observation X = x X = x , we now live in a conditional universe, we need to minimize the conditional MSE, {\\bf E}\\Big[\\big(\\Theta - {\\bf E}\\big[\\Theta \\big] \\big)^2 \\mid X = x \\Big] {\\bf E}\\Big[\\big(\\Theta - {\\bf E}\\big[\\Theta \\big] \\big)^2 \\mid X = x \\Big] , the optimal value is \\hat \\theta = {\\bf E}\\big[\\Theta \\mid X = x\\big] \\hat \\theta = {\\bf E}\\big[\\Theta \\mid X = x\\big] By the \"happens to take\" reasoning and the iterated expection rule, we can achieve the conclusion: that \\hat \\Theta_{LMS} = {\\bf E}\\big[\\Theta \\mid X\\big] \\hat \\Theta_{LMS} = {\\bf E}\\big[\\Theta \\mid X\\big] minimize the MSE {\\bf E}\\Big[\\big(\\Theta - g(X) \\big)^2 \\Big] {\\bf E}\\Big[\\big(\\Theta - g(X) \\big)^2 \\Big] over all estimators \\hat \\Theta = g(X) \\hat \\Theta = g(X) .","title":"LMS estimation; single unknown and observation"},{"location":"courses/6.431-probability/notes/#lms-performance-evaluation","text":"MSE: {\\bf E}\\Big[\\big(\\Theta - {\\bf E}\\big[\\Theta \\mid X = x\\big] \\big)^2 \\mid X = x \\Big] = \\textbf{Var}(\\Theta \\mid X = x) {\\bf E}\\Big[\\big(\\Theta - {\\bf E}\\big[\\Theta \\mid X = x\\big] \\big)^2 \\mid X = x \\Big] = \\textbf{Var}(\\Theta \\mid X = x) Expected performance of the design: {\\bf E}\\Big[\\big(\\Theta - {\\bf E}\\big[\\Theta \\mid X\\big] \\big)^2 \\Big] = {\\bf E}\\Big[\\textbf{Var}(\\Theta \\mid X)\\Big] {\\bf E}\\Big[\\big(\\Theta - {\\bf E}\\big[\\Theta \\mid X\\big] \\big)^2 \\Big] = {\\bf E}\\Big[\\textbf{Var}(\\Theta \\mid X)\\Big] LMS relavant to estimation (not hypothesis testing) Same as MAP if the posterior is unimodal and symmetric around the mean. e.g. when the posterior is normal (the case in \"linear-normal\" models)","title":"LMS performance evaluation"},{"location":"courses/6.431-probability/notes/#the-multidimensional-case","text":"","title":"The multidimensional case"},{"location":"courses/6.431-probability/notes/#lecture-17-linear-least-mean-squares-llms-estimation","text":"","title":"Lecture 17 Linear least mean squares (LLMS) estimation"},{"location":"courses/6.431-probability/notes/#lecture-18-inequalities-convergence-and-the-weak-law-of-large-numbers","text":"","title":"Lecture 18 Inequalities, convergence, and the Weak Law of Large Numbers"},{"location":"courses/6.431-probability/notes/#the-weak-law-of-large-numbers","text":"X_1, X_2, \\cdots, X_n X_1, X_2, \\cdots, X_n , i.i.d.; finite mean \\mu \\mu and variance \\sigma^2 \\sigma^2 Sample mean M_n M_n : it is not a constant, but a function of multiple random variables. You can understand it this way. In a experiment which you draw the exam score of student one by one, once you finish the first draw X_1 X_1 , the first sample is a fixed real number. But before your first draw, X_1 X_1 can be any score and it is a random variable. After you draw a total of n n scores, you have all the fixed value of X_1, X_2, \\cdots, X_n X_1, X_2, \\cdots, X_n , the sample mean is determined for this draw. However, abstractly, sample mean is a random variable, if you draw another n n example scores to get the sample mean, you get a different set of X_1, X_2, \\cdots, X_n X_1, X_2, \\cdots, X_n . The randomness comes from the randomness of different experiments, in each of the experiment, the sample X_i X_i is not random. {\\bf E}\\big[M_n\\big] =\\mu {\\bf E}\\big[M_n\\big] =\\mu . Two level of averaging. M_n M_n itself is averaging over n samples. Since these n samples are drawn randomly, taking the expectation of M_n M_n is averaging all possible sample means obtained through many (infinity) experiments. \\textbf{Var}(M_n) = \\frac{1}{n^2} \\cdot n \\cdot \\textbf{Var}(X_1) = \\frac{\\sigma^2}{n} \\textbf{Var}(M_n) = \\frac{1}{n^2} \\cdot n \\cdot \\textbf{Var}(X_1) = \\frac{\\sigma^2}{n} . Apply Chebyshev inequality: \\textbf{P}(|M_n - \\mu| \\ge \\epsilon) \\le \\frac{\\textbf{var}(M_n)}{\\epsilon^2} = \\frac{\\sigma^2}{n\\epsilon^2} \\rightarrow 0 \\textbf{P}(|M_n - \\mu| \\ge \\epsilon) \\le \\frac{\\textbf{var}(M_n)}{\\epsilon^2} = \\frac{\\sigma^2}{n\\epsilon^2} \\rightarrow 0 , as n \\rightarrow \\infty n \\rightarrow \\infty Weak Law of Large Numbers For \\epsilon > 0 \\epsilon > 0 , \\textbf{P}(|M_n - \\mu| \\ge \\epsilon) = \\textbf{P}\\Big(\\Big|\\frac{X_1 + X_2 + \\cdots + X_n}{n} - \\mu\\Big| \\ge \\epsilon \\Big) \\rightarrow 0 \\textbf{P}(|M_n - \\mu| \\ge \\epsilon) = \\textbf{P}\\Big(\\Big|\\frac{X_1 + X_2 + \\cdots + X_n}{n} - \\mu\\Big| \\ge \\epsilon \\Big) \\rightarrow 0 , as n \\rightarrow \\infty n \\rightarrow \\infty .","title":"The Weak Law of Large Numbers"},{"location":"courses/6.431-probability/notes/#interpreting-the-wlln","text":"One experiment many measurements X_i = \\mu + W_i X_i = \\mu + W_i , where W_i W_i is noise, \\textbf{E}[W_i] = 0 \\textbf{E}[W_i] = 0 , independent W_i W_i . Sample mean M_n M_n is unlikely to be far off from true mean \\mu \\mu Many independent repetitions of the same experiments event A A , with p = \\textbf{P}(A) p = \\textbf{P}(A) X_i X_i : indicator of event A A The sampel mean M_n M_n is the empirical frequency of event A A","title":"Interpreting the WLLN"},{"location":"courses/6.431-probability/notes/#application-of-wlln-polling","text":"The probability of error greater than \\epsilon \\epsilon is smaller than a certain probability. You can use Chebyshev inequality and WLLN to estimate how many samples you need to fulfill a specific error probability requirement.","title":"Application of WLLN - polling"},{"location":"courses/6.431-probability/notes/#convergence-in-probability","text":"Definition: A sequence Y_n Y_n converges in probability to a number a a if For any \\epsilon > 0 \\epsilon > 0 , \\displaystyle \\lim_{n\\rightarrow\\infty}\\textbf{P}(|Y_n - a| \\ge \\epsilon) = 0 \\displaystyle \\lim_{n\\rightarrow\\infty}\\textbf{P}(|Y_n - a| \\ge \\epsilon) = 0 * Comparison between ordinary convergence and convergence in probability:","title":"Convergence in probability"},{"location":"courses/6.431-probability/notes/#convergence-in-probability-examples","text":"convergence in probability doesn't imply convergence of the expectations. How to find what value it converges to? Make an educated conjecture about the limit \\tau \\tau , write {\\bf P}(|Y_n - \\tau| \\ge \\epsilon) {\\bf P}(|Y_n - \\tau| \\ge \\epsilon) , and derive the value of it to where you can observe from the expression that the probability \\to 0 \\to 0 , when n \\to \\infty n \\to \\infty .","title":"Convergence in probability examples"},{"location":"courses/6.431-probability/notes/#related-topics","text":"Better bounds/approximations on tail probabilities Markov and Chebyshev inequalities Chernoff bound Central limit theorem Different types of convergence Convergence in probability Convergence \"with probability 1\" Strong law of large numbers Convergence of a sequence of distributions (CDFs) to a limiting CDF.","title":"Related topics"},{"location":"courses/6.431-probability/notes/#lecture-19-the-central-limit-theorem-clt","text":"Considering the sum of random variable S_n = X_1 + X_2, + \\cdots + X_n S_n = X_1 + X_2, + \\cdots + X_n , ( X_1, X_2, \\cdots, X_n X_1, X_2, \\cdots, X_n are i.i.d. with finite mean \\mu \\mu and variance \\sigma^2 \\sigma^2 ) Z_n = \\frac{S_n - n\\mu}{\\sqrt{n}\\sigma} Z_n = \\frac{S_n - n\\mu}{\\sqrt{n}\\sigma} , then we have the central limit Theorem, Central Limit Theorem: For every z z : \\displaystyle \\lim_{n \\to \\infty}\\textbf{P}(Z_n \\le z) = \\textbf{P}(Z \\le z) \\displaystyle \\lim_{n \\to \\infty}\\textbf{P}(Z_n \\le z) = \\textbf{P}(Z \\le z)","title":"Lecture 19 The Central Limit Theorem (CLT)"},{"location":"courses/6.431-probability/notes/#what-exactly-does-the-clt-say-practice","text":"The practice of normal approximations: We have linear expression between S_n S_n and Z_n Z_n : S_n = \\sqrt n \\sigma Z_n + n\\mu S_n = \\sqrt n \\sigma Z_n + n\\mu , since Z_n Z_n can be treated as if it were normal, S_n S_n can be treated as if normal: N(n\\mu, n\\sigma^2) N(n\\mu, n\\sigma^2) Can we use the CLT when n is \"moderate\"? i.e. n = 30? Usually, yes. When the distribution of X X has common features with a normal distribution. symmetry and unimodality help","title":"What exactly does the CLT say? - Practice"},{"location":"courses/6.431-probability/notes/#central-limit-theory-examples","text":"\\textbf{P}(S_{\\color{red}n} \\le {\\color{red}a}) \\approx {\\color{red}b} \\textbf{P}(S_{\\color{red}n} \\le {\\color{red}a}) \\approx {\\color{red}b} Given two parameters, find the third. Package weight X_i X_i , i.i.d. exponential, \\lambda = 1/2 \\lambda = 1/2 , \\mu = \\sigma = 2 \\mu = \\sigma = 2 Load container with n = 100 n = 100 packages, what's the probability that the overall weight is heavier than 210? \\textbf{P}(S_n \\ge 210) = ? \\textbf{P}(S_n \\ge 210) = ? Load container with n = 100 n = 100 packages. Choose the \"capacity\" a so that \\textbf{P}(S_n \\ge a) \\approx 0.05 \\textbf{P}(S_n \\ge a) \\approx 0.05 Fix the capacity at 210, how large can n n be, so that \\textbf{P}(S_n \\ge 210) \\approx 0.05 \\textbf{P}(S_n \\ge 210) \\approx 0.05 Load container until wight exceeds 210, N N is the number of packages loaded, find \\textbf{P}(N > 100) \\textbf{P}(N > 100)","title":"Central Limit Theory examples"},{"location":"courses/6.431-probability/notes/#airline-booking","text":"For any given flight, an airline tries to sell as many tickets as possible. Suppose that on average, of ticket holders fail to show up, all independent of one another. Knowing this, an airline will sell more tickets than there are seats available (i.e., overbook the flight) and hope that there is a sufficient number of ticket holders who do not show up, to compensate for its overbooking. Using the Central Limit Theorem, determine, the maximum number of tickets an airline can sell on a flight with 400 seats so that it can be approximately confident that all ticket holders who do show up will be able to board the plane. Use the de Moivre-Laplace -correction in your calculations. Hint: You may have to solve numerically a quadratic equation. Solution: Each ticket can either be used or not used by the the passager. It can be modeled with the random variable: X \\sim \\operatorname{Bern} \\left({p}\\right) X \\sim \\operatorname{Bern} \\left({p}\\right) , p=0.8 p=0.8 , then the total passergers presented at airport is S_n = X_1 + X_2 + \\cdots + X_n S_n = X_1 + X_2 + \\cdots + X_n , which is a Binomial: S_n \\sim \\operatorname{B} \\left({n, p}\\right) S_n \\sim \\operatorname{B} \\left({n, p}\\right) . For binomial random variable, \\mu = np = 0.8n, \\sigma^2 = np(1-p) = 0.16n \\mu = np = 0.8n, \\sigma^2 = np(1-p) = 0.16n . Our requirements is \\textbf{P}(S_n \\le 400) \\approx 0.99 \\textbf{P}(S_n \\le 400) \\approx 0.99 . Normalize S_n S_n in the probability and treat the normalized random variable Z_n Z_n as a normal distribution, refer to the normal table, and solve the number n n . \\begin{align*} \\textbf{P}(S_n \\le 400) &\\approx 0.99 \\\\ \\textbf{P}\\Big(\\frac{S_n - 0.8n}{0.4\\sqrt{n}} \\le \\frac{400.5 - 0.8n}{0.4\\sqrt{n}}\\Big) &\\approx 0.99 \\\\ \\textbf{P}\\Big(Z_n \\le \\frac{400.5 - 0.8n}{0.4\\sqrt{n}}\\Big) &\\approx 0.99 \\\\ \\Phi\\Big(\\frac{400.5 - 0.8n}{0.4\\sqrt{n}}\\Big) &\\approx 0.99 \\\\ \\frac{400.5 - 0.8n}{0.4\\sqrt{n}} &= 2.33 \\end{align*} \\begin{align*} \\textbf{P}(S_n \\le 400) &\\approx 0.99 \\\\ \\textbf{P}\\Big(\\frac{S_n - 0.8n}{0.4\\sqrt{n}} \\le \\frac{400.5 - 0.8n}{0.4\\sqrt{n}}\\Big) &\\approx 0.99 \\\\ \\textbf{P}\\Big(Z_n \\le \\frac{400.5 - 0.8n}{0.4\\sqrt{n}}\\Big) &\\approx 0.99 \\\\ \\Phi\\Big(\\frac{400.5 - 0.8n}{0.4\\sqrt{n}}\\Big) &\\approx 0.99 \\\\ \\frac{400.5 - 0.8n}{0.4\\sqrt{n}} &= 2.33 \\end{align*} Solve n n to obtain n = 475 n = 475","title":"Airline booking"},{"location":"courses/6.431-probability/notes/#normal-approximation-to-the-binomial","text":"Take X_i X_i as independent Bernuolli random variables: X \\sim \\operatorname{Bern} \\left({p}\\right) X \\sim \\operatorname{Bern} \\left({p}\\right) , 0 < p < 1 0 < p < 1 , then S_n = X_1 + X_2 + \\cdots + X_n S_n = X_1 + X_2 + \\cdots + X_n is Binomial: X \\sim \\operatorname{B} \\left({n, p}\\right) X \\sim \\operatorname{B} \\left({n, p}\\right) . Binomial random variable S_n S_n have \\mu = np \\mu = np , \\sigma^2=np(1-p) \\sigma^2=np(1-p) . According CLT, The normalized random variable \\frac{S_n-np}{\\sqrt{np(1-p)}} \\frac{S_n-np}{\\sqrt{np(1-p)}} is a standard normal. In order to find the \\textbf{P}(S_n \\le 21) \\textbf{P}(S_n \\le 21) (given n = 36 n = 36 , p = 0.5 p = 0.5 ), we use the equvilent of events to transform the probability \\textbf{P}(S_n \\le 21) \\textbf{P}(S_n \\le 21) into another probability about a normal random vairable \\textbf{P}\\Big(\\frac{S_n - 18}{3} \\le \\frac{21-28}{3}\\Big) = \\textbf{P}(Z_n \\le 1) = \\Phi(1) = 0.8413 \\textbf{P}\\Big(\\frac{S_n - 18}{3} \\le \\frac{21-28}{3}\\Big) = \\textbf{P}(Z_n \\le 1) = \\Phi(1) = 0.8413 . The 1/2 correction for integer random variables. To get a improved estimate of binomial from normal, we can take the middle point of two integers. Instead using \\textbf{P}(S_n \\le 21) \\textbf{P}(S_n \\le 21) or \\textbf{P}(S_n \\lt 22) \\textbf{P}(S_n \\lt 22) , we use \\textbf{P}(S_n \\lt 21.5) \\textbf{P}(S_n \\lt 21.5) which is very accurate De Moivre-Laplace CLT to the binomial. To estimate \\textbf{P}(S_n=19) \\textbf{P}(S_n=19) , we take \\textbf{P}(18.5 \\le S_n \\le 19.5) \\textbf{P}(18.5 \\le S_n \\le 19.5) and get an accurate estimate.","title":"Normal approximation to the binomial"},{"location":"courses/6.431-probability/notes/#lecture-20-introduction-to-classical-statistics","text":"","title":"Lecture 20 Introduction to classical statistics"},{"location":"courses/6.431-probability/notes/#overview-of-the-classical-statistical-framework","text":"attributes Bayesian classical unkonwn \\Theta \\Theta is r.v. \\theta \\theta is a fixed value known p_\\Theta p_\\Theta prior distribution and samples X_i X_i only sample X_i X_i model p_{X \\mid \\Theta} p_{X \\mid \\Theta} , where observation is generated p_X(x; \\theta) p_X(x; \\theta) , \\theta \\theta is a real-valued parameter of the model, method use bayes rule to obtain p_{\\Theta \\mid X} p_{\\Theta \\mid X} , \\big(p_X = {\\int_{-\\infty}^{+\\infty}} p_\\Theta \\cdot p_{X \\mid \\Theta}d\\theta\\big) \\big(p_X = {\\int_{-\\infty}^{+\\infty}} p_\\Theta \\cdot p_{X \\mid \\Theta}d\\theta\\big) , then determind the estimate Design a estimator \\hat\\Theta \\hat\\Theta , to keep estimate error \\hat\\Theta - \\theta \\hat\\Theta - \\theta small estimator MAP, LMS ML","title":"Overview of the classical statistical framework"},{"location":"courses/6.431-probability/notes/#confidence-intervals-interpretation","text":"Every day, I try to estimate an unknown parameter using a fresh data set. I look at the data and then I use some formulas to calculate a 70% confidence interval, [\\hat\\Theta^-, \\hat\\Theta^+] [\\hat\\Theta^-, \\hat\\Theta^+] , based on the day's data.","title":"Confidence intervals interpretation"},{"location":"courses/6.431-probability/notes/#classical-statsistic-interpretation","text":"If today I got the confidence interval [0.41, 0.47] [0.41, 0.47] , that doesn't mean there is 70\\% 70\\% probability that the true value will be inside [0.41, 0.47] [0.41, 0.47] . What the confidence interval is that 70% of those confidence intervals you obtained in all those days will include the true value. The \"confidence\" (or probability) can be think of \\frac{\\text{All CIs that include the true value}}{\\text{All CIs that include the true value + CIs that exclude the true value}} \\frac{\\text{All CIs that include the true value}}{\\text{All CIs that include the true value + CIs that exclude the true value}} . You cannot speak about a particular interval, and once you do that, there is no randomness anymore, the true value is either inside the interval or not, there are no other probabilities so to speak.","title":"Classical statsistic interpretation"},{"location":"courses/6.431-probability/notes/#bayes-interpretation-bayesians-confidence-interval","text":"Alternatively, I decided to use a Bayesian approach, by viewing the unknown parameter, denoted by \\Theta \\Theta , as a continuous random variable and assuming a prior PDF for \\Theta \\Theta . I observe a specific value x x , calculate the posterior f_{\\Theta}{X}(\\theta|x) f_{\\Theta}{X}(\\theta|x) , and find out that \\begin{align*} \\int _{0.41}^{0.47} f_{\\Theta |X}(\\theta \\, |\\, x)\\, d\\theta =0.70. \\end{align*} \\begin{align*} \\int _{0.41}^{0.47} f_{\\Theta |X}(\\theta \\, |\\, x)\\, d\\theta =0.70. \\end{align*} This time, I can say that there is a probability 70\\% 70\\% that the unknown parameter is inside the (Bayesian) confidence interval [0.41, 0.47] [0.41, 0.47] .","title":"Bayes' interpretation (Bayesian's Confidence Interval)"},{"location":"courses/6.431-probability/notes/#confidence-intervals-for-the-estimation-of-the-mean","text":"I asked you to estimate the mean of i.i.d variables X_1, X_2, \\cdots, X_n X_1, X_2, \\cdots, X_n with true mean \\theta \\theta , and variance \\sigma^2 \\sigma^2 , and the estimate should achieve 95\\% 95\\% confidence interval. How you proceed with it? By Central Limit Theory, your estimate values \\hat\\Theta \\hat\\Theta is an normal distribution. By standardizing it, you get the standard normal Z_n = \\frac{\\hat\\Theta - \\theta}{\\sigma/\\sqrt{n}} Z_n = \\frac{\\hat\\Theta - \\theta}{\\sigma/\\sqrt{n}} . 95\\% 95\\% confidence interverl means standard normal is between symetric 95\\% 95\\% intervals [-b, b] [-b, b] . By looking up the normal table, b = 1.69 b = 1.69 corresponds to the probability 97.5\\% 97.5\\% , so \\textbf{P}(-1.69 \\le Z_n \\le 1.69) = 95\\% \\textbf{P}(-1.69 \\le Z_n \\le 1.69) = 95\\% thus \\textbf{P}\\big(\\frac{|\\hat\\Theta - \\theta|}{\\sigma/\\sqrt{n}} \\le 1.69\\big) = 95\\% \\textbf{P}\\big(\\frac{|\\hat\\Theta - \\theta|}{\\sigma/\\sqrt{n}} \\le 1.69\\big) = 95\\% , which can be rewrite as \\begin{align*} \\textbf{P}\\Big(\\hat\\Theta -\\frac{1.96\\sigma}{\\sqrt n} \\le \\theta \\le \\hat\\Theta +\\frac{1.96\\sigma}{\\sqrt n}\\Big) = 95\\%. \\end{align*} \\begin{align*} \\textbf{P}\\Big(\\hat\\Theta -\\frac{1.96\\sigma}{\\sqrt n} \\le \\theta \\le \\hat\\Theta +\\frac{1.96\\sigma}{\\sqrt n}\\Big) = 95\\%. \\end{align*} This is the way to construct the confidence interval. If you have the \\sigma \\sigma and n n , you have a concrete 95\\% 95\\% confidence interval to report. However, the \\sigma \\sigma is usually unknown, what you can do is to use the estimated \\hat\\Theta \\hat\\Theta to estimate the \\sigma \\sigma .","title":"Confidence intervals for the estimation of the mean"},{"location":"courses/6.431-probability/notes/#lecture-21","text":"","title":"Lecture 21"},{"location":"courses/6.431-probability/notes/#lecture-22","text":"","title":"Lecture 22"},{"location":"courses/6.431-probability/notes/#lecture-23","text":"","title":"Lecture 23"},{"location":"courses/6.431-probability/notes/#lecture-24","text":"","title":"Lecture 24"},{"location":"courses/6.431-probability/notes/#lecture-25","text":"","title":"Lecture 25"},{"location":"courses/6.431-probability/notes/#lecture-26","text":"","title":"Lecture 26"},{"location":"courses/cs224n/Part%202%20Neural%20Networks%20and%20Backpropagation/","text":"Part 2 Neural Networks and Backpropagation \u00b6 Date: April 15th 2019 We discussed the softmax classifier in Part 1 and its major drawback that the classifier only gives linear decision boundaries. In Part 2, Neural Networks will be introduced to demonstrate that it can learn much more complex functions and nonlinear decision boundaries. Intro to Neural Networks \u00b6 Biological Neuron Mathematical Model Simplified Neuron The neuron can be modeled as a binary logistic regression unit as in the right figure above. It can be further simplified as following functions, \\begin{equation*} h_{w, b} (\\boldsymbol{x})=f(\\boldsymbol{w}^\\mathsf{T} \\boldsymbol{x} + b) \\\\ f(z) = \\frac{1}{1 + e^{-z}}. \\end{equation*} \\begin{equation*} h_{w, b} (\\boldsymbol{x})=f(\\boldsymbol{w}^\\mathsf{T} \\boldsymbol{x} + b) \\\\ f(z) = \\frac{1}{1 + e^{-z}}. \\end{equation*} \\boldsymbol{x} \\boldsymbol{x} is the inputs \\boldsymbol{w} \\boldsymbol{w} is the weights b b is a bias term h h is a hidden layer function f f is a nonlinear activation function (sigmoid, tanh, etc.) If we feed a vector of inputs through a bunch of logistic regression (sigmoid) functions, then we get a vector of outputs. The CS224N lecture note 3 section 1.2 has a complete derivation on multiple sigmoid units. But we don\u2019t have to decide ahead of time what variables these logistic regressions are trying to predict! It is the loss function that will direct what the intermediate hidden variables should be, to do a good job at predicting the targets for the next layer, etc. Forward Propagation in Matrix Notation \u00b6 In a multilayer neural network, not only did we have multiple sigmoid units, but we also have more than one layer. Let's explicitly write down the signal transformation (aka forward propagation) from one layer to another referring to this network from Andrew's ML course. We will use the following notation convention a^{(j)}_i a^{(j)}_i to represent the \"activation\" of unit i i in layer j j . W^{(j)} W^{(j)} to represent the matrix of weights controlling function mapping from layer j j to layer j + 1 j + 1 . The value at each node can be calculated as \\begin{equation*} a^{(2)}_1 = f(w^{(1)}_{10} x_0 + w^{(1)}_{11} x_1 + w^{(1)}_{12} x_2 + w^{(1)}_{13} x_3) \\\\ a^{(2)}_2 = f(w^{(1)}_{20} x_0 + w^{(1)}_{21} x_1 + w^{(1)}_{22} x_2 + w^{(1)}_{23} x_3) \\\\ a^{(2)}_3 = f(w^{(1)}_{30} x_0 + w^{(1)}_{31} x_1 + w^{(1)}_{32} x_2 + w^{(1)}_{33} x_3) \\\\ \\text{and} \\\\ h_{w} (\\boldsymbol{x}) = a^{(3)}_1 = f(w^{(2)}_{10} a^{(2)}_0 + w^{(2)}_{11} a^{(2)}_1 + w^{(2)}_{12} a^{(2)}_2 + w^{(2)}_{13} a^{(2)}_3) \\end{equation*} \\begin{equation*} a^{(2)}_1 = f(w^{(1)}_{10} x_0 + w^{(1)}_{11} x_1 + w^{(1)}_{12} x_2 + w^{(1)}_{13} x_3) \\\\ a^{(2)}_2 = f(w^{(1)}_{20} x_0 + w^{(1)}_{21} x_1 + w^{(1)}_{22} x_2 + w^{(1)}_{23} x_3) \\\\ a^{(2)}_3 = f(w^{(1)}_{30} x_0 + w^{(1)}_{31} x_1 + w^{(1)}_{32} x_2 + w^{(1)}_{33} x_3) \\\\ \\text{and} \\\\ h_{w} (\\boldsymbol{x}) = a^{(3)}_1 = f(w^{(2)}_{10} a^{(2)}_0 + w^{(2)}_{11} a^{(2)}_1 + w^{(2)}_{12} a^{(2)}_2 + w^{(2)}_{13} a^{(2)}_3) \\end{equation*} write the matrix W^{(j)} W^{(j)} explicity, \\begin{equation*} W^{(1)} = \\begin{bmatrix} w^{(1)}_{10} & w^{(1)}_{11} & w^{(1)}_{12} & w^{(1)}_{13} \\\\ w^{(1)}_{20} & w^{(1)}_{21} & w^{(1)}_{22} & w^{(1)}_{23} \\\\ w^{(1)}_{30} & w^{(1)}_{31} & w^{(1)}_{32} & w^{(1)}_{33} \\end{bmatrix} \\end{equation*} \\begin{equation*} W^{(1)} = \\begin{bmatrix} w^{(1)}_{10} & w^{(1)}_{11} & w^{(1)}_{12} & w^{(1)}_{13} \\\\ w^{(1)}_{20} & w^{(1)}_{21} & w^{(1)}_{22} & w^{(1)}_{23} \\\\ w^{(1)}_{30} & w^{(1)}_{31} & w^{(1)}_{32} & w^{(1)}_{33} \\end{bmatrix} \\end{equation*} \\begin{equation*} W^{(2)} = \\begin{bmatrix} w^{(2)}_{10} & w^{(2)}_{11} & w^{(2)}_{12} & w^{(2)}_{13} \\end{bmatrix} \\end{equation*} \\begin{equation*} W^{(2)} = \\begin{bmatrix} w^{(2)}_{10} & w^{(2)}_{11} & w^{(2)}_{12} & w^{(2)}_{13} \\end{bmatrix} \\end{equation*} With the above form, we can use matrix notation as \\begin{align*} & \\boldsymbol{a}^{(2)} = f(\\boldsymbol{W}^{(1)} \\boldsymbol{x}) \\\\ h_{w} (\\boldsymbol{x}) = & \\boldsymbol{a}^{(3)} = f(\\boldsymbol{W}^{(2)} \\boldsymbol{a}^{(2)}) \\end{align*} \\begin{align*} & \\boldsymbol{a}^{(2)} = f(\\boldsymbol{W}^{(1)} \\boldsymbol{x}) \\\\ h_{w} (\\boldsymbol{x}) = & \\boldsymbol{a}^{(3)} = f(\\boldsymbol{W}^{(2)} \\boldsymbol{a}^{(2)}) \\end{align*} We can see from above notations that if the network has s_j s_j units in layer j j and s_{j+1} s_{j+1} in layer j + 1 j + 1 , the matrix W^{(j)} W^{(j)} will be of dimention s_{j+1} \\times s_{j}+1 s_{j+1} \\times s_{j}+1 . It could be interpreted as \"the dimention of W^{(j)} W^{(j)} is the number of nodes in the next layer (layer j + 1 j + 1 ) \\times \\times the number of nodes in the current layer + + 1. Note that in cs224n the matrix notation is slightly different, \\begin{align } & \\boldsymbol{a}^{(2)} = f(\\boldsymbol{W}^{(1)} \\boldsymbol{x} + \\boldsymbol{b}^{(1)}) \\ h_{w} (\\boldsymbol{x}) = & \\boldsymbol{a}^{(3)} = f(\\boldsymbol{W}^{(2)} \\boldsymbol{a}^{(2)} + \\boldsymbol{b}^{(2)}) \\end{align } the difference is from how we denote the bias. The two are enssentially the same, but be cautions that the matrix dimentions are different. Word Window Classification Using Neural Networks \u00b6 From now on, let's switch the notation to cs224n so that we can derive the backpropagation algorithm for word window classification and get more intuition about the backprop. The drawing of the neural nets in cs224n for word window classification is less dynamic and slitly different from the drawing from Andrew's ML class. The figure from cs224n may be slightly confusing at first, but it is good to understand it from this particular application, word window classification. Forward propagation \u00b6 Firstly, the goal of this classification task is to classify whether the center word is a location. Similar to word2vec, we will go over all positions in a corpus. But this time, it will be supervised and only some positions should get a high score. The figure above illustrate the feed-forward process. We use the method by Collobert & Weston (2008, 2011). An unnormalized score will be calculated from the activation \\boldsymbol{a} = [a_1, a_2, a_3, \\cdots]^\\mathsf{T} \\boldsymbol{a} = [a_1, a_2, a_3, \\cdots]^\\mathsf{T} . \\begin{equation*} \\text{scroe}(\\boldsymbol{x}) = U^\\mathsf{T} \\boldsymbol{a} \\in \\mathbb{R} \\end{equation*} \\begin{equation*} \\text{scroe}(\\boldsymbol{x}) = U^\\mathsf{T} \\boldsymbol{a} \\in \\mathbb{R} \\end{equation*} We will use max-margin loss as our loss function. The training is essentially to find the optimal weights W W by minimize the max-margin loss \\begin{equation } J = \\max(0, 1 - s + s_c) \\end{equation } s s is the score of a window that have a location in the center. s_c s_c is the score of a window that doesn't have a location in the center. For full objective function: Sample several corrupt windows per true one. Sum over all training windows. It is similar to negative sampling in word2vec. We will use gradient descent to update the parameter so as to minimize the loss function. The key is how to calculate the gradient with respect to the model parameters, namely \\nabla_{\\theta} J(\\theta) \\nabla_{\\theta} J(\\theta) . Here we use \\theta \\theta to represent the hyperthetic parameters, it can include the W W and other parameters of the model. \\begin{equation } \\theta^{\\text{new}} = \\theta^{\\text{old}} - \\alpha\\nabla_{\\theta} J(\\theta) \\end{equation } Gradients and Jacobians Matrix \u00b6 At first, let us layout the input and all the equations in this simple neural network. | Input Layer | Hidden Layer | Output Layer | |:---:|:---:|:---:| | \\boldsymbol{x} \\boldsymbol{x} | \\boldsymbol{z} = \\boldsymbol{W} \\boldsymbol{x} + \\boldsymbol{b}, \\\\ \\boldsymbol{h} = f(\\boldsymbol{z}) \\boldsymbol{z} = \\boldsymbol{W} \\boldsymbol{x} + \\boldsymbol{b}, \\\\ \\boldsymbol{h} = f(\\boldsymbol{z}) | $ s = \\boldsymbol{u}^{\\mathsf{T}}\\boldsymbol{h}$ | To update the parameters in this model, namely \\boldsymbol{x}, \\boldsymbol{W}, \\boldsymbol{b} \\boldsymbol{x}, \\boldsymbol{W}, \\boldsymbol{b} , we would like to compute the derivitavies of s s with respect to all these parameters. We have to use the chain rule to compute it. What chain rule says is that we can compute the partial derivatives of each individual functions and then multiply them together to get the derivative with respect the specific variable. For example, \\frac{\\partial s}{\\partial \\boldsymbol{b}} \\frac{\\partial s}{\\partial \\boldsymbol{b}} is computed as \\frac{\\partial s}{\\partial \\boldsymbol{b}} = \\frac{\\partial s}{\\partial \\boldsymbol{h}} \\cdot \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\cdot \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} \\frac{\\partial s}{\\partial \\boldsymbol{b}} = \\frac{\\partial s}{\\partial \\boldsymbol{h}} \\cdot \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\cdot \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} . This seems very easy to understand, but when it comes to implemenation in vectorized format, it become confusing for those who doesn't work on matrix calculus for quite a while like me. I want to get the points straight here. What exactly is \\frac{\\partial s}{\\partial \\boldsymbol{b}} \\frac{\\partial s}{\\partial \\boldsymbol{b}} and \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} . Note both \\boldsymbol{h} \\boldsymbol{h} and \\boldsymbol{z} \\boldsymbol{z} are vectors. To calculate these two gradient, simply remember the following two rules: Given a function with 1 output and n n inputs f(\\boldsymbol{x}) = f(x_1, x_2, \\cdots, x_n) f(\\boldsymbol{x}) = f(x_1, x_2, \\cdots, x_n) , it's gradient is a vector of partial derivatives with respect to each input(take the gradient element wise). \\begin{equation } \\frac{\\partial f}{\\partial \\boldsymbol{x}} = \\Bigg [ \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\cdots, \\frac{\\partial f}{\\partial x_n}, \\Bigg ] \\end{equation } Given a function with m m output and n n inputs \\boldsymbol{f}(\\boldsymbol{x}) = \\big [f_1(x_1, x_2, \\cdots, x_n), \\cdots, f_m(x_1, x_2, \\cdots, x_n) \\big ] \\boldsymbol{f}(\\boldsymbol{x}) = \\big [f_1(x_1, x_2, \\cdots, x_n), \\cdots, f_m(x_1, x_2, \\cdots, x_n) \\big ] , it's gradient is an m \\times n m \\times n matrix of partial derivatives (\\frac{\\partial \\boldsymbol{f}}{\\partial \\boldsymbol{x}})_{ij} = \\dfrac{\\partial f_i}{\\partial x_j} (\\frac{\\partial \\boldsymbol{f}}{\\partial \\boldsymbol{x}})_{ij} = \\dfrac{\\partial f_i}{\\partial x_j} . This matrix is also called Jacobian matrix. \\begin{equation*} \\frac{\\partial \\boldsymbol{f}}{\\partial \\boldsymbol{x}} = \\begin{bmatrix} \\dfrac{\\partial \\boldsymbol{f}}{\\partial x_1} & \\cdots & \\dfrac{\\partial \\boldsymbol{f}}{\\partial x_n} \\end{bmatrix} = \\begin{bmatrix} \\dfrac{\\partial f_1}{\\partial x_1} & \\cdots & \\dfrac{\\partial f_1}{\\partial x_n}\\\\ \\vdots & \\ddots & \\vdots\\\\ \\dfrac{\\partial f_m}{\\partial x_1} & \\cdots & \\dfrac{\\partial f_m}{\\partial x_n} \\end{bmatrix} \\end{equation*} \\begin{equation*} \\frac{\\partial \\boldsymbol{f}}{\\partial \\boldsymbol{x}} = \\begin{bmatrix} \\dfrac{\\partial \\boldsymbol{f}}{\\partial x_1} & \\cdots & \\dfrac{\\partial \\boldsymbol{f}}{\\partial x_n} \\end{bmatrix} = \\begin{bmatrix} \\dfrac{\\partial f_1}{\\partial x_1} & \\cdots & \\dfrac{\\partial f_1}{\\partial x_n}\\\\ \\vdots & \\ddots & \\vdots\\\\ \\dfrac{\\partial f_m}{\\partial x_1} & \\cdots & \\dfrac{\\partial f_m}{\\partial x_n} \\end{bmatrix} \\end{equation*} Computing gradients with the chain rule \u00b6 With these two rules we can calculate the partials. We will use \\frac{\\partial s}{\\partial \\boldsymbol{b}} \\frac{\\partial s}{\\partial \\boldsymbol{b}} as an example. \\begin{equation*} \\frac{\\partial s}{\\partial \\boldsymbol{b}} = \\frac{\\partial s}{\\partial \\boldsymbol{h}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} \\end{equation*} \\begin{equation*} \\frac{\\partial s}{\\partial \\boldsymbol{b}} = \\frac{\\partial s}{\\partial \\boldsymbol{h}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} \\end{equation*} \\frac{\\partial s}{\\partial \\boldsymbol{h}} = \\frac{\\partial}{\\partial \\boldsymbol{h}}(\\boldsymbol{u}^{\\mathsf{T}}\\boldsymbol{h}) = \\boldsymbol{u}^{\\mathsf{T}} \\frac{\\partial s}{\\partial \\boldsymbol{h}} = \\frac{\\partial}{\\partial \\boldsymbol{h}}(\\boldsymbol{u}^{\\mathsf{T}}\\boldsymbol{h}) = \\boldsymbol{u}^{\\mathsf{T}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} = \\frac{\\partial}{\\partial \\boldsymbol{z}}(f(\\boldsymbol{z})) = \\begin{pmatrix} f'(z_1) & & 0 \\\\ & \\ddots & \\\\ 0 & & f'(z_n) \\end{pmatrix} = \\mathrm{diag} (f' (\\boldsymbol{z})) \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} = \\frac{\\partial}{\\partial \\boldsymbol{z}}(f(\\boldsymbol{z})) = \\begin{pmatrix} f'(z_1) & & 0 \\\\ & \\ddots & \\\\ 0 & & f'(z_n) \\end{pmatrix} = \\mathrm{diag} (f' (\\boldsymbol{z})) \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} = \\frac{\\partial}{\\partial \\boldsymbol{b}}(\\boldsymbol{W}\\boldsymbol{x} + \\boldsymbol{b}) = \\boldsymbol{I} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} = \\frac{\\partial}{\\partial \\boldsymbol{b}}(\\boldsymbol{W}\\boldsymbol{x} + \\boldsymbol{b}) = \\boldsymbol{I} Therefore, \\begin{equation } \\frac{\\partial s}{\\partial \\boldsymbol{b}} = \\boldsymbol{u}^{\\mathsf{T}} \\mathrm{diag}(f'(\\boldsymbol{z})) \\boldsymbol{I} = \\boldsymbol{u}^{\\mathsf{T}} \\circ f'(\\boldsymbol{z}) \\end{equation } Similarly we can calculate the partial with respect to \\boldsymbol{W} \\boldsymbol{W} and \\boldsymbol{x} \\boldsymbol{x} . Since the top layer partials are already calculated, we can reuese the results. We denote those reusable partials as \\delta \\delta meaning local error signal. \\begin{align*} \\frac{\\partial s}{\\partial \\boldsymbol{W}} & = \\frac{\\partial s}{\\partial \\boldsymbol{h}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{W}} \\\\ \\\\ \\frac{\\partial s}{\\partial \\boldsymbol{x}} & = \\frac{\\partial s}{\\partial \\boldsymbol{h}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{x}} \\\\ \\\\ \\frac{\\partial s}{\\partial \\boldsymbol{b}} & = \\underbrace{\\frac{\\partial s}{\\partial \\boldsymbol{h}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}}}_{\\mathrm{reuse}} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} = \\boldsymbol{\\delta} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} = \\boldsymbol{\\delta} = \\boldsymbol{u}^{\\mathsf{T}} \\circ f'(\\boldsymbol{z}) \\end{align*} \\begin{align*} \\frac{\\partial s}{\\partial \\boldsymbol{W}} & = \\frac{\\partial s}{\\partial \\boldsymbol{h}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{W}} \\\\ \\\\ \\frac{\\partial s}{\\partial \\boldsymbol{x}} & = \\frac{\\partial s}{\\partial \\boldsymbol{h}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{x}} \\\\ \\\\ \\frac{\\partial s}{\\partial \\boldsymbol{b}} & = \\underbrace{\\frac{\\partial s}{\\partial \\boldsymbol{h}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}}}_{\\mathrm{reuse}} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} = \\boldsymbol{\\delta} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} = \\boldsymbol{\\delta} = \\boldsymbol{u}^{\\mathsf{T}} \\circ f'(\\boldsymbol{z}) \\end{align*} Shape convention \u00b6 What does the shape of the derivatives looks like in practice? How can we make the chain rule computation efficient? According to the aforementioned gradient calculation rules, \\frac{\\partial s}{\\partial \\boldsymbol{W}} \\frac{\\partial s}{\\partial \\boldsymbol{W}} is a row vector. The chain rule gave \\begin{equation*} \\frac{\\partial s}{\\partial \\boldsymbol{W}} = \\boldsymbol{\\delta} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{W}}. \\end{equation*} \\begin{equation*} \\frac{\\partial s}{\\partial \\boldsymbol{W}} = \\boldsymbol{\\delta} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{W}}. \\end{equation*} We know from Jacobians that \\frac{\\partial\\boldsymbol{z}}{\\partial \\boldsymbol{W}} = \\frac{\\partial}{\\partial \\boldsymbol{W}}(\\boldsymbol{W}\\boldsymbol{x} + \\boldsymbol{b}) = \\boldsymbol{x}^{\\mathsf{T}} \\frac{\\partial\\boldsymbol{z}}{\\partial \\boldsymbol{W}} = \\frac{\\partial}{\\partial \\boldsymbol{W}}(\\boldsymbol{W}\\boldsymbol{x} + \\boldsymbol{b}) = \\boldsymbol{x}^{\\mathsf{T}} . We may arrived the result that \\frac{\\partial s}{\\partial \\boldsymbol{W}} = \\boldsymbol{\\delta} \\ \\boldsymbol{x}^{\\mathsf{T}} \\frac{\\partial s}{\\partial \\boldsymbol{W}} = \\boldsymbol{\\delta} \\ \\boldsymbol{x}^{\\mathsf{T}} . This is actually not quite wirte. The correct form should be \\begin{align*} \\underbrace{\\frac{\\partial s}{\\partial \\boldsymbol{W}}}_{n \\times m} = \\underbrace{\\boldsymbol{\\delta}^{\\mathsf{T}}}_{n \\times 1} \\ \\underbrace{\\boldsymbol{x}^{\\mathsf{T}}}_{1 \\times m}. \\end{align*} \\begin{align*} \\underbrace{\\frac{\\partial s}{\\partial \\boldsymbol{W}}}_{n \\times m} = \\underbrace{\\boldsymbol{\\delta}^{\\mathsf{T}}}_{n \\times 1} \\ \\underbrace{\\boldsymbol{x}^{\\mathsf{T}}}_{1 \\times m}. \\end{align*} You may wonder why is this form instead of the one we derived directly. The explanation from the CS224N is that we would like to follow the shape convention so as to make the chain rule implementation more efficient (matrix multiplication instead of loops). Different from the Jacobian form, shape convention states that the shape of the gradient is the same shape of parameters. The resolution here is to use Jacobian form as much as possible and reshape to follow the convention at the end. Because Jacobian form makes the chain rule easy and the shape convention makes the implementation of gradient descent easy. Computation graphs and backpropagation \u00b6 We have shown how to compute the partial derivatives using chain rule. This is almost the backpropagation alrogithm. If we want to add on to that and make the algorithm complete, the only thing we need is how to reuse gradient computed for higher layers. We leverage the computation graph to explain this. From the computation graph, you'll see the intuiation of reusing the partial derivatives computed for higher layers in computing derivatives for lower layers so as to minimize computation. We represent our neural net equations as a computation graph as following: source nodes represent inputs interior nodes represent operations edges pass along result of the operation When following computation order to carry out the computations from inputs, it is enssetially forward propagation. Backpropagation is to pass along gradients backwards. This can be illustrated in the following computation graph. The partial derivative with respect to a parameter reflect how changing the parameter would effect the output value. The output value in the backprop is usually a loss function (or error). Intuitively, you can see backprop is push the error back to the lower layer through a bunch of operation nodes, the arrived error is a measure of the error at that particular layer, training is try to reduce this backprop-ed error by adjusting the local parameters, the effect of reducing the local error will be forward propagated to the output, the error at the output should also reduced. We use gradient descent to make this process to converge as soon as possible. To understand it better, let's look at a single node as following. We define \"local gradient\" for each node as the gradient of it's output with respect to it's input. By chain rule, the downstream gradient is equal to the multiplication of the upstream gradient and the local gradient. When having multipe local gradients, they are pushed back to each input using chain rule. CS224N lecture 4 slides have step by step example of backprop. From the example, we can got some intuitions about some nodes' effects. For example, when push back gradients along outward branches from a single node, the gradients should be sumed up; \"+\" node distributes the upstream gradient to each summand; \"max\" node simply routes the upstream gradients. When update gradient with backprop, you should compute all the gradients at once. With this computation graph notion, following routine captures the gist of backprop in a very decent manner. Fprop: visit nodes in topological sorted order Compute value of node given predecessors Bprop: initialize output gradient = 1 visit nodes in reverse order: Compute gradient wrt each node using gradient wrt successors \\{y_1, y_2, \\cdots, y_2\\} = \\mathrm{successors\\ of\\ } x \\{y_1, y_2, \\cdots, y_2\\} = \\mathrm{successors\\ of\\ } x \\begin{equation }\\frac{\\partial z}{\\partial \\boldsymbol{x}} = \\sum_{i=i}^n \\frac{\\partial z}{\\partial y_i} \\frac{\\partial y_i}{\\partial x} \\end{equation } If done correctly, the big O O complexity of forward propagation and backpropagation is the same. Automatic differentiation \u00b6 The gradient computation can be automatically inferred from the symbolic expression of the fprop. but this is not commonly used in practice. Each node type needs to know how to compute its output and how to compute the gradient wrt its inputs given the gradient wrt its output. Modern DL frameworks (Tensorflow, PyTorch, etc.) do backpropagation for you but mainly leave layer/node writer to hand-calculate the local derivative. Following is a simple demo of how to implement forward propagation and backpropagation. class MultiplyGate ( object ): def forward ( x , y ): z = x * y self . x = x # must keep these around! self . y = y return z def backword ( dz ): dx = self . y * dz # [dz/dx * dL/dz] dy = self . x * dz # [dz/dy * dL/dz] return [ dx , dy ] nonlinear activation functions: sigmoid, tanh, hard tanh, ReLU Learning rates: start 0.001. power of 10. halve the learning rate every k epochs. formula: lr = lr_0 e^{-k t} lr = lr_0 e^{-k t} for each epoch t t Regularization \u00b6 LFD starting point: L2 regularization f","title":"Part 2 Neural Networks and Backpropagation"},{"location":"courses/cs224n/Part%202%20Neural%20Networks%20and%20Backpropagation/#part-2-neural-networks-and-backpropagation","text":"Date: April 15th 2019 We discussed the softmax classifier in Part 1 and its major drawback that the classifier only gives linear decision boundaries. In Part 2, Neural Networks will be introduced to demonstrate that it can learn much more complex functions and nonlinear decision boundaries.","title":"Part 2 Neural Networks and Backpropagation"},{"location":"courses/cs224n/Part%202%20Neural%20Networks%20and%20Backpropagation/#intro-to-neural-networks","text":"Biological Neuron Mathematical Model Simplified Neuron The neuron can be modeled as a binary logistic regression unit as in the right figure above. It can be further simplified as following functions, \\begin{equation*} h_{w, b} (\\boldsymbol{x})=f(\\boldsymbol{w}^\\mathsf{T} \\boldsymbol{x} + b) \\\\ f(z) = \\frac{1}{1 + e^{-z}}. \\end{equation*} \\begin{equation*} h_{w, b} (\\boldsymbol{x})=f(\\boldsymbol{w}^\\mathsf{T} \\boldsymbol{x} + b) \\\\ f(z) = \\frac{1}{1 + e^{-z}}. \\end{equation*} \\boldsymbol{x} \\boldsymbol{x} is the inputs \\boldsymbol{w} \\boldsymbol{w} is the weights b b is a bias term h h is a hidden layer function f f is a nonlinear activation function (sigmoid, tanh, etc.) If we feed a vector of inputs through a bunch of logistic regression (sigmoid) functions, then we get a vector of outputs. The CS224N lecture note 3 section 1.2 has a complete derivation on multiple sigmoid units. But we don\u2019t have to decide ahead of time what variables these logistic regressions are trying to predict! It is the loss function that will direct what the intermediate hidden variables should be, to do a good job at predicting the targets for the next layer, etc.","title":"Intro to Neural Networks"},{"location":"courses/cs224n/Part%202%20Neural%20Networks%20and%20Backpropagation/#forward-propagation-in-matrix-notation","text":"In a multilayer neural network, not only did we have multiple sigmoid units, but we also have more than one layer. Let's explicitly write down the signal transformation (aka forward propagation) from one layer to another referring to this network from Andrew's ML course. We will use the following notation convention a^{(j)}_i a^{(j)}_i to represent the \"activation\" of unit i i in layer j j . W^{(j)} W^{(j)} to represent the matrix of weights controlling function mapping from layer j j to layer j + 1 j + 1 . The value at each node can be calculated as \\begin{equation*} a^{(2)}_1 = f(w^{(1)}_{10} x_0 + w^{(1)}_{11} x_1 + w^{(1)}_{12} x_2 + w^{(1)}_{13} x_3) \\\\ a^{(2)}_2 = f(w^{(1)}_{20} x_0 + w^{(1)}_{21} x_1 + w^{(1)}_{22} x_2 + w^{(1)}_{23} x_3) \\\\ a^{(2)}_3 = f(w^{(1)}_{30} x_0 + w^{(1)}_{31} x_1 + w^{(1)}_{32} x_2 + w^{(1)}_{33} x_3) \\\\ \\text{and} \\\\ h_{w} (\\boldsymbol{x}) = a^{(3)}_1 = f(w^{(2)}_{10} a^{(2)}_0 + w^{(2)}_{11} a^{(2)}_1 + w^{(2)}_{12} a^{(2)}_2 + w^{(2)}_{13} a^{(2)}_3) \\end{equation*} \\begin{equation*} a^{(2)}_1 = f(w^{(1)}_{10} x_0 + w^{(1)}_{11} x_1 + w^{(1)}_{12} x_2 + w^{(1)}_{13} x_3) \\\\ a^{(2)}_2 = f(w^{(1)}_{20} x_0 + w^{(1)}_{21} x_1 + w^{(1)}_{22} x_2 + w^{(1)}_{23} x_3) \\\\ a^{(2)}_3 = f(w^{(1)}_{30} x_0 + w^{(1)}_{31} x_1 + w^{(1)}_{32} x_2 + w^{(1)}_{33} x_3) \\\\ \\text{and} \\\\ h_{w} (\\boldsymbol{x}) = a^{(3)}_1 = f(w^{(2)}_{10} a^{(2)}_0 + w^{(2)}_{11} a^{(2)}_1 + w^{(2)}_{12} a^{(2)}_2 + w^{(2)}_{13} a^{(2)}_3) \\end{equation*} write the matrix W^{(j)} W^{(j)} explicity, \\begin{equation*} W^{(1)} = \\begin{bmatrix} w^{(1)}_{10} & w^{(1)}_{11} & w^{(1)}_{12} & w^{(1)}_{13} \\\\ w^{(1)}_{20} & w^{(1)}_{21} & w^{(1)}_{22} & w^{(1)}_{23} \\\\ w^{(1)}_{30} & w^{(1)}_{31} & w^{(1)}_{32} & w^{(1)}_{33} \\end{bmatrix} \\end{equation*} \\begin{equation*} W^{(1)} = \\begin{bmatrix} w^{(1)}_{10} & w^{(1)}_{11} & w^{(1)}_{12} & w^{(1)}_{13} \\\\ w^{(1)}_{20} & w^{(1)}_{21} & w^{(1)}_{22} & w^{(1)}_{23} \\\\ w^{(1)}_{30} & w^{(1)}_{31} & w^{(1)}_{32} & w^{(1)}_{33} \\end{bmatrix} \\end{equation*} \\begin{equation*} W^{(2)} = \\begin{bmatrix} w^{(2)}_{10} & w^{(2)}_{11} & w^{(2)}_{12} & w^{(2)}_{13} \\end{bmatrix} \\end{equation*} \\begin{equation*} W^{(2)} = \\begin{bmatrix} w^{(2)}_{10} & w^{(2)}_{11} & w^{(2)}_{12} & w^{(2)}_{13} \\end{bmatrix} \\end{equation*} With the above form, we can use matrix notation as \\begin{align*} & \\boldsymbol{a}^{(2)} = f(\\boldsymbol{W}^{(1)} \\boldsymbol{x}) \\\\ h_{w} (\\boldsymbol{x}) = & \\boldsymbol{a}^{(3)} = f(\\boldsymbol{W}^{(2)} \\boldsymbol{a}^{(2)}) \\end{align*} \\begin{align*} & \\boldsymbol{a}^{(2)} = f(\\boldsymbol{W}^{(1)} \\boldsymbol{x}) \\\\ h_{w} (\\boldsymbol{x}) = & \\boldsymbol{a}^{(3)} = f(\\boldsymbol{W}^{(2)} \\boldsymbol{a}^{(2)}) \\end{align*} We can see from above notations that if the network has s_j s_j units in layer j j and s_{j+1} s_{j+1} in layer j + 1 j + 1 , the matrix W^{(j)} W^{(j)} will be of dimention s_{j+1} \\times s_{j}+1 s_{j+1} \\times s_{j}+1 . It could be interpreted as \"the dimention of W^{(j)} W^{(j)} is the number of nodes in the next layer (layer j + 1 j + 1 ) \\times \\times the number of nodes in the current layer + + 1. Note that in cs224n the matrix notation is slightly different, \\begin{align } & \\boldsymbol{a}^{(2)} = f(\\boldsymbol{W}^{(1)} \\boldsymbol{x} + \\boldsymbol{b}^{(1)}) \\ h_{w} (\\boldsymbol{x}) = & \\boldsymbol{a}^{(3)} = f(\\boldsymbol{W}^{(2)} \\boldsymbol{a}^{(2)} + \\boldsymbol{b}^{(2)}) \\end{align } the difference is from how we denote the bias. The two are enssentially the same, but be cautions that the matrix dimentions are different.","title":"Forward Propagation in Matrix Notation"},{"location":"courses/cs224n/Part%202%20Neural%20Networks%20and%20Backpropagation/#word-window-classification-using-neural-networks","text":"From now on, let's switch the notation to cs224n so that we can derive the backpropagation algorithm for word window classification and get more intuition about the backprop. The drawing of the neural nets in cs224n for word window classification is less dynamic and slitly different from the drawing from Andrew's ML class. The figure from cs224n may be slightly confusing at first, but it is good to understand it from this particular application, word window classification.","title":"Word Window Classification Using Neural Networks"},{"location":"courses/cs224n/Part%202%20Neural%20Networks%20and%20Backpropagation/#forward-propagation","text":"Firstly, the goal of this classification task is to classify whether the center word is a location. Similar to word2vec, we will go over all positions in a corpus. But this time, it will be supervised and only some positions should get a high score. The figure above illustrate the feed-forward process. We use the method by Collobert & Weston (2008, 2011). An unnormalized score will be calculated from the activation \\boldsymbol{a} = [a_1, a_2, a_3, \\cdots]^\\mathsf{T} \\boldsymbol{a} = [a_1, a_2, a_3, \\cdots]^\\mathsf{T} . \\begin{equation*} \\text{scroe}(\\boldsymbol{x}) = U^\\mathsf{T} \\boldsymbol{a} \\in \\mathbb{R} \\end{equation*} \\begin{equation*} \\text{scroe}(\\boldsymbol{x}) = U^\\mathsf{T} \\boldsymbol{a} \\in \\mathbb{R} \\end{equation*} We will use max-margin loss as our loss function. The training is essentially to find the optimal weights W W by minimize the max-margin loss \\begin{equation } J = \\max(0, 1 - s + s_c) \\end{equation } s s is the score of a window that have a location in the center. s_c s_c is the score of a window that doesn't have a location in the center. For full objective function: Sample several corrupt windows per true one. Sum over all training windows. It is similar to negative sampling in word2vec. We will use gradient descent to update the parameter so as to minimize the loss function. The key is how to calculate the gradient with respect to the model parameters, namely \\nabla_{\\theta} J(\\theta) \\nabla_{\\theta} J(\\theta) . Here we use \\theta \\theta to represent the hyperthetic parameters, it can include the W W and other parameters of the model. \\begin{equation } \\theta^{\\text{new}} = \\theta^{\\text{old}} - \\alpha\\nabla_{\\theta} J(\\theta) \\end{equation }","title":"Forward propagation"},{"location":"courses/cs224n/Part%202%20Neural%20Networks%20and%20Backpropagation/#gradients-and-jacobians-matrix","text":"At first, let us layout the input and all the equations in this simple neural network. | Input Layer | Hidden Layer | Output Layer | |:---:|:---:|:---:| | \\boldsymbol{x} \\boldsymbol{x} | \\boldsymbol{z} = \\boldsymbol{W} \\boldsymbol{x} + \\boldsymbol{b}, \\\\ \\boldsymbol{h} = f(\\boldsymbol{z}) \\boldsymbol{z} = \\boldsymbol{W} \\boldsymbol{x} + \\boldsymbol{b}, \\\\ \\boldsymbol{h} = f(\\boldsymbol{z}) | $ s = \\boldsymbol{u}^{\\mathsf{T}}\\boldsymbol{h}$ | To update the parameters in this model, namely \\boldsymbol{x}, \\boldsymbol{W}, \\boldsymbol{b} \\boldsymbol{x}, \\boldsymbol{W}, \\boldsymbol{b} , we would like to compute the derivitavies of s s with respect to all these parameters. We have to use the chain rule to compute it. What chain rule says is that we can compute the partial derivatives of each individual functions and then multiply them together to get the derivative with respect the specific variable. For example, \\frac{\\partial s}{\\partial \\boldsymbol{b}} \\frac{\\partial s}{\\partial \\boldsymbol{b}} is computed as \\frac{\\partial s}{\\partial \\boldsymbol{b}} = \\frac{\\partial s}{\\partial \\boldsymbol{h}} \\cdot \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\cdot \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} \\frac{\\partial s}{\\partial \\boldsymbol{b}} = \\frac{\\partial s}{\\partial \\boldsymbol{h}} \\cdot \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\cdot \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} . This seems very easy to understand, but when it comes to implemenation in vectorized format, it become confusing for those who doesn't work on matrix calculus for quite a while like me. I want to get the points straight here. What exactly is \\frac{\\partial s}{\\partial \\boldsymbol{b}} \\frac{\\partial s}{\\partial \\boldsymbol{b}} and \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} . Note both \\boldsymbol{h} \\boldsymbol{h} and \\boldsymbol{z} \\boldsymbol{z} are vectors. To calculate these two gradient, simply remember the following two rules: Given a function with 1 output and n n inputs f(\\boldsymbol{x}) = f(x_1, x_2, \\cdots, x_n) f(\\boldsymbol{x}) = f(x_1, x_2, \\cdots, x_n) , it's gradient is a vector of partial derivatives with respect to each input(take the gradient element wise). \\begin{equation } \\frac{\\partial f}{\\partial \\boldsymbol{x}} = \\Bigg [ \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\cdots, \\frac{\\partial f}{\\partial x_n}, \\Bigg ] \\end{equation } Given a function with m m output and n n inputs \\boldsymbol{f}(\\boldsymbol{x}) = \\big [f_1(x_1, x_2, \\cdots, x_n), \\cdots, f_m(x_1, x_2, \\cdots, x_n) \\big ] \\boldsymbol{f}(\\boldsymbol{x}) = \\big [f_1(x_1, x_2, \\cdots, x_n), \\cdots, f_m(x_1, x_2, \\cdots, x_n) \\big ] , it's gradient is an m \\times n m \\times n matrix of partial derivatives (\\frac{\\partial \\boldsymbol{f}}{\\partial \\boldsymbol{x}})_{ij} = \\dfrac{\\partial f_i}{\\partial x_j} (\\frac{\\partial \\boldsymbol{f}}{\\partial \\boldsymbol{x}})_{ij} = \\dfrac{\\partial f_i}{\\partial x_j} . This matrix is also called Jacobian matrix. \\begin{equation*} \\frac{\\partial \\boldsymbol{f}}{\\partial \\boldsymbol{x}} = \\begin{bmatrix} \\dfrac{\\partial \\boldsymbol{f}}{\\partial x_1} & \\cdots & \\dfrac{\\partial \\boldsymbol{f}}{\\partial x_n} \\end{bmatrix} = \\begin{bmatrix} \\dfrac{\\partial f_1}{\\partial x_1} & \\cdots & \\dfrac{\\partial f_1}{\\partial x_n}\\\\ \\vdots & \\ddots & \\vdots\\\\ \\dfrac{\\partial f_m}{\\partial x_1} & \\cdots & \\dfrac{\\partial f_m}{\\partial x_n} \\end{bmatrix} \\end{equation*} \\begin{equation*} \\frac{\\partial \\boldsymbol{f}}{\\partial \\boldsymbol{x}} = \\begin{bmatrix} \\dfrac{\\partial \\boldsymbol{f}}{\\partial x_1} & \\cdots & \\dfrac{\\partial \\boldsymbol{f}}{\\partial x_n} \\end{bmatrix} = \\begin{bmatrix} \\dfrac{\\partial f_1}{\\partial x_1} & \\cdots & \\dfrac{\\partial f_1}{\\partial x_n}\\\\ \\vdots & \\ddots & \\vdots\\\\ \\dfrac{\\partial f_m}{\\partial x_1} & \\cdots & \\dfrac{\\partial f_m}{\\partial x_n} \\end{bmatrix} \\end{equation*}","title":"Gradients and Jacobians Matrix"},{"location":"courses/cs224n/Part%202%20Neural%20Networks%20and%20Backpropagation/#computing-gradients-with-the-chain-rule","text":"With these two rules we can calculate the partials. We will use \\frac{\\partial s}{\\partial \\boldsymbol{b}} \\frac{\\partial s}{\\partial \\boldsymbol{b}} as an example. \\begin{equation*} \\frac{\\partial s}{\\partial \\boldsymbol{b}} = \\frac{\\partial s}{\\partial \\boldsymbol{h}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} \\end{equation*} \\begin{equation*} \\frac{\\partial s}{\\partial \\boldsymbol{b}} = \\frac{\\partial s}{\\partial \\boldsymbol{h}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} \\end{equation*} \\frac{\\partial s}{\\partial \\boldsymbol{h}} = \\frac{\\partial}{\\partial \\boldsymbol{h}}(\\boldsymbol{u}^{\\mathsf{T}}\\boldsymbol{h}) = \\boldsymbol{u}^{\\mathsf{T}} \\frac{\\partial s}{\\partial \\boldsymbol{h}} = \\frac{\\partial}{\\partial \\boldsymbol{h}}(\\boldsymbol{u}^{\\mathsf{T}}\\boldsymbol{h}) = \\boldsymbol{u}^{\\mathsf{T}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} = \\frac{\\partial}{\\partial \\boldsymbol{z}}(f(\\boldsymbol{z})) = \\begin{pmatrix} f'(z_1) & & 0 \\\\ & \\ddots & \\\\ 0 & & f'(z_n) \\end{pmatrix} = \\mathrm{diag} (f' (\\boldsymbol{z})) \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} = \\frac{\\partial}{\\partial \\boldsymbol{z}}(f(\\boldsymbol{z})) = \\begin{pmatrix} f'(z_1) & & 0 \\\\ & \\ddots & \\\\ 0 & & f'(z_n) \\end{pmatrix} = \\mathrm{diag} (f' (\\boldsymbol{z})) \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} = \\frac{\\partial}{\\partial \\boldsymbol{b}}(\\boldsymbol{W}\\boldsymbol{x} + \\boldsymbol{b}) = \\boldsymbol{I} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} = \\frac{\\partial}{\\partial \\boldsymbol{b}}(\\boldsymbol{W}\\boldsymbol{x} + \\boldsymbol{b}) = \\boldsymbol{I} Therefore, \\begin{equation } \\frac{\\partial s}{\\partial \\boldsymbol{b}} = \\boldsymbol{u}^{\\mathsf{T}} \\mathrm{diag}(f'(\\boldsymbol{z})) \\boldsymbol{I} = \\boldsymbol{u}^{\\mathsf{T}} \\circ f'(\\boldsymbol{z}) \\end{equation } Similarly we can calculate the partial with respect to \\boldsymbol{W} \\boldsymbol{W} and \\boldsymbol{x} \\boldsymbol{x} . Since the top layer partials are already calculated, we can reuese the results. We denote those reusable partials as \\delta \\delta meaning local error signal. \\begin{align*} \\frac{\\partial s}{\\partial \\boldsymbol{W}} & = \\frac{\\partial s}{\\partial \\boldsymbol{h}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{W}} \\\\ \\\\ \\frac{\\partial s}{\\partial \\boldsymbol{x}} & = \\frac{\\partial s}{\\partial \\boldsymbol{h}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{x}} \\\\ \\\\ \\frac{\\partial s}{\\partial \\boldsymbol{b}} & = \\underbrace{\\frac{\\partial s}{\\partial \\boldsymbol{h}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}}}_{\\mathrm{reuse}} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} = \\boldsymbol{\\delta} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} = \\boldsymbol{\\delta} = \\boldsymbol{u}^{\\mathsf{T}} \\circ f'(\\boldsymbol{z}) \\end{align*} \\begin{align*} \\frac{\\partial s}{\\partial \\boldsymbol{W}} & = \\frac{\\partial s}{\\partial \\boldsymbol{h}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{W}} \\\\ \\\\ \\frac{\\partial s}{\\partial \\boldsymbol{x}} & = \\frac{\\partial s}{\\partial \\boldsymbol{h}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{x}} \\\\ \\\\ \\frac{\\partial s}{\\partial \\boldsymbol{b}} & = \\underbrace{\\frac{\\partial s}{\\partial \\boldsymbol{h}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}}}_{\\mathrm{reuse}} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} = \\boldsymbol{\\delta} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} = \\boldsymbol{\\delta} = \\boldsymbol{u}^{\\mathsf{T}} \\circ f'(\\boldsymbol{z}) \\end{align*}","title":"Computing gradients with the chain rule"},{"location":"courses/cs224n/Part%202%20Neural%20Networks%20and%20Backpropagation/#shape-convention","text":"What does the shape of the derivatives looks like in practice? How can we make the chain rule computation efficient? According to the aforementioned gradient calculation rules, \\frac{\\partial s}{\\partial \\boldsymbol{W}} \\frac{\\partial s}{\\partial \\boldsymbol{W}} is a row vector. The chain rule gave \\begin{equation*} \\frac{\\partial s}{\\partial \\boldsymbol{W}} = \\boldsymbol{\\delta} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{W}}. \\end{equation*} \\begin{equation*} \\frac{\\partial s}{\\partial \\boldsymbol{W}} = \\boldsymbol{\\delta} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{W}}. \\end{equation*} We know from Jacobians that \\frac{\\partial\\boldsymbol{z}}{\\partial \\boldsymbol{W}} = \\frac{\\partial}{\\partial \\boldsymbol{W}}(\\boldsymbol{W}\\boldsymbol{x} + \\boldsymbol{b}) = \\boldsymbol{x}^{\\mathsf{T}} \\frac{\\partial\\boldsymbol{z}}{\\partial \\boldsymbol{W}} = \\frac{\\partial}{\\partial \\boldsymbol{W}}(\\boldsymbol{W}\\boldsymbol{x} + \\boldsymbol{b}) = \\boldsymbol{x}^{\\mathsf{T}} . We may arrived the result that \\frac{\\partial s}{\\partial \\boldsymbol{W}} = \\boldsymbol{\\delta} \\ \\boldsymbol{x}^{\\mathsf{T}} \\frac{\\partial s}{\\partial \\boldsymbol{W}} = \\boldsymbol{\\delta} \\ \\boldsymbol{x}^{\\mathsf{T}} . This is actually not quite wirte. The correct form should be \\begin{align*} \\underbrace{\\frac{\\partial s}{\\partial \\boldsymbol{W}}}_{n \\times m} = \\underbrace{\\boldsymbol{\\delta}^{\\mathsf{T}}}_{n \\times 1} \\ \\underbrace{\\boldsymbol{x}^{\\mathsf{T}}}_{1 \\times m}. \\end{align*} \\begin{align*} \\underbrace{\\frac{\\partial s}{\\partial \\boldsymbol{W}}}_{n \\times m} = \\underbrace{\\boldsymbol{\\delta}^{\\mathsf{T}}}_{n \\times 1} \\ \\underbrace{\\boldsymbol{x}^{\\mathsf{T}}}_{1 \\times m}. \\end{align*} You may wonder why is this form instead of the one we derived directly. The explanation from the CS224N is that we would like to follow the shape convention so as to make the chain rule implementation more efficient (matrix multiplication instead of loops). Different from the Jacobian form, shape convention states that the shape of the gradient is the same shape of parameters. The resolution here is to use Jacobian form as much as possible and reshape to follow the convention at the end. Because Jacobian form makes the chain rule easy and the shape convention makes the implementation of gradient descent easy.","title":"Shape convention"},{"location":"courses/cs224n/Part%202%20Neural%20Networks%20and%20Backpropagation/#computation-graphs-and-backpropagation","text":"We have shown how to compute the partial derivatives using chain rule. This is almost the backpropagation alrogithm. If we want to add on to that and make the algorithm complete, the only thing we need is how to reuse gradient computed for higher layers. We leverage the computation graph to explain this. From the computation graph, you'll see the intuiation of reusing the partial derivatives computed for higher layers in computing derivatives for lower layers so as to minimize computation. We represent our neural net equations as a computation graph as following: source nodes represent inputs interior nodes represent operations edges pass along result of the operation When following computation order to carry out the computations from inputs, it is enssetially forward propagation. Backpropagation is to pass along gradients backwards. This can be illustrated in the following computation graph. The partial derivative with respect to a parameter reflect how changing the parameter would effect the output value. The output value in the backprop is usually a loss function (or error). Intuitively, you can see backprop is push the error back to the lower layer through a bunch of operation nodes, the arrived error is a measure of the error at that particular layer, training is try to reduce this backprop-ed error by adjusting the local parameters, the effect of reducing the local error will be forward propagated to the output, the error at the output should also reduced. We use gradient descent to make this process to converge as soon as possible. To understand it better, let's look at a single node as following. We define \"local gradient\" for each node as the gradient of it's output with respect to it's input. By chain rule, the downstream gradient is equal to the multiplication of the upstream gradient and the local gradient. When having multipe local gradients, they are pushed back to each input using chain rule. CS224N lecture 4 slides have step by step example of backprop. From the example, we can got some intuitions about some nodes' effects. For example, when push back gradients along outward branches from a single node, the gradients should be sumed up; \"+\" node distributes the upstream gradient to each summand; \"max\" node simply routes the upstream gradients. When update gradient with backprop, you should compute all the gradients at once. With this computation graph notion, following routine captures the gist of backprop in a very decent manner. Fprop: visit nodes in topological sorted order Compute value of node given predecessors Bprop: initialize output gradient = 1 visit nodes in reverse order: Compute gradient wrt each node using gradient wrt successors \\{y_1, y_2, \\cdots, y_2\\} = \\mathrm{successors\\ of\\ } x \\{y_1, y_2, \\cdots, y_2\\} = \\mathrm{successors\\ of\\ } x \\begin{equation }\\frac{\\partial z}{\\partial \\boldsymbol{x}} = \\sum_{i=i}^n \\frac{\\partial z}{\\partial y_i} \\frac{\\partial y_i}{\\partial x} \\end{equation } If done correctly, the big O O complexity of forward propagation and backpropagation is the same.","title":"Computation graphs and backpropagation"},{"location":"courses/cs224n/Part%202%20Neural%20Networks%20and%20Backpropagation/#automatic-differentiation","text":"The gradient computation can be automatically inferred from the symbolic expression of the fprop. but this is not commonly used in practice. Each node type needs to know how to compute its output and how to compute the gradient wrt its inputs given the gradient wrt its output. Modern DL frameworks (Tensorflow, PyTorch, etc.) do backpropagation for you but mainly leave layer/node writer to hand-calculate the local derivative. Following is a simple demo of how to implement forward propagation and backpropagation. class MultiplyGate ( object ): def forward ( x , y ): z = x * y self . x = x # must keep these around! self . y = y return z def backword ( dz ): dx = self . y * dz # [dz/dx * dL/dz] dy = self . x * dz # [dz/dy * dL/dz] return [ dx , dy ] nonlinear activation functions: sigmoid, tanh, hard tanh, ReLU Learning rates: start 0.001. power of 10. halve the learning rate every k epochs. formula: lr = lr_0 e^{-k t} lr = lr_0 e^{-k t} for each epoch t t","title":"Automatic differentiation"},{"location":"courses/cs224n/Part%202%20Neural%20Networks%20and%20Backpropagation/#regularization","text":"LFD starting point: L2 regularization f","title":"Regularization"},{"location":"courses/cs224n/Part%203%20Language%20Model%20and%20RNN/","text":"Part 3 Language Model and RNN \u00b6 n-gram \u00b6 assumption: \\boldsymbol{x}^{(t+1)} \\boldsymbol{x}^{(t+1)} depends only on previous n-1 n-1 words. Sparsity problem, --> backoff (n-1)-gram practically, n has to be less than 5. Neural language model \u00b6 fixed-window natural language model Recurrent Neural Net","title":"Part 3 Language Model and RNN"},{"location":"courses/cs224n/Part%203%20Language%20Model%20and%20RNN/#part-3-language-model-and-rnn","text":"","title":"Part 3 Language Model and RNN"},{"location":"courses/cs224n/Part%203%20Language%20Model%20and%20RNN/#n-gram","text":"assumption: \\boldsymbol{x}^{(t+1)} \\boldsymbol{x}^{(t+1)} depends only on previous n-1 n-1 words. Sparsity problem, --> backoff (n-1)-gram practically, n has to be less than 5.","title":"n-gram"},{"location":"courses/cs224n/Part%203%20Language%20Model%20and%20RNN/#neural-language-model","text":"fixed-window natural language model Recurrent Neural Net","title":"Neural language model"},{"location":"courses/cs224n/notes/","text":"CS224N NLP with Deep Learning \u00b6 Part 1 Word Embeddings Based on Distributional Semantic \u00b6 Date: April 20th 2019 If you have no experience in the Natrual Language Processing (NLP) field and asked to name a few practical NLP applications, what would you list? Google Translation or Amazon Echo voice assistant may first come into your mind. How did those products understand a sentence or human conversation? Engineers built computational models that understand our language as our brain does. To build such a computational model, the following components are necessary. a large corpus of text (input training data) a method to represent each word from the corpus (feature representation) a starting model that barely understands English at the beginning but can be improved by \"reading\" more words from the corpus (parametric function). an algorithm for the model to correct itself if it makes a mistake in understanding (learning algorithm/optimization method) a measurement that can qualify the mistake the model made (loss function) Introduction \u00b6 Starting with this post, I will write materials from my understanding of the Stanford CS224N. The plan is to anchor all the applications I learned in the above 5 components so as to systematically understand the gist of each of those applications and corresponding solutions. Part 1 is about language models and word embeddings. Part 2 will discuss neural networks and backprop. Part 3 revisits the language model and introduces recurrent neural networks. Part 4 study advanced RNN, Long short-term memory (LSTM), and gated recurrent networks (GRN). Future parts will depend on my availability and the schedule of CS224N. Let's get started! Word representations \u00b6 How do we represent word with meaning on a computer? Before 2013, wordNet and one-hot vector are most popular in word meaning representations. WordNet is a manually compiled thesaurus containing lists of synonym sets and hypernyms. Like most of the manual stuff, it is subjective, unscalable, inaccurate in computing word similarity, and impossible to maintain and keep up-to-data. One-hot vectors represent word meaning using discrete symbolic 1 1 s in a long stream of 0 0 of vector elements. It suffers from sparcity issues and many other drawbacks. We will not spend time on that outdated method Instead, We will focus on the embedding method using the idea of word vector. The core idea of this embedding method is based on the remarkable insight on word meaning called distributional semantics . It conjectures that a word\u2019s meaning is given by the words that frequently appear close-by. It is proposed by J. R. Firth. Here is the famous quote: Quote \"You shall know a word by the company it keeps\" -- J. R. Firth 1957: 11 In this method, we will build a dense vector for each word, chosen so that it is similar to vectors of words that appear in similar contexts. word vector representation also called distributed representation, or word embeddings. \\mathrm{banking} = \\begin{pmatrix} 0.286\\\\ 0.792\\\\ \u22120.177\\\\ \u22120.107\\\\ 0.109\\\\ \u22120.542\\\\ 0.349\\\\ 0.271 \\end{pmatrix} \\mathrm{banking} = \\begin{pmatrix} 0.286\\\\ 0.792\\\\ \u22120.177\\\\ \u22120.107\\\\ 0.109\\\\ \u22120.542\\\\ 0.349\\\\ 0.271 \\end{pmatrix} word2vec \u00b6 Word2vec (Mikolov et al. 2013) is a framework for learning word vectors. Word2vec go through windows of words and calculate the probability of the context word given the center word (or vice versa) using the similarity of the word vectors. It keeps adjusting the word vectors to maximize this probability. Vividly, these two pictures show the general idea of how word2vec works. Formally, for a single prediction, the probability is P(o|c) P(o|c) , interpreted as the probability of the outer word o o given the center word c c . For the large corpus including T T words and each position t = 1, 2, \\cdots, T t = 1, 2, \\cdots, T , we predict context words within a window of fixed size m m , given center word w_t w_t . The model likelihood can be written as the following \\begin{equation*} \\mathrm{likelihood} = L(\\theta) = \\prod_{t=1}^T\\prod_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} p(w_{t+j}|w_t; \\theta) \\end{equation*} \\begin{equation*} \\mathrm{likelihood} = L(\\theta) = \\prod_{t=1}^T\\prod_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} p(w_{t+j}|w_t; \\theta) \\end{equation*} The objective function J(\\theta) J(\\theta) is the average negative log likelihood: \\begin{equation*} J(\\theta) = -\\frac{1}{T}\\log{L(\\theta)} = -\\frac{1}{T} \\sum_{t=1}^T\\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} \\log p(w_{t+j}|w_t; \\theta) \\end{equation*} \\begin{equation*} J(\\theta) = -\\frac{1}{T}\\log{L(\\theta)} = -\\frac{1}{T} \\sum_{t=1}^T\\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} \\log p(w_{t+j}|w_t; \\theta) \\end{equation*} We've defined the cost function by now. In order to minimize the loss, we need to know how p(w_{t+j}|w_t; \\theta) p(w_{t+j}|w_t; \\theta) can be calculated. One function we can use to calculate the probability value is the \\mathrm{softmax} \\mathrm{softmax} function. \\begin{equation*} \\mathrm{softmax}(x)_i = \\frac{\\exp(x_i)}{\\sum_{j=1}^{n}\\exp(x_j)} \\end{equation*} \\begin{equation*} \\mathrm{softmax}(x)_i = \\frac{\\exp(x_i)}{\\sum_{j=1}^{n}\\exp(x_j)} \\end{equation*} Particularly we will write the probability as the following format. \\begin{equation*} p(o|c) = \\frac{\\exp(u_o^T v_c)}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\end{equation*} \\begin{equation*} p(o|c) = \\frac{\\exp(u_o^T v_c)}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\end{equation*} There are several points need to be emphasized. Two vectors will be obtained for each individual word, one as center word v_w v_w , and the other context word u_w u_w . d d is the dimension of the word vector. V V is the vocabulary size. \\begin{equation*} \\theta = \\begin{bmatrix} v_{\\mathrm{aardvark}}\\\\ v_{\\mathrm{a}}\\\\ \\cdots \\\\ v_{\\mathrm{zebra}}\\\\ u_{\\mathrm{aardvark}}\\\\ u_{\\mathrm{a}}\\\\ \\cdots \\\\ u_{\\mathrm{zebra}}\\\\ \\end{bmatrix} \\in \\mathbb{R}^{2dV} \\end{equation*} \\begin{equation*} \\theta = \\begin{bmatrix} v_{\\mathrm{aardvark}}\\\\ v_{\\mathrm{a}}\\\\ \\cdots \\\\ v_{\\mathrm{zebra}}\\\\ u_{\\mathrm{aardvark}}\\\\ u_{\\mathrm{a}}\\\\ \\cdots \\\\ u_{\\mathrm{zebra}}\\\\ \\end{bmatrix} \\in \\mathbb{R}^{2dV} \\end{equation*} The dot product in the exponet compares similarity of o o and c c . Larger dot product indicates larger probability. The denorminator sum over entire vocabulary to give normalized probability distribution. The \\mathrm{softmax} \\mathrm{softmax} function maps aribitrary values x_i x_i to a probability distribution p_i p_i . \u201c \\mathrm{max} \\mathrm{max} \u201d because it amplifies the probability of the largest x_i x_i , \u201c \\mathrm{soft} \\mathrm{soft} \u201d because it still assigns some probabilities to smaller x_i x_i . It is very commonly used in deep learning. Gradient descent to optimize log likelihood loss function \u00b6 This section I will purely focus on how to derive the gradient of the log likelihood loss function with respect to center word using the chain rule. Once we have the computed gradients, we are ready to implement it in matrix form and train the word vectors. This model is called skip-gram. | loss function in p(w_{t+j}|w_t) p(w_{t+j}|w_t) | p(w_{t+j}|w_t) p(w_{t+j}|w_t) in softmax form | |:---:|:---:| | \\begin{equation }J(\\theta) = -\\frac{1}{T} \\sum_{t=1}^T\\sum_{\\substack{-m \\le j \\le m\\ j \\ne 0}} \\log p(w_{t+j}|w_t; \\theta)\\end{equation } | \\begin{equation }p(o|c) = \\frac{\\exp(u_o^T v_c)}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)}\\end{equation } | We would like to find the following derivatives: \\begin{equation*} 1. \\frac{\\partial}{\\partial v_c} \\log p(o|c) \\\\ 2. \\frac{\\partial}{\\partial u_o} \\log p(o|c) \\end{equation*} \\begin{equation*} 1. \\frac{\\partial}{\\partial v_c} \\log p(o|c) \\\\ 2. \\frac{\\partial}{\\partial u_o} \\log p(o|c) \\end{equation*} Let's now start working with the first one, the derivative wrt. v_c v_c . \\begin{align*} \\frac{\\partial}{\\partial v_c} \\log p(o|c) & = \\frac{\\partial}{\\partial v_c} \\log{\\frac{\\exp(u_o^T v_c)}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)}} \\\\ &= \\frac{\\partial}{\\partial v_c} \\log{\\exp(u_o^T v_c)} - \\frac{\\partial}{\\partial v_c} \\log{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\;\\;\\;\\cdots (\\log\\frac{a}{b} = \\log a - \\log b) \\\\ & = \\frac{\\partial}{\\partial v_c} (u_o^T v_c) - \\frac{1}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\sum_{x=1}^{V} \\frac{\\partial}{\\partial v_c} \\exp(u_x^T v_c) \\;\\;\\;\\cdots \\mathrm{chain\\ rule}\\\\ & = u_o - \\frac{1}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\sum_{x=1}^{V} \\exp(u_x^{T} v_c) \\frac{\\partial}{\\partial v_c} (u_x^T v_c) \\;\\;\\;\\cdots \\mathrm{chain\\ rule}\\\\ & = u_o - \\frac{1}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\sum_{x=1}^{V} \\exp(u_x^{T} v_c) \\cdot u_x \\\\ & = u_o - \\sum_{x=1}^{V} \\frac{\\exp(u_x^{T} v_c)}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\cdot u_x \\\\ & = \\underbrace{u_o}_{\\mathrm{current}} - \\underbrace{\\sum_{x=1}^{V} p(x|c) \\cdot u_x}_{\\mathrm{expectation}} \\end{align*} \\begin{align*} \\frac{\\partial}{\\partial v_c} \\log p(o|c) & = \\frac{\\partial}{\\partial v_c} \\log{\\frac{\\exp(u_o^T v_c)}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)}} \\\\ &= \\frac{\\partial}{\\partial v_c} \\log{\\exp(u_o^T v_c)} - \\frac{\\partial}{\\partial v_c} \\log{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\;\\;\\;\\cdots (\\log\\frac{a}{b} = \\log a - \\log b) \\\\ & = \\frac{\\partial}{\\partial v_c} (u_o^T v_c) - \\frac{1}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\sum_{x=1}^{V} \\frac{\\partial}{\\partial v_c} \\exp(u_x^T v_c) \\;\\;\\;\\cdots \\mathrm{chain\\ rule}\\\\ & = u_o - \\frac{1}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\sum_{x=1}^{V} \\exp(u_x^{T} v_c) \\frac{\\partial}{\\partial v_c} (u_x^T v_c) \\;\\;\\;\\cdots \\mathrm{chain\\ rule}\\\\ & = u_o - \\frac{1}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\sum_{x=1}^{V} \\exp(u_x^{T} v_c) \\cdot u_x \\\\ & = u_o - \\sum_{x=1}^{V} \\frac{\\exp(u_x^{T} v_c)}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\cdot u_x \\\\ & = \\underbrace{u_o}_{\\mathrm{current}} - \\underbrace{\\sum_{x=1}^{V} p(x|c) \\cdot u_x}_{\\mathrm{expectation}} \\end{align*} The result is remarkable. It have a great intuition in it. The gradient represent the slop in the multidimentional space that we should walk along to reach the optima. The result gradient we got tell us that the slop actully equals to the difference of current context vector \\boldsymbol{u}_o \\boldsymbol{u}_o and the expected context vector (the weighted average over all context vectors). It has nothing todo with the center word c c . To compute the gradient of J(\\theta) J(\\theta) with respect to the center word c c , you have to sum up all the gradients obtained from word windows when c c is the center word. The gradient with respect to the context word will be very similar, chain rule is also handy in that case. Once all the gradient with resepct to center words and context words are calculated. We can use gradient descent to update the model parameters, which in this case is all the word vectors. Because we have two vectors for each word, when update the parameters we will use the average of the two vectors to update. Gradient descent to optimize cross entropy loss function \u00b6 Alternatively, we could also use cross entrypy loss function. CS224N 2017 assignment 1 requires to derive the gradient of cross entropy loss function. This section, we will go step by step to derive the gradient when using cross entropy loss function and \\mathrm{softmax} \\mathrm{softmax} activation function in the out put layer. the cross entropy function is defined as follows \\begin{equation*} \\mathrm{CE}(\\boldsymbol{y}, \\boldsymbol{\\hat y}) = - \\sum_i y_i \\log (\\hat y_i) \\end{equation*} \\begin{equation*} \\mathrm{CE}(\\boldsymbol{y}, \\boldsymbol{\\hat y}) = - \\sum_i y_i \\log (\\hat y_i) \\end{equation*} Notice the \\boldsymbol{y} \\boldsymbol{y} is the one-hot label vector, and \\boldsymbol{\\hat y} \\boldsymbol{\\hat y} is the predicted probability for all classes. The index i i is the index of the one-hot element in individual label vectors. Therefore the definition of cross entropy loss function is defined as followinng if we use \\mathrm{softmax} \\mathrm{softmax} to predict \\hat y_i \\hat y_i , \\begin{equation*} J_{\\mathrm{CE}}(\\theta) = - \\sum_i^N y_i \\log (\\hat y_i) = -\\sum_i^N y_i \\log (\\mathrm{softmax}(\\theta)_i) \\end{equation*} \\begin{equation*} J_{\\mathrm{CE}}(\\theta) = - \\sum_i^N y_i \\log (\\hat y_i) = -\\sum_i^N y_i \\log (\\mathrm{softmax}(\\theta)_i) \\end{equation*} The gradient of \\frac{\\partial}{\\partial \\theta}J_{\\mathrm{CE}}(\\theta) \\frac{\\partial}{\\partial \\theta}J_{\\mathrm{CE}}(\\theta) can be derived using chain rule. Because we will use the gradient of the \\mathrm{softmax} \\mathrm{softmax} for the derivation, Let's derive \\mathrm{softmax} \\mathrm{softmax} gradient first. \\begin{equation*} \\frac{\\partial \\hat y_i}{\\theta_j} = \\frac{\\partial }{\\theta_j}\\mathrm{softmax}(\\theta)_i = \\frac{\\partial }{\\theta_j} \\frac{e^{\\theta_i}}{\\sum_{k=1}^{n}e^{\\theta_k}} \\end{equation*} \\begin{equation*} \\frac{\\partial \\hat y_i}{\\theta_j} = \\frac{\\partial }{\\theta_j}\\mathrm{softmax}(\\theta)_i = \\frac{\\partial }{\\theta_j} \\frac{e^{\\theta_i}}{\\sum_{k=1}^{n}e^{\\theta_k}} \\end{equation*} Here we should use two tricks to derive this gradient. Quotient rule separate into 2 cases: when i = j i = j and i \\ne j i \\ne j . If i = j i = j , we have \\begin{align*} \\frac{\\partial }{\\theta_j} \\frac{e^{\\theta_i}}{\\sum_{k=1}^{n}e^{\\theta_k}} & = \\frac{e^{\\theta_i}\\sum_{k=1}^{n}e^{\\theta_k} - e^{\\theta_i}e^{\\theta_j} }{\\Big(\\sum_{k=1}^{n}e^{\\theta_k}\\Big)^2} \\\\ & = \\frac{e^{\\theta_i}\\Big(\\sum_{k=1}^{n}e^{\\theta_k} - e^{\\theta_j}\\Big)}{\\Big(\\sum_{k=1}^{n}e^{\\theta_k}\\Big)^2} \\\\ & = \\frac{e^{\\theta_i}}{\\sum_{k=1}^{n}e^{\\theta_k}} \\frac{\\sum_{k=1}^{n}e^{\\theta_k} - e^{\\theta_j}}{\\sum_{k=1}^{n}e^{\\theta_k}} \\\\ & = \\hat y_i(1 - \\hat y_j) \\end{align*} \\begin{align*} \\frac{\\partial }{\\theta_j} \\frac{e^{\\theta_i}}{\\sum_{k=1}^{n}e^{\\theta_k}} & = \\frac{e^{\\theta_i}\\sum_{k=1}^{n}e^{\\theta_k} - e^{\\theta_i}e^{\\theta_j} }{\\Big(\\sum_{k=1}^{n}e^{\\theta_k}\\Big)^2} \\\\ & = \\frac{e^{\\theta_i}\\Big(\\sum_{k=1}^{n}e^{\\theta_k} - e^{\\theta_j}\\Big)}{\\Big(\\sum_{k=1}^{n}e^{\\theta_k}\\Big)^2} \\\\ & = \\frac{e^{\\theta_i}}{\\sum_{k=1}^{n}e^{\\theta_k}} \\frac{\\sum_{k=1}^{n}e^{\\theta_k} - e^{\\theta_j}}{\\sum_{k=1}^{n}e^{\\theta_k}} \\\\ & = \\hat y_i(1 - \\hat y_j) \\end{align*} if i \\ne j i \\ne j , we have \\begin{align*} \\frac{\\partial }{\\theta_j} \\frac{e^{\\theta_i}}{\\sum_{k=1}^{n}e^{\\theta_k}} & = \\frac{0 - e^{\\theta_i}e^{\\theta_j} }{\\Big(\\sum_{k=1}^{n}e^{\\theta_k}\\Big)^2} \\\\ & = \\frac{e^{\\theta_i}}{\\sum_{k=1}^{n}e^{\\theta_k}} \\frac{ - e^{\\theta_j}}{\\sum_{k=1}^{n}e^{\\theta_k}} \\\\ & = -\\hat y_i \\hat y_j \\end{align*} \\begin{align*} \\frac{\\partial }{\\theta_j} \\frac{e^{\\theta_i}}{\\sum_{k=1}^{n}e^{\\theta_k}} & = \\frac{0 - e^{\\theta_i}e^{\\theta_j} }{\\Big(\\sum_{k=1}^{n}e^{\\theta_k}\\Big)^2} \\\\ & = \\frac{e^{\\theta_i}}{\\sum_{k=1}^{n}e^{\\theta_k}} \\frac{ - e^{\\theta_j}}{\\sum_{k=1}^{n}e^{\\theta_k}} \\\\ & = -\\hat y_i \\hat y_j \\end{align*} Now let calculate the gradient of the cross-entropy loss function. Notice the gradient is concerning the i i th parameter. This is because we can use the gradient of the \\mathrm{softmax} \\mathrm{softmax} (the \\frac{\\partial \\hat y_k}{\\partial \\theta_i} \\frac{\\partial \\hat y_k}{\\partial \\theta_i} term) conveniently. \\begin{align*} \\frac{\\partial J_{\\mathrm{CE}}(\\theta)}{\\partial \\theta_i} & = - \\sum_k^N y_k \\frac{\\partial \\log (\\hat y_k)}{\\partial \\theta_i} \\\\ & = - \\sum_k^N y_k \\frac{\\partial \\log (\\hat y_k)}{\\partial \\hat y_k} \\frac{\\partial \\hat y_k}{\\partial \\theta_i} \\\\ & = - \\sum_k^N y_k \\frac{1}{\\hat y_k} \\frac{\\partial \\hat y_k}{\\partial \\theta_i} \\\\ & = - \\Big(\\underbrace{y_k \\frac{1}{\\hat y_k} \\hat y_k(1 - \\hat y_i)}_{k = i} + \\underbrace{\\sum_{k\\ne i}^N y_k \\frac{1}{\\hat y_k} (-\\hat y_k \\hat y_i) \\Big)}_{k \\ne i} \\\\ & = - \\Big(y_i (1 - \\hat y_i) - \\sum_{k\\ne i}^N y_k \\hat y_i \\Big) \\\\ & = - y_i + y_i \\hat y_i + \\sum_{k\\ne i}^N y_k \\hat y_i \\\\ & = \\hat y_i\\Big(y_i + \\sum_{k\\ne i}^N y_k\\Big) - \\hat y_i \\\\ & = \\hat y_i \\cdot 1 - y_i = \\hat y_i - y_i \\end{align*} \\begin{align*} \\frac{\\partial J_{\\mathrm{CE}}(\\theta)}{\\partial \\theta_i} & = - \\sum_k^N y_k \\frac{\\partial \\log (\\hat y_k)}{\\partial \\theta_i} \\\\ & = - \\sum_k^N y_k \\frac{\\partial \\log (\\hat y_k)}{\\partial \\hat y_k} \\frac{\\partial \\hat y_k}{\\partial \\theta_i} \\\\ & = - \\sum_k^N y_k \\frac{1}{\\hat y_k} \\frac{\\partial \\hat y_k}{\\partial \\theta_i} \\\\ & = - \\Big(\\underbrace{y_k \\frac{1}{\\hat y_k} \\hat y_k(1 - \\hat y_i)}_{k = i} + \\underbrace{\\sum_{k\\ne i}^N y_k \\frac{1}{\\hat y_k} (-\\hat y_k \\hat y_i) \\Big)}_{k \\ne i} \\\\ & = - \\Big(y_i (1 - \\hat y_i) - \\sum_{k\\ne i}^N y_k \\hat y_i \\Big) \\\\ & = - y_i + y_i \\hat y_i + \\sum_{k\\ne i}^N y_k \\hat y_i \\\\ & = \\hat y_i\\Big(y_i + \\sum_{k\\ne i}^N y_k\\Big) - \\hat y_i \\\\ & = \\hat y_i \\cdot 1 - y_i = \\hat y_i - y_i \\end{align*} Write in vector form, we will have \\frac{\\partial J_{\\mathrm{CE}}(\\boldsymbol{y}, \\boldsymbol{\\hat y})}{\\partial \\boldsymbol{\\theta}} = \\boldsymbol{\\hat y} - \\boldsymbol{y} \\frac{\\partial J_{\\mathrm{CE}}(\\boldsymbol{y}, \\boldsymbol{\\hat y})}{\\partial \\boldsymbol{\\theta}} = \\boldsymbol{\\hat y} - \\boldsymbol{y} . With this gradient, we can update the model parameters, namely the word vectors. In the next post, I will discuss neural networks, which have more layers than the word2vec. In neural networks, there are more parameters need to be trained and the gradients with respect all the parameters need to be derived. Skip-gram models \u00b6 Skip-gram model uses the center word to predict the surrounding. We have derived the gradient for the skip-gram model using log-likelihood loss function and \\mathrm{softmax} \\mathrm{softmax} . This section will derive the gradient for cross-entropy loss function. The probability output function will keep using the \\mathrm{softmax} \\mathrm{softmax} . Cross entropy loss function \u00b6 Since we have derived above that \\frac{\\partial J_{\\mathrm{CE}}(\\boldsymbol{y}, \\boldsymbol{\\hat y})}{\\partial \\boldsymbol{\\theta}} = \\boldsymbol{\\hat y} - \\boldsymbol{y} \\frac{\\partial J_{\\mathrm{CE}}(\\boldsymbol{y}, \\boldsymbol{\\hat y})}{\\partial \\boldsymbol{\\theta}} = \\boldsymbol{\\hat y} - \\boldsymbol{y} . In this case, the vector \\boldsymbol{\\theta} = [\\boldsymbol{u_1^{\\mathsf{T}}}\\boldsymbol{v_c}, \\boldsymbol{u_2^{\\mathsf{T}}}\\boldsymbol{v_c}, \\cdots, \\boldsymbol{u_W^{\\mathsf{T}}}\\boldsymbol{v_c}] \\boldsymbol{\\theta} = [\\boldsymbol{u_1^{\\mathsf{T}}}\\boldsymbol{v_c}, \\boldsymbol{u_2^{\\mathsf{T}}}\\boldsymbol{v_c}, \\cdots, \\boldsymbol{u_W^{\\mathsf{T}}}\\boldsymbol{v_c}] . Gradient for center word \u00b6 Borrow the above steps to derive the gradient with respect to \\boldsymbol{\\theta} \\boldsymbol{\\theta} , we have \\begin{align*} \\frac{\\partial }{\\partial \\boldsymbol{v_c}}J_{\\mathrm{softmax-CE}}(\\boldsymbol{o}, \\boldsymbol{v_c}, \\boldsymbol{U}) & = \\frac{\\partial J_{\\mathrm{CE}}(\\theta)}{\\partial \\theta_i} \\frac{\\partial \\theta_i}{\\boldsymbol{v_c}} \\\\ & = (\\hat y_i - y_i) \\boldsymbol{u_i^{\\mathsf{T}}} \\end{align*} \\begin{align*} \\frac{\\partial }{\\partial \\boldsymbol{v_c}}J_{\\mathrm{softmax-CE}}(\\boldsymbol{o}, \\boldsymbol{v_c}, \\boldsymbol{U}) & = \\frac{\\partial J_{\\mathrm{CE}}(\\theta)}{\\partial \\theta_i} \\frac{\\partial \\theta_i}{\\boldsymbol{v_c}} \\\\ & = (\\hat y_i - y_i) \\boldsymbol{u_i^{\\mathsf{T}}} \\end{align*} Notice here the \\boldsymbol{u_i} \\boldsymbol{u_i} is a column vector. And the derivative of \\boldsymbol{y} = \\boldsymbol{a}^{\\mathsf{T}}\\boldsymbol{x} \\boldsymbol{y} = \\boldsymbol{a}^{\\mathsf{T}}\\boldsymbol{x} with respect to \\boldsymbol{x} \\boldsymbol{x} is \\boldsymbol{a}^{\\mathsf{T}} \\boldsymbol{a}^{\\mathsf{T}} . Written in vector form, we will get \\begin{equation*} \\frac{\\partial J}{\\partial \\boldsymbol{v_c}} = \\boldsymbol{U}(\\boldsymbol{\\hat y} - \\boldsymbol{y}) \\end{equation*} \\begin{equation*} \\frac{\\partial J}{\\partial \\boldsymbol{v_c}} = \\boldsymbol{U}(\\boldsymbol{\\hat y} - \\boldsymbol{y}) \\end{equation*} In the above gradient, U = [\\boldsymbol{u_1}, \\boldsymbol{u_1}, \\cdots, \\boldsymbol{u_W}] U = [\\boldsymbol{u_1}, \\boldsymbol{u_1}, \\cdots, \\boldsymbol{u_W}] is the matrix of all the output vectors. \\boldsymbol{u_1} \\boldsymbol{u_1} is a column word vector. The component \\boldsymbol{\\hat y} - \\boldsymbol{y} \\boldsymbol{\\hat y} - \\boldsymbol{y} is a also a column vector with length of W W . The above gradient can be viewed as scaling each output vector \\boldsymbol{u_i} \\boldsymbol{u_i} by the scaler \\hat y_i - y. \\hat y_i - y. Alternatively, the gradient can also be wrote as distributive form \\begin{equation*} \\frac{\\partial J}{\\partial \\boldsymbol{v_c}} = \\boldsymbol{U}\\boldsymbol{\\hat y} - \\boldsymbol{U}\\boldsymbol{y} = -\\boldsymbol{u_i} + \\sum_{w=1}^{W}\\hat y_w \\boldsymbol{u_w} \\end{equation*} \\begin{equation*} \\frac{\\partial J}{\\partial \\boldsymbol{v_c}} = \\boldsymbol{U}\\boldsymbol{\\hat y} - \\boldsymbol{U}\\boldsymbol{y} = -\\boldsymbol{u_i} + \\sum_{w=1}^{W}\\hat y_w \\boldsymbol{u_w} \\end{equation*} The index i i in the above equation is corresponding to the index of the none zero element in the one-hot vector \\boldsymbol{y} \\boldsymbol{y} . Here we can see \\boldsymbol{y} \\boldsymbol{y} as a the true label of the output word. Gradient for output word \u00b6 We can also calculate the gradient with respect to the output word. \\begin{align*} \\frac{\\partial }{\\partial \\boldsymbol{u_w}}J_{\\mathrm{softmax-CE}}(\\boldsymbol{o}, \\boldsymbol{v_c}, \\boldsymbol{U}) & = \\frac{\\partial J_{\\mathrm{CE}}(\\theta)}{\\partial \\theta_i} \\frac{\\partial \\theta_i}{\\boldsymbol{u_w}} \\\\ & = (\\hat y_i - y_i) \\boldsymbol{v_c} \\end{align*} \\begin{align*} \\frac{\\partial }{\\partial \\boldsymbol{u_w}}J_{\\mathrm{softmax-CE}}(\\boldsymbol{o}, \\boldsymbol{v_c}, \\boldsymbol{U}) & = \\frac{\\partial J_{\\mathrm{CE}}(\\theta)}{\\partial \\theta_i} \\frac{\\partial \\theta_i}{\\boldsymbol{u_w}} \\\\ & = (\\hat y_i - y_i) \\boldsymbol{v_c} \\end{align*} Notice here we apply \\frac{\\partial \\boldsymbol{a}^{\\mathsf{T}} \\boldsymbol{x}}{\\partial \\boldsymbol{a}} = x \\frac{\\partial \\boldsymbol{a}^{\\mathsf{T}} \\boldsymbol{x}}{\\partial \\boldsymbol{a}} = x . Writen the gradient in matrix format, we have \\begin{equation*} \\frac{\\partial J}{\\partial \\boldsymbol{U}} = \\boldsymbol{v_c}(\\boldsymbol{\\hat y} - \\boldsymbol{y})^{\\mathsf{T}} \\end{equation*} \\begin{equation*} \\frac{\\partial J}{\\partial \\boldsymbol{U}} = \\boldsymbol{v_c}(\\boldsymbol{\\hat y} - \\boldsymbol{y})^{\\mathsf{T}} \\end{equation*} Notice the shape of the gradient. It determines how the notation looks like. In the above notation notice the shape of the output word gradient \\frac{\\partial J}{\\partial \\boldsymbol{U}} \\frac{\\partial J}{\\partial \\boldsymbol{U}} is d \\times W d \\times W . As we will discussion in the next post that it is a convention to make the shape of gradient as the shape of the input vectors. in this case the shape of U U and \\frac{\\partial J}{\\partial \\boldsymbol{U}} \\frac{\\partial J}{\\partial \\boldsymbol{U}} are the same. Negative sampling \u00b6 The cost function for a single word prediction using nagative sampling is the following \\begin{align*} J_{\\mathrm{neg-sample}}(\\boldsymbol{o},\\boldsymbol{v_c},\\boldsymbol{U}) & = -\\log(\\sigma(\\boldsymbol{u_o}^{\\mathsf{T}} \\boldsymbol{v_c})) - \\sum_{k=1}^{K} \\log(\\sigma(-\\boldsymbol{u_k}^{\\mathsf{T}} \\boldsymbol{v_c})) \\end{align*} \\begin{align*} J_{\\mathrm{neg-sample}}(\\boldsymbol{o},\\boldsymbol{v_c},\\boldsymbol{U}) & = -\\log(\\sigma(\\boldsymbol{u_o}^{\\mathsf{T}} \\boldsymbol{v_c})) - \\sum_{k=1}^{K} \\log(\\sigma(-\\boldsymbol{u_k}^{\\mathsf{T}} \\boldsymbol{v_c})) \\end{align*} It comes from the original paper by Mikolov et al. \\sigma \\sigma is the sigmoid function \\sigma(x) = \\frac{1}{1+e^{-x}} \\sigma(x) = \\frac{1}{1+e^{-x}} . The ideas is to reduce the optimization computation by only sampling a small part of the vocabulary that have a lower probability of being context words of one another. The first lecture notes from CS224N discussed briefly the origin and intuition of the negative sampling loss funciton. Here we will focus on deriving the gradient and implementation ideas. With the fact that \\frac{\\mathrm{d}\\sigma(x)}{\\mathrm{d} x} = \\sigma(x)(1-\\sigma(x)) \\frac{\\mathrm{d}\\sigma(x)}{\\mathrm{d} x} = \\sigma(x)(1-\\sigma(x)) and the chain rule, it is not hard to derive the gradients result as following \\begin{align*} \\frac{\\partial J}{\\partial \\boldsymbol{v_c}} & = (\\sigma(\\boldsymbol{u_o}^{\\mathsf{T}}\\boldsymbol{v_c}) - 1)\\boldsymbol{u_o} - \\sum_{k=1}^{K}(\\sigma(-\\boldsymbol{u_k}^{\\mathsf{T}}\\boldsymbol{v_c}) - 1)\\boldsymbol{u_k} \\\\ \\frac{\\partial J}{\\partial \\boldsymbol{u_o}} & = (\\sigma(\\boldsymbol{u_o}^{\\mathsf{T}}\\boldsymbol{v_c}) - 1)\\boldsymbol{v_c} \\\\ \\frac{\\partial J}{\\partial \\boldsymbol{u_k}} & = -\\sigma(-\\boldsymbol{u_k}^{\\mathsf{T}}\\boldsymbol{v_c}) - 1)\\boldsymbol{v_c}, \\mathrm{for\\ all\\ } k = 1, 2, \\cdots, K \\end{align*} \\begin{align*} \\frac{\\partial J}{\\partial \\boldsymbol{v_c}} & = (\\sigma(\\boldsymbol{u_o}^{\\mathsf{T}}\\boldsymbol{v_c}) - 1)\\boldsymbol{u_o} - \\sum_{k=1}^{K}(\\sigma(-\\boldsymbol{u_k}^{\\mathsf{T}}\\boldsymbol{v_c}) - 1)\\boldsymbol{u_k} \\\\ \\frac{\\partial J}{\\partial \\boldsymbol{u_o}} & = (\\sigma(\\boldsymbol{u_o}^{\\mathsf{T}}\\boldsymbol{v_c}) - 1)\\boldsymbol{v_c} \\\\ \\frac{\\partial J}{\\partial \\boldsymbol{u_k}} & = -\\sigma(-\\boldsymbol{u_k}^{\\mathsf{T}}\\boldsymbol{v_c}) - 1)\\boldsymbol{v_c}, \\mathrm{for\\ all\\ } k = 1, 2, \\cdots, K \\end{align*} How to sample the \\boldsymbol{u_k} \\boldsymbol{u_k} in practice? The best known sampling method is based on the Unigram Model raise to the power of 3/4. Unigram Model is the counts of each word in the particular corpus (not the vocabulary). Gradient for all of word vectors \u00b6 Since skip-gram model is using one center word to predict all its context words. Given a word context size of m m , we obtain a set of context words [\\mathrm{word}_{c-m}, \\cdots, \\mathrm{word}_{c-1}, \\mathrm{word}_{c}, \\mathrm{word}_{c+1}, \\cdots, \\mathrm{word}_{c+m}] [\\mathrm{word}_{c-m}, \\cdots, \\mathrm{word}_{c-1}, \\mathrm{word}_{c}, \\mathrm{word}_{c+1}, \\cdots, \\mathrm{word}_{c+m}] . For each window, We need to predict 2m 2m context word given the center word. Denote the \"input\" and \"output\" word vectors for \\mathrm{word}_k \\mathrm{word}_k as \\boldsymbol{v}_k \\boldsymbol{v}_k and \\boldsymbol{u}_k \\boldsymbol{u}_k respectively. The cost for the entire context window with size m m centered around \\mathrm{word}_c \\mathrm{word}_c would be \\begin{equation*} J_{\\mathrm{skip-gram}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m}) = \\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} F(\\boldsymbol{u}_{c+j}, \\boldsymbol{v}_c). \\end{equation*} \\begin{equation*} J_{\\mathrm{skip-gram}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m}) = \\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} F(\\boldsymbol{u}_{c+j}, \\boldsymbol{v}_c). \\end{equation*} F F is a placeholder notation to represent the cost function given the center word for different model. Therefore for skip-gram, the gradients for the cost of one context window are \\begin{align*} \\frac{J_{\\mathrm{skip-gram}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{U}} & = \\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} \\frac{\\partial F(\\boldsymbol{u}_{c+j}, \\boldsymbol{v}_c)}{\\partial \\boldsymbol{v}_c},\\\\ \\frac{J_{\\mathrm{skip-gram}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{v}_c} & = \\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} \\frac{\\partial F(\\boldsymbol{u}_{c+j}, \\boldsymbol{v}_c)}{\\partial \\boldsymbol{v}_c}, \\\\ \\frac{J_{\\mathrm{skip-gram}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{v}_j} & = 0, \\forall j \\ne c \\end{align*} \\begin{align*} \\frac{J_{\\mathrm{skip-gram}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{U}} & = \\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} \\frac{\\partial F(\\boldsymbol{u}_{c+j}, \\boldsymbol{v}_c)}{\\partial \\boldsymbol{v}_c},\\\\ \\frac{J_{\\mathrm{skip-gram}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{v}_c} & = \\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} \\frac{\\partial F(\\boldsymbol{u}_{c+j}, \\boldsymbol{v}_c)}{\\partial \\boldsymbol{v}_c}, \\\\ \\frac{J_{\\mathrm{skip-gram}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{v}_j} & = 0, \\forall j \\ne c \\end{align*} CBOW model \u00b6 Continuous bag-of-words (CBOW) is using the context words to predect the center words. Different from skip-gram model, in CBOW, we will use 2m 2m context word vectors as we predict probability of a word is the center word. For a simple variant of CBOW, we could sum up all the 2m 2m word vectors in one context vector \\hat{\\boldsymbol{v}} \\hat{\\boldsymbol{v}} and use the similar cost function with \\mathrm{softmax} \\mathrm{softmax} as we did in skip-gram model. \\begin{equation*} \\hat{\\boldsymbol{v}} = \\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} \\boldsymbol{v}_{c+j}. \\end{equation*} \\begin{equation*} \\hat{\\boldsymbol{v}} = \\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} \\boldsymbol{v}_{c+j}. \\end{equation*} Similar to skip-gram, we have \\begin{align*} \\frac{J_{\\mathrm{CBOW}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{U}} & = \\frac{\\partial F(\\boldsymbol{u}_{c}, \\hat{\\boldsymbol{v}})}{\\partial \\boldsymbol{U}},\\\\ \\frac{J_{\\mathrm{CBOW}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{v}_j} & = \\frac{\\partial F(\\boldsymbol{u}_{c}, \\hat{\\boldsymbol{v}})}{\\partial \\boldsymbol{v}_j}, \\forall j \\in \\{c-m, \\cdots, c-1, c+1, \\cdots, c+m\\}\\\\ \\frac{J_{\\mathrm{CBOW}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{v}_j} & = 0, \\forall j \\notin \\{c-m, \\cdots, c-1, c+1, \\cdots, c+m\\}. \\end{align*} \\begin{align*} \\frac{J_{\\mathrm{CBOW}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{U}} & = \\frac{\\partial F(\\boldsymbol{u}_{c}, \\hat{\\boldsymbol{v}})}{\\partial \\boldsymbol{U}},\\\\ \\frac{J_{\\mathrm{CBOW}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{v}_j} & = \\frac{\\partial F(\\boldsymbol{u}_{c}, \\hat{\\boldsymbol{v}})}{\\partial \\boldsymbol{v}_j}, \\forall j \\in \\{c-m, \\cdots, c-1, c+1, \\cdots, c+m\\}\\\\ \\frac{J_{\\mathrm{CBOW}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{v}_j} & = 0, \\forall j \\notin \\{c-m, \\cdots, c-1, c+1, \\cdots, c+m\\}. \\end{align*} GloVe model \u00b6 todo, the paper and lecture. Word vector evaluation \u00b6 Summary \u00b6 This post focused on gradient derivation for various word embedding models. The bootstrap model is based on the distributional semantic, by which to predict the probability of a word given some other words from the fixed corpus. We use the \\mathrm{softmax} \\mathrm{softmax} function to computer the probability. To update the word vectors, we introduce the likelihood function and derived its gradient. In that derivation, the parameter \\boldsymbol{\\theta} \\boldsymbol{\\theta} is the hyper parameters related to word vector that can be used to compute the probability. We continue to derive the gradient of the cross entropy loss function for a single word prediction. The result gradient of the cross entropy loss function is the key for all our later gradient derivation. After that we introduce the word2vec family of word embedding, skip-gram and CBOW models. In word2vec we use the dot product of the center word and its context words to compute the probability. We derive the gradient with respect both center word and context word when using cross entropy loss funciton. Negative sampling is also discussed as it improves the computation cost by many factors. With the discussion of skip-gram model, CBOW model is easier to present. One thing you need to distinguish is that whether the gradient is for a particualr word prediction, for the whole window of the words, or the over all objective for the corpus. For skip-gram, we first compute the gradient for each word prediction in the given context window, then sum up all the gradient to update the cost function of that window. We move the window over the corpus until finish all the word updates. This whole process is only one update. While using negative sampling, the process becomes more efficient, we don't have to go through all the window, only sampling K K windows with Unigram Model raise to the 3/4 3/4 power. In the simple CBOW model discussed, we add up all the context vectors first, then only update a single gradient for that window corresponding to the center word. We repeat this process for different windows to complete one update. Part 2 Neural Networks and Backpropagation \u00b6 Date: April 15th 2019 We discussed the softmax classifier in Part 1 and its major drawback that the classifier only gives linear decision boundaries. In Part 2, Neural Networks will be introduced to demonstrate that it can learn much more complex functions and nonlinear decision boundaries. Intro to Neural Networks \u00b6 Biological Neuron Mathematical Model Simplified Neuron The neuron can be modeled as a binary logistic regression unit as in the right figure above. It can be further simplified as following functions, \\begin{equation*} h_{w, b} (\\boldsymbol{x})=f(\\boldsymbol{w}^\\mathsf{T} \\boldsymbol{x} + b) \\\\ f(z) = \\frac{1}{1 + e^{-z}}. \\end{equation*} \\begin{equation*} h_{w, b} (\\boldsymbol{x})=f(\\boldsymbol{w}^\\mathsf{T} \\boldsymbol{x} + b) \\\\ f(z) = \\frac{1}{1 + e^{-z}}. \\end{equation*} \\boldsymbol{x} \\boldsymbol{x} is the inputs \\boldsymbol{w} \\boldsymbol{w} is the weights b b is a bias term h h is a hidden layer function f f is a nonlinear activation function (sigmoid, tanh, etc.) If we feed a vector of inputs through a bunch of logistic regression (sigmoid) functions, then we get a vector of outputs. The CS224N lecture note 3 secion 1.2 have a complete derivation on multiple sigmoid units. But we don\u2019t have to decide ahead of time what variables these logistic regressions are trying to predict! It is the loss function that will direct what the intermediate hidden variables should be, so as to do a good job at predicting the targets for the next layer, etc. Forward Propagation in Matrix Notation \u00b6 In a multilayer neural network, not only did we have multiple sigmoid units, but we also have more than one layer. Let's explicitly write down the signal transformation (aka forward propagation) from one layer to another referring to this network from Andrew's ML course. We will use the following notation convention a^{(j)}_i a^{(j)}_i to represent the \"activation\" of unit i i in layer j j . W^{(j)} W^{(j)} to represent the matrix of weights controlling function mapping from layer j j to layer j + 1 j + 1 . The value at each node can be calculated as \\begin{equation*} a^{(2)}_1 = f(w^{(1)}_{10} x_0 + w^{(1)}_{11} x_1 + w^{(1)}_{12} x_2 + w^{(1)}_{13} x_3) \\\\ a^{(2)}_2 = f(w^{(1)}_{20} x_0 + w^{(1)}_{21} x_1 + w^{(1)}_{22} x_2 + w^{(1)}_{23} x_3) \\\\ a^{(2)}_3 = f(w^{(1)}_{30} x_0 + w^{(1)}_{31} x_1 + w^{(1)}_{32} x_2 + w^{(1)}_{33} x_3) \\\\ \\text{and} \\\\ h_{w} (\\boldsymbol{x}) = a^{(3)}_1 = f(w^{(2)}_{10} a^{(2)}_0 + w^{(2)}_{11} a^{(2)}_1 + w^{(2)}_{12} a^{(2)}_2 + w^{(2)}_{13} a^{(2)}_3) \\end{equation*} \\begin{equation*} a^{(2)}_1 = f(w^{(1)}_{10} x_0 + w^{(1)}_{11} x_1 + w^{(1)}_{12} x_2 + w^{(1)}_{13} x_3) \\\\ a^{(2)}_2 = f(w^{(1)}_{20} x_0 + w^{(1)}_{21} x_1 + w^{(1)}_{22} x_2 + w^{(1)}_{23} x_3) \\\\ a^{(2)}_3 = f(w^{(1)}_{30} x_0 + w^{(1)}_{31} x_1 + w^{(1)}_{32} x_2 + w^{(1)}_{33} x_3) \\\\ \\text{and} \\\\ h_{w} (\\boldsymbol{x}) = a^{(3)}_1 = f(w^{(2)}_{10} a^{(2)}_0 + w^{(2)}_{11} a^{(2)}_1 + w^{(2)}_{12} a^{(2)}_2 + w^{(2)}_{13} a^{(2)}_3) \\end{equation*} write the matrix W^{(j)} W^{(j)} explicity, \\begin{equation } W^{(1)} = \\begin{bmatrix} w^{(1)} {10} & w^{(1)} {11} & w^{(1)} {12} & w^{(1)} {13} \\ w^{(1)} {20} & w^{(1)} {21} & w^{(1)} {22} & w^{(1)} {23} \\ w^{(1)} {30} & w^{(1)} {31} & w^{(1)} {32} & w^{(1)} {33} \\end{bmatrix} \\end{equation } \\begin{equation*} W^{(2)} = \\begin{bmatrix} w^{(2)}_{10} & w^{(2)}_{11} & w^{(2)}_{12} & w^{(2)}_{13} \\end{bmatrix} \\end{equation*} \\begin{equation*} W^{(2)} = \\begin{bmatrix} w^{(2)}_{10} & w^{(2)}_{11} & w^{(2)}_{12} & w^{(2)}_{13} \\end{bmatrix} \\end{equation*} With the above form, we can use matrix notation as \\begin{align*} & \\boldsymbol{a}^{(2)} = f(\\boldsymbol{W}^{(1)} \\boldsymbol{x}) \\\\ h_{w} (\\boldsymbol{x}) = & \\boldsymbol{a}^{(3)} = f(\\boldsymbol{W}^{(2)} \\boldsymbol{a}^{(2)}) \\end{align*} \\begin{align*} & \\boldsymbol{a}^{(2)} = f(\\boldsymbol{W}^{(1)} \\boldsymbol{x}) \\\\ h_{w} (\\boldsymbol{x}) = & \\boldsymbol{a}^{(3)} = f(\\boldsymbol{W}^{(2)} \\boldsymbol{a}^{(2)}) \\end{align*} We can see from above notations that if the network has s_j s_j units in layer j j and s_{j+1} s_{j+1} in layer j + 1 j + 1 , the matrix W^{(j)} W^{(j)} will be of dimention s_{j+1} \\times s_{j}+1 s_{j+1} \\times s_{j}+1 . It could be interpreted as \"the dimention of W^{(j)} W^{(j)} is the number of nodes in the next layer (layer j + 1 j + 1 ) \\times \\times the number of nodes in the current layer + + 1. Note that in cs224n the matrix notation is slightly different, \\begin{align } & \\boldsymbol{a}^{(2)} = f(\\boldsymbol{W}^{(1)} \\boldsymbol{x} + \\boldsymbol{b}^{(1)}) \\ h_{w} (\\boldsymbol{x}) = & \\boldsymbol{a}^{(3)} = f(\\boldsymbol{W}^{(2)} \\boldsymbol{a}^{(2)} + \\boldsymbol{b}^{(2)}) \\end{align } the different is from how we denote the bias. The two are enssentially the same, but be cautions that the matrix dimentions are different. Word Window Classification Using Neural Networks \u00b6 From now on, let's switch the notation to cs224n so that we can derive the backpropagation algorithm for word window classification and get more intuition about the backprop. The drawing of the neural nets in cs224n for word window classification is less dynamic and slitly different from the drawing from Andrew's ML class. The figure from cs224n may be slightly confusing at first, but it is good to understand it from this particular application, word window classification. Forward propagation \u00b6 Firstly, the goal of this classification task is to classify whether the center word is a location. Similar to word2vec, we will go over all positions in a corpus. But this time, it will be supervised and only some positions should get a high score. The figure above illustrate the feed-forward process. We use the method by Collobert & Weston (2008, 2011). An unnormalized score will be calculated from the activation \\boldsymbol{a} = [a_1, a_2, a_3, \\cdots]^\\mathsf{T} \\boldsymbol{a} = [a_1, a_2, a_3, \\cdots]^\\mathsf{T} . \\begin{equation*} \\text{scroe}(\\boldsymbol{x}) = U^\\mathsf{T} \\boldsymbol{a} \\in \\mathbb{R} \\end{equation*} \\begin{equation*} \\text{scroe}(\\boldsymbol{x}) = U^\\mathsf{T} \\boldsymbol{a} \\in \\mathbb{R} \\end{equation*} We will use max-margin loss as our loss function. The training is essentially to find the optimal weights W W by minimize the max-margin loss \\begin{equation*} J = \\max(0, 1 - s + s_c) \\end{equation*} \\begin{equation*} J = \\max(0, 1 - s + s_c) \\end{equation*} s s is the score of a window that have a location in the center. s_c s_c is the score of a window that doesn't have a location in the center. For full objective function: Sample several corrupt windows per true one. Sum over all training windows. It is similar to negative sampling in word2vec. We will use gradient descent to update the parameter so as to minimize the loss function. The key is how to calculate the gradient with respect to the model parameters, namely \\nabla_{\\theta} J(\\theta) \\nabla_{\\theta} J(\\theta) . Here we use \\theta \\theta to represent the hyperthetic parameters, it can include the W W and other parameters of the model. \\begin{equation*} \\theta^{\\text{new}} = \\theta^{\\text{old}} - \\alpha\\nabla_{\\theta} J(\\theta) \\end{equation*} \\begin{equation*} \\theta^{\\text{new}} = \\theta^{\\text{old}} - \\alpha\\nabla_{\\theta} J(\\theta) \\end{equation*} Gradients and Jacobians Matrix \u00b6 At first, let us layout the input and all the equations in this simple neural network. | Input Layer | Hidden Layer | Output Layer | |:---:|:---:|:---:| | \\boldsymbol{x} \\boldsymbol{x} | \\boldsymbol{z} = \\boldsymbol{W} \\boldsymbol{x} + \\boldsymbol{b}, \\\\ \\boldsymbol{h} = f(\\boldsymbol{z}) \\boldsymbol{z} = \\boldsymbol{W} \\boldsymbol{x} + \\boldsymbol{b}, \\\\ \\boldsymbol{h} = f(\\boldsymbol{z}) | $ s = \\boldsymbol{u}^{\\mathsf{T}}\\boldsymbol{h}$ | To update the parameters in this model, namely \\boldsymbol{x}, \\boldsymbol{W}, \\boldsymbol{b} \\boldsymbol{x}, \\boldsymbol{W}, \\boldsymbol{b} , we would like to compute the derivitavies of s s with respect to all these parameters. We have to use the chain rule to compute it. What chain rule says is that we can compute the partial derivatives of each individual functions and then multiply them together to get the derivative with respect the specific variable. For example, \\frac{\\partial s}{\\partial \\boldsymbol{b}} \\frac{\\partial s}{\\partial \\boldsymbol{b}} is computed as \\frac{\\partial s}{\\partial \\boldsymbol{b}} = \\frac{\\partial s}{\\partial \\boldsymbol{h}} \\cdot \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\cdot \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} \\frac{\\partial s}{\\partial \\boldsymbol{b}} = \\frac{\\partial s}{\\partial \\boldsymbol{h}} \\cdot \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\cdot \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} . This seems very easy to understand, but when it comes to implemenation in vectorized format, it become confusing for those who doesn't work on matrix calculus for quite a while like me. I want to get the points straight here. What exactly is \\frac{\\partial s}{\\partial \\boldsymbol{b}} \\frac{\\partial s}{\\partial \\boldsymbol{b}} and \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} . Note both \\boldsymbol{h} \\boldsymbol{h} and \\boldsymbol{z} \\boldsymbol{z} are vectors. To calculate these two gradient, simply remember the following two rules: Given a function with 1 output and n n inputs f(\\boldsymbol{x}) = f(x_1, x_2, \\cdots, x_n) f(\\boldsymbol{x}) = f(x_1, x_2, \\cdots, x_n) , it's gradient is a vector of partial derivatives with respect to each input(take the gradient element wise). \\begin{equation } \\frac{\\partial f}{\\partial \\boldsymbol{x}} = \\Bigg [ \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\cdots, \\frac{\\partial f}{\\partial x_n}, \\Bigg ] \\end{equation } Given a function with m m output and n n inputs \\boldsymbol{f}(\\boldsymbol{x}) = \\big [f_1(x_1, x_2, \\cdots, x_n), \\cdots, f_m(x_1, x_2, \\cdots, x_n) \\big ] \\boldsymbol{f}(\\boldsymbol{x}) = \\big [f_1(x_1, x_2, \\cdots, x_n), \\cdots, f_m(x_1, x_2, \\cdots, x_n) \\big ] , it's gradient is an m \\times n m \\times n matrix of partial derivatives (\\frac{\\partial \\boldsymbol{f}}{\\partial \\boldsymbol{x}})_{ij} = \\dfrac{\\partial f_i}{\\partial x_j} (\\frac{\\partial \\boldsymbol{f}}{\\partial \\boldsymbol{x}})_{ij} = \\dfrac{\\partial f_i}{\\partial x_j} . This matrix is also called Jacobian matrix. \\begin{equation*} \\frac{\\partial \\boldsymbol{f}}{\\partial \\boldsymbol{x}} = \\begin{bmatrix} \\dfrac{\\partial \\boldsymbol{f}}{\\partial x_1} & \\cdots & \\dfrac{\\partial \\boldsymbol{f}}{\\partial x_n} \\end{bmatrix} = \\begin{bmatrix} \\dfrac{\\partial f_1}{\\partial x_1} & \\cdots & \\dfrac{\\partial f_1}{\\partial x_n}\\\\ \\vdots & \\ddots & \\vdots\\\\ \\dfrac{\\partial f_m}{\\partial x_1} & \\cdots & \\dfrac{\\partial f_m}{\\partial x_n} \\end{bmatrix} \\end{equation*} \\begin{equation*} \\frac{\\partial \\boldsymbol{f}}{\\partial \\boldsymbol{x}} = \\begin{bmatrix} \\dfrac{\\partial \\boldsymbol{f}}{\\partial x_1} & \\cdots & \\dfrac{\\partial \\boldsymbol{f}}{\\partial x_n} \\end{bmatrix} = \\begin{bmatrix} \\dfrac{\\partial f_1}{\\partial x_1} & \\cdots & \\dfrac{\\partial f_1}{\\partial x_n}\\\\ \\vdots & \\ddots & \\vdots\\\\ \\dfrac{\\partial f_m}{\\partial x_1} & \\cdots & \\dfrac{\\partial f_m}{\\partial x_n} \\end{bmatrix} \\end{equation*} Computing gradients with the chain rule \u00b6 With these two rules we can calculate the partials. We will use \\frac{\\partial s}{\\partial \\boldsymbol{b}} \\frac{\\partial s}{\\partial \\boldsymbol{b}} as an example. \\begin{equation*} \\frac{\\partial s}{\\partial \\boldsymbol{b}} = \\frac{\\partial s}{\\partial \\boldsymbol{h}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} \\end{equation*} \\begin{equation*} \\frac{\\partial s}{\\partial \\boldsymbol{b}} = \\frac{\\partial s}{\\partial \\boldsymbol{h}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} \\end{equation*} \\frac{\\partial s}{\\partial \\boldsymbol{h}} = \\frac{\\partial}{\\partial \\boldsymbol{h}}(\\boldsymbol{u}^{\\mathsf{T}}\\boldsymbol{h}) = \\boldsymbol{u}^{\\mathsf{T}} \\frac{\\partial s}{\\partial \\boldsymbol{h}} = \\frac{\\partial}{\\partial \\boldsymbol{h}}(\\boldsymbol{u}^{\\mathsf{T}}\\boldsymbol{h}) = \\boldsymbol{u}^{\\mathsf{T}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} = \\frac{\\partial}{\\partial \\boldsymbol{z}}(f(\\boldsymbol{z})) = \\begin{pmatrix} f'(z_1) & & 0 \\\\ & \\ddots & \\\\ 0 & & f'(z_n) \\end{pmatrix} = \\mathrm{diag} (f' (\\boldsymbol{z})) \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} = \\frac{\\partial}{\\partial \\boldsymbol{z}}(f(\\boldsymbol{z})) = \\begin{pmatrix} f'(z_1) & & 0 \\\\ & \\ddots & \\\\ 0 & & f'(z_n) \\end{pmatrix} = \\mathrm{diag} (f' (\\boldsymbol{z})) \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} = \\frac{\\partial}{\\partial \\boldsymbol{b}}(\\boldsymbol{W}\\boldsymbol{x} + \\boldsymbol{b}) = \\boldsymbol{I} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} = \\frac{\\partial}{\\partial \\boldsymbol{b}}(\\boldsymbol{W}\\boldsymbol{x} + \\boldsymbol{b}) = \\boldsymbol{I} Therefore, \\begin{equation*} \\frac{\\partial s}{\\partial \\boldsymbol{b}} = \\boldsymbol{u}^{\\mathsf{T}} \\mathrm{diag}(f'(\\boldsymbol{z})) \\boldsymbol{I} = \\boldsymbol{u}^{\\mathsf{T}} \\circ f'(\\boldsymbol{z}) \\end{equation*} \\begin{equation*} \\frac{\\partial s}{\\partial \\boldsymbol{b}} = \\boldsymbol{u}^{\\mathsf{T}} \\mathrm{diag}(f'(\\boldsymbol{z})) \\boldsymbol{I} = \\boldsymbol{u}^{\\mathsf{T}} \\circ f'(\\boldsymbol{z}) \\end{equation*} Similarly we can calculate the partial with respect to \\boldsymbol{W} \\boldsymbol{W} and \\boldsymbol{x} \\boldsymbol{x} . Since the top layer partials are already calculated, we can reuese the results. We denote those reusable partials as \\delta \\delta meaning local error signal. \\begin{align*} \\frac{\\partial s}{\\partial \\boldsymbol{W}} & = \\frac{\\partial s}{\\partial \\boldsymbol{h}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{W}} \\\\ \\\\ \\frac{\\partial s}{\\partial \\boldsymbol{x}} & = \\frac{\\partial s}{\\partial \\boldsymbol{h}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{x}} \\\\ \\\\ \\frac{\\partial s}{\\partial \\boldsymbol{b}} & = \\underbrace{\\frac{\\partial s}{\\partial \\boldsymbol{h}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}}}_{\\mathrm{reuse}} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} = \\boldsymbol{\\delta} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} = \\boldsymbol{\\delta} = \\boldsymbol{u}^{\\mathsf{T}} \\circ f'(\\boldsymbol{z}) \\end{align*} \\begin{align*} \\frac{\\partial s}{\\partial \\boldsymbol{W}} & = \\frac{\\partial s}{\\partial \\boldsymbol{h}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{W}} \\\\ \\\\ \\frac{\\partial s}{\\partial \\boldsymbol{x}} & = \\frac{\\partial s}{\\partial \\boldsymbol{h}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{x}} \\\\ \\\\ \\frac{\\partial s}{\\partial \\boldsymbol{b}} & = \\underbrace{\\frac{\\partial s}{\\partial \\boldsymbol{h}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}}}_{\\mathrm{reuse}} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} = \\boldsymbol{\\delta} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} = \\boldsymbol{\\delta} = \\boldsymbol{u}^{\\mathsf{T}} \\circ f'(\\boldsymbol{z}) \\end{align*} Shape convention \u00b6 What does the shape of the derivatives looks like in practice? How can we make the chain rule computation efficient? According to the aforementioned gradient calculation rules, \\frac{\\partial s}{\\partial \\boldsymbol{W}} \\frac{\\partial s}{\\partial \\boldsymbol{W}} is a row vector. The chain rule gave \\begin{equation*} \\frac{\\partial s}{\\partial \\boldsymbol{W}} = \\boldsymbol{\\delta} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{W}}. \\end{equation*} \\begin{equation*} \\frac{\\partial s}{\\partial \\boldsymbol{W}} = \\boldsymbol{\\delta} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{W}}. \\end{equation*} We know from Jacobians that \\frac{\\partial\\boldsymbol{z}}{\\partial \\boldsymbol{W}} = \\frac{\\partial}{\\partial \\boldsymbol{W}}(\\boldsymbol{W}\\boldsymbol{x} + \\boldsymbol{b}) = \\boldsymbol{x}^{\\mathsf{T}} \\frac{\\partial\\boldsymbol{z}}{\\partial \\boldsymbol{W}} = \\frac{\\partial}{\\partial \\boldsymbol{W}}(\\boldsymbol{W}\\boldsymbol{x} + \\boldsymbol{b}) = \\boldsymbol{x}^{\\mathsf{T}} . We may arrived the result that \\frac{\\partial s}{\\partial \\boldsymbol{W}} = \\boldsymbol{\\delta} \\ \\boldsymbol{x}^{\\mathsf{T}} \\frac{\\partial s}{\\partial \\boldsymbol{W}} = \\boldsymbol{\\delta} \\ \\boldsymbol{x}^{\\mathsf{T}} . This is actually not quite wirte. The correct form should be \\begin{align*} \\underbrace{\\frac{\\partial s}{\\partial \\boldsymbol{W}}}_{n \\times m} = \\underbrace{\\boldsymbol{\\delta}^{\\mathsf{T}}}_{n \\times 1} \\ \\underbrace{\\boldsymbol{x}^{\\mathsf{T}}}_{1 \\times m}. \\end{align*} \\begin{align*} \\underbrace{\\frac{\\partial s}{\\partial \\boldsymbol{W}}}_{n \\times m} = \\underbrace{\\boldsymbol{\\delta}^{\\mathsf{T}}}_{n \\times 1} \\ \\underbrace{\\boldsymbol{x}^{\\mathsf{T}}}_{1 \\times m}. \\end{align*} You may wonder why is this form instead of the one we derived directly. The explanation from the CS224N is that we would like to follow the shape convention so as to make the chain rule implementation more efficient (matrix multiplication instead of loops). Different from the Jacobian form, shape convention states that the shape of the gradient is the same shape of parameters. The resolution here is to use Jacobian form as much as possible and reshape to follow the convention at the end. Because Jacobian form makes the chain rule easy and the shape convention makes the implementation of gradient descent easy. Computation graphs and backpropagation \u00b6 We have shown how to compute the partial derivatives using chain rule. This is almost the backpropagation alrogithm. If we want to add on to that and make the algorithm complete, the only thing we need is how to reuse gradient computed for higher layers. We leverage the computation graph to explain this. From the computation graph, you'll see the intuiation of reusing the partial derivatives computed for higher layers in computing derivatives for lower layers so as to minimize computation. We represent our neural net equations as a computation graph as following: source nodes represent inputs interior nodes represent operations edges pass along result of the operation When following computation order to carry out the computations from inputs, it is enssetially forward propagation. Backpropagation is to pass along gradients backwards. This can be illustrated in the following computation graph. The partial derivative with respect to a parameter reflect how changing the parameter would effect the output value. The output value in the backprop is usually a loss function (or error). Intuitively, you can see backprop is push the error back to the lower layer through a bunch of operation nodes, the arrived error is a measure of the error at that particular layer, training is try to reduce this backprop-ed error by adjusting the local parameters, the effect of reducing the local error will be forward propagated to the output, the error at the output should also reduced. We use gradient descent to make this process to converge as soon as possible. To understand it better, let's look at a single node as following. We define \"local gradient\" for each node as the gradient of it's output with respect to it's input. By chain rule, the downstream gradient is equal to the multiplication of the upstream gradient and the local gradient. When having multipe local gradients, they are pushed back to each input using chain rule. CS224N lecture 4 slides have step by step example of backprop. From the example, we can got some intuitions about some nodes' effects. For example, when push back gradients along outward branches from a single node, the gradients should be sumed up; \"+\" node distributes the upstream gradient to each summand; \"max\" node simply routes the upstream gradients. When update gradient with backprop, you should compute all the gradients at once. With this computation graph notion, following routine captures the gist of backprop in a very decent manner. Fprop: visit nodes in topological sorted order Compute value of node given predecessors Bprop: initialize output gradient = 1 visit nodes in reverse order: Compute gradient wrt each node using gradient wrt successors \\{y_1, y_2, \\cdots, y_2\\} = \\mathrm{successors\\ of\\ } x \\{y_1, y_2, \\cdots, y_2\\} = \\mathrm{successors\\ of\\ } x \\begin{equation*}\\frac{\\partial z}{\\partial \\boldsymbol{x}} = \\sum_{i=i}^n \\frac{\\partial z}{\\partial y_i} \\frac{\\partial y_i}{\\partial x} \\end{equation*} \\begin{equation*}\\frac{\\partial z}{\\partial \\boldsymbol{x}} = \\sum_{i=i}^n \\frac{\\partial z}{\\partial y_i} \\frac{\\partial y_i}{\\partial x} \\end{equation*} If done correctly, the big O O complexity of forward propagation and backpropagation is the same. Automatic differentiation \u00b6 The gradient computation can be automatically inferred from the symbolic expression of the fprop. but this is not commonly used in practice. Each node type needs to know how to compute its output and how to compute the gradient wrt its inputs given the gradient wrt its output. Modern DL frameworks (Tensorflow, PyTorch, etc.) do backpropagation for you but mainly leave layer/node writer to hand-calculate the local derivative. Following is a simple demo of how to implement forward propagation and backpropagation. class MultiplyGate ( object ): def forward ( x , y ): z = x * y self . x = x # must keep these around! self . y = y return z def backword ( dz ): dx = self . y * dz # [dz/dx * dL/dz] dy = self . x * dz # [dz/dy * dL/dz] return [ dx , dy ] nonlinear activation functions: sigmoid, tanh, hard tanh, ReLU Learning rates: start 0.001. power of 10. halve the learning rate every k epochs. formula: lr = lr_0 e^{-k t} lr = lr_0 e^{-k t} for each epoch t t Regularization \u00b6 LFD starting point: L2 regularization Part 3 Language Model and RNN \u00b6 n-gram \u00b6 assumption: \\boldsymbol{x}^{(t+1)} \\boldsymbol{x}^{(t+1)} depends only on previous n-1 n-1 words. Sparsity problem, --> backoff (n-1)-gram practically, n has to be less than 5. Neural language model \u00b6 fixed-window neural language model Recurrent Neural Ne","title":"CS224N NLP with Deep Learning"},{"location":"courses/cs224n/notes/#cs224n-nlp-with-deep-learning","text":"","title":"CS224N NLP with Deep Learning"},{"location":"courses/cs224n/notes/#part-1-word-embeddings-based-on-distributional-semantic","text":"Date: April 20th 2019 If you have no experience in the Natrual Language Processing (NLP) field and asked to name a few practical NLP applications, what would you list? Google Translation or Amazon Echo voice assistant may first come into your mind. How did those products understand a sentence or human conversation? Engineers built computational models that understand our language as our brain does. To build such a computational model, the following components are necessary. a large corpus of text (input training data) a method to represent each word from the corpus (feature representation) a starting model that barely understands English at the beginning but can be improved by \"reading\" more words from the corpus (parametric function). an algorithm for the model to correct itself if it makes a mistake in understanding (learning algorithm/optimization method) a measurement that can qualify the mistake the model made (loss function)","title":"Part 1 Word Embeddings Based on Distributional Semantic"},{"location":"courses/cs224n/notes/#introduction","text":"Starting with this post, I will write materials from my understanding of the Stanford CS224N. The plan is to anchor all the applications I learned in the above 5 components so as to systematically understand the gist of each of those applications and corresponding solutions. Part 1 is about language models and word embeddings. Part 2 will discuss neural networks and backprop. Part 3 revisits the language model and introduces recurrent neural networks. Part 4 study advanced RNN, Long short-term memory (LSTM), and gated recurrent networks (GRN). Future parts will depend on my availability and the schedule of CS224N. Let's get started!","title":"Introduction"},{"location":"courses/cs224n/notes/#word-representations","text":"How do we represent word with meaning on a computer? Before 2013, wordNet and one-hot vector are most popular in word meaning representations. WordNet is a manually compiled thesaurus containing lists of synonym sets and hypernyms. Like most of the manual stuff, it is subjective, unscalable, inaccurate in computing word similarity, and impossible to maintain and keep up-to-data. One-hot vectors represent word meaning using discrete symbolic 1 1 s in a long stream of 0 0 of vector elements. It suffers from sparcity issues and many other drawbacks. We will not spend time on that outdated method Instead, We will focus on the embedding method using the idea of word vector. The core idea of this embedding method is based on the remarkable insight on word meaning called distributional semantics . It conjectures that a word\u2019s meaning is given by the words that frequently appear close-by. It is proposed by J. R. Firth. Here is the famous quote: Quote \"You shall know a word by the company it keeps\" -- J. R. Firth 1957: 11 In this method, we will build a dense vector for each word, chosen so that it is similar to vectors of words that appear in similar contexts. word vector representation also called distributed representation, or word embeddings. \\mathrm{banking} = \\begin{pmatrix} 0.286\\\\ 0.792\\\\ \u22120.177\\\\ \u22120.107\\\\ 0.109\\\\ \u22120.542\\\\ 0.349\\\\ 0.271 \\end{pmatrix} \\mathrm{banking} = \\begin{pmatrix} 0.286\\\\ 0.792\\\\ \u22120.177\\\\ \u22120.107\\\\ 0.109\\\\ \u22120.542\\\\ 0.349\\\\ 0.271 \\end{pmatrix}","title":"Word representations"},{"location":"courses/cs224n/notes/#word2vec","text":"Word2vec (Mikolov et al. 2013) is a framework for learning word vectors. Word2vec go through windows of words and calculate the probability of the context word given the center word (or vice versa) using the similarity of the word vectors. It keeps adjusting the word vectors to maximize this probability. Vividly, these two pictures show the general idea of how word2vec works. Formally, for a single prediction, the probability is P(o|c) P(o|c) , interpreted as the probability of the outer word o o given the center word c c . For the large corpus including T T words and each position t = 1, 2, \\cdots, T t = 1, 2, \\cdots, T , we predict context words within a window of fixed size m m , given center word w_t w_t . The model likelihood can be written as the following \\begin{equation*} \\mathrm{likelihood} = L(\\theta) = \\prod_{t=1}^T\\prod_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} p(w_{t+j}|w_t; \\theta) \\end{equation*} \\begin{equation*} \\mathrm{likelihood} = L(\\theta) = \\prod_{t=1}^T\\prod_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} p(w_{t+j}|w_t; \\theta) \\end{equation*} The objective function J(\\theta) J(\\theta) is the average negative log likelihood: \\begin{equation*} J(\\theta) = -\\frac{1}{T}\\log{L(\\theta)} = -\\frac{1}{T} \\sum_{t=1}^T\\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} \\log p(w_{t+j}|w_t; \\theta) \\end{equation*} \\begin{equation*} J(\\theta) = -\\frac{1}{T}\\log{L(\\theta)} = -\\frac{1}{T} \\sum_{t=1}^T\\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} \\log p(w_{t+j}|w_t; \\theta) \\end{equation*} We've defined the cost function by now. In order to minimize the loss, we need to know how p(w_{t+j}|w_t; \\theta) p(w_{t+j}|w_t; \\theta) can be calculated. One function we can use to calculate the probability value is the \\mathrm{softmax} \\mathrm{softmax} function. \\begin{equation*} \\mathrm{softmax}(x)_i = \\frac{\\exp(x_i)}{\\sum_{j=1}^{n}\\exp(x_j)} \\end{equation*} \\begin{equation*} \\mathrm{softmax}(x)_i = \\frac{\\exp(x_i)}{\\sum_{j=1}^{n}\\exp(x_j)} \\end{equation*} Particularly we will write the probability as the following format. \\begin{equation*} p(o|c) = \\frac{\\exp(u_o^T v_c)}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\end{equation*} \\begin{equation*} p(o|c) = \\frac{\\exp(u_o^T v_c)}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\end{equation*} There are several points need to be emphasized. Two vectors will be obtained for each individual word, one as center word v_w v_w , and the other context word u_w u_w . d d is the dimension of the word vector. V V is the vocabulary size. \\begin{equation*} \\theta = \\begin{bmatrix} v_{\\mathrm{aardvark}}\\\\ v_{\\mathrm{a}}\\\\ \\cdots \\\\ v_{\\mathrm{zebra}}\\\\ u_{\\mathrm{aardvark}}\\\\ u_{\\mathrm{a}}\\\\ \\cdots \\\\ u_{\\mathrm{zebra}}\\\\ \\end{bmatrix} \\in \\mathbb{R}^{2dV} \\end{equation*} \\begin{equation*} \\theta = \\begin{bmatrix} v_{\\mathrm{aardvark}}\\\\ v_{\\mathrm{a}}\\\\ \\cdots \\\\ v_{\\mathrm{zebra}}\\\\ u_{\\mathrm{aardvark}}\\\\ u_{\\mathrm{a}}\\\\ \\cdots \\\\ u_{\\mathrm{zebra}}\\\\ \\end{bmatrix} \\in \\mathbb{R}^{2dV} \\end{equation*} The dot product in the exponet compares similarity of o o and c c . Larger dot product indicates larger probability. The denorminator sum over entire vocabulary to give normalized probability distribution. The \\mathrm{softmax} \\mathrm{softmax} function maps aribitrary values x_i x_i to a probability distribution p_i p_i . \u201c \\mathrm{max} \\mathrm{max} \u201d because it amplifies the probability of the largest x_i x_i , \u201c \\mathrm{soft} \\mathrm{soft} \u201d because it still assigns some probabilities to smaller x_i x_i . It is very commonly used in deep learning.","title":"word2vec"},{"location":"courses/cs224n/notes/#gradient-descent-to-optimize-log-likelihood-loss-function","text":"This section I will purely focus on how to derive the gradient of the log likelihood loss function with respect to center word using the chain rule. Once we have the computed gradients, we are ready to implement it in matrix form and train the word vectors. This model is called skip-gram. | loss function in p(w_{t+j}|w_t) p(w_{t+j}|w_t) | p(w_{t+j}|w_t) p(w_{t+j}|w_t) in softmax form | |:---:|:---:| | \\begin{equation }J(\\theta) = -\\frac{1}{T} \\sum_{t=1}^T\\sum_{\\substack{-m \\le j \\le m\\ j \\ne 0}} \\log p(w_{t+j}|w_t; \\theta)\\end{equation } | \\begin{equation }p(o|c) = \\frac{\\exp(u_o^T v_c)}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)}\\end{equation } | We would like to find the following derivatives: \\begin{equation*} 1. \\frac{\\partial}{\\partial v_c} \\log p(o|c) \\\\ 2. \\frac{\\partial}{\\partial u_o} \\log p(o|c) \\end{equation*} \\begin{equation*} 1. \\frac{\\partial}{\\partial v_c} \\log p(o|c) \\\\ 2. \\frac{\\partial}{\\partial u_o} \\log p(o|c) \\end{equation*} Let's now start working with the first one, the derivative wrt. v_c v_c . \\begin{align*} \\frac{\\partial}{\\partial v_c} \\log p(o|c) & = \\frac{\\partial}{\\partial v_c} \\log{\\frac{\\exp(u_o^T v_c)}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)}} \\\\ &= \\frac{\\partial}{\\partial v_c} \\log{\\exp(u_o^T v_c)} - \\frac{\\partial}{\\partial v_c} \\log{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\;\\;\\;\\cdots (\\log\\frac{a}{b} = \\log a - \\log b) \\\\ & = \\frac{\\partial}{\\partial v_c} (u_o^T v_c) - \\frac{1}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\sum_{x=1}^{V} \\frac{\\partial}{\\partial v_c} \\exp(u_x^T v_c) \\;\\;\\;\\cdots \\mathrm{chain\\ rule}\\\\ & = u_o - \\frac{1}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\sum_{x=1}^{V} \\exp(u_x^{T} v_c) \\frac{\\partial}{\\partial v_c} (u_x^T v_c) \\;\\;\\;\\cdots \\mathrm{chain\\ rule}\\\\ & = u_o - \\frac{1}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\sum_{x=1}^{V} \\exp(u_x^{T} v_c) \\cdot u_x \\\\ & = u_o - \\sum_{x=1}^{V} \\frac{\\exp(u_x^{T} v_c)}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\cdot u_x \\\\ & = \\underbrace{u_o}_{\\mathrm{current}} - \\underbrace{\\sum_{x=1}^{V} p(x|c) \\cdot u_x}_{\\mathrm{expectation}} \\end{align*} \\begin{align*} \\frac{\\partial}{\\partial v_c} \\log p(o|c) & = \\frac{\\partial}{\\partial v_c} \\log{\\frac{\\exp(u_o^T v_c)}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)}} \\\\ &= \\frac{\\partial}{\\partial v_c} \\log{\\exp(u_o^T v_c)} - \\frac{\\partial}{\\partial v_c} \\log{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\;\\;\\;\\cdots (\\log\\frac{a}{b} = \\log a - \\log b) \\\\ & = \\frac{\\partial}{\\partial v_c} (u_o^T v_c) - \\frac{1}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\sum_{x=1}^{V} \\frac{\\partial}{\\partial v_c} \\exp(u_x^T v_c) \\;\\;\\;\\cdots \\mathrm{chain\\ rule}\\\\ & = u_o - \\frac{1}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\sum_{x=1}^{V} \\exp(u_x^{T} v_c) \\frac{\\partial}{\\partial v_c} (u_x^T v_c) \\;\\;\\;\\cdots \\mathrm{chain\\ rule}\\\\ & = u_o - \\frac{1}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\sum_{x=1}^{V} \\exp(u_x^{T} v_c) \\cdot u_x \\\\ & = u_o - \\sum_{x=1}^{V} \\frac{\\exp(u_x^{T} v_c)}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\cdot u_x \\\\ & = \\underbrace{u_o}_{\\mathrm{current}} - \\underbrace{\\sum_{x=1}^{V} p(x|c) \\cdot u_x}_{\\mathrm{expectation}} \\end{align*} The result is remarkable. It have a great intuition in it. The gradient represent the slop in the multidimentional space that we should walk along to reach the optima. The result gradient we got tell us that the slop actully equals to the difference of current context vector \\boldsymbol{u}_o \\boldsymbol{u}_o and the expected context vector (the weighted average over all context vectors). It has nothing todo with the center word c c . To compute the gradient of J(\\theta) J(\\theta) with respect to the center word c c , you have to sum up all the gradients obtained from word windows when c c is the center word. The gradient with respect to the context word will be very similar, chain rule is also handy in that case. Once all the gradient with resepct to center words and context words are calculated. We can use gradient descent to update the model parameters, which in this case is all the word vectors. Because we have two vectors for each word, when update the parameters we will use the average of the two vectors to update.","title":"Gradient descent to optimize log likelihood loss function"},{"location":"courses/cs224n/notes/#gradient-descent-to-optimize-cross-entropy-loss-function","text":"Alternatively, we could also use cross entrypy loss function. CS224N 2017 assignment 1 requires to derive the gradient of cross entropy loss function. This section, we will go step by step to derive the gradient when using cross entropy loss function and \\mathrm{softmax} \\mathrm{softmax} activation function in the out put layer. the cross entropy function is defined as follows \\begin{equation*} \\mathrm{CE}(\\boldsymbol{y}, \\boldsymbol{\\hat y}) = - \\sum_i y_i \\log (\\hat y_i) \\end{equation*} \\begin{equation*} \\mathrm{CE}(\\boldsymbol{y}, \\boldsymbol{\\hat y}) = - \\sum_i y_i \\log (\\hat y_i) \\end{equation*} Notice the \\boldsymbol{y} \\boldsymbol{y} is the one-hot label vector, and \\boldsymbol{\\hat y} \\boldsymbol{\\hat y} is the predicted probability for all classes. The index i i is the index of the one-hot element in individual label vectors. Therefore the definition of cross entropy loss function is defined as followinng if we use \\mathrm{softmax} \\mathrm{softmax} to predict \\hat y_i \\hat y_i , \\begin{equation*} J_{\\mathrm{CE}}(\\theta) = - \\sum_i^N y_i \\log (\\hat y_i) = -\\sum_i^N y_i \\log (\\mathrm{softmax}(\\theta)_i) \\end{equation*} \\begin{equation*} J_{\\mathrm{CE}}(\\theta) = - \\sum_i^N y_i \\log (\\hat y_i) = -\\sum_i^N y_i \\log (\\mathrm{softmax}(\\theta)_i) \\end{equation*} The gradient of \\frac{\\partial}{\\partial \\theta}J_{\\mathrm{CE}}(\\theta) \\frac{\\partial}{\\partial \\theta}J_{\\mathrm{CE}}(\\theta) can be derived using chain rule. Because we will use the gradient of the \\mathrm{softmax} \\mathrm{softmax} for the derivation, Let's derive \\mathrm{softmax} \\mathrm{softmax} gradient first. \\begin{equation*} \\frac{\\partial \\hat y_i}{\\theta_j} = \\frac{\\partial }{\\theta_j}\\mathrm{softmax}(\\theta)_i = \\frac{\\partial }{\\theta_j} \\frac{e^{\\theta_i}}{\\sum_{k=1}^{n}e^{\\theta_k}} \\end{equation*} \\begin{equation*} \\frac{\\partial \\hat y_i}{\\theta_j} = \\frac{\\partial }{\\theta_j}\\mathrm{softmax}(\\theta)_i = \\frac{\\partial }{\\theta_j} \\frac{e^{\\theta_i}}{\\sum_{k=1}^{n}e^{\\theta_k}} \\end{equation*} Here we should use two tricks to derive this gradient. Quotient rule separate into 2 cases: when i = j i = j and i \\ne j i \\ne j . If i = j i = j , we have \\begin{align*} \\frac{\\partial }{\\theta_j} \\frac{e^{\\theta_i}}{\\sum_{k=1}^{n}e^{\\theta_k}} & = \\frac{e^{\\theta_i}\\sum_{k=1}^{n}e^{\\theta_k} - e^{\\theta_i}e^{\\theta_j} }{\\Big(\\sum_{k=1}^{n}e^{\\theta_k}\\Big)^2} \\\\ & = \\frac{e^{\\theta_i}\\Big(\\sum_{k=1}^{n}e^{\\theta_k} - e^{\\theta_j}\\Big)}{\\Big(\\sum_{k=1}^{n}e^{\\theta_k}\\Big)^2} \\\\ & = \\frac{e^{\\theta_i}}{\\sum_{k=1}^{n}e^{\\theta_k}} \\frac{\\sum_{k=1}^{n}e^{\\theta_k} - e^{\\theta_j}}{\\sum_{k=1}^{n}e^{\\theta_k}} \\\\ & = \\hat y_i(1 - \\hat y_j) \\end{align*} \\begin{align*} \\frac{\\partial }{\\theta_j} \\frac{e^{\\theta_i}}{\\sum_{k=1}^{n}e^{\\theta_k}} & = \\frac{e^{\\theta_i}\\sum_{k=1}^{n}e^{\\theta_k} - e^{\\theta_i}e^{\\theta_j} }{\\Big(\\sum_{k=1}^{n}e^{\\theta_k}\\Big)^2} \\\\ & = \\frac{e^{\\theta_i}\\Big(\\sum_{k=1}^{n}e^{\\theta_k} - e^{\\theta_j}\\Big)}{\\Big(\\sum_{k=1}^{n}e^{\\theta_k}\\Big)^2} \\\\ & = \\frac{e^{\\theta_i}}{\\sum_{k=1}^{n}e^{\\theta_k}} \\frac{\\sum_{k=1}^{n}e^{\\theta_k} - e^{\\theta_j}}{\\sum_{k=1}^{n}e^{\\theta_k}} \\\\ & = \\hat y_i(1 - \\hat y_j) \\end{align*} if i \\ne j i \\ne j , we have \\begin{align*} \\frac{\\partial }{\\theta_j} \\frac{e^{\\theta_i}}{\\sum_{k=1}^{n}e^{\\theta_k}} & = \\frac{0 - e^{\\theta_i}e^{\\theta_j} }{\\Big(\\sum_{k=1}^{n}e^{\\theta_k}\\Big)^2} \\\\ & = \\frac{e^{\\theta_i}}{\\sum_{k=1}^{n}e^{\\theta_k}} \\frac{ - e^{\\theta_j}}{\\sum_{k=1}^{n}e^{\\theta_k}} \\\\ & = -\\hat y_i \\hat y_j \\end{align*} \\begin{align*} \\frac{\\partial }{\\theta_j} \\frac{e^{\\theta_i}}{\\sum_{k=1}^{n}e^{\\theta_k}} & = \\frac{0 - e^{\\theta_i}e^{\\theta_j} }{\\Big(\\sum_{k=1}^{n}e^{\\theta_k}\\Big)^2} \\\\ & = \\frac{e^{\\theta_i}}{\\sum_{k=1}^{n}e^{\\theta_k}} \\frac{ - e^{\\theta_j}}{\\sum_{k=1}^{n}e^{\\theta_k}} \\\\ & = -\\hat y_i \\hat y_j \\end{align*} Now let calculate the gradient of the cross-entropy loss function. Notice the gradient is concerning the i i th parameter. This is because we can use the gradient of the \\mathrm{softmax} \\mathrm{softmax} (the \\frac{\\partial \\hat y_k}{\\partial \\theta_i} \\frac{\\partial \\hat y_k}{\\partial \\theta_i} term) conveniently. \\begin{align*} \\frac{\\partial J_{\\mathrm{CE}}(\\theta)}{\\partial \\theta_i} & = - \\sum_k^N y_k \\frac{\\partial \\log (\\hat y_k)}{\\partial \\theta_i} \\\\ & = - \\sum_k^N y_k \\frac{\\partial \\log (\\hat y_k)}{\\partial \\hat y_k} \\frac{\\partial \\hat y_k}{\\partial \\theta_i} \\\\ & = - \\sum_k^N y_k \\frac{1}{\\hat y_k} \\frac{\\partial \\hat y_k}{\\partial \\theta_i} \\\\ & = - \\Big(\\underbrace{y_k \\frac{1}{\\hat y_k} \\hat y_k(1 - \\hat y_i)}_{k = i} + \\underbrace{\\sum_{k\\ne i}^N y_k \\frac{1}{\\hat y_k} (-\\hat y_k \\hat y_i) \\Big)}_{k \\ne i} \\\\ & = - \\Big(y_i (1 - \\hat y_i) - \\sum_{k\\ne i}^N y_k \\hat y_i \\Big) \\\\ & = - y_i + y_i \\hat y_i + \\sum_{k\\ne i}^N y_k \\hat y_i \\\\ & = \\hat y_i\\Big(y_i + \\sum_{k\\ne i}^N y_k\\Big) - \\hat y_i \\\\ & = \\hat y_i \\cdot 1 - y_i = \\hat y_i - y_i \\end{align*} \\begin{align*} \\frac{\\partial J_{\\mathrm{CE}}(\\theta)}{\\partial \\theta_i} & = - \\sum_k^N y_k \\frac{\\partial \\log (\\hat y_k)}{\\partial \\theta_i} \\\\ & = - \\sum_k^N y_k \\frac{\\partial \\log (\\hat y_k)}{\\partial \\hat y_k} \\frac{\\partial \\hat y_k}{\\partial \\theta_i} \\\\ & = - \\sum_k^N y_k \\frac{1}{\\hat y_k} \\frac{\\partial \\hat y_k}{\\partial \\theta_i} \\\\ & = - \\Big(\\underbrace{y_k \\frac{1}{\\hat y_k} \\hat y_k(1 - \\hat y_i)}_{k = i} + \\underbrace{\\sum_{k\\ne i}^N y_k \\frac{1}{\\hat y_k} (-\\hat y_k \\hat y_i) \\Big)}_{k \\ne i} \\\\ & = - \\Big(y_i (1 - \\hat y_i) - \\sum_{k\\ne i}^N y_k \\hat y_i \\Big) \\\\ & = - y_i + y_i \\hat y_i + \\sum_{k\\ne i}^N y_k \\hat y_i \\\\ & = \\hat y_i\\Big(y_i + \\sum_{k\\ne i}^N y_k\\Big) - \\hat y_i \\\\ & = \\hat y_i \\cdot 1 - y_i = \\hat y_i - y_i \\end{align*} Write in vector form, we will have \\frac{\\partial J_{\\mathrm{CE}}(\\boldsymbol{y}, \\boldsymbol{\\hat y})}{\\partial \\boldsymbol{\\theta}} = \\boldsymbol{\\hat y} - \\boldsymbol{y} \\frac{\\partial J_{\\mathrm{CE}}(\\boldsymbol{y}, \\boldsymbol{\\hat y})}{\\partial \\boldsymbol{\\theta}} = \\boldsymbol{\\hat y} - \\boldsymbol{y} . With this gradient, we can update the model parameters, namely the word vectors. In the next post, I will discuss neural networks, which have more layers than the word2vec. In neural networks, there are more parameters need to be trained and the gradients with respect all the parameters need to be derived.","title":"Gradient descent to optimize cross entropy loss function"},{"location":"courses/cs224n/notes/#skip-gram-models","text":"Skip-gram model uses the center word to predict the surrounding. We have derived the gradient for the skip-gram model using log-likelihood loss function and \\mathrm{softmax} \\mathrm{softmax} . This section will derive the gradient for cross-entropy loss function. The probability output function will keep using the \\mathrm{softmax} \\mathrm{softmax} .","title":"Skip-gram models"},{"location":"courses/cs224n/notes/#cross-entropy-loss-function","text":"Since we have derived above that \\frac{\\partial J_{\\mathrm{CE}}(\\boldsymbol{y}, \\boldsymbol{\\hat y})}{\\partial \\boldsymbol{\\theta}} = \\boldsymbol{\\hat y} - \\boldsymbol{y} \\frac{\\partial J_{\\mathrm{CE}}(\\boldsymbol{y}, \\boldsymbol{\\hat y})}{\\partial \\boldsymbol{\\theta}} = \\boldsymbol{\\hat y} - \\boldsymbol{y} . In this case, the vector \\boldsymbol{\\theta} = [\\boldsymbol{u_1^{\\mathsf{T}}}\\boldsymbol{v_c}, \\boldsymbol{u_2^{\\mathsf{T}}}\\boldsymbol{v_c}, \\cdots, \\boldsymbol{u_W^{\\mathsf{T}}}\\boldsymbol{v_c}] \\boldsymbol{\\theta} = [\\boldsymbol{u_1^{\\mathsf{T}}}\\boldsymbol{v_c}, \\boldsymbol{u_2^{\\mathsf{T}}}\\boldsymbol{v_c}, \\cdots, \\boldsymbol{u_W^{\\mathsf{T}}}\\boldsymbol{v_c}] .","title":"Cross entropy loss function"},{"location":"courses/cs224n/notes/#gradient-for-center-word","text":"Borrow the above steps to derive the gradient with respect to \\boldsymbol{\\theta} \\boldsymbol{\\theta} , we have \\begin{align*} \\frac{\\partial }{\\partial \\boldsymbol{v_c}}J_{\\mathrm{softmax-CE}}(\\boldsymbol{o}, \\boldsymbol{v_c}, \\boldsymbol{U}) & = \\frac{\\partial J_{\\mathrm{CE}}(\\theta)}{\\partial \\theta_i} \\frac{\\partial \\theta_i}{\\boldsymbol{v_c}} \\\\ & = (\\hat y_i - y_i) \\boldsymbol{u_i^{\\mathsf{T}}} \\end{align*} \\begin{align*} \\frac{\\partial }{\\partial \\boldsymbol{v_c}}J_{\\mathrm{softmax-CE}}(\\boldsymbol{o}, \\boldsymbol{v_c}, \\boldsymbol{U}) & = \\frac{\\partial J_{\\mathrm{CE}}(\\theta)}{\\partial \\theta_i} \\frac{\\partial \\theta_i}{\\boldsymbol{v_c}} \\\\ & = (\\hat y_i - y_i) \\boldsymbol{u_i^{\\mathsf{T}}} \\end{align*} Notice here the \\boldsymbol{u_i} \\boldsymbol{u_i} is a column vector. And the derivative of \\boldsymbol{y} = \\boldsymbol{a}^{\\mathsf{T}}\\boldsymbol{x} \\boldsymbol{y} = \\boldsymbol{a}^{\\mathsf{T}}\\boldsymbol{x} with respect to \\boldsymbol{x} \\boldsymbol{x} is \\boldsymbol{a}^{\\mathsf{T}} \\boldsymbol{a}^{\\mathsf{T}} . Written in vector form, we will get \\begin{equation*} \\frac{\\partial J}{\\partial \\boldsymbol{v_c}} = \\boldsymbol{U}(\\boldsymbol{\\hat y} - \\boldsymbol{y}) \\end{equation*} \\begin{equation*} \\frac{\\partial J}{\\partial \\boldsymbol{v_c}} = \\boldsymbol{U}(\\boldsymbol{\\hat y} - \\boldsymbol{y}) \\end{equation*} In the above gradient, U = [\\boldsymbol{u_1}, \\boldsymbol{u_1}, \\cdots, \\boldsymbol{u_W}] U = [\\boldsymbol{u_1}, \\boldsymbol{u_1}, \\cdots, \\boldsymbol{u_W}] is the matrix of all the output vectors. \\boldsymbol{u_1} \\boldsymbol{u_1} is a column word vector. The component \\boldsymbol{\\hat y} - \\boldsymbol{y} \\boldsymbol{\\hat y} - \\boldsymbol{y} is a also a column vector with length of W W . The above gradient can be viewed as scaling each output vector \\boldsymbol{u_i} \\boldsymbol{u_i} by the scaler \\hat y_i - y. \\hat y_i - y. Alternatively, the gradient can also be wrote as distributive form \\begin{equation*} \\frac{\\partial J}{\\partial \\boldsymbol{v_c}} = \\boldsymbol{U}\\boldsymbol{\\hat y} - \\boldsymbol{U}\\boldsymbol{y} = -\\boldsymbol{u_i} + \\sum_{w=1}^{W}\\hat y_w \\boldsymbol{u_w} \\end{equation*} \\begin{equation*} \\frac{\\partial J}{\\partial \\boldsymbol{v_c}} = \\boldsymbol{U}\\boldsymbol{\\hat y} - \\boldsymbol{U}\\boldsymbol{y} = -\\boldsymbol{u_i} + \\sum_{w=1}^{W}\\hat y_w \\boldsymbol{u_w} \\end{equation*} The index i i in the above equation is corresponding to the index of the none zero element in the one-hot vector \\boldsymbol{y} \\boldsymbol{y} . Here we can see \\boldsymbol{y} \\boldsymbol{y} as a the true label of the output word.","title":"Gradient for center word"},{"location":"courses/cs224n/notes/#gradient-for-output-word","text":"We can also calculate the gradient with respect to the output word. \\begin{align*} \\frac{\\partial }{\\partial \\boldsymbol{u_w}}J_{\\mathrm{softmax-CE}}(\\boldsymbol{o}, \\boldsymbol{v_c}, \\boldsymbol{U}) & = \\frac{\\partial J_{\\mathrm{CE}}(\\theta)}{\\partial \\theta_i} \\frac{\\partial \\theta_i}{\\boldsymbol{u_w}} \\\\ & = (\\hat y_i - y_i) \\boldsymbol{v_c} \\end{align*} \\begin{align*} \\frac{\\partial }{\\partial \\boldsymbol{u_w}}J_{\\mathrm{softmax-CE}}(\\boldsymbol{o}, \\boldsymbol{v_c}, \\boldsymbol{U}) & = \\frac{\\partial J_{\\mathrm{CE}}(\\theta)}{\\partial \\theta_i} \\frac{\\partial \\theta_i}{\\boldsymbol{u_w}} \\\\ & = (\\hat y_i - y_i) \\boldsymbol{v_c} \\end{align*} Notice here we apply \\frac{\\partial \\boldsymbol{a}^{\\mathsf{T}} \\boldsymbol{x}}{\\partial \\boldsymbol{a}} = x \\frac{\\partial \\boldsymbol{a}^{\\mathsf{T}} \\boldsymbol{x}}{\\partial \\boldsymbol{a}} = x . Writen the gradient in matrix format, we have \\begin{equation*} \\frac{\\partial J}{\\partial \\boldsymbol{U}} = \\boldsymbol{v_c}(\\boldsymbol{\\hat y} - \\boldsymbol{y})^{\\mathsf{T}} \\end{equation*} \\begin{equation*} \\frac{\\partial J}{\\partial \\boldsymbol{U}} = \\boldsymbol{v_c}(\\boldsymbol{\\hat y} - \\boldsymbol{y})^{\\mathsf{T}} \\end{equation*} Notice the shape of the gradient. It determines how the notation looks like. In the above notation notice the shape of the output word gradient \\frac{\\partial J}{\\partial \\boldsymbol{U}} \\frac{\\partial J}{\\partial \\boldsymbol{U}} is d \\times W d \\times W . As we will discussion in the next post that it is a convention to make the shape of gradient as the shape of the input vectors. in this case the shape of U U and \\frac{\\partial J}{\\partial \\boldsymbol{U}} \\frac{\\partial J}{\\partial \\boldsymbol{U}} are the same.","title":"Gradient for output word"},{"location":"courses/cs224n/notes/#negative-sampling","text":"The cost function for a single word prediction using nagative sampling is the following \\begin{align*} J_{\\mathrm{neg-sample}}(\\boldsymbol{o},\\boldsymbol{v_c},\\boldsymbol{U}) & = -\\log(\\sigma(\\boldsymbol{u_o}^{\\mathsf{T}} \\boldsymbol{v_c})) - \\sum_{k=1}^{K} \\log(\\sigma(-\\boldsymbol{u_k}^{\\mathsf{T}} \\boldsymbol{v_c})) \\end{align*} \\begin{align*} J_{\\mathrm{neg-sample}}(\\boldsymbol{o},\\boldsymbol{v_c},\\boldsymbol{U}) & = -\\log(\\sigma(\\boldsymbol{u_o}^{\\mathsf{T}} \\boldsymbol{v_c})) - \\sum_{k=1}^{K} \\log(\\sigma(-\\boldsymbol{u_k}^{\\mathsf{T}} \\boldsymbol{v_c})) \\end{align*} It comes from the original paper by Mikolov et al. \\sigma \\sigma is the sigmoid function \\sigma(x) = \\frac{1}{1+e^{-x}} \\sigma(x) = \\frac{1}{1+e^{-x}} . The ideas is to reduce the optimization computation by only sampling a small part of the vocabulary that have a lower probability of being context words of one another. The first lecture notes from CS224N discussed briefly the origin and intuition of the negative sampling loss funciton. Here we will focus on deriving the gradient and implementation ideas. With the fact that \\frac{\\mathrm{d}\\sigma(x)}{\\mathrm{d} x} = \\sigma(x)(1-\\sigma(x)) \\frac{\\mathrm{d}\\sigma(x)}{\\mathrm{d} x} = \\sigma(x)(1-\\sigma(x)) and the chain rule, it is not hard to derive the gradients result as following \\begin{align*} \\frac{\\partial J}{\\partial \\boldsymbol{v_c}} & = (\\sigma(\\boldsymbol{u_o}^{\\mathsf{T}}\\boldsymbol{v_c}) - 1)\\boldsymbol{u_o} - \\sum_{k=1}^{K}(\\sigma(-\\boldsymbol{u_k}^{\\mathsf{T}}\\boldsymbol{v_c}) - 1)\\boldsymbol{u_k} \\\\ \\frac{\\partial J}{\\partial \\boldsymbol{u_o}} & = (\\sigma(\\boldsymbol{u_o}^{\\mathsf{T}}\\boldsymbol{v_c}) - 1)\\boldsymbol{v_c} \\\\ \\frac{\\partial J}{\\partial \\boldsymbol{u_k}} & = -\\sigma(-\\boldsymbol{u_k}^{\\mathsf{T}}\\boldsymbol{v_c}) - 1)\\boldsymbol{v_c}, \\mathrm{for\\ all\\ } k = 1, 2, \\cdots, K \\end{align*} \\begin{align*} \\frac{\\partial J}{\\partial \\boldsymbol{v_c}} & = (\\sigma(\\boldsymbol{u_o}^{\\mathsf{T}}\\boldsymbol{v_c}) - 1)\\boldsymbol{u_o} - \\sum_{k=1}^{K}(\\sigma(-\\boldsymbol{u_k}^{\\mathsf{T}}\\boldsymbol{v_c}) - 1)\\boldsymbol{u_k} \\\\ \\frac{\\partial J}{\\partial \\boldsymbol{u_o}} & = (\\sigma(\\boldsymbol{u_o}^{\\mathsf{T}}\\boldsymbol{v_c}) - 1)\\boldsymbol{v_c} \\\\ \\frac{\\partial J}{\\partial \\boldsymbol{u_k}} & = -\\sigma(-\\boldsymbol{u_k}^{\\mathsf{T}}\\boldsymbol{v_c}) - 1)\\boldsymbol{v_c}, \\mathrm{for\\ all\\ } k = 1, 2, \\cdots, K \\end{align*} How to sample the \\boldsymbol{u_k} \\boldsymbol{u_k} in practice? The best known sampling method is based on the Unigram Model raise to the power of 3/4. Unigram Model is the counts of each word in the particular corpus (not the vocabulary).","title":"Negative sampling"},{"location":"courses/cs224n/notes/#gradient-for-all-of-word-vectors","text":"Since skip-gram model is using one center word to predict all its context words. Given a word context size of m m , we obtain a set of context words [\\mathrm{word}_{c-m}, \\cdots, \\mathrm{word}_{c-1}, \\mathrm{word}_{c}, \\mathrm{word}_{c+1}, \\cdots, \\mathrm{word}_{c+m}] [\\mathrm{word}_{c-m}, \\cdots, \\mathrm{word}_{c-1}, \\mathrm{word}_{c}, \\mathrm{word}_{c+1}, \\cdots, \\mathrm{word}_{c+m}] . For each window, We need to predict 2m 2m context word given the center word. Denote the \"input\" and \"output\" word vectors for \\mathrm{word}_k \\mathrm{word}_k as \\boldsymbol{v}_k \\boldsymbol{v}_k and \\boldsymbol{u}_k \\boldsymbol{u}_k respectively. The cost for the entire context window with size m m centered around \\mathrm{word}_c \\mathrm{word}_c would be \\begin{equation*} J_{\\mathrm{skip-gram}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m}) = \\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} F(\\boldsymbol{u}_{c+j}, \\boldsymbol{v}_c). \\end{equation*} \\begin{equation*} J_{\\mathrm{skip-gram}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m}) = \\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} F(\\boldsymbol{u}_{c+j}, \\boldsymbol{v}_c). \\end{equation*} F F is a placeholder notation to represent the cost function given the center word for different model. Therefore for skip-gram, the gradients for the cost of one context window are \\begin{align*} \\frac{J_{\\mathrm{skip-gram}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{U}} & = \\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} \\frac{\\partial F(\\boldsymbol{u}_{c+j}, \\boldsymbol{v}_c)}{\\partial \\boldsymbol{v}_c},\\\\ \\frac{J_{\\mathrm{skip-gram}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{v}_c} & = \\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} \\frac{\\partial F(\\boldsymbol{u}_{c+j}, \\boldsymbol{v}_c)}{\\partial \\boldsymbol{v}_c}, \\\\ \\frac{J_{\\mathrm{skip-gram}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{v}_j} & = 0, \\forall j \\ne c \\end{align*} \\begin{align*} \\frac{J_{\\mathrm{skip-gram}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{U}} & = \\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} \\frac{\\partial F(\\boldsymbol{u}_{c+j}, \\boldsymbol{v}_c)}{\\partial \\boldsymbol{v}_c},\\\\ \\frac{J_{\\mathrm{skip-gram}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{v}_c} & = \\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} \\frac{\\partial F(\\boldsymbol{u}_{c+j}, \\boldsymbol{v}_c)}{\\partial \\boldsymbol{v}_c}, \\\\ \\frac{J_{\\mathrm{skip-gram}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{v}_j} & = 0, \\forall j \\ne c \\end{align*}","title":"Gradient for all of word vectors"},{"location":"courses/cs224n/notes/#cbow-model","text":"Continuous bag-of-words (CBOW) is using the context words to predect the center words. Different from skip-gram model, in CBOW, we will use 2m 2m context word vectors as we predict probability of a word is the center word. For a simple variant of CBOW, we could sum up all the 2m 2m word vectors in one context vector \\hat{\\boldsymbol{v}} \\hat{\\boldsymbol{v}} and use the similar cost function with \\mathrm{softmax} \\mathrm{softmax} as we did in skip-gram model. \\begin{equation*} \\hat{\\boldsymbol{v}} = \\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} \\boldsymbol{v}_{c+j}. \\end{equation*} \\begin{equation*} \\hat{\\boldsymbol{v}} = \\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} \\boldsymbol{v}_{c+j}. \\end{equation*} Similar to skip-gram, we have \\begin{align*} \\frac{J_{\\mathrm{CBOW}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{U}} & = \\frac{\\partial F(\\boldsymbol{u}_{c}, \\hat{\\boldsymbol{v}})}{\\partial \\boldsymbol{U}},\\\\ \\frac{J_{\\mathrm{CBOW}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{v}_j} & = \\frac{\\partial F(\\boldsymbol{u}_{c}, \\hat{\\boldsymbol{v}})}{\\partial \\boldsymbol{v}_j}, \\forall j \\in \\{c-m, \\cdots, c-1, c+1, \\cdots, c+m\\}\\\\ \\frac{J_{\\mathrm{CBOW}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{v}_j} & = 0, \\forall j \\notin \\{c-m, \\cdots, c-1, c+1, \\cdots, c+m\\}. \\end{align*} \\begin{align*} \\frac{J_{\\mathrm{CBOW}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{U}} & = \\frac{\\partial F(\\boldsymbol{u}_{c}, \\hat{\\boldsymbol{v}})}{\\partial \\boldsymbol{U}},\\\\ \\frac{J_{\\mathrm{CBOW}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{v}_j} & = \\frac{\\partial F(\\boldsymbol{u}_{c}, \\hat{\\boldsymbol{v}})}{\\partial \\boldsymbol{v}_j}, \\forall j \\in \\{c-m, \\cdots, c-1, c+1, \\cdots, c+m\\}\\\\ \\frac{J_{\\mathrm{CBOW}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{v}_j} & = 0, \\forall j \\notin \\{c-m, \\cdots, c-1, c+1, \\cdots, c+m\\}. \\end{align*}","title":"CBOW model"},{"location":"courses/cs224n/notes/#glove-model","text":"todo, the paper and lecture.","title":"GloVe model"},{"location":"courses/cs224n/notes/#word-vector-evaluation","text":"","title":"Word vector evaluation"},{"location":"courses/cs224n/notes/#summary","text":"This post focused on gradient derivation for various word embedding models. The bootstrap model is based on the distributional semantic, by which to predict the probability of a word given some other words from the fixed corpus. We use the \\mathrm{softmax} \\mathrm{softmax} function to computer the probability. To update the word vectors, we introduce the likelihood function and derived its gradient. In that derivation, the parameter \\boldsymbol{\\theta} \\boldsymbol{\\theta} is the hyper parameters related to word vector that can be used to compute the probability. We continue to derive the gradient of the cross entropy loss function for a single word prediction. The result gradient of the cross entropy loss function is the key for all our later gradient derivation. After that we introduce the word2vec family of word embedding, skip-gram and CBOW models. In word2vec we use the dot product of the center word and its context words to compute the probability. We derive the gradient with respect both center word and context word when using cross entropy loss funciton. Negative sampling is also discussed as it improves the computation cost by many factors. With the discussion of skip-gram model, CBOW model is easier to present. One thing you need to distinguish is that whether the gradient is for a particualr word prediction, for the whole window of the words, or the over all objective for the corpus. For skip-gram, we first compute the gradient for each word prediction in the given context window, then sum up all the gradient to update the cost function of that window. We move the window over the corpus until finish all the word updates. This whole process is only one update. While using negative sampling, the process becomes more efficient, we don't have to go through all the window, only sampling K K windows with Unigram Model raise to the 3/4 3/4 power. In the simple CBOW model discussed, we add up all the context vectors first, then only update a single gradient for that window corresponding to the center word. We repeat this process for different windows to complete one update.","title":"Summary"},{"location":"courses/cs224n/notes/#part-2-neural-networks-and-backpropagation","text":"Date: April 15th 2019 We discussed the softmax classifier in Part 1 and its major drawback that the classifier only gives linear decision boundaries. In Part 2, Neural Networks will be introduced to demonstrate that it can learn much more complex functions and nonlinear decision boundaries.","title":"Part 2 Neural Networks and Backpropagation"},{"location":"courses/cs224n/notes/#intro-to-neural-networks","text":"Biological Neuron Mathematical Model Simplified Neuron The neuron can be modeled as a binary logistic regression unit as in the right figure above. It can be further simplified as following functions, \\begin{equation*} h_{w, b} (\\boldsymbol{x})=f(\\boldsymbol{w}^\\mathsf{T} \\boldsymbol{x} + b) \\\\ f(z) = \\frac{1}{1 + e^{-z}}. \\end{equation*} \\begin{equation*} h_{w, b} (\\boldsymbol{x})=f(\\boldsymbol{w}^\\mathsf{T} \\boldsymbol{x} + b) \\\\ f(z) = \\frac{1}{1 + e^{-z}}. \\end{equation*} \\boldsymbol{x} \\boldsymbol{x} is the inputs \\boldsymbol{w} \\boldsymbol{w} is the weights b b is a bias term h h is a hidden layer function f f is a nonlinear activation function (sigmoid, tanh, etc.) If we feed a vector of inputs through a bunch of logistic regression (sigmoid) functions, then we get a vector of outputs. The CS224N lecture note 3 secion 1.2 have a complete derivation on multiple sigmoid units. But we don\u2019t have to decide ahead of time what variables these logistic regressions are trying to predict! It is the loss function that will direct what the intermediate hidden variables should be, so as to do a good job at predicting the targets for the next layer, etc.","title":"Intro to Neural Networks"},{"location":"courses/cs224n/notes/#forward-propagation-in-matrix-notation","text":"In a multilayer neural network, not only did we have multiple sigmoid units, but we also have more than one layer. Let's explicitly write down the signal transformation (aka forward propagation) from one layer to another referring to this network from Andrew's ML course. We will use the following notation convention a^{(j)}_i a^{(j)}_i to represent the \"activation\" of unit i i in layer j j . W^{(j)} W^{(j)} to represent the matrix of weights controlling function mapping from layer j j to layer j + 1 j + 1 . The value at each node can be calculated as \\begin{equation*} a^{(2)}_1 = f(w^{(1)}_{10} x_0 + w^{(1)}_{11} x_1 + w^{(1)}_{12} x_2 + w^{(1)}_{13} x_3) \\\\ a^{(2)}_2 = f(w^{(1)}_{20} x_0 + w^{(1)}_{21} x_1 + w^{(1)}_{22} x_2 + w^{(1)}_{23} x_3) \\\\ a^{(2)}_3 = f(w^{(1)}_{30} x_0 + w^{(1)}_{31} x_1 + w^{(1)}_{32} x_2 + w^{(1)}_{33} x_3) \\\\ \\text{and} \\\\ h_{w} (\\boldsymbol{x}) = a^{(3)}_1 = f(w^{(2)}_{10} a^{(2)}_0 + w^{(2)}_{11} a^{(2)}_1 + w^{(2)}_{12} a^{(2)}_2 + w^{(2)}_{13} a^{(2)}_3) \\end{equation*} \\begin{equation*} a^{(2)}_1 = f(w^{(1)}_{10} x_0 + w^{(1)}_{11} x_1 + w^{(1)}_{12} x_2 + w^{(1)}_{13} x_3) \\\\ a^{(2)}_2 = f(w^{(1)}_{20} x_0 + w^{(1)}_{21} x_1 + w^{(1)}_{22} x_2 + w^{(1)}_{23} x_3) \\\\ a^{(2)}_3 = f(w^{(1)}_{30} x_0 + w^{(1)}_{31} x_1 + w^{(1)}_{32} x_2 + w^{(1)}_{33} x_3) \\\\ \\text{and} \\\\ h_{w} (\\boldsymbol{x}) = a^{(3)}_1 = f(w^{(2)}_{10} a^{(2)}_0 + w^{(2)}_{11} a^{(2)}_1 + w^{(2)}_{12} a^{(2)}_2 + w^{(2)}_{13} a^{(2)}_3) \\end{equation*} write the matrix W^{(j)} W^{(j)} explicity, \\begin{equation } W^{(1)} = \\begin{bmatrix} w^{(1)} {10} & w^{(1)} {11} & w^{(1)} {12} & w^{(1)} {13} \\ w^{(1)} {20} & w^{(1)} {21} & w^{(1)} {22} & w^{(1)} {23} \\ w^{(1)} {30} & w^{(1)} {31} & w^{(1)} {32} & w^{(1)} {33} \\end{bmatrix} \\end{equation } \\begin{equation*} W^{(2)} = \\begin{bmatrix} w^{(2)}_{10} & w^{(2)}_{11} & w^{(2)}_{12} & w^{(2)}_{13} \\end{bmatrix} \\end{equation*} \\begin{equation*} W^{(2)} = \\begin{bmatrix} w^{(2)}_{10} & w^{(2)}_{11} & w^{(2)}_{12} & w^{(2)}_{13} \\end{bmatrix} \\end{equation*} With the above form, we can use matrix notation as \\begin{align*} & \\boldsymbol{a}^{(2)} = f(\\boldsymbol{W}^{(1)} \\boldsymbol{x}) \\\\ h_{w} (\\boldsymbol{x}) = & \\boldsymbol{a}^{(3)} = f(\\boldsymbol{W}^{(2)} \\boldsymbol{a}^{(2)}) \\end{align*} \\begin{align*} & \\boldsymbol{a}^{(2)} = f(\\boldsymbol{W}^{(1)} \\boldsymbol{x}) \\\\ h_{w} (\\boldsymbol{x}) = & \\boldsymbol{a}^{(3)} = f(\\boldsymbol{W}^{(2)} \\boldsymbol{a}^{(2)}) \\end{align*} We can see from above notations that if the network has s_j s_j units in layer j j and s_{j+1} s_{j+1} in layer j + 1 j + 1 , the matrix W^{(j)} W^{(j)} will be of dimention s_{j+1} \\times s_{j}+1 s_{j+1} \\times s_{j}+1 . It could be interpreted as \"the dimention of W^{(j)} W^{(j)} is the number of nodes in the next layer (layer j + 1 j + 1 ) \\times \\times the number of nodes in the current layer + + 1. Note that in cs224n the matrix notation is slightly different, \\begin{align } & \\boldsymbol{a}^{(2)} = f(\\boldsymbol{W}^{(1)} \\boldsymbol{x} + \\boldsymbol{b}^{(1)}) \\ h_{w} (\\boldsymbol{x}) = & \\boldsymbol{a}^{(3)} = f(\\boldsymbol{W}^{(2)} \\boldsymbol{a}^{(2)} + \\boldsymbol{b}^{(2)}) \\end{align } the different is from how we denote the bias. The two are enssentially the same, but be cautions that the matrix dimentions are different.","title":"Forward Propagation in Matrix Notation"},{"location":"courses/cs224n/notes/#word-window-classification-using-neural-networks","text":"From now on, let's switch the notation to cs224n so that we can derive the backpropagation algorithm for word window classification and get more intuition about the backprop. The drawing of the neural nets in cs224n for word window classification is less dynamic and slitly different from the drawing from Andrew's ML class. The figure from cs224n may be slightly confusing at first, but it is good to understand it from this particular application, word window classification.","title":"Word Window Classification Using Neural Networks"},{"location":"courses/cs224n/notes/#forward-propagation","text":"Firstly, the goal of this classification task is to classify whether the center word is a location. Similar to word2vec, we will go over all positions in a corpus. But this time, it will be supervised and only some positions should get a high score. The figure above illustrate the feed-forward process. We use the method by Collobert & Weston (2008, 2011). An unnormalized score will be calculated from the activation \\boldsymbol{a} = [a_1, a_2, a_3, \\cdots]^\\mathsf{T} \\boldsymbol{a} = [a_1, a_2, a_3, \\cdots]^\\mathsf{T} . \\begin{equation*} \\text{scroe}(\\boldsymbol{x}) = U^\\mathsf{T} \\boldsymbol{a} \\in \\mathbb{R} \\end{equation*} \\begin{equation*} \\text{scroe}(\\boldsymbol{x}) = U^\\mathsf{T} \\boldsymbol{a} \\in \\mathbb{R} \\end{equation*} We will use max-margin loss as our loss function. The training is essentially to find the optimal weights W W by minimize the max-margin loss \\begin{equation*} J = \\max(0, 1 - s + s_c) \\end{equation*} \\begin{equation*} J = \\max(0, 1 - s + s_c) \\end{equation*} s s is the score of a window that have a location in the center. s_c s_c is the score of a window that doesn't have a location in the center. For full objective function: Sample several corrupt windows per true one. Sum over all training windows. It is similar to negative sampling in word2vec. We will use gradient descent to update the parameter so as to minimize the loss function. The key is how to calculate the gradient with respect to the model parameters, namely \\nabla_{\\theta} J(\\theta) \\nabla_{\\theta} J(\\theta) . Here we use \\theta \\theta to represent the hyperthetic parameters, it can include the W W and other parameters of the model. \\begin{equation*} \\theta^{\\text{new}} = \\theta^{\\text{old}} - \\alpha\\nabla_{\\theta} J(\\theta) \\end{equation*} \\begin{equation*} \\theta^{\\text{new}} = \\theta^{\\text{old}} - \\alpha\\nabla_{\\theta} J(\\theta) \\end{equation*}","title":"Forward propagation"},{"location":"courses/cs224n/notes/#gradients-and-jacobians-matrix","text":"At first, let us layout the input and all the equations in this simple neural network. | Input Layer | Hidden Layer | Output Layer | |:---:|:---:|:---:| | \\boldsymbol{x} \\boldsymbol{x} | \\boldsymbol{z} = \\boldsymbol{W} \\boldsymbol{x} + \\boldsymbol{b}, \\\\ \\boldsymbol{h} = f(\\boldsymbol{z}) \\boldsymbol{z} = \\boldsymbol{W} \\boldsymbol{x} + \\boldsymbol{b}, \\\\ \\boldsymbol{h} = f(\\boldsymbol{z}) | $ s = \\boldsymbol{u}^{\\mathsf{T}}\\boldsymbol{h}$ | To update the parameters in this model, namely \\boldsymbol{x}, \\boldsymbol{W}, \\boldsymbol{b} \\boldsymbol{x}, \\boldsymbol{W}, \\boldsymbol{b} , we would like to compute the derivitavies of s s with respect to all these parameters. We have to use the chain rule to compute it. What chain rule says is that we can compute the partial derivatives of each individual functions and then multiply them together to get the derivative with respect the specific variable. For example, \\frac{\\partial s}{\\partial \\boldsymbol{b}} \\frac{\\partial s}{\\partial \\boldsymbol{b}} is computed as \\frac{\\partial s}{\\partial \\boldsymbol{b}} = \\frac{\\partial s}{\\partial \\boldsymbol{h}} \\cdot \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\cdot \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} \\frac{\\partial s}{\\partial \\boldsymbol{b}} = \\frac{\\partial s}{\\partial \\boldsymbol{h}} \\cdot \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\cdot \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} . This seems very easy to understand, but when it comes to implemenation in vectorized format, it become confusing for those who doesn't work on matrix calculus for quite a while like me. I want to get the points straight here. What exactly is \\frac{\\partial s}{\\partial \\boldsymbol{b}} \\frac{\\partial s}{\\partial \\boldsymbol{b}} and \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} . Note both \\boldsymbol{h} \\boldsymbol{h} and \\boldsymbol{z} \\boldsymbol{z} are vectors. To calculate these two gradient, simply remember the following two rules: Given a function with 1 output and n n inputs f(\\boldsymbol{x}) = f(x_1, x_2, \\cdots, x_n) f(\\boldsymbol{x}) = f(x_1, x_2, \\cdots, x_n) , it's gradient is a vector of partial derivatives with respect to each input(take the gradient element wise). \\begin{equation } \\frac{\\partial f}{\\partial \\boldsymbol{x}} = \\Bigg [ \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\cdots, \\frac{\\partial f}{\\partial x_n}, \\Bigg ] \\end{equation } Given a function with m m output and n n inputs \\boldsymbol{f}(\\boldsymbol{x}) = \\big [f_1(x_1, x_2, \\cdots, x_n), \\cdots, f_m(x_1, x_2, \\cdots, x_n) \\big ] \\boldsymbol{f}(\\boldsymbol{x}) = \\big [f_1(x_1, x_2, \\cdots, x_n), \\cdots, f_m(x_1, x_2, \\cdots, x_n) \\big ] , it's gradient is an m \\times n m \\times n matrix of partial derivatives (\\frac{\\partial \\boldsymbol{f}}{\\partial \\boldsymbol{x}})_{ij} = \\dfrac{\\partial f_i}{\\partial x_j} (\\frac{\\partial \\boldsymbol{f}}{\\partial \\boldsymbol{x}})_{ij} = \\dfrac{\\partial f_i}{\\partial x_j} . This matrix is also called Jacobian matrix. \\begin{equation*} \\frac{\\partial \\boldsymbol{f}}{\\partial \\boldsymbol{x}} = \\begin{bmatrix} \\dfrac{\\partial \\boldsymbol{f}}{\\partial x_1} & \\cdots & \\dfrac{\\partial \\boldsymbol{f}}{\\partial x_n} \\end{bmatrix} = \\begin{bmatrix} \\dfrac{\\partial f_1}{\\partial x_1} & \\cdots & \\dfrac{\\partial f_1}{\\partial x_n}\\\\ \\vdots & \\ddots & \\vdots\\\\ \\dfrac{\\partial f_m}{\\partial x_1} & \\cdots & \\dfrac{\\partial f_m}{\\partial x_n} \\end{bmatrix} \\end{equation*} \\begin{equation*} \\frac{\\partial \\boldsymbol{f}}{\\partial \\boldsymbol{x}} = \\begin{bmatrix} \\dfrac{\\partial \\boldsymbol{f}}{\\partial x_1} & \\cdots & \\dfrac{\\partial \\boldsymbol{f}}{\\partial x_n} \\end{bmatrix} = \\begin{bmatrix} \\dfrac{\\partial f_1}{\\partial x_1} & \\cdots & \\dfrac{\\partial f_1}{\\partial x_n}\\\\ \\vdots & \\ddots & \\vdots\\\\ \\dfrac{\\partial f_m}{\\partial x_1} & \\cdots & \\dfrac{\\partial f_m}{\\partial x_n} \\end{bmatrix} \\end{equation*}","title":"Gradients and Jacobians Matrix"},{"location":"courses/cs224n/notes/#computing-gradients-with-the-chain-rule","text":"With these two rules we can calculate the partials. We will use \\frac{\\partial s}{\\partial \\boldsymbol{b}} \\frac{\\partial s}{\\partial \\boldsymbol{b}} as an example. \\begin{equation*} \\frac{\\partial s}{\\partial \\boldsymbol{b}} = \\frac{\\partial s}{\\partial \\boldsymbol{h}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} \\end{equation*} \\begin{equation*} \\frac{\\partial s}{\\partial \\boldsymbol{b}} = \\frac{\\partial s}{\\partial \\boldsymbol{h}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} \\end{equation*} \\frac{\\partial s}{\\partial \\boldsymbol{h}} = \\frac{\\partial}{\\partial \\boldsymbol{h}}(\\boldsymbol{u}^{\\mathsf{T}}\\boldsymbol{h}) = \\boldsymbol{u}^{\\mathsf{T}} \\frac{\\partial s}{\\partial \\boldsymbol{h}} = \\frac{\\partial}{\\partial \\boldsymbol{h}}(\\boldsymbol{u}^{\\mathsf{T}}\\boldsymbol{h}) = \\boldsymbol{u}^{\\mathsf{T}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} = \\frac{\\partial}{\\partial \\boldsymbol{z}}(f(\\boldsymbol{z})) = \\begin{pmatrix} f'(z_1) & & 0 \\\\ & \\ddots & \\\\ 0 & & f'(z_n) \\end{pmatrix} = \\mathrm{diag} (f' (\\boldsymbol{z})) \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} = \\frac{\\partial}{\\partial \\boldsymbol{z}}(f(\\boldsymbol{z})) = \\begin{pmatrix} f'(z_1) & & 0 \\\\ & \\ddots & \\\\ 0 & & f'(z_n) \\end{pmatrix} = \\mathrm{diag} (f' (\\boldsymbol{z})) \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} = \\frac{\\partial}{\\partial \\boldsymbol{b}}(\\boldsymbol{W}\\boldsymbol{x} + \\boldsymbol{b}) = \\boldsymbol{I} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} = \\frac{\\partial}{\\partial \\boldsymbol{b}}(\\boldsymbol{W}\\boldsymbol{x} + \\boldsymbol{b}) = \\boldsymbol{I} Therefore, \\begin{equation*} \\frac{\\partial s}{\\partial \\boldsymbol{b}} = \\boldsymbol{u}^{\\mathsf{T}} \\mathrm{diag}(f'(\\boldsymbol{z})) \\boldsymbol{I} = \\boldsymbol{u}^{\\mathsf{T}} \\circ f'(\\boldsymbol{z}) \\end{equation*} \\begin{equation*} \\frac{\\partial s}{\\partial \\boldsymbol{b}} = \\boldsymbol{u}^{\\mathsf{T}} \\mathrm{diag}(f'(\\boldsymbol{z})) \\boldsymbol{I} = \\boldsymbol{u}^{\\mathsf{T}} \\circ f'(\\boldsymbol{z}) \\end{equation*} Similarly we can calculate the partial with respect to \\boldsymbol{W} \\boldsymbol{W} and \\boldsymbol{x} \\boldsymbol{x} . Since the top layer partials are already calculated, we can reuese the results. We denote those reusable partials as \\delta \\delta meaning local error signal. \\begin{align*} \\frac{\\partial s}{\\partial \\boldsymbol{W}} & = \\frac{\\partial s}{\\partial \\boldsymbol{h}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{W}} \\\\ \\\\ \\frac{\\partial s}{\\partial \\boldsymbol{x}} & = \\frac{\\partial s}{\\partial \\boldsymbol{h}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{x}} \\\\ \\\\ \\frac{\\partial s}{\\partial \\boldsymbol{b}} & = \\underbrace{\\frac{\\partial s}{\\partial \\boldsymbol{h}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}}}_{\\mathrm{reuse}} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} = \\boldsymbol{\\delta} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} = \\boldsymbol{\\delta} = \\boldsymbol{u}^{\\mathsf{T}} \\circ f'(\\boldsymbol{z}) \\end{align*} \\begin{align*} \\frac{\\partial s}{\\partial \\boldsymbol{W}} & = \\frac{\\partial s}{\\partial \\boldsymbol{h}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{W}} \\\\ \\\\ \\frac{\\partial s}{\\partial \\boldsymbol{x}} & = \\frac{\\partial s}{\\partial \\boldsymbol{h}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{x}} \\\\ \\\\ \\frac{\\partial s}{\\partial \\boldsymbol{b}} & = \\underbrace{\\frac{\\partial s}{\\partial \\boldsymbol{h}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}}}_{\\mathrm{reuse}} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} = \\boldsymbol{\\delta} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} = \\boldsymbol{\\delta} = \\boldsymbol{u}^{\\mathsf{T}} \\circ f'(\\boldsymbol{z}) \\end{align*}","title":"Computing gradients with the chain rule"},{"location":"courses/cs224n/notes/#shape-convention","text":"What does the shape of the derivatives looks like in practice? How can we make the chain rule computation efficient? According to the aforementioned gradient calculation rules, \\frac{\\partial s}{\\partial \\boldsymbol{W}} \\frac{\\partial s}{\\partial \\boldsymbol{W}} is a row vector. The chain rule gave \\begin{equation*} \\frac{\\partial s}{\\partial \\boldsymbol{W}} = \\boldsymbol{\\delta} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{W}}. \\end{equation*} \\begin{equation*} \\frac{\\partial s}{\\partial \\boldsymbol{W}} = \\boldsymbol{\\delta} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{W}}. \\end{equation*} We know from Jacobians that \\frac{\\partial\\boldsymbol{z}}{\\partial \\boldsymbol{W}} = \\frac{\\partial}{\\partial \\boldsymbol{W}}(\\boldsymbol{W}\\boldsymbol{x} + \\boldsymbol{b}) = \\boldsymbol{x}^{\\mathsf{T}} \\frac{\\partial\\boldsymbol{z}}{\\partial \\boldsymbol{W}} = \\frac{\\partial}{\\partial \\boldsymbol{W}}(\\boldsymbol{W}\\boldsymbol{x} + \\boldsymbol{b}) = \\boldsymbol{x}^{\\mathsf{T}} . We may arrived the result that \\frac{\\partial s}{\\partial \\boldsymbol{W}} = \\boldsymbol{\\delta} \\ \\boldsymbol{x}^{\\mathsf{T}} \\frac{\\partial s}{\\partial \\boldsymbol{W}} = \\boldsymbol{\\delta} \\ \\boldsymbol{x}^{\\mathsf{T}} . This is actually not quite wirte. The correct form should be \\begin{align*} \\underbrace{\\frac{\\partial s}{\\partial \\boldsymbol{W}}}_{n \\times m} = \\underbrace{\\boldsymbol{\\delta}^{\\mathsf{T}}}_{n \\times 1} \\ \\underbrace{\\boldsymbol{x}^{\\mathsf{T}}}_{1 \\times m}. \\end{align*} \\begin{align*} \\underbrace{\\frac{\\partial s}{\\partial \\boldsymbol{W}}}_{n \\times m} = \\underbrace{\\boldsymbol{\\delta}^{\\mathsf{T}}}_{n \\times 1} \\ \\underbrace{\\boldsymbol{x}^{\\mathsf{T}}}_{1 \\times m}. \\end{align*} You may wonder why is this form instead of the one we derived directly. The explanation from the CS224N is that we would like to follow the shape convention so as to make the chain rule implementation more efficient (matrix multiplication instead of loops). Different from the Jacobian form, shape convention states that the shape of the gradient is the same shape of parameters. The resolution here is to use Jacobian form as much as possible and reshape to follow the convention at the end. Because Jacobian form makes the chain rule easy and the shape convention makes the implementation of gradient descent easy.","title":"Shape convention"},{"location":"courses/cs224n/notes/#computation-graphs-and-backpropagation","text":"We have shown how to compute the partial derivatives using chain rule. This is almost the backpropagation alrogithm. If we want to add on to that and make the algorithm complete, the only thing we need is how to reuse gradient computed for higher layers. We leverage the computation graph to explain this. From the computation graph, you'll see the intuiation of reusing the partial derivatives computed for higher layers in computing derivatives for lower layers so as to minimize computation. We represent our neural net equations as a computation graph as following: source nodes represent inputs interior nodes represent operations edges pass along result of the operation When following computation order to carry out the computations from inputs, it is enssetially forward propagation. Backpropagation is to pass along gradients backwards. This can be illustrated in the following computation graph. The partial derivative with respect to a parameter reflect how changing the parameter would effect the output value. The output value in the backprop is usually a loss function (or error). Intuitively, you can see backprop is push the error back to the lower layer through a bunch of operation nodes, the arrived error is a measure of the error at that particular layer, training is try to reduce this backprop-ed error by adjusting the local parameters, the effect of reducing the local error will be forward propagated to the output, the error at the output should also reduced. We use gradient descent to make this process to converge as soon as possible. To understand it better, let's look at a single node as following. We define \"local gradient\" for each node as the gradient of it's output with respect to it's input. By chain rule, the downstream gradient is equal to the multiplication of the upstream gradient and the local gradient. When having multipe local gradients, they are pushed back to each input using chain rule. CS224N lecture 4 slides have step by step example of backprop. From the example, we can got some intuitions about some nodes' effects. For example, when push back gradients along outward branches from a single node, the gradients should be sumed up; \"+\" node distributes the upstream gradient to each summand; \"max\" node simply routes the upstream gradients. When update gradient with backprop, you should compute all the gradients at once. With this computation graph notion, following routine captures the gist of backprop in a very decent manner. Fprop: visit nodes in topological sorted order Compute value of node given predecessors Bprop: initialize output gradient = 1 visit nodes in reverse order: Compute gradient wrt each node using gradient wrt successors \\{y_1, y_2, \\cdots, y_2\\} = \\mathrm{successors\\ of\\ } x \\{y_1, y_2, \\cdots, y_2\\} = \\mathrm{successors\\ of\\ } x \\begin{equation*}\\frac{\\partial z}{\\partial \\boldsymbol{x}} = \\sum_{i=i}^n \\frac{\\partial z}{\\partial y_i} \\frac{\\partial y_i}{\\partial x} \\end{equation*} \\begin{equation*}\\frac{\\partial z}{\\partial \\boldsymbol{x}} = \\sum_{i=i}^n \\frac{\\partial z}{\\partial y_i} \\frac{\\partial y_i}{\\partial x} \\end{equation*} If done correctly, the big O O complexity of forward propagation and backpropagation is the same.","title":"Computation graphs and backpropagation"},{"location":"courses/cs224n/notes/#automatic-differentiation","text":"The gradient computation can be automatically inferred from the symbolic expression of the fprop. but this is not commonly used in practice. Each node type needs to know how to compute its output and how to compute the gradient wrt its inputs given the gradient wrt its output. Modern DL frameworks (Tensorflow, PyTorch, etc.) do backpropagation for you but mainly leave layer/node writer to hand-calculate the local derivative. Following is a simple demo of how to implement forward propagation and backpropagation. class MultiplyGate ( object ): def forward ( x , y ): z = x * y self . x = x # must keep these around! self . y = y return z def backword ( dz ): dx = self . y * dz # [dz/dx * dL/dz] dy = self . x * dz # [dz/dy * dL/dz] return [ dx , dy ] nonlinear activation functions: sigmoid, tanh, hard tanh, ReLU Learning rates: start 0.001. power of 10. halve the learning rate every k epochs. formula: lr = lr_0 e^{-k t} lr = lr_0 e^{-k t} for each epoch t t","title":"Automatic differentiation"},{"location":"courses/cs224n/notes/#regularization","text":"LFD starting point: L2 regularization","title":"Regularization"},{"location":"courses/cs224n/notes/#part-3-language-model-and-rnn","text":"","title":"Part 3 Language Model and RNN"},{"location":"courses/cs224n/notes/#n-gram","text":"assumption: \\boldsymbol{x}^{(t+1)} \\boldsymbol{x}^{(t+1)} depends only on previous n-1 n-1 words. Sparsity problem, --> backoff (n-1)-gram practically, n has to be less than 5.","title":"n-gram"},{"location":"courses/cs224n/notes/#neural-language-model","text":"fixed-window neural language model Recurrent Neural Ne","title":"Neural language model"},{"location":"leetcode/binary-search/notes/","text":"Binary Search \u00b6 Binary search problem characteristics \u00b6 Ordered binary search. You need to find index or array element where ordering info is available, either explicitly (sorted array) or implicitly (partially sorted or other special info). monotony pattern. If the ordering info isn't available, but you can exclude \"all\" the possible cases from left or right by a condition camparing f(mid) to the target . Binary search problem solving techniques \u00b6 Clarify that you are trying to find the first one or find the last one. Clarify that you are trying to move the index or move the value (i.e. kth smallest number in multiplicative table). Use an \"ordering abstraction\" vless(target, f(mid)) . This ordering abstraction will produce a boolean array that indicate the ordering information between the target value and the f(mid) . Decide whether the left or right part the f(mid) should fall into. The principle to determine the predicate is simple: don't rule out the possible result (maintain the loop invariant). Shrink the range accordingly based on the predicate decided in step 3. Test the case that the search range is small, such as only have one or two elements. Binary search practical use case \u00b6 Find whether the given target is in the array. Find the the position of the first value equal to the given target. Find the insertion position of the given target in the array. Find the the position of the last value equal to the given target. Find the total number of x in a sorted array. Find the last element less than the target. Find the first element greater than the target. Binary search in C++ STL \u00b6 lower_bound : return iterator point to the element no less than the target. upper_bound : return iterator point to the element greater than the target. equal_range : return a pair of iterators, first of which is lower_bound , second is upper_bound . binary_search : return true if an element equivalent to val is found, and false otherwise. Caveat of binary search implementation \u00b6 Specify the range: [start, end) or [start, end] ? C++ STL used [start, end) to denote a range, which bring in many conveniences. We will stick on this convention. Which while loop condition? start < end ? start <= end ? start != end ? start + 1 < end ? The calculation of the mid . mid = start + (end - start) / 2 or mid = (start + end) / 2 ? To proof mid is always in the range [begin, end) . The \"bisection\": start = mid + 1 , start = mid , or end = mid - 1 or end = mid ? Where is the result? start ? end ? How to make sure? A \"universial\" binary search implementation \u00b6 Despite the above caveas, just remember that there are two version of binary search one can write based on the range [begin, end) and [begin, end] . Iterator type in C++ using the former, it have many benefits in reduce the code complexity. Among all the binary search implementation you have seen, the following one is the most powerful version and it equivalent to C++ STL lower_bound algorithm. /** * return index to an element no less than x. Be more specifically, if there is * an element in the given array equal to x, it returns the index of first such * element; if there is no element that is equal to x, it returns the index * where x can be inserted into the position without changing the ordering of * the elements. * * All possible return value for calling this function with array.size() == n is * [0, 1, ..., n - 1, n] * */ size_t binary_search ( int x , vector < int >& array , size_t n ) { size_t begin = 0 , end = n ; while ( begin != end ) { size_t mid = begin + ( end - begin ) / 2 ; if ( array [ mid ] < x ) { begin = mid + 1 ; } else { end = mid ; } } return begin ; } Important observations about this implementation \u00b6 mid cannot less than begin , they can be equal. This will ensure begin = mid + 1 in the if statement at least reduce the size of [begin, end] by 1. Informal proof: if (array[mid] < x) , it indicate x can only possible be in array[mid + 1, mid + 2, ... n - 1]. mid + 1 is at least 1 greater than begin. mid and end never equal inside the while loop, mid < end is always hold. This will ensure end = mid in the else statement at least reduce the size of [begin, end] by 1. Informal proof: we have begain < end , so begin + end < 2 * end , thus (begin + end) / 2 < end , because integer divisioin truncate down, mid = (begin + end) / 2 always less than end. begin and end never cross. Informal proof: Inside the while loop, at the begining, we have begin < end . If the current iteration executes the if condition, begin = mid + 1 at most advance begin to end but not exceed end . If it execute the else condition, end = mid would at worst case change end point to the minimum value of mid , because we have begin <= mid . Thus, we can conclude that executing the statement end = mid will not change end less than begin , at worst equal to begin . Claims regarding this binary search routine \u00b6 The range [begin, end) is used, which comply to the convention used in C++ iterator. It is impossible that mid == end . If they are equal, array[n] is invalid memory access. We use the loop condition while (begin != end) to indicate that once the loop terminates, we have begin == end . By checking whether begin is a valid index to the array or not, we can know whether x is greater than all the elements in the array or not. If we want to check whether x is found in the array, we simply check array[begin] == x . However, this condition is based on the assumption that begin < end initially. Considering that, using while (begin < end) is better if you cannot ensure begin < end before the loop. Setting begin = mid + 1 reduces the size of the remaining interested sub-array and maintains the invariant, which is if x in the array, x is in [begin, end) . Setting end = mid reduces the size of the remaining interested sub-array (mid never equal to end) and maintains the invariant, which is if x in the array, x is in [begin, end) . This claim is a little hard to absorb. On way to understand is like the following: ~~Because we need keep searching x in the range [begin, mid] if we get in the else statement. In the else case there are two possibilities: 1) array[mid] > x . 2) array[mid] = x . For 1) it indicates x is in [begin, mid) , setting end = mid maintains the loop invariant correctly, which is that x is in the shrinked range. For the 2) it is a little complex. If array[mid] is the only element equal to x, setting end = mid appears violate the loop invariant by exclude x from the range [begin, end) . however, remember array[mid] is the only element equal to x, after the while loop, begin = end , we have the x found by begin even though theoretically [begin, end) is already an empty range since begin = end and array[begin] = array[end] = x . If there are more values are equal to x before and after the element array[mid] the loop will always end up finding the first x value in the array. If we use end = mid + 1 . Try test case [1, 3, 5, 7] , with x = 0 . deadloop will accur. i.e. begin = 0, mid = 1, end = 2 . Category 1 Binary search basics and binary search on special array (i.e. rotated sorted) \u00b6 To solve this type of binary search problem. You should focus on the following: Come up test cases to verify your solution. Be able to find which side to drop for each iteration. Be extremly careful \"off by 1\" bugs. (1. reasoning: is mid value possible to be the solution or not. 2. exercise test cases: especially short ones) 34. Search for a Range \u00b6 C++ Use STL function class Solution { public : vector < int > searchRange ( vector < int >& nums , int target ) { vector < int > res ( 2 , - 1 ); int low = lower_bound ( nums . begin (), nums . end (), target ) - nums . begin (); int high = upper_bound ( nums . begin (), nums . end (), target ) - nums . begin (); if ( low == high ) return res ; return { low , hight - 1 }; } }; C++ Implementation of binary search class Solution { public : vector < int > searchRange ( vector < int >& nums , int target ) { vector < int > res ( 2 , - 1 ); int low = lower_bound ( nums , target ); //int high = lower_bound(nums, target + 1); // also works. int high = upper_bound ( nums , target ); if ( low == high ) { return res ; } return { low , high - 1 }; } int lower_bound ( vector < int >& nums , int target ) { if ( nums . size () == 0 ) return 0 ; int l = 0 , r = nums . size (); while ( l < r ) { int m = l + ( r - l ) / 2 ; if ( nums [ m ] < target ) { l = m + 1 ; } else { r = m ; } } return l ; } int upper_bound ( vector < int >& nums , int target ) { if ( nums . size () == 0 ) return 0 ; int l = 0 , r = nums . size (); while ( l < r ) { int m = l + ( r - l ) / 2 ; if ( nums [ m ] <= target ) { l = m + 1 ; } else { r = m ; } } return l ; } }; Python solution class Solution ( object ): def searchRange ( self , nums , target ): \"\"\" :type nums: List[int] :type target: int :rtype: List[int] \"\"\" if len ( nums ) == 0 : return [ - 1 , - 1 ] begin = 0 end = len ( nums ) while begin != end : mid = begin + ( end - begin ) / 2 if nums [ mid ] < target : begin = mid + 1 else : end = mid if begin == len ( nums ): return [ - 1 , - 1 ] if nums [ begin ] == target : lower = begin else : lower = - 1 begin = 0 end = len ( nums ) while begin != end : mid = begin + ( end - begin ) / 2 if nums [ mid ] <= target : begin = mid + 1 else : end = mid if nums [ begin - 1 ] == target : upper = begin - 1 else : upper = - 1 return [ lower , upper ] 35. Search Insert Position \u00b6 C++ solution lower_bound class Solution { public : int searchInsert ( vector < int >& nums , int target ) { if ( nums . size () == 0 ) return 0 ; int l = 0 , r = nums . size (); while ( l < r ) { int m = l + ( r - l ) / 2 ; if ( nums [ m ] < target ) { l = m + 1 ; } else { r = m ; } } return l ; } }; 33. Search in Rotated Sorted Array \u00b6 How to locate the sorted half? If left half is sorted, check where the target t is like be. else if right half is sorted, check where the target t is like to be. else if mid element is equal to left or right. Remove one of them. Although no duplicate, should consider short input like [3 1], 1 will have the equal case. C++ /** t = 1 t = 3 t = 5 t = 4 t = -1 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 3 4 5 1 1 3 5 4 1 <--need check */ class Solution { public : int search ( vector < int >& A , int t ) { if ( A . empty ()) return - 1 ; int l = 0 , r = A . size () - 1 ; while ( l < r ) { int m = l + ( r - l ) / 2 ; if ( A [ m ] == t ) return m ; if ( A [ l ] < A [ m ]) { // left is sorted if ( A [ l ] <= t && t < A [ m ]) { r = m - 1 ; } else { l = m + 1 ; } } else if ( A [ m ] < A [ r ]) { // right is sorted if ( A [ m ] < t && t <= A [ r ]) { l = m + 1 ; } else { r = m - 1 ; } } else { // if equal, remove one. case: [3, 1], 1 if ( A [ l ] == A [ m ]) l ++ ; if ( A [ m ] == A [ r ]) r -- ; } } return A [ l ] == t ? l : - 1 ; } }; Python class Solution ( object ): def search ( self , nums , target ): \"\"\" :type nums: List[int] :type target: int :rtype: int \"\"\" if len ( nums ) == 0 : return - 1 left = 0 right = len ( nums ) - 1 while left < right : mid = left + ( right - left ) / 2 if nums [ mid ] == target : return mid ; if nums [ left ] < nums [ mid ]: if nums [ left ] <= target and target < nums [ mid ]: right = mid - 1 else : left = mid + 1 elif nums [ mid ] < nums [ right ]: if nums [ mid ] < target and target <= nums [ right ]: left = mid + 1 else : right = mid - 1 else : if nums [ left ] == nums [ mid ]: left += 1 if nums [ right ] == nums [ mid ]: right -= 1 if nums [ left ] == target : return left return - 1 81. Search in Rotated Sorted Array II \u00b6 How to locate the sorted half? C++ class Solution { public : bool search ( vector < int >& A , int t ) { if ( A . empty ()) return false ; int l = 0 , r = A . size () - 1 ; while ( l < r ) { int m = l + ( r - l ) / 2 ; if ( A [ m ] == t ) return true ; if ( A [ l ] < A [ m ]) { if ( A [ l ] <= t && t < A [ m ]) { r = m - 1 ; } else { l = m + 1 ; } } else if ( A [ m ] < A [ r ]) { if ( A [ m ] < t && t <= A [ r ]) { l = m + 1 ; } else { r = m - 1 ; } } else { if ( A [ l ] == A [ m ]) l ++ ; if ( A [ m ] == A [ r ]) r -- ; } } return A [ l ] == t ? true : false ; } }; Python class Solution ( object ): def search ( self , nums , target ): \"\"\" :type nums: List[int] :type target: int :rtype: int \"\"\" if len ( nums ) == 0 : return False left = 0 right = len ( nums ) - 1 while left < right : mid = left + ( right - left ) / 2 if nums [ mid ] == target : return True if nums [ left ] < nums [ mid ]: if nums [ left ] <= target and target < nums [ mid ]: right = mid - 1 else : left = mid + 1 elif nums [ mid ] < nums [ right ]: if nums [ mid ] < target and target <= nums [ right ]: left = mid + 1 else : right = mid - 1 else : if nums [ left ] == nums [ mid ]: left += 1 if nums [ right ] == nums [ mid ]: right -= 1 if nums [ left ] == target : return True return False 153. Find Minimum in Rotated Sorted Array \u00b6 Try to locate the valley which contains the min. Notice when A[0] < A[n - 1] , return A[0] . Draw a monotonic curve and then split the curve into two half, swith the order. This can help you to write the code. C++ class Solution { public : int findMin ( vector < int >& A ) { int l = 0 , r = A . size () - 1 ; while ( l < r ) { if ( A [ l ] < A [ r ]) // serve as base case. return A [ l ]; int m = l + ( r - l ) / 2 ; if ( A [ m ] > A [ r ]) { // also works. looking for not sorted half l = m + 1 ; } else if ( A [ m ] < A [ r ]) { // don't really need if statement r = m ; } } return A [ l ]; } }; Python class Solution ( object ): def findMin ( self , nums ): \"\"\" :type nums: List[int] :rtype: int \"\"\" if len ( nums ) == 0 : return - 1 left = 0 right = len ( nums ) - 1 while left < right : if nums [ left ] == nums [ right ]: return nums [ left ] mid = left + ( right - left ) / 2 if nums [ mid ] > nums [ right ]: left = mid + 1 else : right = mid return nums [ left ] 154. Find Minimum in Rotated Sorted Array II \u00b6 Locate the valley which contains the min. Since duplicates exist. we cannot use the observation A[l] == A[r] . Here we deal with duplicates using decrease by one step. C++ class Solution { public : int findMin ( vector < int >& A ) { int l = 0 , r = A . size () - 1 ; while ( l < r ) { int m = l + ( r - l ) / 2 ; if ( A [ m ] > A [ r ]) { l = m + 1 ; } else if ( A [ m ] < A [ r ]) { r = m ; } else { r -- ; } } return A [ l ]; } }; Python class Solution ( object ): def findMin ( self , nums ): \"\"\" :type nums: List[int] :rtype: int \"\"\" if len ( nums ) == 0 : return - 1 left = 0 right = len ( nums ) - 1 while left < right : mid = left + ( right - left ) / 2 if nums [ mid ] > nums [ right ]: left = mid + 1 elif nums [ mid ] < nums [ right ]: right = mid else : right -= 1 return nums [ left ] 162 Find Peak Element \u00b6 Use Binary search Use the neighboring relation to determin which side a peak value may occur then eliminate the other side. C++ class Solution { public : int findPeakElement ( vector < int >& A ) { int l = 0 , r = A . size () - 1 ; while ( l < r ) { int m = l + ( r - l ) / 2 ; if ( A [ m ] < A [ m + 1 ]) { l = m + 1 ; } else if ( A [ m ] > A [ m + 1 ]) { r = m ; } } return l ; } }; 278. First Bad Version \u00b6 Binary search Notice how this can be related to the ordering abstraction. // Forward declaration of isBadVersion API. bool isBadVersion ( int version ); class Solution { public : int firstBadVersion ( int n ) { int l = 0 , r = n ; while ( l < r ) { int m = l + ( r - l ) / 2 ; if ( ! isBadVersion ( m )) { l = m + 1 ; } else { r = m ; } } return l ; } }; 374. Guess Number Higher or Lower \u00b6 C++ binary search // Forward declaration of guess API. // @param num, your guess // @return -1 if my number is lower, 1 if my number is higher, otherwise return 0 int guess ( int num ); class Solution { public : int guessNumber ( int n ) { int start = 1 , end = n ; while ( start < end ) { int mid = start + ( end - start ) / 2 ; if ( guess ( mid ) == 0 ) return mid ; if ( guess ( mid ) == 1 ) { start = mid + 1 ; } else { end = mid ; } } return start ; } }; Python # The guess API is already defined for you. # @param num, your guess # @return -1 if my number is lower, 1 if my number is higher, otherwise return 0 # def guess(num): class Solution ( object ): def guessNumber ( self , n ): \"\"\" :type n: int :rtype: int \"\"\" begin = 0 end = n while begin != end : mid = begin + ( end - begin ) / 2 if guess ( mid ) == 0 : return mid if guess ( mid ) == 1 : begin = mid + 1 else : end = mid return begin 475. Heaters \u00b6 Sort then brute force The solution we are looking for is the max value of the smallest house-heater distance. Think through what is the distance you want to keep, min or max C++ class Solution { public : int findRadius ( vector < int >& houses , vector < int >& heaters ) { int m = houses . size (); int n = heaters . size (); sort ( houses . begin (), houses . end ()); sort ( heaters . begin (), heaters . end ()); int res = INT_MIN ; int i , j = 0 ; for ( i = 0 ; i < m ; ++ i ) { while ( j < n - 1 && abs ( heaters [ j + 1 ] - houses [ i ]) <= abs ( heaters [ j ] - houses [ i ])) { j ++ ; } res = max ( res , abs ( houses [ i ] - heaters [ j ])); } return res ; } }; Python class Solution ( object ): def findRadius ( self , houses , heaters ): \"\"\" :type houses: List[int] :type heaters: List[int] :rtype: int \"\"\" m = len ( houses ) n = len ( heaters ) houses . sort () heaters . sort () i = 0 j = 0 res = 0 for i in range ( m ): while j < n - 1 and abs ( heaters [ j + 1 ] - houses [ i ]) <= abs ( heaters [ j ] - houses [ i ]): j += 1 res = max ( res , abs ( houses [ i ] - heaters [ j ])) return res Binary search the neighboring heaters get max of min Notice we cannot sort hourses and then search each heater's position. A special cases [1, 2, 3] 2 , the result is 0 whereis it should be 1 . C++ class Solution { public : int findRadius ( vector < int >& houses , vector < int >& heaters ) { int n = heaters . size (); sort ( heaters . begin (), heaters . end ()); int res = INT_MIN ; for ( int house : houses ) { int start = 0 , end = n ; while ( start < end ) { int mid = start + ( end - start ) / 2 ; if ( heaters [ mid ] < house ) start = mid + 1 ; else end = mid ; } int dist1 = ( start == n ) ? INT_MAX : heaters [ start ] - house ; int dist2 = ( start == 0 ) ? INT_MAX : house - heaters [ start - 1 ]; res = max ( res , min ( dist1 , dist2 )); } return res ; } }; Python class Solution ( object ): def findRadius ( self , houses , heaters ): \"\"\" :type houses: List[int] :type heaters: List[int] :rtype: int \"\"\" m = len ( houses ) n = len ( heaters ) heaters . sort () i = 0 j = 0 res = float ( '-inf' ) for i in range ( m ): start = 0 end = n while start != end : mid = start + ( end - start ) / 2 if heaters [ mid ] < houses [ i ]: start = mid + 1 else : end = mid dist1 = float ( 'inf' ) dist2 = float ( 'inf' ) if start != n : dist1 = heaters [ start ] - houses [ i ] if start != 0 : dist2 = houses [ i ] - heaters [ start - 1 ] res = max ( res , min ( dist1 , dist2 )) return res 74. Search a 2D Matrix \u00b6 Binary search We can view the matrix as a big sorted array and then binary search the target. Notice test your finished routine using edge cases. (i.e. the initial value of end) C++ class Solution { public : bool searchMatrix ( vector < vector < int >>& matrix , int target ) { int m = matrix . size (); int n = m ? matrix [ 0 ]. size () : 0 ; if ( m == 0 || n == 0 ) return false ; int start = 0 , end = m * n - 1 ; while ( start < end ) { int mid = start + ( end - start ) / 2 ; int i = mid / n , j = mid % n ; if ( matrix [ i ][ j ] < target ) { start = mid + 1 ; } else { end = mid ; } } return matrix [ start / n ][ start % n ] == target ? true : false ; } }; 240. Search a 2D Matrix II \u00b6 Binary search to exclude whole column or whole row the key is you decide where to start the compare. If you start from left bottom or right top, the solution should be abvious. Notice the idea is from binary search, if ordering info available, we want to exclude as many as impossible values as we can. class Solution { public : bool searchMatrix ( vector < vector < int >>& matrix , int target ) { int m = matrix . size (); int n = m ? matrix [ 0 ]. size () : 0 ; if ( m == 0 || n == 0 ) return false ; int x = m - 1 , y = 0 ; while ( x >= 0 && y < n ) { if ( matrix [ x ][ y ] == target ) { return true ; } if ( matrix [ x ][ y ] < target ) { y ++ ; } else { x -- ; } } return false ; } }; 302. Smallest Rectangle Enclosing Black Pixels \u00b6 C++ Brute Force class Solution { public : int minArea ( vector < vector < char >>& image , int x , int y ) { int m = image . size (); int n = m ? image [ 0 ]. size () : 0 ; int top = m , bottom = 0 , left = n , right = 0 ; for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { if ( image [ i ][ j ] == '1' ) { top = min ( top , i ); bottom = max ( bottom , i + 1 ); left = min ( left , j ); right = max ( right , j + 1 ); } } } return ( right - left ) * ( bottom - top ); } }; Binary search Notice the binary search idea is related to the problem Smallest Good Base and Wood Cut. The basic idea is seach each of furthest 1 from 4 directions. First make sure you can search one boundary and the others are similar. For example, to search the first row that contains 1 , we can look at the whole column/row to see whether this col/row have 1 . Because we are searching the first row that have 1 top down, bisec based on the count of 1 on each row we can know whether we ignore upper half or lower half. C++ Binary Search class Solution { public : int minArea ( vector < vector < char >>& image , int x , int y ) { int m = image . size (); int n = m ? image [ 0 ]. size () : 0 ; int top = bsearch_byrows ( image , 0 , x , 0 , n , true ); // search top int bottom = bsearch_byrows ( image , x + 1 , m , 0 , n , false ); int left = bsearch_bycols ( image , 0 , y , top , bottom , true ); int right = bsearch_bycols ( image , y + 1 , n , top , bottom , false ); return ( bottom - top ) * ( right - left ); } int bsearch_byrows ( vector < vector < char >>& image , int x , int y , int left , int right , bool white2black ) { while ( x < y ) { int m = ( x + y ) / 2 ; int k = left ; while ( k < right && image [ m ][ k ] == '0' ) k ++ ; if ( k < right == white2black ) { // mth row have '1' y = m ; } else { x = m + 1 ; } } return x ; } int bsearch_bycols ( vector < vector < char >>& image , int x , int y , int top , int bottom , bool white2black ) { while ( x < y ) { int m = ( x + y ) / 2 ; int k = top ; while ( k < bottom && image [ k ][ m ] == '0' ) k ++ ; if ( k < bottom == white2black ) { // mth column have '1' y = m ; } else { x = m + 1 ; } } return x ; } }; Python Binary Search class Solution ( object ): def minArea ( self , image , x , y ): \"\"\" :type image: List[List[str]] :type x: int :type y: int :rtype: int \"\"\" m = len ( image ) n = 0 if m != 0 : n = len ( image [ 0 ]) top = self . bsearch_row ( image , 0 , x , 0 , n , True ) bottom = self . bsearch_row ( image , x + 1 , m , 0 , n , False ) left = self . bsearch_col ( image , 0 , y , top , bottom , True ) right = self . bsearch_col ( image , y + 1 , n , top , bottom , False ) return ( bottom - top ) * ( right - left ) def bsearch_row ( self , image , start , end , lower , upper , white2black ): while start < end : m = ( start + end ) / 2 k = lower while k < upper and image [ m ][ k ] == '0' : k += 1 if ( k < upper ) == white2black : end = m else : start = m + 1 return start def bsearch_col ( self , image , start , end , lower , upper , white2black ): while start < end : m = ( start + end ) / 2 k = lower while k < upper and image [ k ][ m ] == '0' : k += 1 if ( k < upper ) == white2black : end = m else : start = m + 1 return start BFS class Solution { public : int minArea ( vector < vector < char >>& image , int x , int y ) { int m = image . size (); int n = m ? image [ 0 ]. size () : 0 ; int top = m , bottom = 0 , left = n , right = 0 ; int xx [ 4 ] = { - 1 , 0 , 1 , 0 }; int yy [ 4 ] = { 0 , 1 , 0 , - 1 }; queue < pair < int , int >> q ; q . push ({ x , y }); image [ x ][ y ] = '0' ; while ( ! q . empty ()) { pair < int , int > t = q . front (); q . pop (); top = min ( top , t . first ); bottom = max ( bottom , t . first + 1 ); left = min ( left , t . second ); right = max ( right , t . second + 1 ); for ( int k = 0 ; k < 4 ; ++ k ) { int a = t . first + xx [ k ]; int b = t . second + yy [ k ]; if ( a >= 0 && a < m && b >= 0 && b < n && image [ a ][ b ] == '1' ) { q . push ({ a , b }); image [ a ][ b ] = '0' ; } } } return ( right - left ) * ( bottom - top ); } }; BFS from collections import deque class Solution ( object ): def minArea ( self , image , x , y ): \"\"\" :type image: List[List[str]] :type x: int :type y: int :rtype: int \"\"\" m = len ( image ) n = 0 if m != 0 : n = len ( image [ 0 ]) xx = [ - 1 , 0 , 1 , 0 ] yy = [ 0 , - 1 , 0 , 1 ] top = m bottom = 0 left = n right = 0 q = deque () q . append ([ x , y ]) image [ x ][ y ] = '0' while len ( q ) > 0 : t = q . popleft () top = min ( top , t [ 0 ]) bottom = max ( bottom , t [ 0 ] + 1 ) left = min ( left , t [ 1 ]) right = max ( right , t [ 1 ] + 1 ) for k in range ( 4 ): a = t [ 0 ] + xx [ k ] b = t [ 1 ] + yy [ k ] if a >= 0 and a < m and b >= 0 and b < n and image [ a ][ b ] == '1' : q . append ([ a , b ]) image [ a ][ b ] = '0' return ( right - left ) * ( bottom - top ) DF class Solution { private : int m , n ; int top , bottom , left , right ; public : int minArea ( vector < vector < char >>& image , int x , int y ) { m = image . size (); n = m ? image [ 0 ]. size () : 0 ; top = m , bottom = 0 , left = n , right = 0 ; dfs_helper ( image , x , y ); return ( right - left ) * ( bottom - top ); } void dfs_helper ( vector < vector < char >>& image , int x , int y ) { if ( x < 0 || x >= m || y < 0 || y >= n || image [ x ][ y ] == '0' ) { return ; } image [ x ][ y ] = '0' ; top = min ( top , x ); bottom = max ( bottom , x + 1 ); left = min ( left , y ); right = max ( right , y + 1 ); dfs_helper ( image , x - 1 , y ); dfs_helper ( image , x , y + 1 ); dfs_helper ( image , x + 1 , y ); dfs_helper ( image , x , y - 1 ); } }; 363. Max Sum of Rectangle No Larger Than K \u00b6 Iterate the wide of the matrix and using prefix sum and set lower_bound . From the problem Max Sum of Subarry No Larger Than K, we have to enumerate the width of the sub-matrix and sum up all row elements and get an array of length m , m is the number of rows of the matrix. Then apply the method. C++ presum class Solution { public : int maxSumSubmatrix ( vector < vector < int >>& matrix , int k ) { if ( matrix . empty ()) return 0 ; int m = matrix . size (); int n = m ? matrix [ 0 ]. size () : 0 ; int res = INT_MIN ; for ( int l = 0 ; l < n ; ++ l ) { vector < int > sums ( m , 0 ); for ( int r = l ; r < n ; ++ r ) { for ( int i = 0 ; i < m ; ++ i ) { sums [ i ] += matrix [ i ][ r ]; } set < int > preSumSet ; preSumSet . insert ( 0 ); int preSum = 0 , curMax = INT_MIN ; for ( int sum : sums ) { preSum += sum ; set < int >:: iterator it = preSumSet . lower_bound ( preSum - k ); if ( it != preSumSet . end ()) { curMax = max ( curMax , preSum - * it ); } preSumSet . insert ( preSum ); } res = max ( res , curMax ); } } return res ; } }; merge sort The idea is similar that solution 1. Instead of calculate preSum on the fly, we finish calculation and pass it to a mergeSort routine. The use mergeSort here is to find the A[j] - A[i] <= k efficiently, O(nlogn) . C++ Merge Sort class Solution { public : int maxSumSubmatrix ( vector < vector < int >>& matrix , int k ) { int m = matrix . size (); int n = m ? matrix [ 0 ]. size () : 0 ; int res = INT_MIN ; vector < long long > sums ( m + 1 , 0 ); for ( int l = 0 ; l < n ; ++ l ) { vector < long long > sumInRow ( m , 0 ); for ( int r = l ; r < n ; ++ r ) { for ( int i = 0 ; i < m ; ++ i ) { sumInRow [ i ] += matrix [ i ][ r ]; sums [ i + 1 ] = sums [ i ] + sumInRow [ i ]; } res = max ( res , mergeSort ( sums , 0 , m + 1 , k )); if ( res == k ) return k ; } } return res ; } int mergeSort ( vector < long long >& sums , int start , int end , int k ) { if ( end == start + 1 ) return INT_MIN ; int mid = start + ( end - start ) / 2 ; int res = mergeSort ( sums , start , mid , k ); if ( res == k ) return k ; res = max ( res , mergeSort ( sums , mid , end , k )); if ( res == k ) return k ; long long cache [ end - start ]; int j = mid , c = 0 , t = mid ; for ( int i = start ; i < mid ; ++ i ) { while ( j < end && sums [ j ] - sums [ i ] <= k ) ++ j ; // search first time sums[j] - sums[i] > k if ( j - 1 >= mid ) { // sums[j - 1] - sums[i] <= k, make sure j - 1 is still in right side res = max ( res , ( int )( sums [ j - 1 ] - sums [ i ])); if ( res == k ) return k ; } while ( t < end && sums [ t ] < sums [ i ]) { cache [ c ++ ] = sums [ t ++ ]; } cache [ c ++ ] = sums [ i ]; } for ( int i = start ; i < t ; ++ i ) { sums [ i ] = cache [ i - start ]; } return res ; } }; Category 2 Using ordering abstration \u00b6 69. Sqrt(x) \u00b6 Solution 1 using ordering abstraction definition To find a square root of a integer x using binary search. We need to first determin the range [left, right] that the target value sqrt(x) may in. The potential range we can search is [0, x/2 + 1] . Then we should clarify this binary search is the \"find the first one\" type or the \"find the last one\" type. Basically, we want to determine our ordering abstraction f(target, g(i)) that is able to produce a boolean array. The boolean array have true part and false part seperated. Here target = sqrt(x) and g(i) = i . We define f(sqrt(x), i) = true when i <= sqrt(x) and f(sqrt(x), i) = false when i > sqrt(x) . This came from the following intuition: We are looking for the \"last\" integer whose square is less than x . Why not the otherwise? Because if you change to find the \"first\" integer whose square is greater than the x from right section of the boolean array, it is hard to define our ordering abstraction f . Of cause, we can search the \"first\" integer whose square is greater than x and find the previous integer next to it as the solution, but this later solution is a bit complex and counter intuitive. We prefer the first definition of ordering abstraction. Although a workable solution following the second ordering abstraction is also given below. For example: to solve the sqrt(8) and sqrt(9) using our definition: k, i = 0 1 2 3 4 5 6 7 8 9 10 n = 11 A = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] f(sqrt(8), k) = [T T T F F] f(sqrt(9), k) = [T T T T F] The binary search routine will be: C++ class Solution { public : int mySqrt ( int x ) { int l = 0 , r = x / 2 + 1 ; while ( l < r ) { // int m = l + (r - l) / 2; // will deadloop for 4, why? int m = r - ( r - l ) / 2 ; if ( m <= x / m ) { l = m ; } else { r = m - 1 ; } } return l ; } }; Solution 2 using the alternative ordering abstraction definition Second ordering abstraction (find first value whose square is greater than x) k, i = 0 1 2 3 4 5 6 7 8 9 10 n = 11 A = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] f(sqrt(8), k) = [F F F T T] f(sqrt(9), k) = [F F F F T] C++ class Solution { public : int mySqrt ( int x ) { if ( x == 0 ) return 0 ; // should handle, but will got division by zero in line 9. int l = 0 , r = x / 2 + 2 ; // r = x / 2 + 1 will not working for x = 1, have to have the one past last; while ( l < r ) { //int m = r - (r - l) / 2; // will dead loop for 4 int m = l + ( r - l ) / 2 ; if ( m > x / m ) { r = m ; } else { l = m + 1 ; } } return l - 1 ; } }; 367. Valid Perfect Square \u00b6 Solution 1 Binary search using ordering abstraction Notice you have to run tests for cases from 1 to 5. C++ class Solution { public : bool isPerfectSquare ( int num ) { if ( num == 1 ) return true ; int begin = 1 , end = num / 2 ; while ( begin < end ) { //long long mid = begin + (end - begin) / 2; // not working, deadloop for 5 long long mid = end - ( end - begin ) / 2 ; if ( mid * mid == num ) return true ; if ( mid * mid < num ) { begin = mid ; } else { end = mid - 1 ; } } return false ; } }; Python class Solution ( object ): def isPerfectSquare ( self , num ): \"\"\" :type num: int :rtype: bool \"\"\" if num == 1 : return True lower = 1 upper = num / 2 while lower < upper : mid = upper - ( upper - lower ) / 2 if mid * mid == num : return True if mid * mid < num : lower = mid else : upper = mid - 1 return False 633. Sum of Square Numbers \u00b6 Solution 1 Binary search Once you have derived value b from a and c , you can binary search b . C++ class Solution { public : bool judgeSquareSum ( int c ) { if ( c == 0 ) return true ; for ( long long a = 0 ; a * a <= c ; ++ a ) { int b = c - ( int ) ( a * a ); int l = 0 , r = b / 2 + 1 ; while ( l < r ) { long long m = r - ( r - l ) / 2 ; if ( m * m == b ) return true ; if ( m * m < b ) { l = m ; } else { r = m - 1 ; } } } return false ; } }; Solution 2 Two pointers Notice this square sum can be found efficiently using two pointers. C++ class Solution { public : bool judgeSquareSum ( int c ) { int a = 0 , b = sqrt ( c ); while ( a <= b ){ int sum = a * a + b * b ; if ( sum < c ) a ++ ; else if ( sum > c ) b -- ; else return true ; } return false ; } }; Solution 3 Using set Keep inserting the value into a set, in the meantime also look up the other C++ class Solution { public : bool judgeSquareSum ( int c ) { set < int > s ; for ( int i = 0 ; i <= sqrt ( c ); ++ i ) { s . insert ( c - i * i ); if ( s . count ( i * i )) return true ; } return false ; } }; 658. Find K Closest Elements \u00b6 Solution 1 Binary search Compare to problem 475. Heaters Our search target is to find the starting index of the subarray of length K. C++ class Solution { public : vector < int > findClosestElements ( vector < int >& arr , int k , int x ) { int start = 0 , end = arr . size () - k ; while ( start < end ) { int mid = start + ( end - start ) / 2 ; // looking for a \"mid\" that if ( x - arr [ mid ] > arr [ mid + k ] - x ) { start = mid + 1 ; } else { end = mid ; } } return vector < int > ( arr . begin () + start , arr . begin () + start + k ); } }; Solution 2 Binary search and Two pointers We first use binary search to locate the x value then expand to left and right looking for the k closest elements Notice the i < 0 in the if condition, it is very important to be there. otherwise the array index will be out of bound. C++ class Solution { public : vector < int > findClosestElements ( vector < int >& arr , int k , int x ) { int index = lower_bound ( arr . begin (), arr . end (), x ) - arr . begin (); int i = index - 1 , j = index ; while ( k -- ) { if ( i < 0 || j < arr . size () && abs ( arr [ j ] - x ) < abs ( arr [ i ] - x )) { j ++ ; } else { i -- ; } } return vector < int > ( arr . begin () + i + 1 , arr . begin () + j ); } }; 611. Valid Triangle Number \u00b6 The main idea comes from the triangle lateral property, in which the triple should fullfil: a + b > c , a + c > b , and b + c > a . Once we sort it. We are able to gain some advantages that we don't have to check all these 3 relations. Instead, we should only take care of A[i] + A[j] > A[k] , in which i < j < k . Because we sorted the array, we can also fix the i and j , using binary search to find the k in the ragne of A[j + 1] ~ A[n - 1] . We can use our classic binary search template to achieve the goal. C++ class Solution { public : int triangleNumber ( vector < int >& nums ) { int n = nums . size (); int res = 0 ; sort ( nums . begin (), nums . end ()); for ( int i = 0 ; i < n - 2 ; ++ i ) { for ( int j = i + 1 ; j < n - 1 ; ++ j ) { int l = j + 1 , r = n ; // range of all possible k, notice l start with j + 1 int t = nums [ i ] + nums [ j ]; while ( l < r ) { int m = l + ( r - l ) / 2 ; if ( nums [ m ] < t ) { l = m + 1 ; } else { r = m ; } } res += l - j - 1 ; // notice the count start from j + 1 to l - 1. } } return res ; } }; Category 3 Using ordering abstration (counter as a gueesing guage) \u00b6 287. Find the Duplicate Number \u00b6 Solution 1 Binary search The problem asking for better than O(n^2) we could check to see whether binary search will work. If you count how many value <= the mid elements of [1, ..., n-1] , it will give you enough information to discard part of the array elements. Here you should distinguish what will be halfed and what will be searched. The answer to that is the [1, ..., n-1] sequence, not the given array. The simple proof of why it works can be put in this the following way. if the count of elements that <=mid in the array is less than mid , we can learn that the duplicate is in the higher end. If the count is greater, we can know that the duplicate element is in the lower end of the sequence [1, ..., n-1] . C++ class Solution { public : int findDuplicate ( vector < int >& nums ) { int begin = 1 , end = nums . size () - 1 ; while ( begin < end ) { int mid = begin + ( end - begin ) / 2 ; int count = 0 ; for ( int a : nums ) { if ( a <= mid ) ++ count ; } if ( count <= mid ) // \"=\" for [1,2,2] begin = mid + 1 ; else end = mid ; } return begin ; } }; Solution 2 tortoise and hare algorithm This problem is very similar to the the find circle in linked list. Generally, if you repeate A[A[i]] , the out put will show some periodic patterns. In fact you can imagine a rho shaped sequence. Image there is a function f(i) = A[i] , it mapping from 1, 2, 3, ... n to 1, 2, 3, ... n . Try to traverse A[i] , you will finally get circle through some same sequence of elements again and again, thus you obtain a rho shaped sequency like a circle in a linked list. The reason of it being a rho shape is becuase at least one element you will not come back to it if you leave it. http://www.keithschwarz.com/interesting/code/find-duplicate/FindDuplicate.python.html C++ class Solution { public : int findDuplicate ( vector < int >& nums ) { int n = nums . size (); if ( n == 0 ) return 0 ; int slow = 0 , fast = 0 , find = 0 ; while ( slow != fast || ( slow == 0 && fast == 0 )) { slow = nums [ slow ]; fast = nums [ nums [ fast ]]; } while ( slow != find ) { slow = nums [ slow ]; find = nums [ find ]; } return find ; } }; 360. Sort Transformed Array \u00b6 410. Split Array Largest Sum \u00b6 Copy books (lintcode) \u00b6 183. Wood cut (lintcode) \u00b6 Solution 1 Binary search It requires to get equal or more than k pieces of wood with same length. So you have to cut the wood to fulfill the requirement. However, you need to promise that each of the k wood is the longest that is possible. Imagine that you are given bunch of wood to cut. How would you do it? You probably want to try to make one cut and see whether you can make it or not. If not, you may want to try make two cuts, and so on. But how could you program such a solution. It is very hard. Start thinking about the length seems a good option. Suppose you know your final maximum length. You would be able to make the cut accordingly. Now given a length out of guessing, can you verify whether it going to work or not? Yes, you can! That's the core idea of this solution. C++ class Solution { public : int woodCut ( vector < int > & L , int k ) { if ( L . empty ()) return 0 ; int maxlen = * max_element ( L . begin (), L . end ()); if ( k == 0 ) return maxlen ; int start = max ( 1 , maxlen / k ), end = maxlen ; while ( start < end ) { int mid = start + ( end - start ) / 2 ; int count = 0 ; for ( int len : L ) { count += len / ( mid + 1 ); } if ( count >= k ) start = mid + 1 ; else end = mid ; } int count = 0 ; for ( int len : L ) count += len / start ; return count >= k ? start : 0 ; } }; 774. Minimize Max Distance to Gas Station \u00b6 Solution 1 Binary search It is very similar to the problem Wood cut. You just need to take care of the accuracy of the results, namely also the int/double casts. It is also the hard part of the problem. Notice the count variable is int type, you should test your solution expecially for the line `count += dist[i] / mid; class Solution { public : double minmaxGasDist ( vector < int >& stations , int K ) { int n = stations . size (); vector < int > dist ( n , 0 ); // dist[0] = null; int d = 0 ; for ( int i = 1 ; i < n ; ++ i ) { dist [ i ] = stations [ i ] - stations [ i - 1 ]; d = max ( d , dist [ i ]); } double low = 0 , high = d ; while ( low + 0.000001 < high ) { double mid = low + ( high - low ) / 2 ; int count = 0 ; for ( int i = 1 ; i < n ; ++ i ) { count += dist [ i ] / mid ; } if ( count > K ) { // mid is too small low = mid ; } else { high = mid ; } } return low ; } }; 644. Maximum Average Subarray II \u00b6 Notice the great trick you used to test whether there is a subarray of length greater than k whose average is larger than current mid . The trick is calculate the diff[i] = nums[i] - mid , and then calculate the prefix sum of the diff array, and compare to another prefix sum of the same array diff , the two prefix sum are calculated at two position at least k distant apart. We actually compare the prefix sum to the smallest prefix sum k distant apart. C++ class Solution { public : double findMaxAverage ( vector < int >& nums , int k ) { int n = nums . size (); double upper = INT_MIN , lower = INT_MAX ; for ( auto num : nums ) { upper = max ( upper , ( double ) num ); lower = min ( lower , ( double ) num ); } while ( lower + 0.00001 < upper ) { double mid = lower + ( upper - lower ) / 2 ; if ( isLarger ( nums , mid , k )) { // is average value >= mid? lower = mid ; } else { upper = mid ; } } return lower ; } /* return true if a greater average value is possible */ bool isLarger ( vector < int >& nums , double mid , int k ) { int n = nums . size (); double sums = 0 , prev = 0 , prev_min = 0 ; for ( int i = 0 ; i < k ; i ++ ) { sums += nums [ i ] - mid ; } if ( sums >= 0 ) { return true ; } /* we keep looking for whether a subarray sum of length >= k in array \"sums\" * is possible to be greater than zero. If such a subarray exist, it means * that the target average value is greater than the \"mid\" value. * we look at the front part of sums that at least k element apart from i. * If we can find the minimum of the sums[0, 1, ..., i - k] and check if * sums[i] - min(sum[0, 1, ..., i - k]) >= 0. If this is the case, it indicate * there exist a subarray of length >= k with sum greater than 0 in sums, * we can return ture, otherwise, it return false. */ for ( int i = k ; i < n ; i ++ ) { sums += nums [ i ] - mid ; prev += nums [ i - k ] - mid ; prev_min = min ( prev_min , prev ); if ( sums >= prev_min ) return true ; } return false ; } }; 778. Swim in Rising Water \u00b6 In This problem we are trying to find a path, in which the maximum element in the path among all paths is minimum. Meaning we look for a target value in the grid, such that there exist a path from grid[0][0] to grid[n-1][n-1] which includes this value and it is the maximum value in the path. C++ class Solution { int x [ 4 ] = { 0 , - 1 , 0 , 1 }; int y [ 4 ] = { - 1 , 0 , 1 , 0 }; public : int swimInWater ( vector < vector < int >>& grid ) { int n = grid . size (); int begin = grid [ 0 ][ 0 ], end = n * n - 1 ; // binary search find a path with mini elevation while ( begin < end ) { int mid = begin + ( end - begin ) / 2 ; if ( pathExist ( grid , mid )) { end = mid ; } else { begin = mid + 1 ; } } return begin ; } bool pathExist ( vector < vector < int >> & grid , int mid ) { int n = grid . size (); vector < vector < int >> visited ( n , vector < int > ( n , 0 )); return dfs_helper ( grid , visited , n , mid , 0 , 0 ); } bool dfs_helper ( vector < vector < int >> & grid , vector < vector < int >>& visited , int n , int mid , int i , int j ) { visited [ i ][ j ] = 1 ; for ( int k = 0 ; k < 4 ; ++ k ) { int a = i + x [ k ]; int b = j + y [ k ]; if ( a < 0 || a >= n || b < 0 || b >= n || visited [ a ][ b ] == 1 || grid [ a ][ b ] > mid ) continue ; if ( a == n - 1 && b == n - 1 ) return true ; if ( dfs_helper ( grid , visited , n , mid , a , b )) return true ; } return false ; } }; 483 Smallest Good Base \u00b6 Solution 1 Binary search This problem requires a bit reasoning to achieve the solution. The starting point is realy mean what's asking by the problem. Here it is asking a minimum base that represent the given number n in a representation like binary representation. For example: 13 = 3^0 + 3^1 + 3^2 so 13 can be representd as 111 (base 3). First of all, there is a special case that such a base may not exist. (precisely, we should seperate the special case when n = (n-1)^0 + (n-1)^1 ; With this special case in mind, we can use binary search to iterate through each m from largest to smallest and check whether the corresponding k is a good base of the given value n . Because when m is the largest, k is the smallest, so if the bianry search find one it must be the smallest k we are looking for. If binary search found nothing, we simpley return the special case n-1 . C++ class Solution { public : string smallestGoodBase ( string n ) { long long num = stoll ( n ); /* for each lenght of the potentional representation, * n = 1 + k + ... + k^{i-1} = (k^i-1)/(k-1), lower bound k is 2, * we have 2^i-1 = n ==> upper bound i = log2(n+1). */ for ( int i = log2 ( num + 1 ); i >= 2 ; -- i ) { /* upper bound is obtained by n = 1 + k + k^2 ... + k^(i-1) > k^(i-1), * n > k^(i-1) ==> k < n^(1/(i-1)); */ long long left = 2 , right = pow ( num , 1.0 / ( i - 1 )) + 1 ; while ( left < right ) { long long mid = left + ( right - left ) / 2 ; long long sum = 0 ; /* calculate i digits value with base \"mid\" */ for ( int j = 0 ; j < i ; ++ j ) { sum = sum * mid + 1 ; } /* binary search for the mid (good base) */ if ( sum == num ) return to_string ( mid ); if ( sum < num ) left = mid + 1 ; else right = mid ; } } return to_string ( num - 1 ); } }; 658. Find K Closest Elements \u00b6 Solution 1 Binary Search C++ class Solution { public : vector < int > findClosestElements ( vector < int >& arr , int k , int x ) { int start = 0 , end = arr . size () - k ; while ( start < end ) { int mid = start + ( end - start ) / 2 ; if ( x - arr [ mid ] > arr [ mid + k ] - x ) { start = mid + 1 ; } else { end = mid ; } } return vector < int > ( arr . begin () + start , arr . begin () + start + k ); } }; 378. Kth Smallest Element in a Sorted Matrix \u00b6 Solution 1 Binary Search The idea of using binary search for this problem my not be straightforward. But the method is very important. The idea is very similar to the problem Search in a rotated sorted array. Because the matrix is sorted row wise and column wise, there are some ordering information we can make use of. Notice we are not try to search using the matrix index, we are searching the matrix element value. Compare to the problem 287. Find the Duplicate Number. The comparison if (count < k) isn't include mid explicitly. but the count is some function f(mid) , with the current mid , the count value is unique and can be use to test a condition that decide which side we can go to shrink the range the target value is possible in. C++ class Solution { public : int kthSmallest ( vector < vector < int >>& matrix , int k ) { int m = matrix . size (); int n = m ? matrix [ 0 ]. size () : 0 ; int start = matrix [ 0 ][ 0 ], end = matrix [ m - 1 ][ n - 1 ]; while ( start < end ) { int mid = start + ( end - start ) / 2 ; int count = 0 ; for ( int i = 0 ; i < m ; ++ i ) { count += upper_bound ( matrix [ i ]. begin (), matrix [ i ]. end (), mid ) - matrix [ i ]. begin (); } if ( count < k ) { // notice no mid here, but count is a function of mid. start = mid + 1 ; } else { end = mid ; } } return start ; } }; Solution 2 Priority Queue Notice when the k <= n^2 , index j < matrix.size() will also make it work. C++ class Solution { public : int kthSmallest ( vector < vector < int >>& matrix , int k ) { priority_queue < int > pq ; for ( int i = 0 ; i < matrix . size (); ++ i ) { for ( int j = 0 ; j < matrix [ 0 ]. size (); ++ j ) { pq . push ( matrix [ i ][ j ]); if ( pq . size () > k ) pq . pop (); } } return pq . top (); } }; 668. Kth Smallest Number in Multiplication Table \u00b6 Solution 1 Binary search While this problem looks simple. But it really isn't unless you observed the following. The condition used for binary search is \"whether there are k smaller elements in the range [start, mid]\". You are looking for the smallest number that has k elements less than or equal to it. Like in the problem Kth Smallest Element in a Sorted Matrix, we will move the number not the index. We move the start or end appropriately based on this condition, if there are more than k, we shrink the range by reduce end: end = mid . If there are less than k numbers, we increase begin hopefully to make mid larger so as to have close to k numbers in the range of [1, mid] . When being == end , we've located the kth number desired. In case k > m*n , we will got begin == end < k , which is not a solution. In counting how many element less than mid, you have to be clever a bit by using the feature that this matrix is multiplicative table. That is for row i , you can at most have x/i number smaller than x , why? Follow up: Does the kth element will be in the range of [1, m*n] ? C++ class Solution { public : int findKthNumber ( int m , int n , int k ) { int begin = 1 , end = m * n ; while ( begin < end ) { int mid = begin + ( end - begin ) / 2 ; int count = 0 ; for ( int i = 1 ; i <= m ; ++ i ) { count += min ( mid / i , n ); } if ( count < k ) begin = mid + 1 ; else end = mid ; } return begin ; } }; 719. Find K-th Smallest Pair Distance \u00b6 Solution 1 Priority Queue TLE C++ class Solution { public : int smallestDistancePair ( vector < int >& nums , int k ) { priority_queue < int > pq ; for ( int i = 0 ; i < nums . size (); ++ i ) { for ( int j = i + 1 ; j < nums . size (); ++ j ) { int dist = abs ( nums [ i ] - nums [ j ]); if ( pq . size () < k ) { pq . push ( dist ); } else if ( dist < pq . top ()) { pq . push ( dist ), pq . pop (); } } } return pq . top (); } }; Solution 2 Binary search Similar to Problem 668. Kth Smallest Number in Multiplication Table. The problem is complicated at firt glass. A brute force solutoin generates all the absolute distances and then sort to find the kth smallest one. We found it is potentially a searchable senario if we sort the elements. We have range [min_distance, max_distance] . We search a distance in this range such that there are exactly k pairs distance including itself. If the count of pair distance less than k, we try to increase it buy start = mid + 1 , vice versa. When the binary search loop stops, if the result exist, start point to the distance we are searching. Since this problem guarrantee solution exist, we return start . C++ class Solution { public : int smallestDistancePair ( vector < int >& nums , int k ) { sort ( nums . begin (), nums . end ()); int start = nums [ 1 ] - nums [ 0 ]; for ( int i = 2 ; i < nums . size (); ++ i ) { start = min ( start , nums [ i ] - nums [ i - 1 ]); } int end = nums . back () - nums [ 0 ]; while ( start < end ) { int mid = start + ( end - start ) / 2 ; // count how many absolute differences that <= mid; int count = 0 ; for ( int i = 0 ; i < nums . size (); ++ i ) { int j = i ; while ( j < nums . size () && nums [ j ] - nums [ i ] <= mid ) j ++ ; count += j - i - 1 ; } if ( count < k ) { start = mid + 1 ; } else { end = mid ; } } return start ; } }; Solution 3 Using binary search to optimize the counting You can also write your own binary search routine upper_bound . class Solution { public : int smallestDistancePair ( vector < int >& nums , int k ) { sort ( nums . begin (), nums . end ()); int start = nums [ 1 ] - nums [ 0 ]; for ( int i = 2 ; i < nums . size (); ++ i ) { start = min ( start , nums [ i ] - nums [ i - 1 ]); } int end = nums . back () - nums [ 0 ]; while ( start < end ) { int mid = start + ( end - start ) / 2 ; // count how many absolute differences that <= mid; int count = 0 ; /* for (int i = 0; i < nums.size(); ++i) { int j = i; while (j < nums.size() && nums[j] - nums[i] <= mid) j++; count += j - i - 1; } */ // optimize the counting use binary search (nested binary search) for ( int i = 0 ; i < nums . size (); ++ i ) { auto iter = upper_bound ( nums . begin () + i , nums . end (), nums [ i ] + mid ); count += iter - ( nums . begin () + i ) - 1 ; } if ( count < k ) { start = mid + 1 ; } else { end = mid ; } } return start ; } }; Category 4 Binary search as an optimization routine \u00b6 300 Longest Increasing Subsequence \u00b6 Solution 1 DP The base case is single char. f[i] is the length of LIS from the begining. C++ class Solution { public : int lengthOfLIS ( vector < int >& nums ) { if ( n == nums . size ()) return 0 ; int f [ n ] = { 0 }; int res = 0 ; for ( int j = 0 ; j < n ; j ++ ) { f [ j ] = 1 ; for ( int i = 0 ; i < j ; i ++ ) { if ( nums [ i ] < nums [ j ] && f [ i ] + 1 > f [ j ]) f [ j ] = f [ i ] + 1 ; } res = max ( res , f [ j ]); } return res ; } }; Solution 2 Using binary search The DP solution is O(n^2) . Using binary search could reduce to O(nlogn) . Binary search solution analysis. For each i , we are looking for the largest f value that has smallest A value. For example, A[0] = 5 could be ignored because of its f value is same as A[1] = 1 , which is smaller. In searching for the LIS, we prefer a small ending value when the length is the same. The following solution using a vector b to record the minimum A value for each length of LIS ( f value), we use binary search to find the last value in b that smaller that current value A[i] . If we found such a value in b , we use A[i] to replace the value next to the found value in b ). i 0 1 2 3 4 5 6 7 A 5 1 3 7 6 4 2 10 f 1 1 2 3 3 3 2 4 f[1] = 1, a[1] = 1 f[6] = 2, a[6] = 2 f[5] = 3, a[5] = 4 f[7] = 4, a[7] = 10 C++ class Solution { public : int lengthOfLIS ( vector < int >& nums ) { vector < int > b ; for ( int i = 0 ; i < nums . size (); ++ i ) { int l = 0 , r = b . size (); while ( l < r ) { int m = l + ( r - l ) / 2 ; if ( b [ m ] < nums [ i ]) { // nums[i] is the target l = m + 1 ; } else { r = m ; } } if ( l == b . size ()) // nums[i] greater than all element in b b . push_back ( nums [ i ]); else // begin point to next element no less than the target nums[i]. b [ l ] = nums [ i ]; } return b . size (); } }; Alternatively, we could use lower_bound . C++ class Solution { public : int lengthOfLIS ( vector < int >& nums ) { vector < int > b ; for ( int i = 0 ; i < nums . size (); ++ i ) { int l = lower_bound ( b . begin (), b . end (), nums [ i ]) - b . begin (); if ( l == b . size ()) // nums[i] greater than all element in b b . push_back ( nums [ i ]); else // begin point to next element no less than the target nums[i]. b [ l ] = nums [ i ]; } return b . size (); } }; 354. Russian Doll Envelopes \u00b6 174. Dungeon Game \u00b6","title":"Binary Search"},{"location":"leetcode/binary-search/notes/#binary-search","text":"","title":"Binary Search"},{"location":"leetcode/binary-search/notes/#binary-search-problem-characteristics","text":"Ordered binary search. You need to find index or array element where ordering info is available, either explicitly (sorted array) or implicitly (partially sorted or other special info). monotony pattern. If the ordering info isn't available, but you can exclude \"all\" the possible cases from left or right by a condition camparing f(mid) to the target .","title":"Binary search problem characteristics"},{"location":"leetcode/binary-search/notes/#binary-search-problem-solving-techniques","text":"Clarify that you are trying to find the first one or find the last one. Clarify that you are trying to move the index or move the value (i.e. kth smallest number in multiplicative table). Use an \"ordering abstraction\" vless(target, f(mid)) . This ordering abstraction will produce a boolean array that indicate the ordering information between the target value and the f(mid) . Decide whether the left or right part the f(mid) should fall into. The principle to determine the predicate is simple: don't rule out the possible result (maintain the loop invariant). Shrink the range accordingly based on the predicate decided in step 3. Test the case that the search range is small, such as only have one or two elements.","title":"Binary search problem solving techniques"},{"location":"leetcode/binary-search/notes/#binary-search-practical-use-case","text":"Find whether the given target is in the array. Find the the position of the first value equal to the given target. Find the insertion position of the given target in the array. Find the the position of the last value equal to the given target. Find the total number of x in a sorted array. Find the last element less than the target. Find the first element greater than the target.","title":"Binary search practical use case"},{"location":"leetcode/binary-search/notes/#binary-search-in-c-stl","text":"lower_bound : return iterator point to the element no less than the target. upper_bound : return iterator point to the element greater than the target. equal_range : return a pair of iterators, first of which is lower_bound , second is upper_bound . binary_search : return true if an element equivalent to val is found, and false otherwise.","title":"Binary search in C++ STL"},{"location":"leetcode/binary-search/notes/#caveat-of-binary-search-implementation","text":"Specify the range: [start, end) or [start, end] ? C++ STL used [start, end) to denote a range, which bring in many conveniences. We will stick on this convention. Which while loop condition? start < end ? start <= end ? start != end ? start + 1 < end ? The calculation of the mid . mid = start + (end - start) / 2 or mid = (start + end) / 2 ? To proof mid is always in the range [begin, end) . The \"bisection\": start = mid + 1 , start = mid , or end = mid - 1 or end = mid ? Where is the result? start ? end ? How to make sure?","title":"Caveat of binary search implementation"},{"location":"leetcode/binary-search/notes/#a-universial-binary-search-implementation","text":"Despite the above caveas, just remember that there are two version of binary search one can write based on the range [begin, end) and [begin, end] . Iterator type in C++ using the former, it have many benefits in reduce the code complexity. Among all the binary search implementation you have seen, the following one is the most powerful version and it equivalent to C++ STL lower_bound algorithm. /** * return index to an element no less than x. Be more specifically, if there is * an element in the given array equal to x, it returns the index of first such * element; if there is no element that is equal to x, it returns the index * where x can be inserted into the position without changing the ordering of * the elements. * * All possible return value for calling this function with array.size() == n is * [0, 1, ..., n - 1, n] * */ size_t binary_search ( int x , vector < int >& array , size_t n ) { size_t begin = 0 , end = n ; while ( begin != end ) { size_t mid = begin + ( end - begin ) / 2 ; if ( array [ mid ] < x ) { begin = mid + 1 ; } else { end = mid ; } } return begin ; }","title":"A \"universial\" binary search implementation"},{"location":"leetcode/binary-search/notes/#important-observations-about-this-implementation","text":"mid cannot less than begin , they can be equal. This will ensure begin = mid + 1 in the if statement at least reduce the size of [begin, end] by 1. Informal proof: if (array[mid] < x) , it indicate x can only possible be in array[mid + 1, mid + 2, ... n - 1]. mid + 1 is at least 1 greater than begin. mid and end never equal inside the while loop, mid < end is always hold. This will ensure end = mid in the else statement at least reduce the size of [begin, end] by 1. Informal proof: we have begain < end , so begin + end < 2 * end , thus (begin + end) / 2 < end , because integer divisioin truncate down, mid = (begin + end) / 2 always less than end. begin and end never cross. Informal proof: Inside the while loop, at the begining, we have begin < end . If the current iteration executes the if condition, begin = mid + 1 at most advance begin to end but not exceed end . If it execute the else condition, end = mid would at worst case change end point to the minimum value of mid , because we have begin <= mid . Thus, we can conclude that executing the statement end = mid will not change end less than begin , at worst equal to begin .","title":"Important observations about this implementation"},{"location":"leetcode/binary-search/notes/#claims-regarding-this-binary-search-routine","text":"The range [begin, end) is used, which comply to the convention used in C++ iterator. It is impossible that mid == end . If they are equal, array[n] is invalid memory access. We use the loop condition while (begin != end) to indicate that once the loop terminates, we have begin == end . By checking whether begin is a valid index to the array or not, we can know whether x is greater than all the elements in the array or not. If we want to check whether x is found in the array, we simply check array[begin] == x . However, this condition is based on the assumption that begin < end initially. Considering that, using while (begin < end) is better if you cannot ensure begin < end before the loop. Setting begin = mid + 1 reduces the size of the remaining interested sub-array and maintains the invariant, which is if x in the array, x is in [begin, end) . Setting end = mid reduces the size of the remaining interested sub-array (mid never equal to end) and maintains the invariant, which is if x in the array, x is in [begin, end) . This claim is a little hard to absorb. On way to understand is like the following: ~~Because we need keep searching x in the range [begin, mid] if we get in the else statement. In the else case there are two possibilities: 1) array[mid] > x . 2) array[mid] = x . For 1) it indicates x is in [begin, mid) , setting end = mid maintains the loop invariant correctly, which is that x is in the shrinked range. For the 2) it is a little complex. If array[mid] is the only element equal to x, setting end = mid appears violate the loop invariant by exclude x from the range [begin, end) . however, remember array[mid] is the only element equal to x, after the while loop, begin = end , we have the x found by begin even though theoretically [begin, end) is already an empty range since begin = end and array[begin] = array[end] = x . If there are more values are equal to x before and after the element array[mid] the loop will always end up finding the first x value in the array. If we use end = mid + 1 . Try test case [1, 3, 5, 7] , with x = 0 . deadloop will accur. i.e. begin = 0, mid = 1, end = 2 .","title":"Claims regarding this binary search routine"},{"location":"leetcode/binary-search/notes/#category-1-binary-search-basics-and-binary-search-on-special-array-ie-rotated-sorted","text":"To solve this type of binary search problem. You should focus on the following: Come up test cases to verify your solution. Be able to find which side to drop for each iteration. Be extremly careful \"off by 1\" bugs. (1. reasoning: is mid value possible to be the solution or not. 2. exercise test cases: especially short ones)","title":"Category 1 Binary search basics and binary search on special array (i.e. rotated sorted)"},{"location":"leetcode/binary-search/notes/#34-search-for-a-range","text":"C++ Use STL function class Solution { public : vector < int > searchRange ( vector < int >& nums , int target ) { vector < int > res ( 2 , - 1 ); int low = lower_bound ( nums . begin (), nums . end (), target ) - nums . begin (); int high = upper_bound ( nums . begin (), nums . end (), target ) - nums . begin (); if ( low == high ) return res ; return { low , hight - 1 }; } }; C++ Implementation of binary search class Solution { public : vector < int > searchRange ( vector < int >& nums , int target ) { vector < int > res ( 2 , - 1 ); int low = lower_bound ( nums , target ); //int high = lower_bound(nums, target + 1); // also works. int high = upper_bound ( nums , target ); if ( low == high ) { return res ; } return { low , high - 1 }; } int lower_bound ( vector < int >& nums , int target ) { if ( nums . size () == 0 ) return 0 ; int l = 0 , r = nums . size (); while ( l < r ) { int m = l + ( r - l ) / 2 ; if ( nums [ m ] < target ) { l = m + 1 ; } else { r = m ; } } return l ; } int upper_bound ( vector < int >& nums , int target ) { if ( nums . size () == 0 ) return 0 ; int l = 0 , r = nums . size (); while ( l < r ) { int m = l + ( r - l ) / 2 ; if ( nums [ m ] <= target ) { l = m + 1 ; } else { r = m ; } } return l ; } }; Python solution class Solution ( object ): def searchRange ( self , nums , target ): \"\"\" :type nums: List[int] :type target: int :rtype: List[int] \"\"\" if len ( nums ) == 0 : return [ - 1 , - 1 ] begin = 0 end = len ( nums ) while begin != end : mid = begin + ( end - begin ) / 2 if nums [ mid ] < target : begin = mid + 1 else : end = mid if begin == len ( nums ): return [ - 1 , - 1 ] if nums [ begin ] == target : lower = begin else : lower = - 1 begin = 0 end = len ( nums ) while begin != end : mid = begin + ( end - begin ) / 2 if nums [ mid ] <= target : begin = mid + 1 else : end = mid if nums [ begin - 1 ] == target : upper = begin - 1 else : upper = - 1 return [ lower , upper ]","title":"34. Search for a Range"},{"location":"leetcode/binary-search/notes/#35-search-insert-position","text":"C++ solution lower_bound class Solution { public : int searchInsert ( vector < int >& nums , int target ) { if ( nums . size () == 0 ) return 0 ; int l = 0 , r = nums . size (); while ( l < r ) { int m = l + ( r - l ) / 2 ; if ( nums [ m ] < target ) { l = m + 1 ; } else { r = m ; } } return l ; } };","title":"35. Search Insert Position"},{"location":"leetcode/binary-search/notes/#33-search-in-rotated-sorted-array","text":"How to locate the sorted half? If left half is sorted, check where the target t is like be. else if right half is sorted, check where the target t is like to be. else if mid element is equal to left or right. Remove one of them. Although no duplicate, should consider short input like [3 1], 1 will have the equal case. C++ /** t = 1 t = 3 t = 5 t = 4 t = -1 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 3 4 5 1 1 3 5 4 1 <--need check */ class Solution { public : int search ( vector < int >& A , int t ) { if ( A . empty ()) return - 1 ; int l = 0 , r = A . size () - 1 ; while ( l < r ) { int m = l + ( r - l ) / 2 ; if ( A [ m ] == t ) return m ; if ( A [ l ] < A [ m ]) { // left is sorted if ( A [ l ] <= t && t < A [ m ]) { r = m - 1 ; } else { l = m + 1 ; } } else if ( A [ m ] < A [ r ]) { // right is sorted if ( A [ m ] < t && t <= A [ r ]) { l = m + 1 ; } else { r = m - 1 ; } } else { // if equal, remove one. case: [3, 1], 1 if ( A [ l ] == A [ m ]) l ++ ; if ( A [ m ] == A [ r ]) r -- ; } } return A [ l ] == t ? l : - 1 ; } }; Python class Solution ( object ): def search ( self , nums , target ): \"\"\" :type nums: List[int] :type target: int :rtype: int \"\"\" if len ( nums ) == 0 : return - 1 left = 0 right = len ( nums ) - 1 while left < right : mid = left + ( right - left ) / 2 if nums [ mid ] == target : return mid ; if nums [ left ] < nums [ mid ]: if nums [ left ] <= target and target < nums [ mid ]: right = mid - 1 else : left = mid + 1 elif nums [ mid ] < nums [ right ]: if nums [ mid ] < target and target <= nums [ right ]: left = mid + 1 else : right = mid - 1 else : if nums [ left ] == nums [ mid ]: left += 1 if nums [ right ] == nums [ mid ]: right -= 1 if nums [ left ] == target : return left return - 1","title":"33. Search in Rotated Sorted Array"},{"location":"leetcode/binary-search/notes/#81-search-in-rotated-sorted-array-ii","text":"How to locate the sorted half? C++ class Solution { public : bool search ( vector < int >& A , int t ) { if ( A . empty ()) return false ; int l = 0 , r = A . size () - 1 ; while ( l < r ) { int m = l + ( r - l ) / 2 ; if ( A [ m ] == t ) return true ; if ( A [ l ] < A [ m ]) { if ( A [ l ] <= t && t < A [ m ]) { r = m - 1 ; } else { l = m + 1 ; } } else if ( A [ m ] < A [ r ]) { if ( A [ m ] < t && t <= A [ r ]) { l = m + 1 ; } else { r = m - 1 ; } } else { if ( A [ l ] == A [ m ]) l ++ ; if ( A [ m ] == A [ r ]) r -- ; } } return A [ l ] == t ? true : false ; } }; Python class Solution ( object ): def search ( self , nums , target ): \"\"\" :type nums: List[int] :type target: int :rtype: int \"\"\" if len ( nums ) == 0 : return False left = 0 right = len ( nums ) - 1 while left < right : mid = left + ( right - left ) / 2 if nums [ mid ] == target : return True if nums [ left ] < nums [ mid ]: if nums [ left ] <= target and target < nums [ mid ]: right = mid - 1 else : left = mid + 1 elif nums [ mid ] < nums [ right ]: if nums [ mid ] < target and target <= nums [ right ]: left = mid + 1 else : right = mid - 1 else : if nums [ left ] == nums [ mid ]: left += 1 if nums [ right ] == nums [ mid ]: right -= 1 if nums [ left ] == target : return True return False","title":"81. Search in Rotated Sorted Array II"},{"location":"leetcode/binary-search/notes/#153-find-minimum-in-rotated-sorted-array","text":"Try to locate the valley which contains the min. Notice when A[0] < A[n - 1] , return A[0] . Draw a monotonic curve and then split the curve into two half, swith the order. This can help you to write the code. C++ class Solution { public : int findMin ( vector < int >& A ) { int l = 0 , r = A . size () - 1 ; while ( l < r ) { if ( A [ l ] < A [ r ]) // serve as base case. return A [ l ]; int m = l + ( r - l ) / 2 ; if ( A [ m ] > A [ r ]) { // also works. looking for not sorted half l = m + 1 ; } else if ( A [ m ] < A [ r ]) { // don't really need if statement r = m ; } } return A [ l ]; } }; Python class Solution ( object ): def findMin ( self , nums ): \"\"\" :type nums: List[int] :rtype: int \"\"\" if len ( nums ) == 0 : return - 1 left = 0 right = len ( nums ) - 1 while left < right : if nums [ left ] == nums [ right ]: return nums [ left ] mid = left + ( right - left ) / 2 if nums [ mid ] > nums [ right ]: left = mid + 1 else : right = mid return nums [ left ]","title":"153. Find Minimum in Rotated Sorted Array"},{"location":"leetcode/binary-search/notes/#154-find-minimum-in-rotated-sorted-array-ii","text":"Locate the valley which contains the min. Since duplicates exist. we cannot use the observation A[l] == A[r] . Here we deal with duplicates using decrease by one step. C++ class Solution { public : int findMin ( vector < int >& A ) { int l = 0 , r = A . size () - 1 ; while ( l < r ) { int m = l + ( r - l ) / 2 ; if ( A [ m ] > A [ r ]) { l = m + 1 ; } else if ( A [ m ] < A [ r ]) { r = m ; } else { r -- ; } } return A [ l ]; } }; Python class Solution ( object ): def findMin ( self , nums ): \"\"\" :type nums: List[int] :rtype: int \"\"\" if len ( nums ) == 0 : return - 1 left = 0 right = len ( nums ) - 1 while left < right : mid = left + ( right - left ) / 2 if nums [ mid ] > nums [ right ]: left = mid + 1 elif nums [ mid ] < nums [ right ]: right = mid else : right -= 1 return nums [ left ]","title":"154. Find Minimum in Rotated Sorted Array II"},{"location":"leetcode/binary-search/notes/#162-find-peak-element","text":"Use Binary search Use the neighboring relation to determin which side a peak value may occur then eliminate the other side. C++ class Solution { public : int findPeakElement ( vector < int >& A ) { int l = 0 , r = A . size () - 1 ; while ( l < r ) { int m = l + ( r - l ) / 2 ; if ( A [ m ] < A [ m + 1 ]) { l = m + 1 ; } else if ( A [ m ] > A [ m + 1 ]) { r = m ; } } return l ; } };","title":"162 Find Peak Element"},{"location":"leetcode/binary-search/notes/#278-first-bad-version","text":"Binary search Notice how this can be related to the ordering abstraction. // Forward declaration of isBadVersion API. bool isBadVersion ( int version ); class Solution { public : int firstBadVersion ( int n ) { int l = 0 , r = n ; while ( l < r ) { int m = l + ( r - l ) / 2 ; if ( ! isBadVersion ( m )) { l = m + 1 ; } else { r = m ; } } return l ; } };","title":"278. First Bad Version"},{"location":"leetcode/binary-search/notes/#374-guess-number-higher-or-lower","text":"C++ binary search // Forward declaration of guess API. // @param num, your guess // @return -1 if my number is lower, 1 if my number is higher, otherwise return 0 int guess ( int num ); class Solution { public : int guessNumber ( int n ) { int start = 1 , end = n ; while ( start < end ) { int mid = start + ( end - start ) / 2 ; if ( guess ( mid ) == 0 ) return mid ; if ( guess ( mid ) == 1 ) { start = mid + 1 ; } else { end = mid ; } } return start ; } }; Python # The guess API is already defined for you. # @param num, your guess # @return -1 if my number is lower, 1 if my number is higher, otherwise return 0 # def guess(num): class Solution ( object ): def guessNumber ( self , n ): \"\"\" :type n: int :rtype: int \"\"\" begin = 0 end = n while begin != end : mid = begin + ( end - begin ) / 2 if guess ( mid ) == 0 : return mid if guess ( mid ) == 1 : begin = mid + 1 else : end = mid return begin","title":"374. Guess Number Higher or Lower"},{"location":"leetcode/binary-search/notes/#475-heaters","text":"Sort then brute force The solution we are looking for is the max value of the smallest house-heater distance. Think through what is the distance you want to keep, min or max C++ class Solution { public : int findRadius ( vector < int >& houses , vector < int >& heaters ) { int m = houses . size (); int n = heaters . size (); sort ( houses . begin (), houses . end ()); sort ( heaters . begin (), heaters . end ()); int res = INT_MIN ; int i , j = 0 ; for ( i = 0 ; i < m ; ++ i ) { while ( j < n - 1 && abs ( heaters [ j + 1 ] - houses [ i ]) <= abs ( heaters [ j ] - houses [ i ])) { j ++ ; } res = max ( res , abs ( houses [ i ] - heaters [ j ])); } return res ; } }; Python class Solution ( object ): def findRadius ( self , houses , heaters ): \"\"\" :type houses: List[int] :type heaters: List[int] :rtype: int \"\"\" m = len ( houses ) n = len ( heaters ) houses . sort () heaters . sort () i = 0 j = 0 res = 0 for i in range ( m ): while j < n - 1 and abs ( heaters [ j + 1 ] - houses [ i ]) <= abs ( heaters [ j ] - houses [ i ]): j += 1 res = max ( res , abs ( houses [ i ] - heaters [ j ])) return res Binary search the neighboring heaters get max of min Notice we cannot sort hourses and then search each heater's position. A special cases [1, 2, 3] 2 , the result is 0 whereis it should be 1 . C++ class Solution { public : int findRadius ( vector < int >& houses , vector < int >& heaters ) { int n = heaters . size (); sort ( heaters . begin (), heaters . end ()); int res = INT_MIN ; for ( int house : houses ) { int start = 0 , end = n ; while ( start < end ) { int mid = start + ( end - start ) / 2 ; if ( heaters [ mid ] < house ) start = mid + 1 ; else end = mid ; } int dist1 = ( start == n ) ? INT_MAX : heaters [ start ] - house ; int dist2 = ( start == 0 ) ? INT_MAX : house - heaters [ start - 1 ]; res = max ( res , min ( dist1 , dist2 )); } return res ; } }; Python class Solution ( object ): def findRadius ( self , houses , heaters ): \"\"\" :type houses: List[int] :type heaters: List[int] :rtype: int \"\"\" m = len ( houses ) n = len ( heaters ) heaters . sort () i = 0 j = 0 res = float ( '-inf' ) for i in range ( m ): start = 0 end = n while start != end : mid = start + ( end - start ) / 2 if heaters [ mid ] < houses [ i ]: start = mid + 1 else : end = mid dist1 = float ( 'inf' ) dist2 = float ( 'inf' ) if start != n : dist1 = heaters [ start ] - houses [ i ] if start != 0 : dist2 = houses [ i ] - heaters [ start - 1 ] res = max ( res , min ( dist1 , dist2 )) return res","title":"475. Heaters"},{"location":"leetcode/binary-search/notes/#74-search-a-2d-matrix","text":"Binary search We can view the matrix as a big sorted array and then binary search the target. Notice test your finished routine using edge cases. (i.e. the initial value of end) C++ class Solution { public : bool searchMatrix ( vector < vector < int >>& matrix , int target ) { int m = matrix . size (); int n = m ? matrix [ 0 ]. size () : 0 ; if ( m == 0 || n == 0 ) return false ; int start = 0 , end = m * n - 1 ; while ( start < end ) { int mid = start + ( end - start ) / 2 ; int i = mid / n , j = mid % n ; if ( matrix [ i ][ j ] < target ) { start = mid + 1 ; } else { end = mid ; } } return matrix [ start / n ][ start % n ] == target ? true : false ; } };","title":"74. Search a 2D Matrix"},{"location":"leetcode/binary-search/notes/#240-search-a-2d-matrix-ii","text":"Binary search to exclude whole column or whole row the key is you decide where to start the compare. If you start from left bottom or right top, the solution should be abvious. Notice the idea is from binary search, if ordering info available, we want to exclude as many as impossible values as we can. class Solution { public : bool searchMatrix ( vector < vector < int >>& matrix , int target ) { int m = matrix . size (); int n = m ? matrix [ 0 ]. size () : 0 ; if ( m == 0 || n == 0 ) return false ; int x = m - 1 , y = 0 ; while ( x >= 0 && y < n ) { if ( matrix [ x ][ y ] == target ) { return true ; } if ( matrix [ x ][ y ] < target ) { y ++ ; } else { x -- ; } } return false ; } };","title":"240. Search a 2D Matrix II"},{"location":"leetcode/binary-search/notes/#302-smallest-rectangle-enclosing-black-pixels","text":"C++ Brute Force class Solution { public : int minArea ( vector < vector < char >>& image , int x , int y ) { int m = image . size (); int n = m ? image [ 0 ]. size () : 0 ; int top = m , bottom = 0 , left = n , right = 0 ; for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { if ( image [ i ][ j ] == '1' ) { top = min ( top , i ); bottom = max ( bottom , i + 1 ); left = min ( left , j ); right = max ( right , j + 1 ); } } } return ( right - left ) * ( bottom - top ); } }; Binary search Notice the binary search idea is related to the problem Smallest Good Base and Wood Cut. The basic idea is seach each of furthest 1 from 4 directions. First make sure you can search one boundary and the others are similar. For example, to search the first row that contains 1 , we can look at the whole column/row to see whether this col/row have 1 . Because we are searching the first row that have 1 top down, bisec based on the count of 1 on each row we can know whether we ignore upper half or lower half. C++ Binary Search class Solution { public : int minArea ( vector < vector < char >>& image , int x , int y ) { int m = image . size (); int n = m ? image [ 0 ]. size () : 0 ; int top = bsearch_byrows ( image , 0 , x , 0 , n , true ); // search top int bottom = bsearch_byrows ( image , x + 1 , m , 0 , n , false ); int left = bsearch_bycols ( image , 0 , y , top , bottom , true ); int right = bsearch_bycols ( image , y + 1 , n , top , bottom , false ); return ( bottom - top ) * ( right - left ); } int bsearch_byrows ( vector < vector < char >>& image , int x , int y , int left , int right , bool white2black ) { while ( x < y ) { int m = ( x + y ) / 2 ; int k = left ; while ( k < right && image [ m ][ k ] == '0' ) k ++ ; if ( k < right == white2black ) { // mth row have '1' y = m ; } else { x = m + 1 ; } } return x ; } int bsearch_bycols ( vector < vector < char >>& image , int x , int y , int top , int bottom , bool white2black ) { while ( x < y ) { int m = ( x + y ) / 2 ; int k = top ; while ( k < bottom && image [ k ][ m ] == '0' ) k ++ ; if ( k < bottom == white2black ) { // mth column have '1' y = m ; } else { x = m + 1 ; } } return x ; } }; Python Binary Search class Solution ( object ): def minArea ( self , image , x , y ): \"\"\" :type image: List[List[str]] :type x: int :type y: int :rtype: int \"\"\" m = len ( image ) n = 0 if m != 0 : n = len ( image [ 0 ]) top = self . bsearch_row ( image , 0 , x , 0 , n , True ) bottom = self . bsearch_row ( image , x + 1 , m , 0 , n , False ) left = self . bsearch_col ( image , 0 , y , top , bottom , True ) right = self . bsearch_col ( image , y + 1 , n , top , bottom , False ) return ( bottom - top ) * ( right - left ) def bsearch_row ( self , image , start , end , lower , upper , white2black ): while start < end : m = ( start + end ) / 2 k = lower while k < upper and image [ m ][ k ] == '0' : k += 1 if ( k < upper ) == white2black : end = m else : start = m + 1 return start def bsearch_col ( self , image , start , end , lower , upper , white2black ): while start < end : m = ( start + end ) / 2 k = lower while k < upper and image [ k ][ m ] == '0' : k += 1 if ( k < upper ) == white2black : end = m else : start = m + 1 return start BFS class Solution { public : int minArea ( vector < vector < char >>& image , int x , int y ) { int m = image . size (); int n = m ? image [ 0 ]. size () : 0 ; int top = m , bottom = 0 , left = n , right = 0 ; int xx [ 4 ] = { - 1 , 0 , 1 , 0 }; int yy [ 4 ] = { 0 , 1 , 0 , - 1 }; queue < pair < int , int >> q ; q . push ({ x , y }); image [ x ][ y ] = '0' ; while ( ! q . empty ()) { pair < int , int > t = q . front (); q . pop (); top = min ( top , t . first ); bottom = max ( bottom , t . first + 1 ); left = min ( left , t . second ); right = max ( right , t . second + 1 ); for ( int k = 0 ; k < 4 ; ++ k ) { int a = t . first + xx [ k ]; int b = t . second + yy [ k ]; if ( a >= 0 && a < m && b >= 0 && b < n && image [ a ][ b ] == '1' ) { q . push ({ a , b }); image [ a ][ b ] = '0' ; } } } return ( right - left ) * ( bottom - top ); } }; BFS from collections import deque class Solution ( object ): def minArea ( self , image , x , y ): \"\"\" :type image: List[List[str]] :type x: int :type y: int :rtype: int \"\"\" m = len ( image ) n = 0 if m != 0 : n = len ( image [ 0 ]) xx = [ - 1 , 0 , 1 , 0 ] yy = [ 0 , - 1 , 0 , 1 ] top = m bottom = 0 left = n right = 0 q = deque () q . append ([ x , y ]) image [ x ][ y ] = '0' while len ( q ) > 0 : t = q . popleft () top = min ( top , t [ 0 ]) bottom = max ( bottom , t [ 0 ] + 1 ) left = min ( left , t [ 1 ]) right = max ( right , t [ 1 ] + 1 ) for k in range ( 4 ): a = t [ 0 ] + xx [ k ] b = t [ 1 ] + yy [ k ] if a >= 0 and a < m and b >= 0 and b < n and image [ a ][ b ] == '1' : q . append ([ a , b ]) image [ a ][ b ] = '0' return ( right - left ) * ( bottom - top ) DF class Solution { private : int m , n ; int top , bottom , left , right ; public : int minArea ( vector < vector < char >>& image , int x , int y ) { m = image . size (); n = m ? image [ 0 ]. size () : 0 ; top = m , bottom = 0 , left = n , right = 0 ; dfs_helper ( image , x , y ); return ( right - left ) * ( bottom - top ); } void dfs_helper ( vector < vector < char >>& image , int x , int y ) { if ( x < 0 || x >= m || y < 0 || y >= n || image [ x ][ y ] == '0' ) { return ; } image [ x ][ y ] = '0' ; top = min ( top , x ); bottom = max ( bottom , x + 1 ); left = min ( left , y ); right = max ( right , y + 1 ); dfs_helper ( image , x - 1 , y ); dfs_helper ( image , x , y + 1 ); dfs_helper ( image , x + 1 , y ); dfs_helper ( image , x , y - 1 ); } };","title":"302. Smallest Rectangle Enclosing Black Pixels"},{"location":"leetcode/binary-search/notes/#363-max-sum-of-rectangle-no-larger-than-k","text":"Iterate the wide of the matrix and using prefix sum and set lower_bound . From the problem Max Sum of Subarry No Larger Than K, we have to enumerate the width of the sub-matrix and sum up all row elements and get an array of length m , m is the number of rows of the matrix. Then apply the method. C++ presum class Solution { public : int maxSumSubmatrix ( vector < vector < int >>& matrix , int k ) { if ( matrix . empty ()) return 0 ; int m = matrix . size (); int n = m ? matrix [ 0 ]. size () : 0 ; int res = INT_MIN ; for ( int l = 0 ; l < n ; ++ l ) { vector < int > sums ( m , 0 ); for ( int r = l ; r < n ; ++ r ) { for ( int i = 0 ; i < m ; ++ i ) { sums [ i ] += matrix [ i ][ r ]; } set < int > preSumSet ; preSumSet . insert ( 0 ); int preSum = 0 , curMax = INT_MIN ; for ( int sum : sums ) { preSum += sum ; set < int >:: iterator it = preSumSet . lower_bound ( preSum - k ); if ( it != preSumSet . end ()) { curMax = max ( curMax , preSum - * it ); } preSumSet . insert ( preSum ); } res = max ( res , curMax ); } } return res ; } }; merge sort The idea is similar that solution 1. Instead of calculate preSum on the fly, we finish calculation and pass it to a mergeSort routine. The use mergeSort here is to find the A[j] - A[i] <= k efficiently, O(nlogn) . C++ Merge Sort class Solution { public : int maxSumSubmatrix ( vector < vector < int >>& matrix , int k ) { int m = matrix . size (); int n = m ? matrix [ 0 ]. size () : 0 ; int res = INT_MIN ; vector < long long > sums ( m + 1 , 0 ); for ( int l = 0 ; l < n ; ++ l ) { vector < long long > sumInRow ( m , 0 ); for ( int r = l ; r < n ; ++ r ) { for ( int i = 0 ; i < m ; ++ i ) { sumInRow [ i ] += matrix [ i ][ r ]; sums [ i + 1 ] = sums [ i ] + sumInRow [ i ]; } res = max ( res , mergeSort ( sums , 0 , m + 1 , k )); if ( res == k ) return k ; } } return res ; } int mergeSort ( vector < long long >& sums , int start , int end , int k ) { if ( end == start + 1 ) return INT_MIN ; int mid = start + ( end - start ) / 2 ; int res = mergeSort ( sums , start , mid , k ); if ( res == k ) return k ; res = max ( res , mergeSort ( sums , mid , end , k )); if ( res == k ) return k ; long long cache [ end - start ]; int j = mid , c = 0 , t = mid ; for ( int i = start ; i < mid ; ++ i ) { while ( j < end && sums [ j ] - sums [ i ] <= k ) ++ j ; // search first time sums[j] - sums[i] > k if ( j - 1 >= mid ) { // sums[j - 1] - sums[i] <= k, make sure j - 1 is still in right side res = max ( res , ( int )( sums [ j - 1 ] - sums [ i ])); if ( res == k ) return k ; } while ( t < end && sums [ t ] < sums [ i ]) { cache [ c ++ ] = sums [ t ++ ]; } cache [ c ++ ] = sums [ i ]; } for ( int i = start ; i < t ; ++ i ) { sums [ i ] = cache [ i - start ]; } return res ; } };","title":"363. Max Sum of Rectangle No Larger Than K"},{"location":"leetcode/binary-search/notes/#category-2-using-ordering-abstration","text":"","title":"Category 2 Using ordering abstration"},{"location":"leetcode/binary-search/notes/#69-sqrtx","text":"Solution 1 using ordering abstraction definition To find a square root of a integer x using binary search. We need to first determin the range [left, right] that the target value sqrt(x) may in. The potential range we can search is [0, x/2 + 1] . Then we should clarify this binary search is the \"find the first one\" type or the \"find the last one\" type. Basically, we want to determine our ordering abstraction f(target, g(i)) that is able to produce a boolean array. The boolean array have true part and false part seperated. Here target = sqrt(x) and g(i) = i . We define f(sqrt(x), i) = true when i <= sqrt(x) and f(sqrt(x), i) = false when i > sqrt(x) . This came from the following intuition: We are looking for the \"last\" integer whose square is less than x . Why not the otherwise? Because if you change to find the \"first\" integer whose square is greater than the x from right section of the boolean array, it is hard to define our ordering abstraction f . Of cause, we can search the \"first\" integer whose square is greater than x and find the previous integer next to it as the solution, but this later solution is a bit complex and counter intuitive. We prefer the first definition of ordering abstraction. Although a workable solution following the second ordering abstraction is also given below. For example: to solve the sqrt(8) and sqrt(9) using our definition: k, i = 0 1 2 3 4 5 6 7 8 9 10 n = 11 A = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] f(sqrt(8), k) = [T T T F F] f(sqrt(9), k) = [T T T T F] The binary search routine will be: C++ class Solution { public : int mySqrt ( int x ) { int l = 0 , r = x / 2 + 1 ; while ( l < r ) { // int m = l + (r - l) / 2; // will deadloop for 4, why? int m = r - ( r - l ) / 2 ; if ( m <= x / m ) { l = m ; } else { r = m - 1 ; } } return l ; } }; Solution 2 using the alternative ordering abstraction definition Second ordering abstraction (find first value whose square is greater than x) k, i = 0 1 2 3 4 5 6 7 8 9 10 n = 11 A = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] f(sqrt(8), k) = [F F F T T] f(sqrt(9), k) = [F F F F T] C++ class Solution { public : int mySqrt ( int x ) { if ( x == 0 ) return 0 ; // should handle, but will got division by zero in line 9. int l = 0 , r = x / 2 + 2 ; // r = x / 2 + 1 will not working for x = 1, have to have the one past last; while ( l < r ) { //int m = r - (r - l) / 2; // will dead loop for 4 int m = l + ( r - l ) / 2 ; if ( m > x / m ) { r = m ; } else { l = m + 1 ; } } return l - 1 ; } };","title":"69. Sqrt(x)"},{"location":"leetcode/binary-search/notes/#367-valid-perfect-square","text":"Solution 1 Binary search using ordering abstraction Notice you have to run tests for cases from 1 to 5. C++ class Solution { public : bool isPerfectSquare ( int num ) { if ( num == 1 ) return true ; int begin = 1 , end = num / 2 ; while ( begin < end ) { //long long mid = begin + (end - begin) / 2; // not working, deadloop for 5 long long mid = end - ( end - begin ) / 2 ; if ( mid * mid == num ) return true ; if ( mid * mid < num ) { begin = mid ; } else { end = mid - 1 ; } } return false ; } }; Python class Solution ( object ): def isPerfectSquare ( self , num ): \"\"\" :type num: int :rtype: bool \"\"\" if num == 1 : return True lower = 1 upper = num / 2 while lower < upper : mid = upper - ( upper - lower ) / 2 if mid * mid == num : return True if mid * mid < num : lower = mid else : upper = mid - 1 return False","title":"367. Valid Perfect Square"},{"location":"leetcode/binary-search/notes/#633-sum-of-square-numbers","text":"Solution 1 Binary search Once you have derived value b from a and c , you can binary search b . C++ class Solution { public : bool judgeSquareSum ( int c ) { if ( c == 0 ) return true ; for ( long long a = 0 ; a * a <= c ; ++ a ) { int b = c - ( int ) ( a * a ); int l = 0 , r = b / 2 + 1 ; while ( l < r ) { long long m = r - ( r - l ) / 2 ; if ( m * m == b ) return true ; if ( m * m < b ) { l = m ; } else { r = m - 1 ; } } } return false ; } }; Solution 2 Two pointers Notice this square sum can be found efficiently using two pointers. C++ class Solution { public : bool judgeSquareSum ( int c ) { int a = 0 , b = sqrt ( c ); while ( a <= b ){ int sum = a * a + b * b ; if ( sum < c ) a ++ ; else if ( sum > c ) b -- ; else return true ; } return false ; } }; Solution 3 Using set Keep inserting the value into a set, in the meantime also look up the other C++ class Solution { public : bool judgeSquareSum ( int c ) { set < int > s ; for ( int i = 0 ; i <= sqrt ( c ); ++ i ) { s . insert ( c - i * i ); if ( s . count ( i * i )) return true ; } return false ; } };","title":"633. Sum of Square Numbers"},{"location":"leetcode/binary-search/notes/#658-find-k-closest-elements","text":"Solution 1 Binary search Compare to problem 475. Heaters Our search target is to find the starting index of the subarray of length K. C++ class Solution { public : vector < int > findClosestElements ( vector < int >& arr , int k , int x ) { int start = 0 , end = arr . size () - k ; while ( start < end ) { int mid = start + ( end - start ) / 2 ; // looking for a \"mid\" that if ( x - arr [ mid ] > arr [ mid + k ] - x ) { start = mid + 1 ; } else { end = mid ; } } return vector < int > ( arr . begin () + start , arr . begin () + start + k ); } }; Solution 2 Binary search and Two pointers We first use binary search to locate the x value then expand to left and right looking for the k closest elements Notice the i < 0 in the if condition, it is very important to be there. otherwise the array index will be out of bound. C++ class Solution { public : vector < int > findClosestElements ( vector < int >& arr , int k , int x ) { int index = lower_bound ( arr . begin (), arr . end (), x ) - arr . begin (); int i = index - 1 , j = index ; while ( k -- ) { if ( i < 0 || j < arr . size () && abs ( arr [ j ] - x ) < abs ( arr [ i ] - x )) { j ++ ; } else { i -- ; } } return vector < int > ( arr . begin () + i + 1 , arr . begin () + j ); } };","title":"658. Find K Closest Elements"},{"location":"leetcode/binary-search/notes/#611-valid-triangle-number","text":"The main idea comes from the triangle lateral property, in which the triple should fullfil: a + b > c , a + c > b , and b + c > a . Once we sort it. We are able to gain some advantages that we don't have to check all these 3 relations. Instead, we should only take care of A[i] + A[j] > A[k] , in which i < j < k . Because we sorted the array, we can also fix the i and j , using binary search to find the k in the ragne of A[j + 1] ~ A[n - 1] . We can use our classic binary search template to achieve the goal. C++ class Solution { public : int triangleNumber ( vector < int >& nums ) { int n = nums . size (); int res = 0 ; sort ( nums . begin (), nums . end ()); for ( int i = 0 ; i < n - 2 ; ++ i ) { for ( int j = i + 1 ; j < n - 1 ; ++ j ) { int l = j + 1 , r = n ; // range of all possible k, notice l start with j + 1 int t = nums [ i ] + nums [ j ]; while ( l < r ) { int m = l + ( r - l ) / 2 ; if ( nums [ m ] < t ) { l = m + 1 ; } else { r = m ; } } res += l - j - 1 ; // notice the count start from j + 1 to l - 1. } } return res ; } };","title":"611. Valid Triangle Number"},{"location":"leetcode/binary-search/notes/#category-3-using-ordering-abstration-counter-as-a-gueesing-guage","text":"","title":"Category 3 Using ordering abstration (counter as a gueesing guage)"},{"location":"leetcode/binary-search/notes/#287-find-the-duplicate-number","text":"Solution 1 Binary search The problem asking for better than O(n^2) we could check to see whether binary search will work. If you count how many value <= the mid elements of [1, ..., n-1] , it will give you enough information to discard part of the array elements. Here you should distinguish what will be halfed and what will be searched. The answer to that is the [1, ..., n-1] sequence, not the given array. The simple proof of why it works can be put in this the following way. if the count of elements that <=mid in the array is less than mid , we can learn that the duplicate is in the higher end. If the count is greater, we can know that the duplicate element is in the lower end of the sequence [1, ..., n-1] . C++ class Solution { public : int findDuplicate ( vector < int >& nums ) { int begin = 1 , end = nums . size () - 1 ; while ( begin < end ) { int mid = begin + ( end - begin ) / 2 ; int count = 0 ; for ( int a : nums ) { if ( a <= mid ) ++ count ; } if ( count <= mid ) // \"=\" for [1,2,2] begin = mid + 1 ; else end = mid ; } return begin ; } }; Solution 2 tortoise and hare algorithm This problem is very similar to the the find circle in linked list. Generally, if you repeate A[A[i]] , the out put will show some periodic patterns. In fact you can imagine a rho shaped sequence. Image there is a function f(i) = A[i] , it mapping from 1, 2, 3, ... n to 1, 2, 3, ... n . Try to traverse A[i] , you will finally get circle through some same sequence of elements again and again, thus you obtain a rho shaped sequency like a circle in a linked list. The reason of it being a rho shape is becuase at least one element you will not come back to it if you leave it. http://www.keithschwarz.com/interesting/code/find-duplicate/FindDuplicate.python.html C++ class Solution { public : int findDuplicate ( vector < int >& nums ) { int n = nums . size (); if ( n == 0 ) return 0 ; int slow = 0 , fast = 0 , find = 0 ; while ( slow != fast || ( slow == 0 && fast == 0 )) { slow = nums [ slow ]; fast = nums [ nums [ fast ]]; } while ( slow != find ) { slow = nums [ slow ]; find = nums [ find ]; } return find ; } };","title":"287. Find the Duplicate Number"},{"location":"leetcode/binary-search/notes/#360-sort-transformed-array","text":"","title":"360. Sort Transformed Array"},{"location":"leetcode/binary-search/notes/#410-split-array-largest-sum","text":"","title":"410. Split Array Largest Sum"},{"location":"leetcode/binary-search/notes/#copy-books-lintcode","text":"","title":"Copy books (lintcode)"},{"location":"leetcode/binary-search/notes/#183-wood-cut-lintcode","text":"Solution 1 Binary search It requires to get equal or more than k pieces of wood with same length. So you have to cut the wood to fulfill the requirement. However, you need to promise that each of the k wood is the longest that is possible. Imagine that you are given bunch of wood to cut. How would you do it? You probably want to try to make one cut and see whether you can make it or not. If not, you may want to try make two cuts, and so on. But how could you program such a solution. It is very hard. Start thinking about the length seems a good option. Suppose you know your final maximum length. You would be able to make the cut accordingly. Now given a length out of guessing, can you verify whether it going to work or not? Yes, you can! That's the core idea of this solution. C++ class Solution { public : int woodCut ( vector < int > & L , int k ) { if ( L . empty ()) return 0 ; int maxlen = * max_element ( L . begin (), L . end ()); if ( k == 0 ) return maxlen ; int start = max ( 1 , maxlen / k ), end = maxlen ; while ( start < end ) { int mid = start + ( end - start ) / 2 ; int count = 0 ; for ( int len : L ) { count += len / ( mid + 1 ); } if ( count >= k ) start = mid + 1 ; else end = mid ; } int count = 0 ; for ( int len : L ) count += len / start ; return count >= k ? start : 0 ; } };","title":"183. Wood cut (lintcode)"},{"location":"leetcode/binary-search/notes/#774-minimize-max-distance-to-gas-station","text":"Solution 1 Binary search It is very similar to the problem Wood cut. You just need to take care of the accuracy of the results, namely also the int/double casts. It is also the hard part of the problem. Notice the count variable is int type, you should test your solution expecially for the line `count += dist[i] / mid; class Solution { public : double minmaxGasDist ( vector < int >& stations , int K ) { int n = stations . size (); vector < int > dist ( n , 0 ); // dist[0] = null; int d = 0 ; for ( int i = 1 ; i < n ; ++ i ) { dist [ i ] = stations [ i ] - stations [ i - 1 ]; d = max ( d , dist [ i ]); } double low = 0 , high = d ; while ( low + 0.000001 < high ) { double mid = low + ( high - low ) / 2 ; int count = 0 ; for ( int i = 1 ; i < n ; ++ i ) { count += dist [ i ] / mid ; } if ( count > K ) { // mid is too small low = mid ; } else { high = mid ; } } return low ; } };","title":"774. Minimize Max Distance to Gas Station"},{"location":"leetcode/binary-search/notes/#644-maximum-average-subarray-ii","text":"Notice the great trick you used to test whether there is a subarray of length greater than k whose average is larger than current mid . The trick is calculate the diff[i] = nums[i] - mid , and then calculate the prefix sum of the diff array, and compare to another prefix sum of the same array diff , the two prefix sum are calculated at two position at least k distant apart. We actually compare the prefix sum to the smallest prefix sum k distant apart. C++ class Solution { public : double findMaxAverage ( vector < int >& nums , int k ) { int n = nums . size (); double upper = INT_MIN , lower = INT_MAX ; for ( auto num : nums ) { upper = max ( upper , ( double ) num ); lower = min ( lower , ( double ) num ); } while ( lower + 0.00001 < upper ) { double mid = lower + ( upper - lower ) / 2 ; if ( isLarger ( nums , mid , k )) { // is average value >= mid? lower = mid ; } else { upper = mid ; } } return lower ; } /* return true if a greater average value is possible */ bool isLarger ( vector < int >& nums , double mid , int k ) { int n = nums . size (); double sums = 0 , prev = 0 , prev_min = 0 ; for ( int i = 0 ; i < k ; i ++ ) { sums += nums [ i ] - mid ; } if ( sums >= 0 ) { return true ; } /* we keep looking for whether a subarray sum of length >= k in array \"sums\" * is possible to be greater than zero. If such a subarray exist, it means * that the target average value is greater than the \"mid\" value. * we look at the front part of sums that at least k element apart from i. * If we can find the minimum of the sums[0, 1, ..., i - k] and check if * sums[i] - min(sum[0, 1, ..., i - k]) >= 0. If this is the case, it indicate * there exist a subarray of length >= k with sum greater than 0 in sums, * we can return ture, otherwise, it return false. */ for ( int i = k ; i < n ; i ++ ) { sums += nums [ i ] - mid ; prev += nums [ i - k ] - mid ; prev_min = min ( prev_min , prev ); if ( sums >= prev_min ) return true ; } return false ; } };","title":"644. Maximum Average Subarray II"},{"location":"leetcode/binary-search/notes/#778-swim-in-rising-water","text":"In This problem we are trying to find a path, in which the maximum element in the path among all paths is minimum. Meaning we look for a target value in the grid, such that there exist a path from grid[0][0] to grid[n-1][n-1] which includes this value and it is the maximum value in the path. C++ class Solution { int x [ 4 ] = { 0 , - 1 , 0 , 1 }; int y [ 4 ] = { - 1 , 0 , 1 , 0 }; public : int swimInWater ( vector < vector < int >>& grid ) { int n = grid . size (); int begin = grid [ 0 ][ 0 ], end = n * n - 1 ; // binary search find a path with mini elevation while ( begin < end ) { int mid = begin + ( end - begin ) / 2 ; if ( pathExist ( grid , mid )) { end = mid ; } else { begin = mid + 1 ; } } return begin ; } bool pathExist ( vector < vector < int >> & grid , int mid ) { int n = grid . size (); vector < vector < int >> visited ( n , vector < int > ( n , 0 )); return dfs_helper ( grid , visited , n , mid , 0 , 0 ); } bool dfs_helper ( vector < vector < int >> & grid , vector < vector < int >>& visited , int n , int mid , int i , int j ) { visited [ i ][ j ] = 1 ; for ( int k = 0 ; k < 4 ; ++ k ) { int a = i + x [ k ]; int b = j + y [ k ]; if ( a < 0 || a >= n || b < 0 || b >= n || visited [ a ][ b ] == 1 || grid [ a ][ b ] > mid ) continue ; if ( a == n - 1 && b == n - 1 ) return true ; if ( dfs_helper ( grid , visited , n , mid , a , b )) return true ; } return false ; } };","title":"778. Swim in Rising Water"},{"location":"leetcode/binary-search/notes/#483-smallest-good-base","text":"Solution 1 Binary search This problem requires a bit reasoning to achieve the solution. The starting point is realy mean what's asking by the problem. Here it is asking a minimum base that represent the given number n in a representation like binary representation. For example: 13 = 3^0 + 3^1 + 3^2 so 13 can be representd as 111 (base 3). First of all, there is a special case that such a base may not exist. (precisely, we should seperate the special case when n = (n-1)^0 + (n-1)^1 ; With this special case in mind, we can use binary search to iterate through each m from largest to smallest and check whether the corresponding k is a good base of the given value n . Because when m is the largest, k is the smallest, so if the bianry search find one it must be the smallest k we are looking for. If binary search found nothing, we simpley return the special case n-1 . C++ class Solution { public : string smallestGoodBase ( string n ) { long long num = stoll ( n ); /* for each lenght of the potentional representation, * n = 1 + k + ... + k^{i-1} = (k^i-1)/(k-1), lower bound k is 2, * we have 2^i-1 = n ==> upper bound i = log2(n+1). */ for ( int i = log2 ( num + 1 ); i >= 2 ; -- i ) { /* upper bound is obtained by n = 1 + k + k^2 ... + k^(i-1) > k^(i-1), * n > k^(i-1) ==> k < n^(1/(i-1)); */ long long left = 2 , right = pow ( num , 1.0 / ( i - 1 )) + 1 ; while ( left < right ) { long long mid = left + ( right - left ) / 2 ; long long sum = 0 ; /* calculate i digits value with base \"mid\" */ for ( int j = 0 ; j < i ; ++ j ) { sum = sum * mid + 1 ; } /* binary search for the mid (good base) */ if ( sum == num ) return to_string ( mid ); if ( sum < num ) left = mid + 1 ; else right = mid ; } } return to_string ( num - 1 ); } };","title":"483 Smallest Good Base"},{"location":"leetcode/binary-search/notes/#658-find-k-closest-elements_1","text":"Solution 1 Binary Search C++ class Solution { public : vector < int > findClosestElements ( vector < int >& arr , int k , int x ) { int start = 0 , end = arr . size () - k ; while ( start < end ) { int mid = start + ( end - start ) / 2 ; if ( x - arr [ mid ] > arr [ mid + k ] - x ) { start = mid + 1 ; } else { end = mid ; } } return vector < int > ( arr . begin () + start , arr . begin () + start + k ); } };","title":"658. Find K Closest Elements"},{"location":"leetcode/binary-search/notes/#378-kth-smallest-element-in-a-sorted-matrix","text":"Solution 1 Binary Search The idea of using binary search for this problem my not be straightforward. But the method is very important. The idea is very similar to the problem Search in a rotated sorted array. Because the matrix is sorted row wise and column wise, there are some ordering information we can make use of. Notice we are not try to search using the matrix index, we are searching the matrix element value. Compare to the problem 287. Find the Duplicate Number. The comparison if (count < k) isn't include mid explicitly. but the count is some function f(mid) , with the current mid , the count value is unique and can be use to test a condition that decide which side we can go to shrink the range the target value is possible in. C++ class Solution { public : int kthSmallest ( vector < vector < int >>& matrix , int k ) { int m = matrix . size (); int n = m ? matrix [ 0 ]. size () : 0 ; int start = matrix [ 0 ][ 0 ], end = matrix [ m - 1 ][ n - 1 ]; while ( start < end ) { int mid = start + ( end - start ) / 2 ; int count = 0 ; for ( int i = 0 ; i < m ; ++ i ) { count += upper_bound ( matrix [ i ]. begin (), matrix [ i ]. end (), mid ) - matrix [ i ]. begin (); } if ( count < k ) { // notice no mid here, but count is a function of mid. start = mid + 1 ; } else { end = mid ; } } return start ; } }; Solution 2 Priority Queue Notice when the k <= n^2 , index j < matrix.size() will also make it work. C++ class Solution { public : int kthSmallest ( vector < vector < int >>& matrix , int k ) { priority_queue < int > pq ; for ( int i = 0 ; i < matrix . size (); ++ i ) { for ( int j = 0 ; j < matrix [ 0 ]. size (); ++ j ) { pq . push ( matrix [ i ][ j ]); if ( pq . size () > k ) pq . pop (); } } return pq . top (); } };","title":"378. Kth Smallest Element in a Sorted Matrix"},{"location":"leetcode/binary-search/notes/#668-kth-smallest-number-in-multiplication-table","text":"Solution 1 Binary search While this problem looks simple. But it really isn't unless you observed the following. The condition used for binary search is \"whether there are k smaller elements in the range [start, mid]\". You are looking for the smallest number that has k elements less than or equal to it. Like in the problem Kth Smallest Element in a Sorted Matrix, we will move the number not the index. We move the start or end appropriately based on this condition, if there are more than k, we shrink the range by reduce end: end = mid . If there are less than k numbers, we increase begin hopefully to make mid larger so as to have close to k numbers in the range of [1, mid] . When being == end , we've located the kth number desired. In case k > m*n , we will got begin == end < k , which is not a solution. In counting how many element less than mid, you have to be clever a bit by using the feature that this matrix is multiplicative table. That is for row i , you can at most have x/i number smaller than x , why? Follow up: Does the kth element will be in the range of [1, m*n] ? C++ class Solution { public : int findKthNumber ( int m , int n , int k ) { int begin = 1 , end = m * n ; while ( begin < end ) { int mid = begin + ( end - begin ) / 2 ; int count = 0 ; for ( int i = 1 ; i <= m ; ++ i ) { count += min ( mid / i , n ); } if ( count < k ) begin = mid + 1 ; else end = mid ; } return begin ; } };","title":"668. Kth Smallest Number in Multiplication Table"},{"location":"leetcode/binary-search/notes/#719-find-k-th-smallest-pair-distance","text":"Solution 1 Priority Queue TLE C++ class Solution { public : int smallestDistancePair ( vector < int >& nums , int k ) { priority_queue < int > pq ; for ( int i = 0 ; i < nums . size (); ++ i ) { for ( int j = i + 1 ; j < nums . size (); ++ j ) { int dist = abs ( nums [ i ] - nums [ j ]); if ( pq . size () < k ) { pq . push ( dist ); } else if ( dist < pq . top ()) { pq . push ( dist ), pq . pop (); } } } return pq . top (); } }; Solution 2 Binary search Similar to Problem 668. Kth Smallest Number in Multiplication Table. The problem is complicated at firt glass. A brute force solutoin generates all the absolute distances and then sort to find the kth smallest one. We found it is potentially a searchable senario if we sort the elements. We have range [min_distance, max_distance] . We search a distance in this range such that there are exactly k pairs distance including itself. If the count of pair distance less than k, we try to increase it buy start = mid + 1 , vice versa. When the binary search loop stops, if the result exist, start point to the distance we are searching. Since this problem guarrantee solution exist, we return start . C++ class Solution { public : int smallestDistancePair ( vector < int >& nums , int k ) { sort ( nums . begin (), nums . end ()); int start = nums [ 1 ] - nums [ 0 ]; for ( int i = 2 ; i < nums . size (); ++ i ) { start = min ( start , nums [ i ] - nums [ i - 1 ]); } int end = nums . back () - nums [ 0 ]; while ( start < end ) { int mid = start + ( end - start ) / 2 ; // count how many absolute differences that <= mid; int count = 0 ; for ( int i = 0 ; i < nums . size (); ++ i ) { int j = i ; while ( j < nums . size () && nums [ j ] - nums [ i ] <= mid ) j ++ ; count += j - i - 1 ; } if ( count < k ) { start = mid + 1 ; } else { end = mid ; } } return start ; } }; Solution 3 Using binary search to optimize the counting You can also write your own binary search routine upper_bound . class Solution { public : int smallestDistancePair ( vector < int >& nums , int k ) { sort ( nums . begin (), nums . end ()); int start = nums [ 1 ] - nums [ 0 ]; for ( int i = 2 ; i < nums . size (); ++ i ) { start = min ( start , nums [ i ] - nums [ i - 1 ]); } int end = nums . back () - nums [ 0 ]; while ( start < end ) { int mid = start + ( end - start ) / 2 ; // count how many absolute differences that <= mid; int count = 0 ; /* for (int i = 0; i < nums.size(); ++i) { int j = i; while (j < nums.size() && nums[j] - nums[i] <= mid) j++; count += j - i - 1; } */ // optimize the counting use binary search (nested binary search) for ( int i = 0 ; i < nums . size (); ++ i ) { auto iter = upper_bound ( nums . begin () + i , nums . end (), nums [ i ] + mid ); count += iter - ( nums . begin () + i ) - 1 ; } if ( count < k ) { start = mid + 1 ; } else { end = mid ; } } return start ; } };","title":"719. Find K-th Smallest Pair Distance"},{"location":"leetcode/binary-search/notes/#category-4-binary-search-as-an-optimization-routine","text":"","title":"Category 4 Binary search as an optimization routine"},{"location":"leetcode/binary-search/notes/#300-longest-increasing-subsequence","text":"Solution 1 DP The base case is single char. f[i] is the length of LIS from the begining. C++ class Solution { public : int lengthOfLIS ( vector < int >& nums ) { if ( n == nums . size ()) return 0 ; int f [ n ] = { 0 }; int res = 0 ; for ( int j = 0 ; j < n ; j ++ ) { f [ j ] = 1 ; for ( int i = 0 ; i < j ; i ++ ) { if ( nums [ i ] < nums [ j ] && f [ i ] + 1 > f [ j ]) f [ j ] = f [ i ] + 1 ; } res = max ( res , f [ j ]); } return res ; } }; Solution 2 Using binary search The DP solution is O(n^2) . Using binary search could reduce to O(nlogn) . Binary search solution analysis. For each i , we are looking for the largest f value that has smallest A value. For example, A[0] = 5 could be ignored because of its f value is same as A[1] = 1 , which is smaller. In searching for the LIS, we prefer a small ending value when the length is the same. The following solution using a vector b to record the minimum A value for each length of LIS ( f value), we use binary search to find the last value in b that smaller that current value A[i] . If we found such a value in b , we use A[i] to replace the value next to the found value in b ). i 0 1 2 3 4 5 6 7 A 5 1 3 7 6 4 2 10 f 1 1 2 3 3 3 2 4 f[1] = 1, a[1] = 1 f[6] = 2, a[6] = 2 f[5] = 3, a[5] = 4 f[7] = 4, a[7] = 10 C++ class Solution { public : int lengthOfLIS ( vector < int >& nums ) { vector < int > b ; for ( int i = 0 ; i < nums . size (); ++ i ) { int l = 0 , r = b . size (); while ( l < r ) { int m = l + ( r - l ) / 2 ; if ( b [ m ] < nums [ i ]) { // nums[i] is the target l = m + 1 ; } else { r = m ; } } if ( l == b . size ()) // nums[i] greater than all element in b b . push_back ( nums [ i ]); else // begin point to next element no less than the target nums[i]. b [ l ] = nums [ i ]; } return b . size (); } }; Alternatively, we could use lower_bound . C++ class Solution { public : int lengthOfLIS ( vector < int >& nums ) { vector < int > b ; for ( int i = 0 ; i < nums . size (); ++ i ) { int l = lower_bound ( b . begin (), b . end (), nums [ i ]) - b . begin (); if ( l == b . size ()) // nums[i] greater than all element in b b . push_back ( nums [ i ]); else // begin point to next element no less than the target nums[i]. b [ l ] = nums [ i ]; } return b . size (); } };","title":"300 Longest Increasing Subsequence"},{"location":"leetcode/binary-search/notes/#354-russian-doll-envelopes","text":"","title":"354. Russian Doll Envelopes"},{"location":"leetcode/binary-search/notes/#174-dungeon-game","text":"","title":"174. Dungeon Game"},{"location":"seedlabs/public-key-cryptography-and-pki/notes/","text":"Public-Key Cryptography and PKI \u00b6 Questions to answer \u00b6 What is a Certificate Authority (CA), and how to become one? How to create a certificate for a customer? (i.e. www.ruihan.org) How PKI is used for web site authentication and encryption? What are the necessary files of such an infrastructure? 0x1 Become a Certificate Authority (CA) \u00b6 CA is a trusted entity to issue digital certificates. The digital certificate certifies the ownership of a public key by the named subject of the certificate. Root CAs are a number of trusted commercial CAs. (VeriSign is one of the largest) However, we are not obligate to the commercial world, we are free to become a root CA, and then use this root CA to issue certificates to others. Remember that the root CAs are self-signed. root CA's certificates are pre-loaded into most OS, browsers, and other software that using PKI. Root CA's certificates are unconditionally trusted. To create a root CA, we use the following openssl command to generate a self-signed certificate for the CA. seed@ubuntu:~/SEED-Labs/PKI-Lab$ openssl req -new -x509 -keyout ca.key -out ca.crt -config openssl.cnf Generating a 1024 bit RSA private key ..........................++++++ ..++++++ unable to write 'random state' writing new private key to 'ca.key' Enter PEM pass phrase: Verifying - Enter PEM pass phrase: ----- You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter '.', the field will be left blank. ----- Country Name (2 letter code) [AU]:US State or Province Name (full name) [Some-State]:CA Locality Name (eg, city) []:San Jose Organization Name (eg, company) [Internet Widgits Pty Ltd]:PKILabServer Organizational Unit Name (eg, section) []: Common Name (e.g. server FQDN or YOUR name) []:PKILabServer.com Email Address []:rui@PKILabServer.com [11/17/2016 11:14] seed@ubuntu:~/SEED-Labs/PKI-Lab$ The Configure File openssl.conf is used by three OpenSSL command: ca , req , and x509 . To get a glimpse of it's purporse, here is partial of the openssl.conf (from [CA_Default} section). dir = ./demoCA # Where everything is kept certs = $dir/certs # Where the issued certs are kept crl_dir = $dir/crl # Where the issued crl are kept new_certs_dir = $dir/newcerts # default place for new certs database = $dir/index.txt # database index file serial = $dir/serial # The current serial number 0x2 Create a Certificate for PKILabServer.com \u00b6 When one becomes a certificate autority(CA), he/she is ready to sign digital certificates for his/her customers. PKILabServer.com is a company that tries to get a digital certificate from a CA. He needs to do the following steps: Generate RSA key pairs Generate a certificate signing request (CSR) Generate a certificate 0x21 Generate RSA Key Paris \u00b6 This step could be done using OpenSSL command line tool like this: seed@ubuntu:~/SEED-Labs/PKI-Lab$ openssl genrsa -aes128 -out server.key 1024 Generating RSA private key, 1024 bit long modulus ....++++++ .............++++++ unable to write 'random state' e is 65537 (0x10001) Enter pass phrase for server.key: Verifying - Enter pass phrase for server.key: [11/17/2016 11:16] seed@ubuntu:~/SEED-Labs/PKI-Lab$ The generated keys will be stored in the file server.key . This file is encrypted as indicated by the option -ase128 , which means using AES-128 encrytpion algorithm. The effect of this is you have to enter a password when executing the above command. To see the plan text content of the server.key file, you can run the following command: seed@ubuntu:~/SEED-Labs/PKI-Lab$ openssl rsa -in server.key -text Enter pass phrase for server.key: Private-Key: (1024 bit) modulus: 00:e1:23:f9:75:2d:2c:d9:21:36:6f:62:d6:28:6a: 2c:c6:1a:3f:d0:77:c2:8c:e0:49:1a:95:b8:99:6d: 7f:15:cb:db:9c:42:6e:a9:c7:fd:af:cd:94:3e:b5: d3:48:1a:14:11:ca:a2:63:d6:27:32:bc:db:75:d7: 5e:05:9a:8c:2b:7d:0d:fc:6f:61:77:69:c3:37:6b: 94:a3:d0:5b:70:0d:80:9a:89:ea:10:04:cd:6f:4a: 72:b9:e7:d6:1b:fa:ff:25:a1:10:0e:05:d5:14:c7: 34:97:78:55:0b:c7:ed:c4:16:7c:0b:fa:df:46:dd: 7f:35:00:7f:72:21:02:38:51 publicExponent: 65537 (0x10001) privateExponent: 00:bb:06:91:72:1f:c7:03:d0:ad:51:b6:7f:45:2f: 7e:66:4a:e3:fc:1f:a0:84:3a:c5:3f:ca:64:81:40: 0e:b4:62:96:da:a5:ed:1c:29:40:ba:cc:42:7c:1d: bb:98:47:e1:a8:2d:f6:dc:8c:c8:1f:43:f7:e3:5a: b7:86:5c:2e:57:5a:ea:fa:8b:48:28:6a:4c:35:6b: fe:80:83:1f:9b:44:80:73:98:62:e2:bc:7e:e1:f0: 15:dd:74:dc:05:e0:5d:a5:59:a4:71:5f:96:13:e5: 8f:a6:4c:c0:31:50:ca:b1:b9:e4:92:de:e1:e9:2a: 2b:3a:2e:07:1c:fb:dc:ac:01 prime1: 00:f9:bf:6b:f0:28:15:b5:ba:2e:5f:c1:62:63:d7: 53:b0:ab:9c:3f:53:e1:9b:76:1c:12:fb:b0:61:c4: 34:fa:28:d7:94:d2:d3:d9:fe:e5:84:57:60:77:23: af:82:75:50:d6:0c:2e:2e:23:55:82:8c:6f:5c:b4: 5a:2f:77:82:b1 prime2: 00:e6:c6:d9:bf:57:3b:a4:1e:b4:62:c9:fd:5f:e9: d8:cc:d7:49:9e:13:db:99:2c:99:2e:7e:78:0a:c2: b1:b4:c4:2f:08:fc:76:89:e2:60:12:7f:b7:47:bb: d2:98:1e:03:99:42:d7:ec:ab:0d:55:52:95:5e:b8: 77:c5:55:37:a1 exponent1: 63:09:e2:fe:f0:96:73:63:6a:a2:74:68:d5:18:fd: ca:30:b3:9c:75:62:21:29:3c:46:d6:e2:82:52:b1: 83:86:90:bf:26:bf:f6:51:db:a8:98:91:db:8d:1e: 3b:88:d4:4e:9d:b3:ee:7d:fa:99:f3:a0:f1:cd:5f: 7a:35:55:71 exponent2: 00:b9:05:4e:48:80:a8:b9:71:40:90:3f:7f:5b:a9: 81:7e:e1:50:0d:63:c0:58:f3:0f:b0:de:06:62:22: 2c:15:50:80:a1:44:bf:c8:d6:6c:ce:08:05:2a:86: e4:55:bf:22:85:7a:b8:e0:ef:56:d6:44:4a:ab:51: 5b:fd:22:d0:61 coefficient: 3d:ec:20:c8:23:3e:d9:f3:88:8f:03:9d:1b:57:c8: 9d:87:14:83:a2:a8:2c:bc:cc:e7:dd:2d:c4:ef:74: 2a:21:2b:5a:ef:fb:79:49:f0:bc:6f:4e:d3:0e:f7: d2:48:af:b1:12:ae:43:e6:1b:03:bb:f2:18:f1:61: 8b:7a:1e:8f writing RSA key -----BEGIN RSA PRIVATE KEY----- MIICXQIBAAKBgQDhI/l1LSzZITZvYtYoaizGGj/Qd8KM4EkalbiZbX8Vy9ucQm6p x/2vzZQ+tdNIGhQRyqJj1icyvNt1114FmowrfQ38b2F3acM3a5Sj0FtwDYCaieoQ BM1vSnK559Yb+v8loRAOBdUUxzSXeFULx+3EFnwL+t9G3X81AH9yIQI4UQIDAQAB AoGBALsGkXIfxwPQrVG2f0UvfmZK4/wfoIQ6xT/KZIFADrRiltql7RwpQLrMQnwd u5hH4agt9tyMyB9D9+Nat4ZcLlda6vqLSChqTDVr/oCDH5tEgHOYYuK8fuHwFd10 3AXgXaVZpHFflhPlj6ZMwDFQyrG55JLe4ekqKzouBxz73KwBAkEA+b9r8CgVtbou X8FiY9dTsKucP1Phm3YcEvuwYcQ0+ijXlNLT2f7lhFdgdyOvgnVQ1gwuLiNVgoxv XLRaL3eCsQJBAObG2b9XO6QetGLJ/V/p2MzXSZ4T25ksmS5+eArCsbTELwj8doni YBJ/t0e70pgeA5lC1+yrDVVSlV64d8VVN6ECQGMJ4v7wlnNjaqJ0aNUY/cows5x1 YiEpPEbW4oJSsYOGkL8mv/ZR26iYkduNHjuI1E6ds+59+pnzoPHNX3o1VXECQQC5 BU5IgKi5cUCQP39bqYF+4VANY8BY8w+w3gZiIiwVUIChRL/I1mzOCAUqhuRVvyKF erjg71bWREqrUVv9ItBhAkA97CDIIz7Z84iPA50bV8idhxSDoqgsvMzn3S3E73Qq ISta7/t5SfC8b07TDvfSSK+xEq5D5hsDu/IY8WGLeh6P -----END RSA PRIVATE KEY----- [11/17/2016 11:57] seed@ubuntu:~/SEED-Labs/PKI-Lab$ The password you entered works like a key to decrypt the ciphertext in the file. 0x22 Generate a certificate signing request (CSR) \u00b6 To request a certificate, the company need to include its public key in a certificate signing request (CSR) and send the CSR to the certificate authority. The command to generate the CSR using OpenSSL is: seed@ubuntu:~/SEED-Labs/PKI-Lab$ openssl req -new -key server.key -out server.csr -config openssl.cnf Enter pass phrase for server.key: You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter '.', the field will be left blank. ----- Country Name (2 letter code) [AU]:US State or Province Name (full name) [Some-State]:CA Locality Name (eg, city) []:San Jose Organization Name (eg, company) [Internet Widgits Pty Ltd]:PKILabServer Organizational Unit Name (eg, section) []: Common Name (e.g. server FQDN or YOUR name) []:PKILabServer.com Email Address []:admin@PKILabServer.com Please enter the following 'extra' attributes to be sent with your certificate request A challenge password []:server.csr An optional company name []: [11/17/2016 11:19] seed@ubuntu:~/SEED-Labs/PKI-Lab$ Note that this command is quite similar to the one we used in creating a self-signed certificate for the certificate authority previously. The only difference is the -x509 option. 0x23 Generate a certificate \u00b6 The CSR file needs to have the CA's signature to form a certificate. When received the CSR file, CA will first verify the identity information in the CSR. In reality, CA might contact the company through physical channel to do the verification. For example, through traditional mails or letters. For this experiment, we can assume we are the CA and will generate the certificate for the company. The following command turns the certificate signing request ( server.csr ) into an X509 certificate ( server.crt ), using the CA's ca.crt and ca.key : seed@ubuntu:~/SEED-Labs/PKI-Lab$ openssl ca -in server.csr -out server.crt -cert ca.crt -keyfile ca.key -config openssl.cnf Using configuration from openssl.cnf Enter pass phrase for ca.key: Check that the request matches the signature Signature ok Certificate Details: Serial Number: 4096 (0x1000) Validity Not Before: Nov 17 19:21:33 2016 GMT Not After : Nov 17 19:21:33 2017 GMT Subject: countryName = US stateOrProvinceName = CA localityName = San Jose organizationName = PKILabServer commonName = PKILabServer.com emailAddress = admin@PKILabServer.com X509v3 extensions: X509v3 Basic Constraints: CA:FALSE Netscape Comment: OpenSSL Generated Certificate X509v3 Subject Key Identifier: B4:F1:39:2A:56:C6:1F:1F:9C:49:AF:0B:45:FC:FD:79:66:90:F2:07 X509v3 Authority Key Identifier: keyid:1D:11:39:21:1C:E2:0D:8B:14:34:B8:36:C4:F5:34:27:09:9D:E8:7A Certificate is to be certified until Nov 17 19:21:33 2017 GMT (365 days) Sign the certificate? [y/n]:y 1 out of 1 certificate requests certified, commit? [y/n]y Write out database with 1 new entries Data Base Updated unable to write 'random state' [11/17/2016 11:22] seed@ubuntu:~/SEED-Labs/PKI-Lab$ If OpenSSL refuses to generate certificates, it is very likely that the names in your requests do not match with those of CA. (yes, think about you are creating a root CA for your company, and use it to sign certificates for different departments.) The matching rules are specified in the configuration file (at the [policy match] section). You can change the names of your requests to comply with the policy, or you can change the policy. The configuration file also includes another policy (called policy_anything ), which is less restrictive. You can choose that policy by changing the following line: \"policy = policy_match\" change to \"policy = policy_anything\". 0x3 Use PKI for Web Sites \u00b6 Following the lab manual didn't work out. While adding the ca.crt to Firefox, I run into the following error that the format isn't supported by Firefox. Failed to decode the file. Either it is not in PKCS #12 format, has been corrupted, or the password you entered was incorrect. Find this stackoverflow post . We should use the following command to combine the server.crt with the ca.crt . openssl pkcs12 -export -in server.crt -inkey server.key -out PKILabServerFirefox.pfx -certfile ca.crt Finally, we can import PKILabServerFirefox.pfx to Firefox and right click the certificate and select \"Edit Trust->this certificate can identify websites\" 0x4 Test the certificate \u00b6 start a openssl simple webserver: seed@ubuntu:~/SEED-Labs/PKI-Lab$ cp server.key server.pem seed@ubuntu:~/SEED-Labs/PKI-Lab$ cat server.crt >> server.pem seed@ubuntu:~/SEED-Labs/PKI-Lab$ openssl s_server -cert server.pem -www -accpet 6666 Now navigate to https://www.pkilabserver.com:6666 , the security verification is passed and show the openssl web server default page. s_server -cert server.pem -www -accept 3633 Ciphers supported in s_server binary TLSv1/SSLv3:ECDHE-RSA-AES256-GCM-SHA384TLSv1/SSLv3:ECDHE-ECDSA-AES256-GCM-SHA384 TLSv1/SSLv3:ECDHE-RSA-AES256-SHA384 TLSv1/SSLv3:ECDHE-ECDSA-AES256-SHA384 TLSv1/SSLv3:ECDHE-RSA-AES256-SHA TLSv1/SSLv3:ECDHE-ECDSA-AES256-SHA TLSv1/SSLv3:SRP-DSS-AES-256-CBC-SHA TLSv1/SSLv3:SRP-RSA-AES-256-CBC-SHA TLSv1/SSLv3:SRP-AES-256-CBC-SHA TLSv1/SSLv3:DHE-DSS-AES256-GCM-SHA384 TLSv1/SSLv3:DHE-RSA-AES256-GCM-SHA384TLSv1/SSLv3:DHE-RSA-AES256-SHA256 TLSv1/SSLv3:DHE-DSS-AES256-SHA256 TLSv1/SSLv3:DHE-RSA-AES256-SHA TLSv1/SSLv3:DHE-DSS-AES256-SHA TLSv1/SSLv3:DHE-RSA-CAMELLIA256-SHA TLSv1/SSLv3:DHE-DSS-CAMELLIA256-SHA TLSv1/SSLv3:ECDH-RSA-AES256-GCM-SHA384 TLSv1/SSLv3:ECDH-ECDSA-AES256-GCM-SHA384TLSv1/SSLv3:ECDH-RSA-AES256-SHA384 TLSv1/SSLv3:ECDH-ECDSA-AES256-SHA384 TLSv1/SSLv3:ECDH-RSA-AES256-SHA TLSv1/SSLv3:ECDH-ECDSA-AES256-SHA TLSv1/SSLv3:AES256-GCM-SHA384 TLSv1/SSLv3:AES256-SHA256 TLSv1/SSLv3:AES256-SHA TLSv1/SSLv3:CAMELLIA256-SHA TLSv1/SSLv3:PSK-AES256-CBC-SHA TLSv1/SSLv3:ECDHE-RSA-DES-CBC3-SHA TLSv1/SSLv3:ECDHE-ECDSA-DES-CBC3-SHA TLSv1/SSLv3:SRP-DSS-3DES-EDE-CBC-SHA TLSv1/SSLv3:SRP-RSA-3DES-EDE-CBC-SHA TLSv1/SSLv3:SRP-3DES-EDE-CBC-SHA TLSv1/SSLv3:EDH-RSA-DES-CBC3-SHA TLSv1/SSLv3:EDH-DSS-DES-CBC3-SHA TLSv1/SSLv3:ECDH-RSA-DES-CBC3-SHA TLSv1/SSLv3:ECDH-ECDSA-DES-CBC3-SHA TLSv1/SSLv3:DES-CBC3-SHA TLSv1/SSLv3:PSK-3DES-EDE-CBC-SHA TLSv1/SSLv3:ECDHE-RSA-AES128-GCM-SHA256 TLSv1/SSLv3:ECDHE-ECDSA-AES128-GCM-SHA256TLSv1/SSLv3:ECDHE-RSA-AES128-SHA256 TLSv1/SSLv3:ECDHE-ECDSA-AES128-SHA256TLSv1/SSLv3:ECDHE-RSA-AES128-SHA TLSv1/SSLv3:ECDHE-ECDSA-AES128-SHA TLSv1/SSLv3:SRP-DSS-AES-128-CBC-SHA TLSv1/SSLv3:SRP-RSA-AES-128-CBC-SHA TLSv1/SSLv3:SRP-AES-128-CBC-SHA TLSv1/SSLv3:DHE-DSS-AES128-GCM-SHA256TLSv1/SSLv3:DHE-RSA-AES128-GCM-SHA256 TLSv1/SSLv3:DHE-RSA-AES128-SHA256 TLSv1/SSLv3:DHE-DSS-AES128-SHA256 TLSv1/SSLv3:DHE-RSA-AES128-SHA TLSv1/SSLv3:DHE-DSS-AES128-SHA TLSv1/SSLv3:DHE-RSA-SEED-SHA TLSv1/SSLv3:DHE-DSS-SEED-SHA TLSv1/SSLv3:DHE-RSA-CAMELLIA128-SHA TLSv1/SSLv3:DHE-DSS-CAMELLIA128-SHA TLSv1/SSLv3:ECDH-RSA-AES128-GCM-SHA256TLSv1/SSLv3:ECDH-ECDSA-AES128-GCM-SHA256 TLSv1/SSLv3:ECDH-RSA-AES128-SHA256 TLSv1/SSLv3:ECDH-ECDSA-AES128-SHA256 TLSv1/SSLv3:ECDH-RSA-AES128-SHA TLSv1/SSLv3:ECDH-ECDSA-AES128-SHA TLSv1/SSLv3:AES128-GCM-SHA256 TLSv1/SSLv3:AES128-SHA256 TLSv1/SSLv3:AES128-SHA TLSv1/SSLv3:SEED-SHA TLSv1/SSLv3:CAMELLIA128-SHA TLSv1/SSLv3:PSK-AES128-CBC-SHA TLSv1/SSLv3:ECDHE-RSA-RC4-SHA TLSv1/SSLv3:ECDHE-ECDSA-RC4-SHA TLSv1/SSLv3:ECDH-RSA-RC4-SHA TLSv1/SSLv3:ECDH-ECDSA-RC4-SHA TLSv1/SSLv3:RC4-SHA TLSv1/SSLv3:RC4-MD5 TLSv1/SSLv3:PSK-RC4-SHA TLSv1/SSLv3:EDH-RSA-DES-CBC-SHA TLSv1/SSLv3:EDH-DSS-DES-CBC-SHA TLSv1/SSLv3:DES-CBC-SHA --- Reused, TLSv1/SSLv3, Cipher is ECDHE-RSA-AES256-SHA SSL-Session: Protocol : TLSv1 Cipher : ECDHE-RSA-AES256-SHA Session-ID: 124BFE9E98B4DF8E5DC0CC87DEB16DDE615FEC0B63A5FD687CD7033012639DD1 Session-ID-ctx: 01000000 Master-Key: A23B112D5A56B98709D5C140EEC5876155C6023CF8D5DCD81858B1EFF3B4E9CE8723E2D9703C217D8C517A762820B32C Key-Arg : None PSK identity: None PSK identity hint: None SRP username: None Start Time: 1479414975 Timeout : 300 (sec) Verify return code: 0 (ok) --- 0 items in the session cache 0 client connects (SSL_connect()) 0 client renegotiates (SSL_connect()) 0 client connects that finished 16 server accepts (SSL_accept()) 0 server renegotiates (SSL_accept()) 16 server accepts that finished 3 session cache hits 0 session cache misses 0 session cache timeouts 0 callback cache hits 0 cache full overflows (128 allowed) --- no client certificate available 0x5 RSA and AES encryption performace \u00b6 We use the following commands to evaluate RSA encryption and decryption: # generate private key openssl genrsa -out private-rsa.pem 1024 # generate public key openssl genrsa -in private-rsa.pem -pubout -out public-rsa.pem # encrypt using public key openssl rsautl -encrypt -pubin -inkey public-rsa.pem -in message.txt -out message_enc.txt # decrypt using private key openssl rsautl -decrypt -inkey public-rsa.pem -in message_enc.txt -out message_dec.txt Commands for AES encrytption and decryption TODO","title":"Public Key Cryptography and PKI"},{"location":"seedlabs/public-key-cryptography-and-pki/notes/#public-key-cryptography-and-pki","text":"","title":"Public-Key Cryptography and PKI"},{"location":"seedlabs/public-key-cryptography-and-pki/notes/#questions-to-answer","text":"What is a Certificate Authority (CA), and how to become one? How to create a certificate for a customer? (i.e. www.ruihan.org) How PKI is used for web site authentication and encryption? What are the necessary files of such an infrastructure?","title":"Questions to answer"},{"location":"seedlabs/public-key-cryptography-and-pki/notes/#0x1-become-a-certificate-authority-ca","text":"CA is a trusted entity to issue digital certificates. The digital certificate certifies the ownership of a public key by the named subject of the certificate. Root CAs are a number of trusted commercial CAs. (VeriSign is one of the largest) However, we are not obligate to the commercial world, we are free to become a root CA, and then use this root CA to issue certificates to others. Remember that the root CAs are self-signed. root CA's certificates are pre-loaded into most OS, browsers, and other software that using PKI. Root CA's certificates are unconditionally trusted. To create a root CA, we use the following openssl command to generate a self-signed certificate for the CA. seed@ubuntu:~/SEED-Labs/PKI-Lab$ openssl req -new -x509 -keyout ca.key -out ca.crt -config openssl.cnf Generating a 1024 bit RSA private key ..........................++++++ ..++++++ unable to write 'random state' writing new private key to 'ca.key' Enter PEM pass phrase: Verifying - Enter PEM pass phrase: ----- You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter '.', the field will be left blank. ----- Country Name (2 letter code) [AU]:US State or Province Name (full name) [Some-State]:CA Locality Name (eg, city) []:San Jose Organization Name (eg, company) [Internet Widgits Pty Ltd]:PKILabServer Organizational Unit Name (eg, section) []: Common Name (e.g. server FQDN or YOUR name) []:PKILabServer.com Email Address []:rui@PKILabServer.com [11/17/2016 11:14] seed@ubuntu:~/SEED-Labs/PKI-Lab$ The Configure File openssl.conf is used by three OpenSSL command: ca , req , and x509 . To get a glimpse of it's purporse, here is partial of the openssl.conf (from [CA_Default} section). dir = ./demoCA # Where everything is kept certs = $dir/certs # Where the issued certs are kept crl_dir = $dir/crl # Where the issued crl are kept new_certs_dir = $dir/newcerts # default place for new certs database = $dir/index.txt # database index file serial = $dir/serial # The current serial number","title":"0x1 Become a Certificate Authority (CA)"},{"location":"seedlabs/public-key-cryptography-and-pki/notes/#0x2-create-a-certificate-for-pkilabservercom","text":"When one becomes a certificate autority(CA), he/she is ready to sign digital certificates for his/her customers. PKILabServer.com is a company that tries to get a digital certificate from a CA. He needs to do the following steps: Generate RSA key pairs Generate a certificate signing request (CSR) Generate a certificate","title":"0x2 Create a Certificate for PKILabServer.com"},{"location":"seedlabs/public-key-cryptography-and-pki/notes/#0x21-generate-rsa-key-paris","text":"This step could be done using OpenSSL command line tool like this: seed@ubuntu:~/SEED-Labs/PKI-Lab$ openssl genrsa -aes128 -out server.key 1024 Generating RSA private key, 1024 bit long modulus ....++++++ .............++++++ unable to write 'random state' e is 65537 (0x10001) Enter pass phrase for server.key: Verifying - Enter pass phrase for server.key: [11/17/2016 11:16] seed@ubuntu:~/SEED-Labs/PKI-Lab$ The generated keys will be stored in the file server.key . This file is encrypted as indicated by the option -ase128 , which means using AES-128 encrytpion algorithm. The effect of this is you have to enter a password when executing the above command. To see the plan text content of the server.key file, you can run the following command: seed@ubuntu:~/SEED-Labs/PKI-Lab$ openssl rsa -in server.key -text Enter pass phrase for server.key: Private-Key: (1024 bit) modulus: 00:e1:23:f9:75:2d:2c:d9:21:36:6f:62:d6:28:6a: 2c:c6:1a:3f:d0:77:c2:8c:e0:49:1a:95:b8:99:6d: 7f:15:cb:db:9c:42:6e:a9:c7:fd:af:cd:94:3e:b5: d3:48:1a:14:11:ca:a2:63:d6:27:32:bc:db:75:d7: 5e:05:9a:8c:2b:7d:0d:fc:6f:61:77:69:c3:37:6b: 94:a3:d0:5b:70:0d:80:9a:89:ea:10:04:cd:6f:4a: 72:b9:e7:d6:1b:fa:ff:25:a1:10:0e:05:d5:14:c7: 34:97:78:55:0b:c7:ed:c4:16:7c:0b:fa:df:46:dd: 7f:35:00:7f:72:21:02:38:51 publicExponent: 65537 (0x10001) privateExponent: 00:bb:06:91:72:1f:c7:03:d0:ad:51:b6:7f:45:2f: 7e:66:4a:e3:fc:1f:a0:84:3a:c5:3f:ca:64:81:40: 0e:b4:62:96:da:a5:ed:1c:29:40:ba:cc:42:7c:1d: bb:98:47:e1:a8:2d:f6:dc:8c:c8:1f:43:f7:e3:5a: b7:86:5c:2e:57:5a:ea:fa:8b:48:28:6a:4c:35:6b: fe:80:83:1f:9b:44:80:73:98:62:e2:bc:7e:e1:f0: 15:dd:74:dc:05:e0:5d:a5:59:a4:71:5f:96:13:e5: 8f:a6:4c:c0:31:50:ca:b1:b9:e4:92:de:e1:e9:2a: 2b:3a:2e:07:1c:fb:dc:ac:01 prime1: 00:f9:bf:6b:f0:28:15:b5:ba:2e:5f:c1:62:63:d7: 53:b0:ab:9c:3f:53:e1:9b:76:1c:12:fb:b0:61:c4: 34:fa:28:d7:94:d2:d3:d9:fe:e5:84:57:60:77:23: af:82:75:50:d6:0c:2e:2e:23:55:82:8c:6f:5c:b4: 5a:2f:77:82:b1 prime2: 00:e6:c6:d9:bf:57:3b:a4:1e:b4:62:c9:fd:5f:e9: d8:cc:d7:49:9e:13:db:99:2c:99:2e:7e:78:0a:c2: b1:b4:c4:2f:08:fc:76:89:e2:60:12:7f:b7:47:bb: d2:98:1e:03:99:42:d7:ec:ab:0d:55:52:95:5e:b8: 77:c5:55:37:a1 exponent1: 63:09:e2:fe:f0:96:73:63:6a:a2:74:68:d5:18:fd: ca:30:b3:9c:75:62:21:29:3c:46:d6:e2:82:52:b1: 83:86:90:bf:26:bf:f6:51:db:a8:98:91:db:8d:1e: 3b:88:d4:4e:9d:b3:ee:7d:fa:99:f3:a0:f1:cd:5f: 7a:35:55:71 exponent2: 00:b9:05:4e:48:80:a8:b9:71:40:90:3f:7f:5b:a9: 81:7e:e1:50:0d:63:c0:58:f3:0f:b0:de:06:62:22: 2c:15:50:80:a1:44:bf:c8:d6:6c:ce:08:05:2a:86: e4:55:bf:22:85:7a:b8:e0:ef:56:d6:44:4a:ab:51: 5b:fd:22:d0:61 coefficient: 3d:ec:20:c8:23:3e:d9:f3:88:8f:03:9d:1b:57:c8: 9d:87:14:83:a2:a8:2c:bc:cc:e7:dd:2d:c4:ef:74: 2a:21:2b:5a:ef:fb:79:49:f0:bc:6f:4e:d3:0e:f7: d2:48:af:b1:12:ae:43:e6:1b:03:bb:f2:18:f1:61: 8b:7a:1e:8f writing RSA key -----BEGIN RSA PRIVATE KEY----- MIICXQIBAAKBgQDhI/l1LSzZITZvYtYoaizGGj/Qd8KM4EkalbiZbX8Vy9ucQm6p x/2vzZQ+tdNIGhQRyqJj1icyvNt1114FmowrfQ38b2F3acM3a5Sj0FtwDYCaieoQ BM1vSnK559Yb+v8loRAOBdUUxzSXeFULx+3EFnwL+t9G3X81AH9yIQI4UQIDAQAB AoGBALsGkXIfxwPQrVG2f0UvfmZK4/wfoIQ6xT/KZIFADrRiltql7RwpQLrMQnwd u5hH4agt9tyMyB9D9+Nat4ZcLlda6vqLSChqTDVr/oCDH5tEgHOYYuK8fuHwFd10 3AXgXaVZpHFflhPlj6ZMwDFQyrG55JLe4ekqKzouBxz73KwBAkEA+b9r8CgVtbou X8FiY9dTsKucP1Phm3YcEvuwYcQ0+ijXlNLT2f7lhFdgdyOvgnVQ1gwuLiNVgoxv XLRaL3eCsQJBAObG2b9XO6QetGLJ/V/p2MzXSZ4T25ksmS5+eArCsbTELwj8doni YBJ/t0e70pgeA5lC1+yrDVVSlV64d8VVN6ECQGMJ4v7wlnNjaqJ0aNUY/cows5x1 YiEpPEbW4oJSsYOGkL8mv/ZR26iYkduNHjuI1E6ds+59+pnzoPHNX3o1VXECQQC5 BU5IgKi5cUCQP39bqYF+4VANY8BY8w+w3gZiIiwVUIChRL/I1mzOCAUqhuRVvyKF erjg71bWREqrUVv9ItBhAkA97CDIIz7Z84iPA50bV8idhxSDoqgsvMzn3S3E73Qq ISta7/t5SfC8b07TDvfSSK+xEq5D5hsDu/IY8WGLeh6P -----END RSA PRIVATE KEY----- [11/17/2016 11:57] seed@ubuntu:~/SEED-Labs/PKI-Lab$ The password you entered works like a key to decrypt the ciphertext in the file.","title":"0x21 Generate RSA Key Paris"},{"location":"seedlabs/public-key-cryptography-and-pki/notes/#0x22-generate-a-certificate-signing-request-csr","text":"To request a certificate, the company need to include its public key in a certificate signing request (CSR) and send the CSR to the certificate authority. The command to generate the CSR using OpenSSL is: seed@ubuntu:~/SEED-Labs/PKI-Lab$ openssl req -new -key server.key -out server.csr -config openssl.cnf Enter pass phrase for server.key: You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter '.', the field will be left blank. ----- Country Name (2 letter code) [AU]:US State or Province Name (full name) [Some-State]:CA Locality Name (eg, city) []:San Jose Organization Name (eg, company) [Internet Widgits Pty Ltd]:PKILabServer Organizational Unit Name (eg, section) []: Common Name (e.g. server FQDN or YOUR name) []:PKILabServer.com Email Address []:admin@PKILabServer.com Please enter the following 'extra' attributes to be sent with your certificate request A challenge password []:server.csr An optional company name []: [11/17/2016 11:19] seed@ubuntu:~/SEED-Labs/PKI-Lab$ Note that this command is quite similar to the one we used in creating a self-signed certificate for the certificate authority previously. The only difference is the -x509 option.","title":"0x22 Generate a certificate signing request (CSR)"},{"location":"seedlabs/public-key-cryptography-and-pki/notes/#0x23-generate-a-certificate","text":"The CSR file needs to have the CA's signature to form a certificate. When received the CSR file, CA will first verify the identity information in the CSR. In reality, CA might contact the company through physical channel to do the verification. For example, through traditional mails or letters. For this experiment, we can assume we are the CA and will generate the certificate for the company. The following command turns the certificate signing request ( server.csr ) into an X509 certificate ( server.crt ), using the CA's ca.crt and ca.key : seed@ubuntu:~/SEED-Labs/PKI-Lab$ openssl ca -in server.csr -out server.crt -cert ca.crt -keyfile ca.key -config openssl.cnf Using configuration from openssl.cnf Enter pass phrase for ca.key: Check that the request matches the signature Signature ok Certificate Details: Serial Number: 4096 (0x1000) Validity Not Before: Nov 17 19:21:33 2016 GMT Not After : Nov 17 19:21:33 2017 GMT Subject: countryName = US stateOrProvinceName = CA localityName = San Jose organizationName = PKILabServer commonName = PKILabServer.com emailAddress = admin@PKILabServer.com X509v3 extensions: X509v3 Basic Constraints: CA:FALSE Netscape Comment: OpenSSL Generated Certificate X509v3 Subject Key Identifier: B4:F1:39:2A:56:C6:1F:1F:9C:49:AF:0B:45:FC:FD:79:66:90:F2:07 X509v3 Authority Key Identifier: keyid:1D:11:39:21:1C:E2:0D:8B:14:34:B8:36:C4:F5:34:27:09:9D:E8:7A Certificate is to be certified until Nov 17 19:21:33 2017 GMT (365 days) Sign the certificate? [y/n]:y 1 out of 1 certificate requests certified, commit? [y/n]y Write out database with 1 new entries Data Base Updated unable to write 'random state' [11/17/2016 11:22] seed@ubuntu:~/SEED-Labs/PKI-Lab$ If OpenSSL refuses to generate certificates, it is very likely that the names in your requests do not match with those of CA. (yes, think about you are creating a root CA for your company, and use it to sign certificates for different departments.) The matching rules are specified in the configuration file (at the [policy match] section). You can change the names of your requests to comply with the policy, or you can change the policy. The configuration file also includes another policy (called policy_anything ), which is less restrictive. You can choose that policy by changing the following line: \"policy = policy_match\" change to \"policy = policy_anything\".","title":"0x23 Generate a certificate"},{"location":"seedlabs/public-key-cryptography-and-pki/notes/#0x3-use-pki-for-web-sites","text":"Following the lab manual didn't work out. While adding the ca.crt to Firefox, I run into the following error that the format isn't supported by Firefox. Failed to decode the file. Either it is not in PKCS #12 format, has been corrupted, or the password you entered was incorrect. Find this stackoverflow post . We should use the following command to combine the server.crt with the ca.crt . openssl pkcs12 -export -in server.crt -inkey server.key -out PKILabServerFirefox.pfx -certfile ca.crt Finally, we can import PKILabServerFirefox.pfx to Firefox and right click the certificate and select \"Edit Trust->this certificate can identify websites\"","title":"0x3 Use PKI for Web Sites"},{"location":"seedlabs/public-key-cryptography-and-pki/notes/#0x4-test-the-certificate","text":"start a openssl simple webserver: seed@ubuntu:~/SEED-Labs/PKI-Lab$ cp server.key server.pem seed@ubuntu:~/SEED-Labs/PKI-Lab$ cat server.crt >> server.pem seed@ubuntu:~/SEED-Labs/PKI-Lab$ openssl s_server -cert server.pem -www -accpet 6666 Now navigate to https://www.pkilabserver.com:6666 , the security verification is passed and show the openssl web server default page. s_server -cert server.pem -www -accept 3633 Ciphers supported in s_server binary TLSv1/SSLv3:ECDHE-RSA-AES256-GCM-SHA384TLSv1/SSLv3:ECDHE-ECDSA-AES256-GCM-SHA384 TLSv1/SSLv3:ECDHE-RSA-AES256-SHA384 TLSv1/SSLv3:ECDHE-ECDSA-AES256-SHA384 TLSv1/SSLv3:ECDHE-RSA-AES256-SHA TLSv1/SSLv3:ECDHE-ECDSA-AES256-SHA TLSv1/SSLv3:SRP-DSS-AES-256-CBC-SHA TLSv1/SSLv3:SRP-RSA-AES-256-CBC-SHA TLSv1/SSLv3:SRP-AES-256-CBC-SHA TLSv1/SSLv3:DHE-DSS-AES256-GCM-SHA384 TLSv1/SSLv3:DHE-RSA-AES256-GCM-SHA384TLSv1/SSLv3:DHE-RSA-AES256-SHA256 TLSv1/SSLv3:DHE-DSS-AES256-SHA256 TLSv1/SSLv3:DHE-RSA-AES256-SHA TLSv1/SSLv3:DHE-DSS-AES256-SHA TLSv1/SSLv3:DHE-RSA-CAMELLIA256-SHA TLSv1/SSLv3:DHE-DSS-CAMELLIA256-SHA TLSv1/SSLv3:ECDH-RSA-AES256-GCM-SHA384 TLSv1/SSLv3:ECDH-ECDSA-AES256-GCM-SHA384TLSv1/SSLv3:ECDH-RSA-AES256-SHA384 TLSv1/SSLv3:ECDH-ECDSA-AES256-SHA384 TLSv1/SSLv3:ECDH-RSA-AES256-SHA TLSv1/SSLv3:ECDH-ECDSA-AES256-SHA TLSv1/SSLv3:AES256-GCM-SHA384 TLSv1/SSLv3:AES256-SHA256 TLSv1/SSLv3:AES256-SHA TLSv1/SSLv3:CAMELLIA256-SHA TLSv1/SSLv3:PSK-AES256-CBC-SHA TLSv1/SSLv3:ECDHE-RSA-DES-CBC3-SHA TLSv1/SSLv3:ECDHE-ECDSA-DES-CBC3-SHA TLSv1/SSLv3:SRP-DSS-3DES-EDE-CBC-SHA TLSv1/SSLv3:SRP-RSA-3DES-EDE-CBC-SHA TLSv1/SSLv3:SRP-3DES-EDE-CBC-SHA TLSv1/SSLv3:EDH-RSA-DES-CBC3-SHA TLSv1/SSLv3:EDH-DSS-DES-CBC3-SHA TLSv1/SSLv3:ECDH-RSA-DES-CBC3-SHA TLSv1/SSLv3:ECDH-ECDSA-DES-CBC3-SHA TLSv1/SSLv3:DES-CBC3-SHA TLSv1/SSLv3:PSK-3DES-EDE-CBC-SHA TLSv1/SSLv3:ECDHE-RSA-AES128-GCM-SHA256 TLSv1/SSLv3:ECDHE-ECDSA-AES128-GCM-SHA256TLSv1/SSLv3:ECDHE-RSA-AES128-SHA256 TLSv1/SSLv3:ECDHE-ECDSA-AES128-SHA256TLSv1/SSLv3:ECDHE-RSA-AES128-SHA TLSv1/SSLv3:ECDHE-ECDSA-AES128-SHA TLSv1/SSLv3:SRP-DSS-AES-128-CBC-SHA TLSv1/SSLv3:SRP-RSA-AES-128-CBC-SHA TLSv1/SSLv3:SRP-AES-128-CBC-SHA TLSv1/SSLv3:DHE-DSS-AES128-GCM-SHA256TLSv1/SSLv3:DHE-RSA-AES128-GCM-SHA256 TLSv1/SSLv3:DHE-RSA-AES128-SHA256 TLSv1/SSLv3:DHE-DSS-AES128-SHA256 TLSv1/SSLv3:DHE-RSA-AES128-SHA TLSv1/SSLv3:DHE-DSS-AES128-SHA TLSv1/SSLv3:DHE-RSA-SEED-SHA TLSv1/SSLv3:DHE-DSS-SEED-SHA TLSv1/SSLv3:DHE-RSA-CAMELLIA128-SHA TLSv1/SSLv3:DHE-DSS-CAMELLIA128-SHA TLSv1/SSLv3:ECDH-RSA-AES128-GCM-SHA256TLSv1/SSLv3:ECDH-ECDSA-AES128-GCM-SHA256 TLSv1/SSLv3:ECDH-RSA-AES128-SHA256 TLSv1/SSLv3:ECDH-ECDSA-AES128-SHA256 TLSv1/SSLv3:ECDH-RSA-AES128-SHA TLSv1/SSLv3:ECDH-ECDSA-AES128-SHA TLSv1/SSLv3:AES128-GCM-SHA256 TLSv1/SSLv3:AES128-SHA256 TLSv1/SSLv3:AES128-SHA TLSv1/SSLv3:SEED-SHA TLSv1/SSLv3:CAMELLIA128-SHA TLSv1/SSLv3:PSK-AES128-CBC-SHA TLSv1/SSLv3:ECDHE-RSA-RC4-SHA TLSv1/SSLv3:ECDHE-ECDSA-RC4-SHA TLSv1/SSLv3:ECDH-RSA-RC4-SHA TLSv1/SSLv3:ECDH-ECDSA-RC4-SHA TLSv1/SSLv3:RC4-SHA TLSv1/SSLv3:RC4-MD5 TLSv1/SSLv3:PSK-RC4-SHA TLSv1/SSLv3:EDH-RSA-DES-CBC-SHA TLSv1/SSLv3:EDH-DSS-DES-CBC-SHA TLSv1/SSLv3:DES-CBC-SHA --- Reused, TLSv1/SSLv3, Cipher is ECDHE-RSA-AES256-SHA SSL-Session: Protocol : TLSv1 Cipher : ECDHE-RSA-AES256-SHA Session-ID: 124BFE9E98B4DF8E5DC0CC87DEB16DDE615FEC0B63A5FD687CD7033012639DD1 Session-ID-ctx: 01000000 Master-Key: A23B112D5A56B98709D5C140EEC5876155C6023CF8D5DCD81858B1EFF3B4E9CE8723E2D9703C217D8C517A762820B32C Key-Arg : None PSK identity: None PSK identity hint: None SRP username: None Start Time: 1479414975 Timeout : 300 (sec) Verify return code: 0 (ok) --- 0 items in the session cache 0 client connects (SSL_connect()) 0 client renegotiates (SSL_connect()) 0 client connects that finished 16 server accepts (SSL_accept()) 0 server renegotiates (SSL_accept()) 16 server accepts that finished 3 session cache hits 0 session cache misses 0 session cache timeouts 0 callback cache hits 0 cache full overflows (128 allowed) --- no client certificate available","title":"0x4 Test the certificate"},{"location":"seedlabs/public-key-cryptography-and-pki/notes/#0x5-rsa-and-aes-encryption-performace","text":"We use the following commands to evaluate RSA encryption and decryption: # generate private key openssl genrsa -out private-rsa.pem 1024 # generate public key openssl genrsa -in private-rsa.pem -pubout -out public-rsa.pem # encrypt using public key openssl rsautl -encrypt -pubin -inkey public-rsa.pem -in message.txt -out message_enc.txt # decrypt using private key openssl rsautl -decrypt -inkey public-rsa.pem -in message_enc.txt -out message_dec.txt Commands for AES encrytption and decryption TODO","title":"0x5 RSA and AES encryption performace"}]}