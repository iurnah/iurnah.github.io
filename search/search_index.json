{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to RUIHAN.ORG \u00b6 Books \u00b6 Course Notes \u00b6 Leetcode \u00b6 Research \u00b6 SEED Labs \u00b6 System Design \u00b6 C #include <stdio.h> int main ( void ) { printf ( \"Hello world! \\n \" ); return 0 ; } C++ #include <iostream> int main ( void ) { std :: cout << \"Hello world!\" << std :: endl ; return 0 ; } \"\"\"This is comments\"\"\" def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] > items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ] def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] > items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ]","title":"Home"},{"location":"#welcome-to-ruihanorg","text":"","title":"Welcome to RUIHAN.ORG"},{"location":"#books","text":"","title":"Books"},{"location":"#course-notes","text":"","title":"Course Notes"},{"location":"#leetcode","text":"","title":"Leetcode"},{"location":"#research","text":"","title":"Research"},{"location":"#seed-labs","text":"","title":"SEED Labs"},{"location":"#system-design","text":"C #include <stdio.h> int main ( void ) { printf ( \"Hello world! \\n \" ); return 0 ; } C++ #include <iostream> int main ( void ) { std :: cout << \"Hello world!\" << std :: endl ; return 0 ; } \"\"\"This is comments\"\"\" def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] > items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ] def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] > items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ]","title":"System Design"},{"location":"books/","text":"Books \u00b6 Accelerated C++ Mining Massive Datasets Dive into Deep Learning","title":"Index"},{"location":"books/#books","text":"Accelerated C++ Mining Massive Datasets Dive into Deep Learning","title":"Books"},{"location":"books/accelerated-cpp/notes/","text":"Accelerated C++ Reading Notes \u00b6 Chapter 0 \u00b6 Expression -> results + side effects Every operand has a type std::out has type std::ostream << is left-associative, mean: (std::cout << \"Hello, world!\") << std::endl int main(){} return 0 if return sccessful, otherwise return failure. exercise 0-7 tell us comments like this wouldn't work /*comments1 /*comment2*/comment3*/ is illigle. exercise 0-8 tell us comments like this would work fine // This is a comment that extends over several lines // by using // at the beginning of each line instead of using /* // or */ to delimit comments. Chapter 1 \u00b6 >> begins by discarding whitespace chars (space, tab, backspace, or the end of line) from the input, then reads chars into a virable until it encounters another whitespace character or end-of-file. input-output library saves its output in an internal data structure called a buffer, which it uses to optimize output operations. it use the buffer to accumulate the characters to be written, and flushes the buffer, only when necessary. 3 events cause the flush: when it is full; when it is asked to read from the std input; when we explicitly say to do so; How to print a framed string? std::string constructor: const std::string greeting(greeting.size(), ' ') will generate a whitespace string with length of greeting.size(). character literal should be enclosed by ' ' , string literal should be enclosed by \" \" . Built-in type char and wchar_t , which is big enough for holding characters for languages such as Chinese. Chapter 2 \u00b6 while statement, loop invariant : which is a property that we assert will be true about a while each time it is about to test its condition. i.e. write r line so far // invariant: we have written r rows so far int r = 0 ; // setting r to 0 makes the invariant true while ( r != rows ) { // we can assume that the invariant is true here // writing a row of output makes the invariant false std :: cout << std :: endl ; // incrementing r makes the invariant true again ++ r ; } // we can conclude that the invariant is true here The std::string::size_type type for strings length or . This is to protect the int from being overflow if we have an arbitrary long input. Asymmetric range [n, m) is better then a symmetric range [n, m]. always start the range with 0 such as [0, n) [n, m) have m-n elements, [n, m] have m-n+1 elements. empty range i.e. [n,n) . modulo operation equivalent : x % y <==> x - ((x/y) * y) pay attention to the problem 2-4, adding code to ensure the invariant for outer while. Chapter 3 \u00b6 double is even faster than float, double has at least 15 significant digits. float has at least 6 digits. while(cin >> x) is equal to cin >> x; while(cin) , because >> operator return its left operand. cin is istream type, using cin as a condition is equivalent to testing whether the last attempt to read from cin was successful. unsuccessful read by cin : We might have reached the end of the input file. We might have encountered input that is incompatible with the type of the variable that we are trying to read, such as might happen if we try to read an int and find something that isn't a number. The system might have detected a hardware failure on the input device. save previous precision (for later reset) and set precision to 4: streamsize prec = cout.pecision(4); . precision manipulator setprecision(3) which means keep 3 significant digits (i.e. 56.5 or 5.65). std::vector<double>::size_type is analogous to the one in string::size_type , its the type used to declare size type variable such as array size or string size. It usually typedef std::size_t string::size_type . It is guaranteed to be able to hold the number of elements in the largest possible vector or string . sort function in <algorithm> header prototype: sort(homework.begin(), homework.end()) , sort in place vector class provide two member functions: begin() and end() . calculate the median of a sorted vector homework: vec_sz mid = size / 2 ; double median ; median = size % 2 == 0 ? ( homework [ mid ] + homework [ mid -1 ]) / 2 : homework [ mid ]; vec.end() return a value that denotes one past the last elelment in v. streamsize The type of the value expected by setprecision() and returned by precision() . Defined in <ios> . The return value of the vector<int>::type_size is unsigned integral number. Doing operation with it could not possibly generate negative value, i.e. if vec.size() is 5, vec.size() - 6 is not negative, it will be positive. Chapter 4 (organize program .h and .cc files, iostream as a argument, exception handling basics) \u00b6 When a program throws an exception, the program stop at the part of the program in which the throw appears. appears, and passes to another part of the program, along with an exception object , which contains information that the caller can use to act on the exception. Domain error: throw domain_error(\"median of an empty vector\") ; Defined in <stdexcept> , it is used in reporting that a function's argument is outside the set of values that the function can accept. domain error is one of logic_error in <stdexcept> , there are other type of exception: runtime_error , which includes overflow_error , underflow_error , etc. runtime_error . & means a reference or an alias. const when define a reference means we will promise not modify the variable. For example, vector < doubel > homework ; const vector < doubel >& chw = homework ; //chw is a READ ONLY synonym for homework A reference to reference is the same as reference to the original variable. when define non const reference, we have to make sure the original variable or reference isn't declared const . Otherwise, it will be illigel. i.e. with above definition, we cannot do this vector<doubel>& hw2=chw; because chw is const when defined. a const argument could take a const parameter. iostream as a parameter to a function: (alwasy keep in mind that a iostream is a type , it has other properties such as a vector or int have.). Notice the returned value is in , which is passed in as a reference parameter. istream & read_hw ( istream & in , vector < double >& hw ) { // statement that modify the input parameters return in ; } // the return of istream allow us to do the following if ( read_hw ( in , homework )) { } // othewise, we have to read_hw ( in , homework ); if ( in ) { } The \"lvalue\" We must pass an lvalue argument to a reference parameter. An \"lvalue\" is a value that denotes a nontemporary object. For example, a variable is an lvalue , as is a reference, or the result of calling a function that returns a reference. An expression that generates an arithmetic value, such as sum / count , is not an lvalue. member functions istream.clear() . This is to ensure the eof or non-valid input data will not effect reading the next data. Alwasy run cin.clear() , before we try to read again . pass by value: vector<double> vec : this will copy the argument, the original will not be modified. pass by reference: vector<double>& vec : this will not copy the argument, but will modify the original argument. This is good convention for object as a parameter, because copy a object have overhead. pass by const reference: const vector<double>& vec : this will not copy the argument as well as promise not to modify the passed argument. try {} catch {} clause, we normally break down to multiple statements in the try clause, because we want to avoid multiple side effect. For example, try { double final_grade = grade ( midterm , final , homework ); streamsize prec = cout . precision (); cout << \"Your final grade is \" << setprecision ( 3 ) << final_grade << setprecision ( prec ) << endl ; } catch ( domain_error ) { cout << endl << \"You must enter your grades. \" \"Please try again.\" << endl ; return 1 ; } is better then written as try { streamsize prec = cout . precision (); cout << \"Your final grade is \" << setprecision ( 3 ) << grade ( midterm , final , homework ) << setprecision ( prec ); } because the later can generate ambigious error message that not easy to debug. sort object Student_info , we have to use another form of sort(student.begin(), students.end()) with a extra parameter, which is a predicate to compare the two object. i.e. bool compare ( const Student_info & x , cosnt Student_info & y ) { return x . name < y . name ; } sort ( students . begin (), students . end (), compare ); Formatting the output, if we want to do the following Bob 88 Christopher 90 we could do this: maxlen = max ( maxlen , record . name . size ()); cout << students [ i ]. name << string ( maxlen + 1 - students [ i ]. name . size (), ' ' ); Notice the string(num, ' ') instantiate a string has num of spaces. hearder file should declare only the names that are necessary. Header files should use fully qualified names rather than using-declarations. (Avoid using namespace std ;) #include <vector> double median ( std :: vector < double > ); Avoid multiple inclusion #ifndef __THIS_HEADER_H__ #define __THIS_HEADER_H__ //your program #endif type of exceptions logic_error domain_error length_error out_of_range invalid_argument runtime_error range_error overflow_error underflow_error exceptional handling try { // code Initiates a block that might throw an exception. } catch ( t ) { // code } //real use case. try { double final_grade = grade ( students [ i ]); streamsize prec = cout . precision (); cout << setprecision ( 3 ) << final_grade << setprecision ( prec ); } catch ( domain_error e ) { cout << e . what (); } Concludes the try block and handles exceptions that match the type t . The code following the catch performs whatever action is appropriate to handle the exception reported in t . throw e ; Terminates the current function; throws the value e back to the caller. e.what() : return a value that report on what happened to cause the error. str.width([n]) and std::setw(n) both used to set the output width. The example program from this chapter is worth of keeping here for references. There is a lot information included in it. //Calculate the grade for many students, including reading the data in, //how to sort according there name and how to format the out put in a nice printing, ect. #include <algorithm> #include <iomanip> #include <iostream> #include <stdexcept> #include <string> #include <vector> using std :: cin ; using std :: cout ; using std :: domain_error ; using std :: endl ; using std :: istream ; using std :: ostream ; using std :: setprecision ; using std :: sort ; using std :: streamsize ; using std :: string ; using std :: vector ; // compute the median of a `vector<double>' // note that calling this function copies the entire argument `vector' double median ( vector < double > vec ) { #ifdef _MSC_VER typedef std :: vector < double >:: size_type vec_sz ; #else typedef vector < double >:: size_type vec_sz ; #endif vec_sz size = vec . size (); if ( size == 0 ) throw domain_error ( \"median of an empty vector\" ); sort ( vec . begin (), vec . end ()); vec_sz mid = size / 2 ; return size % 2 == 0 ? ( vec [ mid ] + vec [ mid -1 ]) / 2 : vec [ mid ]; } // compute a student's overall grade from midterm and final exam grades and homework grade double grade ( double midterm , double final , double homework ) { return 0.2 * midterm + 0.4 * final + 0.4 * homework ; } // compute a student's overall grade from midterm and final exam grades // and vector of homework grades. // this function does not copy its argument, because `median' does so for us. double grade ( double midterm , double final , const vector < double >& hw ) { if ( hw . size () == 0 ) throw domain_error ( \"student has done no homework\" ); return grade ( midterm , final , median ( hw )); } // read homework grades from an input stream into a `vector<double>' istream & read_hw ( istream & in , vector < double >& hw ) { if ( in ) { // get rid of previous contents hw . clear (); // read homework grades double x ; while ( in >> x ) hw . push_back ( x ); // clear the stream so that input will work for the next student in . clear (); } return in ; } int main () { // ask for and read the student's name cout << \"Please enter your first name: \" ; string name ; cin >> name ; cout << \"Hello, \" << name << \"!\" << endl ; // ask for and read the midterm and final grades cout << \"Please enter your midterm and final exam grades: \" ; double midterm , final ; cin >> midterm >> final ; // ask for the homework grades cout << \"Enter all your homework grades, \" \"followed by end-of-file: \" ; vector < double > homework ; // read the homework grades read_hw ( cin , homework ); // compute and generate the final grade, if possible try { double final_grade = grade ( midterm , final , homework ); streamsize prec = cout . precision (); cout << \"Your final grade is \" << setprecision ( 3 ) << final_grade << setprecision ( prec ) << endl ; } catch ( domain_error ) { cout << endl << \"You must enter your grades. \" \"Please try again.\" << endl ; return 1 ; } return 0 ; } Chapter 5 (sequential containers (vector, list) and analyzing strings) \u00b6 5.1 \u00b6 write a function extract_fails() to seperate the students that failed the course. The ideas is to use two seperate vector to hold the ones that passed and the ones that failed. To make it better, we only use one vector fails to hold the failed students, and erase them from the original vector. however, be cautions in using the erase memeber function for vectors. 1. the indexes of element after the removed element will change. 2. the size will change. 3. efficiency problem. Introduced the necessity of using iterator . Briefly, it is for efficiency optimization. The indexing is random access, which is more expensive to maintain the data structure properties, such as for vector. iterator allow us to separate the data access manner (sequential v.s. random) in a container, so as to implement different container to cope with a different need. All this work is because of efficiency concerns. 5.2 \u00b6 Beside providing access or modify operations, a iterator is able to restrict the available operations in ways that correspond to what the iterator can handle efficiently. Generally, two type of iterators: const_iterator and iterator . when we do vector<student>::const_iterator iter = S.begin() , there is an explicit type conversion happened because S.begin() is a type of iterator . The usage of iterator vector<int>::iterator iter; , either iter->name or (*iter).name . 5.5 \u00b6 vector and list differ in that if you call erase(iter) member function of the container. vector will invalidate all the iterators following iter in the vector. Even this is the case, we can use erase to delete a element from the container. i.e. iter = students.erase(iter); , iter will point to the next element of the removed element. Similarly, call push_back member function will invalidate all the iters of the vector. while for list container, call erase(iter) only invalidate the iter erased, not others. and call push_back will not invalidate other iterators. Because list doesn't support random access. We cannot use the sort() function from <algorithm> . Instead we have to use the member function that optimized for list container. 5.8 \u00b6 Use inset function to do vertical concatenation ret.insert(ret.end(), bottom.begin(), bottom.end()) Notes \u00b6 students.erase(students.begin()+i) , remove the ith object in the container students . iterator properties: Identifies a container and an element in the container Lets us examine the value stored in that element Provides operations for moving between elements in the container Restricts the available operations in ways that correspond to what the container can handle efficiently iterator types: container-type::iterator : to change the container value container-type::const_iterator : to only read the container value container-type::reverse_iterator container-type::const_reverse_iterator *iter return a lvalue. we can replace (*iter).name with iter->name students.erase(iter) will invalidate all the iterators following the elements that has been removed. After iter = students.erase(iter) , iter will point to the first element following the removed element. From vector to list . vector\u007f list optimized for fast random access optimized for fast insertion and deletion Using push_back to append an element to a vector invalidates all iterators referring to that vector. the erase and push_back operations do not invalidate iterators to other elements. list : doesn't support random access, so the STL <algorithm> library function sort() doens't apply to list string example, split a sentence into words. vector < string > split ( const string & s ) { vector < string > ret ; typedef string :: size_type string_size ; string_size i = 0 ; // invariant: we have processed characters [original value of i, i) while ( i != s . size ()) { // ignore leading blanks // invariant: characters in range [original i, current i) are all spaces while ( i != s . size () && isspace ( s [ i ])) ++ i ; // find end of next word string_size string_size j = i ; // invariant: none of the characters in range [original j, current j)is a space while ( j != s . size () && ! isspace ( s [ j ])) j ++ ; // if we found some nonwhitespace characters if ( i != j ){ // copy from s starting at i and taking j - i chars ret . push_back ( s . substr ( i , j - i )); i = j ; } } return ret ; } isspace is in the header file <cctype> isspace(c) true if c is a whitespace character. isalpha(c) true if c is an alphabetic character. isdigit(c) true if c is a digit character. isalnum(c) true if c is a letter or a digit. ispunct(c) true if c is a punctuation character. isupper(c) true if c is an uppercase letter. islower(c) true if c is a lowercase letter. toupper(c) Yields the uppercase equivalent to c tolower(c) Yields the lowercase equivalent to c In the ret.push_back(s.str(i, j-i)) , the j-i indicate a open range [s[i], s[j]) while (cin >> s) is read one work at a time, because the std::cin seperated by white spaces. It terminate until a invalid input is entered or a EOF. while (getline(cin, s)) is reading one line at a time, it return false when EOF entered or invalid chars. How to framing a word characters. How to cancatenate two vector?, we can do insert(ret.end(), bottom.begin(), bottom.end()) , note the first argument provide the iterator before which the element will be inserted. How to concatenate two pictures horizontally like the bellow pictures in case 1: this is an ************** example * this is an * to * example * illustrate * to * framing * illustrate * * framing * ************** pictures in case 2: ************** this is an * this is an * example * example * to * to * illustrate * illustrate * framing * framing * ************** vector < string > hcat ( const vector < string >& left , const vector < string >& right ) { vector < string > ret ; // add 1 to leave a space between pictures string :: size_type width1 = width ( left ) + 1 ; // indices to look at elements from left and right respectively vector < string >:: size_type i = 0 , j = 0 ; // continue until we've seen all rows from both pictures while ( i != left . size () || j != right . size ()) { // construct new string to hold characters from both pictures string s ; // copy a row from the left-hand side, if there is one if ( i != left . size ()) s = left [ i ++ ]; // pad to full width s += string ( width1 - s . size (), ' ' ); // copy a row from the right-hand side, if there is one if ( j != right . size ()) s += right [ j ++ ]; // add s to the picture we're creating ret . push_back ( s ); } return ret ; } vec.reserve(n) : Reserves space to hold n elements, but does not initialize them. This operation does not change the size of the container. It affects only the frequency with which vector may have to allocate memory in response to repeated calls to insert or push_back. c.rbegin() and c.rend() are iterator refering to the last and (one beyond) the first element in the container that grant access to the container's elements in reverse order. Chapter 6 \u00b6 Chapter 7 \u00b6 Chapter 8 \u00b6 The language feature that implements generic functions is called template functions. template header template<class T> \"instantiation\" Keyword typename , i.e. typedef typename vector<T>::size_type vec_sz; \"you must precede the entire name by typename to let the implementation know to treat the name as a type.\" The C++ standard says nothing about how implementations should manage template instantiation, so every implementation handles instantiation in its own particular way. While we cannot say exactly how your compiler will handle instantiation, there are two important points to keep in mind: The first is that for C++ implementations that follow the traditional edit-compile-link model, instantiation often happens not at compile time, but at link time. It is not until the templates are instantiated that the implementation can verify that the template code can be used with the types that were specified. Hence, it is possible to get what seem like compile-time errors at link time. The second point matters if you write your own templates: Most current implementations require that in order to instantiate a template, the definition of the template, not just the declaration, has to be accessible to the implementation. Generally, this requirement implies access to the source files that define the template, as well as the header file. How the implementation locates the source file differs from one implementation to another. Many implementations expect the header file for the template to include the source file, either directly or via a #include. The most certain way to know what your implementation expects is to check its documentation. parameter type to a generic function should keep consistent. For example, We cannot pass int and double to the following function: tmeplate < class T > T max ( const T & left , const T & right ) { return left > right ? left : right ; } Data structure indepnedence: why we write the find function as find(c.begin(), c.end(), val) ? (it is the only way to write generic functions that works on more than 1 element types) Why not write as the form c.find(val) or find(c, val) ? iterator categories: 1.Sequential read-only access (input iterator) 2.Sequential write-only access (output iterator) 3.Sequentila read-wirte access (input-output iterator) 4.Reverseible access 5.Random access \"input iterator\" - interator support \"++, ==, !=, unary *, and it->first\". We say we give find two input iterators as parameters. template < class In , class X > In find ( In begin , In end , constX & x ) { if ( begin == end || * begin == x ) return ; begin ++ ; return find ( begin , end , x ); } \"output iterator\" -interator support *dest = _value_, dest++, and ++dest template < class In , class Out > Out copy ( In begin , In end , Out dest ) { while ( begin != end ) * dest ++ = * begin ++ ; return dest ; } \"input-output iterator\" - iterator support *it, ++it, it++, (but not --it or it--), it == j, it != j, it->member template < class For , class X > void replace ( For beg , For end , const X & x , const X & y ) { while ( beg != end ){ if ( * beg == x ) * beg = y ; ++ beg ; } } \"reverse interator\" - also support --it and it-- template < class Bi > void reverse ( Bi begin , Bi end ) { while ( begin != end ) { -- end ; if ( begin != end ) swap ( * begin ++ , * end ); } } Random access - support p + n, p - n, n + p, p-q, p[n], (equivalent to *(p + n)) p < q, p > q, p <= q, and p >= q template < class Ran , class X > bool binary_search ( Ran begin , Ran end , const X & x ) { while ( begin < end ) { // find the midpoint of the range Ran mid = begin + ( end - begin ) / 2 ; // see which part of the range contains x; keep looking only in that part if ( x < * mid ) end = mid ; else if ( * mid < x ) begin = mid + 1 ; // if we got here, then *mid == x so we're done else return true ; } return false ; } off-the-end values, it always ensure the range is [begin, end). The advantage? (see section 8.2.7) Input and output iterators input iterator for copy vector < int > v ; // read ints from the standard input and append them to v copy ( istream_iterator < int > ( cin ), istream_iterator < int > (), back_inserter ( v )); ouput iterator for copy // write the elements of v each separated from the other by a space copy ( v . begin (), v . end (), ostream_iterator < int > ( cout , \" \" )); Chapter 9 \u00b6 Using the :: before the function name of a non-member function called by a member function. doubel Student_info::grad () const { return :: grade ( midterm , final , homework ); } const for member function means this member function will not change the member variable. Only const member functions may be called for const objects. We cannot call non-const functions on const objects. such as read memeber on const Student_info . When we pass a non-const object to a function that take const reference. The function will treat the object as if it were const, and compiler will only permit it to call const memeber functions. When we pass a nonconst object to a function that takes a const reference, then the function treats that object as if it were const, and the compiler will permit it to call only const members of such objects. difference of class and struct : default protection. class --> private between { and first label. explicitly define a accessor read function, string name() const { return n; } will return a copy of member variable n instead return a reference, because we don't want the user to modifiy it. The \"Synthesized constructor\" will initialized the data memebers to a value based on how the object is created. if the object is local variable, will be default-initialized (undefined). If the object is used to init a container element, the members will be value-initialized(zero). Initialization rules: If an object is of a class type that defines one or more constructors, then the appropriate constructor completely controls initialization of the objects of that class. If an object is of built-in type, then value-initializing it sets it to zero, and default-initializing it gives it an undefined value. Otherwise, the object can be only of a class type that does not define any constructors. In that case, value- or default-initializing the object value- or default-initializes each of its data members. This initialization process will be recursive if any of the data members is of a class type with its own constructor. constructor initializers Student_info::Student_info() : final(0), midterm(0){} when we create a object: the implementation allocate memory for the new object. it initializes the object, as directed by the constructor's initializer list. it executes the constructor body. The implementation initializes every data member of every object, regardless of whether the constructor initializer list mentions those members. The constructor body may change these initial values subsequently, but the initialization happens before the constructor body begins execution. It is usually better to give a member an initial value explicitly, rather than assigning to it in the body of the constructor. By initializing rather than assigning a value, we avoid doing the same work twice. Constructors with Arguments: Student_info::Student_info(istream& is) { read(is); } Chapter 10 \u00b6 All you can do with a function is to take its address or call it. Any use of function that is not a call is assumed to be taking its address. function pointer declarition: int (*fp)(int) in which fp is a function pointer, if we have another function definition: int next(int){ return n+1; } we can use it like this fp = &next or fp = next . With & or without it is essentially same. define a function pointer point to a function: vector<string>(*sp)(const string &) = split; we can call the next function such as i = (*fp)(i); or i = fp(i); calling function pointer automatically calling the function itself. function with a return value as a function pointer, can use typedef . For example: //define analysis_fp as the name of the type of an appropriate pointer typedef double ( * analysis_fp )( const vector < Student_info >& ); //get_analysis_ptr returns a pointer to an analysis function analysis_fp get_analysis_ptr (); //the alternative and most important trick that has been played in the S2E tcg components. doubel ( * get_analysis_ptr ())( const vector < Student_info >& ); function pointer as parameter to find_if , Notice the Pred can be any type as long as f(*begin) has meaningful value. bool is_negative ( int n ) { return n < 0 ; } template < class In , class Pred > In find_if ( In begin , In end , Pred f ) { while ( begin != end && ! f ( * begin )) ++ begin ; return begin ; } // call it vector < int >:: iterator i = find_if ( v . begin (), v . end (), is_negative ); <cstddef> header: size_t : unsigned type large enough to hold the size of any object. ptrdiff_t : the type of p - q , p, q are both pointer. static means only initialize once, not everytime the function calle or the object is initialized. sizeof() operator reports the results in bytes . ifstream and ofstream object doesn't like string for file path. It almost always require the name of the file to be a pointer to the initial element of a null-terminated character array. simplicity. What if the string facilities doesn't exist. historical. fstream is earlier than string facilities in c++ compatibility. easier to interface with OS file I/O, which typically use such pointers to communicate. using c_str member function for string literal. `ifstream infile(filepath.c_str()); example that read every file supplied in the commandline. int main ( int argc , char ** argv ) { int fail_count = 0 ; for ( int i = 1 ; i < argc ; i ++ ){ ifstream in ( argv [ i ]); if ( in ){ string s ; while ( getline ( in , s )) cout << s << endl ; } else { cerr << \"cannot open file \" << argv [ i ] << endl ; ++ fail_count ; } } return fail_count ; // very neat trick played here. } Three kinds of memory management automatica memory management (local variable) statically allocated memory ( static int x ) it allocate once and only once before the function contain the statement is ever called. every call to pointer_to_static will return a pointer to the same object. the pointer will be valid as long as the program runs, and invalid afterward. dynamic allocation Allocate object of type T. new T(args) i.e. int* p = new int(32); allocate a object int with initial value is 32. Chapter 11 (Implement a vector class) \u00b6 template function V.S. template class template <typename T> T Vec (T a) { // function body } and template <class T> class Vec { public: // interface private: // implementation } What it does when use new to allocate memory. (i.e. new T[n] ) allocate memory initialize the element by running the default constructor. the class T should have a default constructor. A template class type should have the control over how a object created, copied, assigned, or destroyed. explicit Vec(size_type n) { create(n); } mean using the constructor should be explicitly declared, such as Vec(5) , not vec = 5 Type names for the members. Using typedef such as typedef T* iterator . Define a overloaded operator: like define a function, the type of the operator(uniary or binary) defines how many parameters the function will have. If the operator is a function that is not a member, then the function has as many arguments as the operator has operands. The first argument is bound to the left operand; the second is bound to the right operand. If the operator is defined as a member function, then its left operand is implicitly bound to the object on which the operator is invoked. Member operator functions, therefore, take one less argument than the operator indicates. Index operator MUST be a member function. T& operator[](size_type i) { return data[i]; } , User might also want to only read the element through the index operator, so we can also define another overlaoded version const T& operator[](size_type i) const { return data[i]; } . Notice the index operator will return a reference instead of a value. implicitly copying passing by value in function parameter passing. vector<int> i; double d; d = median(i); return value from a function. ( string line; vector<string> words = split(line); ) explicitly copying assignment: vector<Student_info> vec = vs; Copy constructor: is a member function with the same name as the name of the class template < class T > class Vec { public : Vec ( const Vec & v ); { create ( v . begin (), v . end ()); } //copy constructor }; using reference because we are defining what it means by copy, so we go deep to the granuality of call by reference to avoid copying. copying object shouldn't change the original vector, so we use const. Because the copy of vector object is actually copy the pointer, the new copy of the original object contain the same data, point to the same data area. We should make sure they are not contain the same underlying storage when making copies of objects. We should do this: (note the create function hasn't been implemented yet) template < class T > class Vec { public : Vec ( const Vec & v ) { create ( v . begin (), v . end ()); } //copy constructor made a copy. } assignment operator : it must be defined as a member function.(may have multiple overloaded versions.) Assignment differs from the copy constructor in that assignment always involves obliterating an existing value (the left-hand side) and replacing it with a new value (the right-hand side). template < class T > class Vec { public : Vec & operator = ( const Vec & ); //assignment operator }; template < class T > Vec < T >& Vec < T >:: operator = ( const Vec & rhs ) { if ( & rhs != this ){ uncreate (); create ( rhs . begin (), rhs . end ()); } return * this ; //why we need '*this' here instead of 'this' } return reference uncreate and create return variable scope How to define a tempalte member function outside of the class? When we should have the T in Vec<T>& Vec<T>::operator=(const Vec& rhs) ? the oprator= have two different meanings in C++ Initialization. Such as we do vector<int> vec = v(10); or int a = 10; we are invoking the copy constructor. Initialization involves creating a new object and giving it a value at the same time. Initialization happens: In variable declarations (explicitly) For function parameters on entry to a function (implicitly) For the return value of a function on return from the function (implicitly) In constructor initializers (explicitly) Assignment, we are calling operator= . Assignment (operator=) always obliterates a previous value; initialization never does so. examples: string url_ch = \"~;/?:@=&$-_.+!*'(),\" // initialization,(constructor + copy constructor) string spaces ( url_ch . size (), ' ' ) ; // initialization string y ; // initialization y = url_ch ; // assignment, call the operator= and obliterate a previous value. //more complex ones vector < string > split ( const string & ); // function declaration vector < string > v ; // initialization v = split ( line ); // on entry, initialization of split's parameter from line; // on exit, both initialization of the return value // and assignment to v The declaration of split above is interesting because it defines a return type that is a class type. Assigning a class type return value from a function is a two-step process: First, the copy constructor is run to copy the return value into a temporary at the call site. Then the assignment operator is run to assign the value of that temporary to the left-hand operand. Constructors always control initialization. The operator= member function always controls assignment. Defalut action regarding the copy constructor, assignment operator, and destructor: rule of three: copy constructor, destructor, and assignment operator. if you defind a class, you probably need the following for copy control and assignment operators. T :: T () one or more constructors , perhaps with arguments T ::~ T () the destructor T :: T ( const T & ) the copy constructor T :: operator = ( const T & ) the assignment operator the compiler will invoke them whenever an object of our type is created, copied, assigned, or destroyed. Remember that objects may be created, copied, or destroyed implicitly. Whether implicitly or explicitly, the compiler will invoke the appropriate operation. consideration in design a vector class: constructor type definition index and size (overload operators) copy control destructor Flexible Memory Management, those functions that used to implement the create and uncreate functions. new always initialized every object by using constructor T::T() . If we want to initialized by ourselves, we have to do it twice. allocator<T> class in <memory> library. Members and non member function: T * allocate ( size_t ); void deallocate ( T * , size_t ); void construct ( T * , const T & ) ; void destroy ( T * ); template < class Out , class T > void uninitialized_fill ( Out , Out , const T & ); template < class In , class Out > Out uninitialized_copy ( In , In , Out ); Chapter 12 (Making class objects working like values) \u00b6 Chapter 13 (Inheritance) \u00b6 the derived class will not inherit the following: constuctor, assignment operator, and destructor. Keyword protected allows the derived class to access the private member of the base. derived class is constructed by the following steps: allocate memory for the entire object.(base member and derived class member.) call base constructor to initialize the base part. initialize the member of the derived class by initializer list. call constructor of the derived class. NOTE: However, it doesn't select which base constructor to run, we have to explicitly involke it. \"The derived-class constructor initializer names its base class followed by a (possibly empty) list of arguments. These arguments are the initial values to use in constructing the base- class part; they serve to select the base-class constructor to run in order to initialize the base.\" If we pass Grad* to function that take Core* , Compiler convert grad* to Core* and bind the parameter to a Core* type. Static binding V.S. Dynamic binding. \"The phrase dynamic binding captures the notion that functions may be bound at run time, as opposed to static bindings that happen at compile time.\" Virtual Function: (mainly for pointer and references, not for explicit object, because the later is bind to the function in compile time.) It come into being in the following accasion: bool compare_grade ( const Core & c1 , const Core & c2 ) { return c1 . grade () < c2 . grade (); } which function to call, it has to be decide in run time. The reason is that the parameter type const Core& can also accept a type Grade* . More examples. Core c ; Grad g ; Core * p ; Core & r = g ; c . grade (); // statically bound to Core::grade() g . grade (); // statically bound to Core::grade() p -> grade (); // dynamically bound, depending on the type of the object to which p points r . grade (); // dynamically bound, depending on the type of the object to which p points if we defind the bool compare_grade(const Core C1, const Core C2) , if we pass Grad to it, it cut down to its core part. The two grade() would be identically from Core . If we define pointer parameters, the compiler will convert Grad* to a Core* , and would bind the pointer to the Core part of the Grad object. polymorphism: one type (base type) stand for many types (by reference and poitners). \"C++ supports polymorphism through the dynamic-binding properties of virtual functions. When we call a virtual through a pointer or reference, we make a polymorphic call. The type of the reference (or pointer) is fixed, but the type of the object to which it refers (or points) can be the type of the reference (or pointer) or any type derived from it. Thus, we can potentially call one of many functions through a single type.\" virtual function must be defined, regardless of whether the program calles them. virtual destructor: usually in base not in derived class. it usually empty if not other special thing need todo. virtual properties are inherented, such as virtual function or virtual destructor, the keyword \"virtual\" only need to be defined in the base class, and no need to redeclared in derived class. virtual destructor: when you delete the heap memory using the command delete, the pointer operand for delete might be more than one class types. you have give the compiler right indication what object space to release, we use the virutal destructor to do this, for example: class Core (){ public : virutal ~ Core (){} //empty destructor is enough } In this case the delete will automatically select the synthesized approperiate destructor for base class. A virtual destructor is needed any time it is possible that an object of derived type is destroyed through a pointer to base. A virtual destructor is inherited and we don't need to add the virtual destructor to the derived class such as Grad . Programming technique: handle class. hide the pointer manipulations and encapsulate the pointer to Core . static member function. Static member functions differ from ordinary member functions in that they do not operate on an object of the class type. Unlike other member functions, they are associated with the class, not with a particular object. How to implement copy constructor? give the handle class a virtual function clone() to implement the copy constructor. another wrapper!!! class Core { friend class Student_info ; protected : virtual Core * clone () const { return new Core ( * this );} //as before. }; Notice that the copy constructor didn't defined explicitly. It is synthesized by the implementation. (default copy constructor) Ordinarily, when a derived class redefines a function from the base class, it does so exactly: the parameter list and the return type are identical. However, if the base-class function returns a pointer (or reference) to a base class, then the derived-class function can return a pointer (or reference) to a corresponding derived class. \"Finally, the objects that were allocated inside the read for the Student_info function will be automatically freed when we exit main. On exiting main, the vector will be destroyed. The destructor for vector will destroy each element in students, which will cause the destructor for Student_info to be run. When that destructor runs, it will delete each of the objects allocated in read.\" look at the following piece of code: what will happen, if you mistake on the type of the class. vector < Core > students ; Grad g ( cin ); // read a Grad students . push_back ( g ); // Store only the core part of the object. What will happen is that push_back will expect that it was given a Core object, and will construct a Core element, copying only the Core parts of the object, ignoring whatever is specific to the Grad class. We can control which function to call by specify the scope operator, such as when r is a reference to Grad, we can call the regrade function of Core. r.Core::regrade(100); keep in mind that base function is always hiden if you call the derived class function member when the two are have same form (see 13.6.2 in page 347)","title":"Accelerated C++"},{"location":"books/accelerated-cpp/notes/#accelerated-c-reading-notes","text":"","title":"Accelerated C++ Reading Notes"},{"location":"books/accelerated-cpp/notes/#chapter-0","text":"Expression -> results + side effects Every operand has a type std::out has type std::ostream << is left-associative, mean: (std::cout << \"Hello, world!\") << std::endl int main(){} return 0 if return sccessful, otherwise return failure. exercise 0-7 tell us comments like this wouldn't work /*comments1 /*comment2*/comment3*/ is illigle. exercise 0-8 tell us comments like this would work fine // This is a comment that extends over several lines // by using // at the beginning of each line instead of using /* // or */ to delimit comments.","title":"Chapter 0"},{"location":"books/accelerated-cpp/notes/#chapter-1","text":">> begins by discarding whitespace chars (space, tab, backspace, or the end of line) from the input, then reads chars into a virable until it encounters another whitespace character or end-of-file. input-output library saves its output in an internal data structure called a buffer, which it uses to optimize output operations. it use the buffer to accumulate the characters to be written, and flushes the buffer, only when necessary. 3 events cause the flush: when it is full; when it is asked to read from the std input; when we explicitly say to do so; How to print a framed string? std::string constructor: const std::string greeting(greeting.size(), ' ') will generate a whitespace string with length of greeting.size(). character literal should be enclosed by ' ' , string literal should be enclosed by \" \" . Built-in type char and wchar_t , which is big enough for holding characters for languages such as Chinese.","title":"Chapter 1"},{"location":"books/accelerated-cpp/notes/#chapter-2","text":"while statement, loop invariant : which is a property that we assert will be true about a while each time it is about to test its condition. i.e. write r line so far // invariant: we have written r rows so far int r = 0 ; // setting r to 0 makes the invariant true while ( r != rows ) { // we can assume that the invariant is true here // writing a row of output makes the invariant false std :: cout << std :: endl ; // incrementing r makes the invariant true again ++ r ; } // we can conclude that the invariant is true here The std::string::size_type type for strings length or . This is to protect the int from being overflow if we have an arbitrary long input. Asymmetric range [n, m) is better then a symmetric range [n, m]. always start the range with 0 such as [0, n) [n, m) have m-n elements, [n, m] have m-n+1 elements. empty range i.e. [n,n) . modulo operation equivalent : x % y <==> x - ((x/y) * y) pay attention to the problem 2-4, adding code to ensure the invariant for outer while.","title":"Chapter 2"},{"location":"books/accelerated-cpp/notes/#chapter-3","text":"double is even faster than float, double has at least 15 significant digits. float has at least 6 digits. while(cin >> x) is equal to cin >> x; while(cin) , because >> operator return its left operand. cin is istream type, using cin as a condition is equivalent to testing whether the last attempt to read from cin was successful. unsuccessful read by cin : We might have reached the end of the input file. We might have encountered input that is incompatible with the type of the variable that we are trying to read, such as might happen if we try to read an int and find something that isn't a number. The system might have detected a hardware failure on the input device. save previous precision (for later reset) and set precision to 4: streamsize prec = cout.pecision(4); . precision manipulator setprecision(3) which means keep 3 significant digits (i.e. 56.5 or 5.65). std::vector<double>::size_type is analogous to the one in string::size_type , its the type used to declare size type variable such as array size or string size. It usually typedef std::size_t string::size_type . It is guaranteed to be able to hold the number of elements in the largest possible vector or string . sort function in <algorithm> header prototype: sort(homework.begin(), homework.end()) , sort in place vector class provide two member functions: begin() and end() . calculate the median of a sorted vector homework: vec_sz mid = size / 2 ; double median ; median = size % 2 == 0 ? ( homework [ mid ] + homework [ mid -1 ]) / 2 : homework [ mid ]; vec.end() return a value that denotes one past the last elelment in v. streamsize The type of the value expected by setprecision() and returned by precision() . Defined in <ios> . The return value of the vector<int>::type_size is unsigned integral number. Doing operation with it could not possibly generate negative value, i.e. if vec.size() is 5, vec.size() - 6 is not negative, it will be positive.","title":"Chapter 3"},{"location":"books/accelerated-cpp/notes/#chapter-4-organize-program-h-and-cc-files-iostream-as-a-argument-exception-handling-basics","text":"When a program throws an exception, the program stop at the part of the program in which the throw appears. appears, and passes to another part of the program, along with an exception object , which contains information that the caller can use to act on the exception. Domain error: throw domain_error(\"median of an empty vector\") ; Defined in <stdexcept> , it is used in reporting that a function's argument is outside the set of values that the function can accept. domain error is one of logic_error in <stdexcept> , there are other type of exception: runtime_error , which includes overflow_error , underflow_error , etc. runtime_error . & means a reference or an alias. const when define a reference means we will promise not modify the variable. For example, vector < doubel > homework ; const vector < doubel >& chw = homework ; //chw is a READ ONLY synonym for homework A reference to reference is the same as reference to the original variable. when define non const reference, we have to make sure the original variable or reference isn't declared const . Otherwise, it will be illigel. i.e. with above definition, we cannot do this vector<doubel>& hw2=chw; because chw is const when defined. a const argument could take a const parameter. iostream as a parameter to a function: (alwasy keep in mind that a iostream is a type , it has other properties such as a vector or int have.). Notice the returned value is in , which is passed in as a reference parameter. istream & read_hw ( istream & in , vector < double >& hw ) { // statement that modify the input parameters return in ; } // the return of istream allow us to do the following if ( read_hw ( in , homework )) { } // othewise, we have to read_hw ( in , homework ); if ( in ) { } The \"lvalue\" We must pass an lvalue argument to a reference parameter. An \"lvalue\" is a value that denotes a nontemporary object. For example, a variable is an lvalue , as is a reference, or the result of calling a function that returns a reference. An expression that generates an arithmetic value, such as sum / count , is not an lvalue. member functions istream.clear() . This is to ensure the eof or non-valid input data will not effect reading the next data. Alwasy run cin.clear() , before we try to read again . pass by value: vector<double> vec : this will copy the argument, the original will not be modified. pass by reference: vector<double>& vec : this will not copy the argument, but will modify the original argument. This is good convention for object as a parameter, because copy a object have overhead. pass by const reference: const vector<double>& vec : this will not copy the argument as well as promise not to modify the passed argument. try {} catch {} clause, we normally break down to multiple statements in the try clause, because we want to avoid multiple side effect. For example, try { double final_grade = grade ( midterm , final , homework ); streamsize prec = cout . precision (); cout << \"Your final grade is \" << setprecision ( 3 ) << final_grade << setprecision ( prec ) << endl ; } catch ( domain_error ) { cout << endl << \"You must enter your grades. \" \"Please try again.\" << endl ; return 1 ; } is better then written as try { streamsize prec = cout . precision (); cout << \"Your final grade is \" << setprecision ( 3 ) << grade ( midterm , final , homework ) << setprecision ( prec ); } because the later can generate ambigious error message that not easy to debug. sort object Student_info , we have to use another form of sort(student.begin(), students.end()) with a extra parameter, which is a predicate to compare the two object. i.e. bool compare ( const Student_info & x , cosnt Student_info & y ) { return x . name < y . name ; } sort ( students . begin (), students . end (), compare ); Formatting the output, if we want to do the following Bob 88 Christopher 90 we could do this: maxlen = max ( maxlen , record . name . size ()); cout << students [ i ]. name << string ( maxlen + 1 - students [ i ]. name . size (), ' ' ); Notice the string(num, ' ') instantiate a string has num of spaces. hearder file should declare only the names that are necessary. Header files should use fully qualified names rather than using-declarations. (Avoid using namespace std ;) #include <vector> double median ( std :: vector < double > ); Avoid multiple inclusion #ifndef __THIS_HEADER_H__ #define __THIS_HEADER_H__ //your program #endif type of exceptions logic_error domain_error length_error out_of_range invalid_argument runtime_error range_error overflow_error underflow_error exceptional handling try { // code Initiates a block that might throw an exception. } catch ( t ) { // code } //real use case. try { double final_grade = grade ( students [ i ]); streamsize prec = cout . precision (); cout << setprecision ( 3 ) << final_grade << setprecision ( prec ); } catch ( domain_error e ) { cout << e . what (); } Concludes the try block and handles exceptions that match the type t . The code following the catch performs whatever action is appropriate to handle the exception reported in t . throw e ; Terminates the current function; throws the value e back to the caller. e.what() : return a value that report on what happened to cause the error. str.width([n]) and std::setw(n) both used to set the output width. The example program from this chapter is worth of keeping here for references. There is a lot information included in it. //Calculate the grade for many students, including reading the data in, //how to sort according there name and how to format the out put in a nice printing, ect. #include <algorithm> #include <iomanip> #include <iostream> #include <stdexcept> #include <string> #include <vector> using std :: cin ; using std :: cout ; using std :: domain_error ; using std :: endl ; using std :: istream ; using std :: ostream ; using std :: setprecision ; using std :: sort ; using std :: streamsize ; using std :: string ; using std :: vector ; // compute the median of a `vector<double>' // note that calling this function copies the entire argument `vector' double median ( vector < double > vec ) { #ifdef _MSC_VER typedef std :: vector < double >:: size_type vec_sz ; #else typedef vector < double >:: size_type vec_sz ; #endif vec_sz size = vec . size (); if ( size == 0 ) throw domain_error ( \"median of an empty vector\" ); sort ( vec . begin (), vec . end ()); vec_sz mid = size / 2 ; return size % 2 == 0 ? ( vec [ mid ] + vec [ mid -1 ]) / 2 : vec [ mid ]; } // compute a student's overall grade from midterm and final exam grades and homework grade double grade ( double midterm , double final , double homework ) { return 0.2 * midterm + 0.4 * final + 0.4 * homework ; } // compute a student's overall grade from midterm and final exam grades // and vector of homework grades. // this function does not copy its argument, because `median' does so for us. double grade ( double midterm , double final , const vector < double >& hw ) { if ( hw . size () == 0 ) throw domain_error ( \"student has done no homework\" ); return grade ( midterm , final , median ( hw )); } // read homework grades from an input stream into a `vector<double>' istream & read_hw ( istream & in , vector < double >& hw ) { if ( in ) { // get rid of previous contents hw . clear (); // read homework grades double x ; while ( in >> x ) hw . push_back ( x ); // clear the stream so that input will work for the next student in . clear (); } return in ; } int main () { // ask for and read the student's name cout << \"Please enter your first name: \" ; string name ; cin >> name ; cout << \"Hello, \" << name << \"!\" << endl ; // ask for and read the midterm and final grades cout << \"Please enter your midterm and final exam grades: \" ; double midterm , final ; cin >> midterm >> final ; // ask for the homework grades cout << \"Enter all your homework grades, \" \"followed by end-of-file: \" ; vector < double > homework ; // read the homework grades read_hw ( cin , homework ); // compute and generate the final grade, if possible try { double final_grade = grade ( midterm , final , homework ); streamsize prec = cout . precision (); cout << \"Your final grade is \" << setprecision ( 3 ) << final_grade << setprecision ( prec ) << endl ; } catch ( domain_error ) { cout << endl << \"You must enter your grades. \" \"Please try again.\" << endl ; return 1 ; } return 0 ; }","title":"Chapter 4 (organize program .h and .cc files, iostream as a argument, exception handling basics)"},{"location":"books/accelerated-cpp/notes/#chapter-5-sequential-containers-vector-list-and-analyzing-strings","text":"","title":"Chapter 5 (sequential containers (vector, list) and analyzing strings)"},{"location":"books/accelerated-cpp/notes/#51","text":"write a function extract_fails() to seperate the students that failed the course. The ideas is to use two seperate vector to hold the ones that passed and the ones that failed. To make it better, we only use one vector fails to hold the failed students, and erase them from the original vector. however, be cautions in using the erase memeber function for vectors. 1. the indexes of element after the removed element will change. 2. the size will change. 3. efficiency problem. Introduced the necessity of using iterator . Briefly, it is for efficiency optimization. The indexing is random access, which is more expensive to maintain the data structure properties, such as for vector. iterator allow us to separate the data access manner (sequential v.s. random) in a container, so as to implement different container to cope with a different need. All this work is because of efficiency concerns.","title":"5.1"},{"location":"books/accelerated-cpp/notes/#52","text":"Beside providing access or modify operations, a iterator is able to restrict the available operations in ways that correspond to what the iterator can handle efficiently. Generally, two type of iterators: const_iterator and iterator . when we do vector<student>::const_iterator iter = S.begin() , there is an explicit type conversion happened because S.begin() is a type of iterator . The usage of iterator vector<int>::iterator iter; , either iter->name or (*iter).name .","title":"5.2"},{"location":"books/accelerated-cpp/notes/#55","text":"vector and list differ in that if you call erase(iter) member function of the container. vector will invalidate all the iterators following iter in the vector. Even this is the case, we can use erase to delete a element from the container. i.e. iter = students.erase(iter); , iter will point to the next element of the removed element. Similarly, call push_back member function will invalidate all the iters of the vector. while for list container, call erase(iter) only invalidate the iter erased, not others. and call push_back will not invalidate other iterators. Because list doesn't support random access. We cannot use the sort() function from <algorithm> . Instead we have to use the member function that optimized for list container.","title":"5.5"},{"location":"books/accelerated-cpp/notes/#58","text":"Use inset function to do vertical concatenation ret.insert(ret.end(), bottom.begin(), bottom.end())","title":"5.8"},{"location":"books/accelerated-cpp/notes/#notes","text":"students.erase(students.begin()+i) , remove the ith object in the container students . iterator properties: Identifies a container and an element in the container Lets us examine the value stored in that element Provides operations for moving between elements in the container Restricts the available operations in ways that correspond to what the container can handle efficiently iterator types: container-type::iterator : to change the container value container-type::const_iterator : to only read the container value container-type::reverse_iterator container-type::const_reverse_iterator *iter return a lvalue. we can replace (*iter).name with iter->name students.erase(iter) will invalidate all the iterators following the elements that has been removed. After iter = students.erase(iter) , iter will point to the first element following the removed element. From vector to list . vector\u007f list optimized for fast random access optimized for fast insertion and deletion Using push_back to append an element to a vector invalidates all iterators referring to that vector. the erase and push_back operations do not invalidate iterators to other elements. list : doesn't support random access, so the STL <algorithm> library function sort() doens't apply to list string example, split a sentence into words. vector < string > split ( const string & s ) { vector < string > ret ; typedef string :: size_type string_size ; string_size i = 0 ; // invariant: we have processed characters [original value of i, i) while ( i != s . size ()) { // ignore leading blanks // invariant: characters in range [original i, current i) are all spaces while ( i != s . size () && isspace ( s [ i ])) ++ i ; // find end of next word string_size string_size j = i ; // invariant: none of the characters in range [original j, current j)is a space while ( j != s . size () && ! isspace ( s [ j ])) j ++ ; // if we found some nonwhitespace characters if ( i != j ){ // copy from s starting at i and taking j - i chars ret . push_back ( s . substr ( i , j - i )); i = j ; } } return ret ; } isspace is in the header file <cctype> isspace(c) true if c is a whitespace character. isalpha(c) true if c is an alphabetic character. isdigit(c) true if c is a digit character. isalnum(c) true if c is a letter or a digit. ispunct(c) true if c is a punctuation character. isupper(c) true if c is an uppercase letter. islower(c) true if c is a lowercase letter. toupper(c) Yields the uppercase equivalent to c tolower(c) Yields the lowercase equivalent to c In the ret.push_back(s.str(i, j-i)) , the j-i indicate a open range [s[i], s[j]) while (cin >> s) is read one work at a time, because the std::cin seperated by white spaces. It terminate until a invalid input is entered or a EOF. while (getline(cin, s)) is reading one line at a time, it return false when EOF entered or invalid chars. How to framing a word characters. How to cancatenate two vector?, we can do insert(ret.end(), bottom.begin(), bottom.end()) , note the first argument provide the iterator before which the element will be inserted. How to concatenate two pictures horizontally like the bellow pictures in case 1: this is an ************** example * this is an * to * example * illustrate * to * framing * illustrate * * framing * ************** pictures in case 2: ************** this is an * this is an * example * example * to * to * illustrate * illustrate * framing * framing * ************** vector < string > hcat ( const vector < string >& left , const vector < string >& right ) { vector < string > ret ; // add 1 to leave a space between pictures string :: size_type width1 = width ( left ) + 1 ; // indices to look at elements from left and right respectively vector < string >:: size_type i = 0 , j = 0 ; // continue until we've seen all rows from both pictures while ( i != left . size () || j != right . size ()) { // construct new string to hold characters from both pictures string s ; // copy a row from the left-hand side, if there is one if ( i != left . size ()) s = left [ i ++ ]; // pad to full width s += string ( width1 - s . size (), ' ' ); // copy a row from the right-hand side, if there is one if ( j != right . size ()) s += right [ j ++ ]; // add s to the picture we're creating ret . push_back ( s ); } return ret ; } vec.reserve(n) : Reserves space to hold n elements, but does not initialize them. This operation does not change the size of the container. It affects only the frequency with which vector may have to allocate memory in response to repeated calls to insert or push_back. c.rbegin() and c.rend() are iterator refering to the last and (one beyond) the first element in the container that grant access to the container's elements in reverse order.","title":"Notes"},{"location":"books/accelerated-cpp/notes/#chapter-6","text":"","title":"Chapter 6"},{"location":"books/accelerated-cpp/notes/#chapter-7","text":"","title":"Chapter 7"},{"location":"books/accelerated-cpp/notes/#chapter-8","text":"The language feature that implements generic functions is called template functions. template header template<class T> \"instantiation\" Keyword typename , i.e. typedef typename vector<T>::size_type vec_sz; \"you must precede the entire name by typename to let the implementation know to treat the name as a type.\" The C++ standard says nothing about how implementations should manage template instantiation, so every implementation handles instantiation in its own particular way. While we cannot say exactly how your compiler will handle instantiation, there are two important points to keep in mind: The first is that for C++ implementations that follow the traditional edit-compile-link model, instantiation often happens not at compile time, but at link time. It is not until the templates are instantiated that the implementation can verify that the template code can be used with the types that were specified. Hence, it is possible to get what seem like compile-time errors at link time. The second point matters if you write your own templates: Most current implementations require that in order to instantiate a template, the definition of the template, not just the declaration, has to be accessible to the implementation. Generally, this requirement implies access to the source files that define the template, as well as the header file. How the implementation locates the source file differs from one implementation to another. Many implementations expect the header file for the template to include the source file, either directly or via a #include. The most certain way to know what your implementation expects is to check its documentation. parameter type to a generic function should keep consistent. For example, We cannot pass int and double to the following function: tmeplate < class T > T max ( const T & left , const T & right ) { return left > right ? left : right ; } Data structure indepnedence: why we write the find function as find(c.begin(), c.end(), val) ? (it is the only way to write generic functions that works on more than 1 element types) Why not write as the form c.find(val) or find(c, val) ? iterator categories: 1.Sequential read-only access (input iterator) 2.Sequential write-only access (output iterator) 3.Sequentila read-wirte access (input-output iterator) 4.Reverseible access 5.Random access \"input iterator\" - interator support \"++, ==, !=, unary *, and it->first\". We say we give find two input iterators as parameters. template < class In , class X > In find ( In begin , In end , constX & x ) { if ( begin == end || * begin == x ) return ; begin ++ ; return find ( begin , end , x ); } \"output iterator\" -interator support *dest = _value_, dest++, and ++dest template < class In , class Out > Out copy ( In begin , In end , Out dest ) { while ( begin != end ) * dest ++ = * begin ++ ; return dest ; } \"input-output iterator\" - iterator support *it, ++it, it++, (but not --it or it--), it == j, it != j, it->member template < class For , class X > void replace ( For beg , For end , const X & x , const X & y ) { while ( beg != end ){ if ( * beg == x ) * beg = y ; ++ beg ; } } \"reverse interator\" - also support --it and it-- template < class Bi > void reverse ( Bi begin , Bi end ) { while ( begin != end ) { -- end ; if ( begin != end ) swap ( * begin ++ , * end ); } } Random access - support p + n, p - n, n + p, p-q, p[n], (equivalent to *(p + n)) p < q, p > q, p <= q, and p >= q template < class Ran , class X > bool binary_search ( Ran begin , Ran end , const X & x ) { while ( begin < end ) { // find the midpoint of the range Ran mid = begin + ( end - begin ) / 2 ; // see which part of the range contains x; keep looking only in that part if ( x < * mid ) end = mid ; else if ( * mid < x ) begin = mid + 1 ; // if we got here, then *mid == x so we're done else return true ; } return false ; } off-the-end values, it always ensure the range is [begin, end). The advantage? (see section 8.2.7) Input and output iterators input iterator for copy vector < int > v ; // read ints from the standard input and append them to v copy ( istream_iterator < int > ( cin ), istream_iterator < int > (), back_inserter ( v )); ouput iterator for copy // write the elements of v each separated from the other by a space copy ( v . begin (), v . end (), ostream_iterator < int > ( cout , \" \" ));","title":"Chapter 8"},{"location":"books/accelerated-cpp/notes/#chapter-9","text":"Using the :: before the function name of a non-member function called by a member function. doubel Student_info::grad () const { return :: grade ( midterm , final , homework ); } const for member function means this member function will not change the member variable. Only const member functions may be called for const objects. We cannot call non-const functions on const objects. such as read memeber on const Student_info . When we pass a non-const object to a function that take const reference. The function will treat the object as if it were const, and compiler will only permit it to call const memeber functions. When we pass a nonconst object to a function that takes a const reference, then the function treats that object as if it were const, and the compiler will permit it to call only const members of such objects. difference of class and struct : default protection. class --> private between { and first label. explicitly define a accessor read function, string name() const { return n; } will return a copy of member variable n instead return a reference, because we don't want the user to modifiy it. The \"Synthesized constructor\" will initialized the data memebers to a value based on how the object is created. if the object is local variable, will be default-initialized (undefined). If the object is used to init a container element, the members will be value-initialized(zero). Initialization rules: If an object is of a class type that defines one or more constructors, then the appropriate constructor completely controls initialization of the objects of that class. If an object is of built-in type, then value-initializing it sets it to zero, and default-initializing it gives it an undefined value. Otherwise, the object can be only of a class type that does not define any constructors. In that case, value- or default-initializing the object value- or default-initializes each of its data members. This initialization process will be recursive if any of the data members is of a class type with its own constructor. constructor initializers Student_info::Student_info() : final(0), midterm(0){} when we create a object: the implementation allocate memory for the new object. it initializes the object, as directed by the constructor's initializer list. it executes the constructor body. The implementation initializes every data member of every object, regardless of whether the constructor initializer list mentions those members. The constructor body may change these initial values subsequently, but the initialization happens before the constructor body begins execution. It is usually better to give a member an initial value explicitly, rather than assigning to it in the body of the constructor. By initializing rather than assigning a value, we avoid doing the same work twice. Constructors with Arguments: Student_info::Student_info(istream& is) { read(is); }","title":"Chapter 9"},{"location":"books/accelerated-cpp/notes/#chapter-10","text":"All you can do with a function is to take its address or call it. Any use of function that is not a call is assumed to be taking its address. function pointer declarition: int (*fp)(int) in which fp is a function pointer, if we have another function definition: int next(int){ return n+1; } we can use it like this fp = &next or fp = next . With & or without it is essentially same. define a function pointer point to a function: vector<string>(*sp)(const string &) = split; we can call the next function such as i = (*fp)(i); or i = fp(i); calling function pointer automatically calling the function itself. function with a return value as a function pointer, can use typedef . For example: //define analysis_fp as the name of the type of an appropriate pointer typedef double ( * analysis_fp )( const vector < Student_info >& ); //get_analysis_ptr returns a pointer to an analysis function analysis_fp get_analysis_ptr (); //the alternative and most important trick that has been played in the S2E tcg components. doubel ( * get_analysis_ptr ())( const vector < Student_info >& ); function pointer as parameter to find_if , Notice the Pred can be any type as long as f(*begin) has meaningful value. bool is_negative ( int n ) { return n < 0 ; } template < class In , class Pred > In find_if ( In begin , In end , Pred f ) { while ( begin != end && ! f ( * begin )) ++ begin ; return begin ; } // call it vector < int >:: iterator i = find_if ( v . begin (), v . end (), is_negative ); <cstddef> header: size_t : unsigned type large enough to hold the size of any object. ptrdiff_t : the type of p - q , p, q are both pointer. static means only initialize once, not everytime the function calle or the object is initialized. sizeof() operator reports the results in bytes . ifstream and ofstream object doesn't like string for file path. It almost always require the name of the file to be a pointer to the initial element of a null-terminated character array. simplicity. What if the string facilities doesn't exist. historical. fstream is earlier than string facilities in c++ compatibility. easier to interface with OS file I/O, which typically use such pointers to communicate. using c_str member function for string literal. `ifstream infile(filepath.c_str()); example that read every file supplied in the commandline. int main ( int argc , char ** argv ) { int fail_count = 0 ; for ( int i = 1 ; i < argc ; i ++ ){ ifstream in ( argv [ i ]); if ( in ){ string s ; while ( getline ( in , s )) cout << s << endl ; } else { cerr << \"cannot open file \" << argv [ i ] << endl ; ++ fail_count ; } } return fail_count ; // very neat trick played here. } Three kinds of memory management automatica memory management (local variable) statically allocated memory ( static int x ) it allocate once and only once before the function contain the statement is ever called. every call to pointer_to_static will return a pointer to the same object. the pointer will be valid as long as the program runs, and invalid afterward. dynamic allocation Allocate object of type T. new T(args) i.e. int* p = new int(32); allocate a object int with initial value is 32.","title":"Chapter 10"},{"location":"books/accelerated-cpp/notes/#chapter-11-implement-a-vector-class","text":"template function V.S. template class template <typename T> T Vec (T a) { // function body } and template <class T> class Vec { public: // interface private: // implementation } What it does when use new to allocate memory. (i.e. new T[n] ) allocate memory initialize the element by running the default constructor. the class T should have a default constructor. A template class type should have the control over how a object created, copied, assigned, or destroyed. explicit Vec(size_type n) { create(n); } mean using the constructor should be explicitly declared, such as Vec(5) , not vec = 5 Type names for the members. Using typedef such as typedef T* iterator . Define a overloaded operator: like define a function, the type of the operator(uniary or binary) defines how many parameters the function will have. If the operator is a function that is not a member, then the function has as many arguments as the operator has operands. The first argument is bound to the left operand; the second is bound to the right operand. If the operator is defined as a member function, then its left operand is implicitly bound to the object on which the operator is invoked. Member operator functions, therefore, take one less argument than the operator indicates. Index operator MUST be a member function. T& operator[](size_type i) { return data[i]; } , User might also want to only read the element through the index operator, so we can also define another overlaoded version const T& operator[](size_type i) const { return data[i]; } . Notice the index operator will return a reference instead of a value. implicitly copying passing by value in function parameter passing. vector<int> i; double d; d = median(i); return value from a function. ( string line; vector<string> words = split(line); ) explicitly copying assignment: vector<Student_info> vec = vs; Copy constructor: is a member function with the same name as the name of the class template < class T > class Vec { public : Vec ( const Vec & v ); { create ( v . begin (), v . end ()); } //copy constructor }; using reference because we are defining what it means by copy, so we go deep to the granuality of call by reference to avoid copying. copying object shouldn't change the original vector, so we use const. Because the copy of vector object is actually copy the pointer, the new copy of the original object contain the same data, point to the same data area. We should make sure they are not contain the same underlying storage when making copies of objects. We should do this: (note the create function hasn't been implemented yet) template < class T > class Vec { public : Vec ( const Vec & v ) { create ( v . begin (), v . end ()); } //copy constructor made a copy. } assignment operator : it must be defined as a member function.(may have multiple overloaded versions.) Assignment differs from the copy constructor in that assignment always involves obliterating an existing value (the left-hand side) and replacing it with a new value (the right-hand side). template < class T > class Vec { public : Vec & operator = ( const Vec & ); //assignment operator }; template < class T > Vec < T >& Vec < T >:: operator = ( const Vec & rhs ) { if ( & rhs != this ){ uncreate (); create ( rhs . begin (), rhs . end ()); } return * this ; //why we need '*this' here instead of 'this' } return reference uncreate and create return variable scope How to define a tempalte member function outside of the class? When we should have the T in Vec<T>& Vec<T>::operator=(const Vec& rhs) ? the oprator= have two different meanings in C++ Initialization. Such as we do vector<int> vec = v(10); or int a = 10; we are invoking the copy constructor. Initialization involves creating a new object and giving it a value at the same time. Initialization happens: In variable declarations (explicitly) For function parameters on entry to a function (implicitly) For the return value of a function on return from the function (implicitly) In constructor initializers (explicitly) Assignment, we are calling operator= . Assignment (operator=) always obliterates a previous value; initialization never does so. examples: string url_ch = \"~;/?:@=&$-_.+!*'(),\" // initialization,(constructor + copy constructor) string spaces ( url_ch . size (), ' ' ) ; // initialization string y ; // initialization y = url_ch ; // assignment, call the operator= and obliterate a previous value. //more complex ones vector < string > split ( const string & ); // function declaration vector < string > v ; // initialization v = split ( line ); // on entry, initialization of split's parameter from line; // on exit, both initialization of the return value // and assignment to v The declaration of split above is interesting because it defines a return type that is a class type. Assigning a class type return value from a function is a two-step process: First, the copy constructor is run to copy the return value into a temporary at the call site. Then the assignment operator is run to assign the value of that temporary to the left-hand operand. Constructors always control initialization. The operator= member function always controls assignment. Defalut action regarding the copy constructor, assignment operator, and destructor: rule of three: copy constructor, destructor, and assignment operator. if you defind a class, you probably need the following for copy control and assignment operators. T :: T () one or more constructors , perhaps with arguments T ::~ T () the destructor T :: T ( const T & ) the copy constructor T :: operator = ( const T & ) the assignment operator the compiler will invoke them whenever an object of our type is created, copied, assigned, or destroyed. Remember that objects may be created, copied, or destroyed implicitly. Whether implicitly or explicitly, the compiler will invoke the appropriate operation. consideration in design a vector class: constructor type definition index and size (overload operators) copy control destructor Flexible Memory Management, those functions that used to implement the create and uncreate functions. new always initialized every object by using constructor T::T() . If we want to initialized by ourselves, we have to do it twice. allocator<T> class in <memory> library. Members and non member function: T * allocate ( size_t ); void deallocate ( T * , size_t ); void construct ( T * , const T & ) ; void destroy ( T * ); template < class Out , class T > void uninitialized_fill ( Out , Out , const T & ); template < class In , class Out > Out uninitialized_copy ( In , In , Out );","title":"Chapter 11 (Implement a vector class)"},{"location":"books/accelerated-cpp/notes/#chapter-12-making-class-objects-working-like-values","text":"","title":"Chapter 12 (Making class objects working like values)"},{"location":"books/accelerated-cpp/notes/#chapter-13-inheritance","text":"the derived class will not inherit the following: constuctor, assignment operator, and destructor. Keyword protected allows the derived class to access the private member of the base. derived class is constructed by the following steps: allocate memory for the entire object.(base member and derived class member.) call base constructor to initialize the base part. initialize the member of the derived class by initializer list. call constructor of the derived class. NOTE: However, it doesn't select which base constructor to run, we have to explicitly involke it. \"The derived-class constructor initializer names its base class followed by a (possibly empty) list of arguments. These arguments are the initial values to use in constructing the base- class part; they serve to select the base-class constructor to run in order to initialize the base.\" If we pass Grad* to function that take Core* , Compiler convert grad* to Core* and bind the parameter to a Core* type. Static binding V.S. Dynamic binding. \"The phrase dynamic binding captures the notion that functions may be bound at run time, as opposed to static bindings that happen at compile time.\" Virtual Function: (mainly for pointer and references, not for explicit object, because the later is bind to the function in compile time.) It come into being in the following accasion: bool compare_grade ( const Core & c1 , const Core & c2 ) { return c1 . grade () < c2 . grade (); } which function to call, it has to be decide in run time. The reason is that the parameter type const Core& can also accept a type Grade* . More examples. Core c ; Grad g ; Core * p ; Core & r = g ; c . grade (); // statically bound to Core::grade() g . grade (); // statically bound to Core::grade() p -> grade (); // dynamically bound, depending on the type of the object to which p points r . grade (); // dynamically bound, depending on the type of the object to which p points if we defind the bool compare_grade(const Core C1, const Core C2) , if we pass Grad to it, it cut down to its core part. The two grade() would be identically from Core . If we define pointer parameters, the compiler will convert Grad* to a Core* , and would bind the pointer to the Core part of the Grad object. polymorphism: one type (base type) stand for many types (by reference and poitners). \"C++ supports polymorphism through the dynamic-binding properties of virtual functions. When we call a virtual through a pointer or reference, we make a polymorphic call. The type of the reference (or pointer) is fixed, but the type of the object to which it refers (or points) can be the type of the reference (or pointer) or any type derived from it. Thus, we can potentially call one of many functions through a single type.\" virtual function must be defined, regardless of whether the program calles them. virtual destructor: usually in base not in derived class. it usually empty if not other special thing need todo. virtual properties are inherented, such as virtual function or virtual destructor, the keyword \"virtual\" only need to be defined in the base class, and no need to redeclared in derived class. virtual destructor: when you delete the heap memory using the command delete, the pointer operand for delete might be more than one class types. you have give the compiler right indication what object space to release, we use the virutal destructor to do this, for example: class Core (){ public : virutal ~ Core (){} //empty destructor is enough } In this case the delete will automatically select the synthesized approperiate destructor for base class. A virtual destructor is needed any time it is possible that an object of derived type is destroyed through a pointer to base. A virtual destructor is inherited and we don't need to add the virtual destructor to the derived class such as Grad . Programming technique: handle class. hide the pointer manipulations and encapsulate the pointer to Core . static member function. Static member functions differ from ordinary member functions in that they do not operate on an object of the class type. Unlike other member functions, they are associated with the class, not with a particular object. How to implement copy constructor? give the handle class a virtual function clone() to implement the copy constructor. another wrapper!!! class Core { friend class Student_info ; protected : virtual Core * clone () const { return new Core ( * this );} //as before. }; Notice that the copy constructor didn't defined explicitly. It is synthesized by the implementation. (default copy constructor) Ordinarily, when a derived class redefines a function from the base class, it does so exactly: the parameter list and the return type are identical. However, if the base-class function returns a pointer (or reference) to a base class, then the derived-class function can return a pointer (or reference) to a corresponding derived class. \"Finally, the objects that were allocated inside the read for the Student_info function will be automatically freed when we exit main. On exiting main, the vector will be destroyed. The destructor for vector will destroy each element in students, which will cause the destructor for Student_info to be run. When that destructor runs, it will delete each of the objects allocated in read.\" look at the following piece of code: what will happen, if you mistake on the type of the class. vector < Core > students ; Grad g ( cin ); // read a Grad students . push_back ( g ); // Store only the core part of the object. What will happen is that push_back will expect that it was given a Core object, and will construct a Core element, copying only the Core parts of the object, ignoring whatever is specific to the Grad class. We can control which function to call by specify the scope operator, such as when r is a reference to Grad, we can call the regrade function of Core. r.Core::regrade(100); keep in mind that base function is always hiden if you call the derived class function member when the two are have same form (see 13.6.2 in page 347)","title":"Chapter 13 (Inheritance)"},{"location":"books/mining-massive-datasets/notes/","text":"Ming Massive Datasets \u00b6 Ming Massive Datasets Book Website Chapter 3 Finding Similar Items","title":"Mining Massive Datasets"},{"location":"books/mining-massive-datasets/notes/#ming-massive-datasets","text":"Ming Massive Datasets Book Website Chapter 3 Finding Similar Items","title":"Ming Massive Datasets"},{"location":"courses/","text":"Courses \u00b6 Algorithm and Languages \u00b6 Nine Chapter Dynamic Programming Functional Programming Principles in Scala Artificial Intelligence and Machine Learning \u00b6 Machine Learning CS224N Lecture Notes CS224N Write-up 6.431 Probability Learning From Data Mining Massive Data Sets Convolutional Neural Networks Distributed Systems \u00b6 Nine Chapter System Design Concurrent Programming in Java Project Management \u00b6 USMx: ENCE607.1x Applied Scrum for Agile Project Management Systems and Security \u00b6 6.828 Operating System Engineering","title":"Index"},{"location":"courses/#courses","text":"","title":"Courses"},{"location":"courses/#algorithm-and-languages","text":"Nine Chapter Dynamic Programming Functional Programming Principles in Scala","title":"Algorithm and Languages"},{"location":"courses/#artificial-intelligence-and-machine-learning","text":"Machine Learning CS224N Lecture Notes CS224N Write-up 6.431 Probability Learning From Data Mining Massive Data Sets Convolutional Neural Networks","title":"Artificial Intelligence and Machine Learning"},{"location":"courses/#distributed-systems","text":"Nine Chapter System Design Concurrent Programming in Java","title":"Distributed Systems"},{"location":"courses/#project-management","text":"USMx: ENCE607.1x Applied Scrum for Agile Project Management","title":"Project Management"},{"location":"courses/#systems-and-security","text":"6.828 Operating System Engineering","title":"Systems and Security"},{"location":"courses/6.431-probability/notes/","text":"6.041/6.431 Probability - The Science of Uncertainty and Data \u00b6 Lecture 1 (September 10, 2018) \u00b6 Probability Models \u00b6 To specify such a model, it includes 2 steps: Sample space: describe possible outcomes of an experiment Probability laws: describe beliefs about the likelihood of outcomes (or assign a probability to outcomes) Sample space \u00b6 list of possible outcomes must be mutually exclusive, collectively exhaustive, and at the right granularity. Probability laws \u00b6 Probability axioms \u00b6 Motivation: Assign individual probability to a single dot is a challenge (in the dart example, each dot has zero probability), so we assign a probability to the subset of the sample space. Nonnegativity: {\\bf P}(A) \\geq 0 {\\bf P}(A) \\geq 0 Normalization: {\\bf P}(\\Omega) = 1 {\\bf P}(\\Omega) = 1 (Finite) Additivity: if A \\cap B = \\emptyset A \\cap B = \\emptyset then {\\bf P}(A \\cup B) = {\\bf P}(A) + {\\bf P}(B) {\\bf P}(A \\cup B) = {\\bf P}(A) + {\\bf P}(B) (need to be refined in future) Consequences of axioms (properties of probability axioms) \u00b6 {\\bf P}(A) \\leq 1 {\\bf P}(A) \\leq 1 {\\bf P}(\\emptyset) = 0 {\\bf P}(\\emptyset) = 0 {\\bf P}(A) + {\\bf P}(A^c) = 1 {\\bf P}(A) + {\\bf P}(A^c) = 1 A, B, \\text{and } C A, B, \\text{and } C are disjoint: {\\bf P}(A \\cup B \\cup C) = {\\bf P}(A) + {\\bf P}(B) + {\\bf P}(C) {\\bf P}(A \\cup B \\cup C) = {\\bf P}(A) + {\\bf P}(B) + {\\bf P}(C) A_1, \\cdots A_k, A_1, \\cdots A_k, are disjoint: {\\bf P}(A_1 \\cup \\cdots, \\cup A_k) = \\sum_{i - 1}^k{\\bf P}(A_i) {\\bf P}(A_1 \\cup \\cdots, \\cup A_k) = \\sum_{i - 1}^k{\\bf P}(A_i) If A \\subset B A \\subset B , then {\\bf P}(A) \\leq {\\bf P}(B) {\\bf P}(A) \\leq {\\bf P}(B) {\\bf P}(A \\cup B) = {\\bf P}(A) + {\\bf P}(B) - {\\bf P}(A \\cap B) {\\bf P}(A \\cup B) = {\\bf P}(A) + {\\bf P}(B) - {\\bf P}(A \\cap B) {\\bf P}(A \\cup B \\cup C) = {\\bf P}(A) + {\\bf P}(A^c \\cap B) + {\\bf P}(A^c \\cap B^c \\cap C) {\\bf P}(A \\cup B \\cup C) = {\\bf P}(A) + {\\bf P}(A^c \\cap B) + {\\bf P}(A^c \\cap B^c \\cap C) Union Bound: {\\bf P}(A \\cup B) \\leq {\\bf P}(A) + {\\bf P}(B) {\\bf P}(A \\cup B) \\leq {\\bf P}(A) + {\\bf P}(B) Probability calculation steps \u00b6 Specify the sample space Specify a probability law Identify an event of interest Calculate Countable additivity \u00b6 The 3rd probability axiom previously mentioned can be extended to countable set (all integers), which means that the element in the set can be arranged in a sequence. This is contrary to the concept of uncountable set (i.e. a dart board, 2D plane). So you need to distinguish the following concepts: discrete set (countable set) continuous set (uncountable set, impossible to arrange in a sequence) finite set infinite set (possible to be arranged in a sequence) discrete (countable) finite set (i.e. {1, 2, 3}) discrete (countable) infinite set (i.e. all integers, all even integers, all old integers) continuous (uncountable) infinite set (i.e. \\{x | 0 \\leq x \\leq 1\\} \\{x | 0 \\leq x \\leq 1\\} ) Example: Sample space \\{1, 2, 3, \\cdots\\} \\{1, 2, 3, \\cdots\\} , given {\\bf P}(n) = \\frac{1}{2^n}, n = 1, 2, \\cdots {\\bf P}(n) = \\frac{1}{2^n}, n = 1, 2, \\cdots . Check the against the probability axioms that \\sum_{n = 1}^{\\infty}{\\bf P}(n) = 1 \\sum_{n = 1}^{\\infty}{\\bf P}(n) = 1 . What about the probability {\\bf P}(\\text{outcome is even}) {\\bf P}(\\text{outcome is even}) ? Using countable additivity axiom. Countable Additivity Axiom (refined probability axiom) \u00b6 Note if A_1, A_2, A_3, \\cdot A_1, A_2, A_3, \\cdot is an infinite sequence of disjoint events, then {\\bf P}(A_1 \\cup A_2 \\cup A_3 \\cdots) = {\\bf P}(A_1) + {\\bf P}(A_2) + {\\bf P}(A_3) + \\cdots {\\bf P}(A_1 \\cup A_2 \\cup A_3 \\cdots) = {\\bf P}(A_1) + {\\bf P}(A_2) + {\\bf P}(A_3) + \\cdots To refine the 3rd probability axiom, additivity holds only for \"countable\" sequences of events. That means the additivity axiom must be a sequence , with finite or infinite elements. Note the following contradiction, the additivity axiom can not be applied to continuous sample space. A_1, A_2, A_3, \\cdots A_1, A_2, A_3, \\cdots are real coordinates, if you apply the additivity axiom, you will get: \\begin{align*} 1={\\bf P}(\\Omega ) &={\\bf P}\\big (\\{ A_1\\} \\cup \\{ A_2\\} \\cup \\{ A_3\\} \\cdots \\big )\\\\ &={\\bf P}(\\{ A_1\\} )+{\\bf P}(\\{ A_2\\} )+{\\bf P}(\\{ A_3\\} )+\\cdots \\\\ &= 0+0+0+\\cdots =0, \\end{align*} \\begin{align*} 1={\\bf P}(\\Omega ) &={\\bf P}\\big (\\{ A_1\\} \\cup \\{ A_2\\} \\cup \\{ A_3\\} \\cdots \\big )\\\\ &={\\bf P}(\\{ A_1\\} )+{\\bf P}(\\{ A_2\\} )+{\\bf P}(\\{ A_3\\} )+\\cdots \\\\ &= 0+0+0+\\cdots =0, \\end{align*} which is contradicts. This is because \"unit square\" is not a countable set. Interpretations of probability theory \u00b6 frequency of events A? What is P(the president will be reelected)? Probability is often interpreted as: Description of beliefs Betting preference Lecture 2 Conditioning and Baye's rule \u00b6 Conditional probability \u00b6 use new information to review a model Definition {\\bf P}(A|B) = {\\bf P}(A|B) = \"probability of A given that B occurred\" {\\bf P}(A|B) = \\frac{{\\bf P}(A \\cap B)}{{\\bf P}(B)} {\\bf P}(A|B) = \\frac{{\\bf P}(A \\cap B)}{{\\bf P}(B)} , defined only when {\\bf P}(B) > 0 {\\bf P}(B) > 0 Conditional probability share properties of ordinary probabilities (finite and infinite countable additivity) If A \\cap C = \\emptyset A \\cap C = \\emptyset , then {\\bf P}(A \\cup C|B) = P(A|B) + {\\bf P}(C|B) {\\bf P}(A \\cup C|B) = P(A|B) + {\\bf P}(C|B) Multiplication rule \u00b6 {\\bf P}(A \\cap B) = {\\bf P}(A){\\bf P}(B|A) = {\\bf P}(B){\\bf P}(A|B) {\\bf P}(A \\cap B) = {\\bf P}(A){\\bf P}(B|A) = {\\bf P}(B){\\bf P}(A|B) {\\bf P}(A \\cap B \\cap C) = {\\bf P}(A){\\bf P}(B|A){\\bf P}(C|A \\cap B) {\\bf P}(A \\cap B \\cap C) = {\\bf P}(A){\\bf P}(B|A){\\bf P}(C|A \\cap B) It also applies to n events. Total probability theorem (divide and conquer) \u00b6 The settings: Partition of sample space into A_1, A_2, A_3 A_1, A_2, A_3 , have {\\bf P}(A_i) {\\bf P}(A_i) for every i i have {\\bf P}(B|A_i) {\\bf P}(B|A_i) for every i i {\\bf P}(B) = \\sum_{i} {\\bf P}(A_i){\\bf P}(B|A_i) {\\bf P}(B) = \\sum_{i} {\\bf P}(A_i){\\bf P}(B|A_i) It also applies to infinite countable sets according to the countable additivity axiom. Baye's rule \u00b6 The setting is the same as the total probability theorem. Partition of sample space into A_1, A_2, A_3 A_1, A_2, A_3 , have {\\bf P}(A_i) {\\bf P}(A_i) for every i i (initial believes) have {\\bf P}(B|A_i) {\\bf P}(B|A_i) for every i i revise \"believe\" given that B occurred: {\\bf P}(A_i|B) = \\frac{{\\bf P}(A_i \\cap B)}{{\\bf P}(B)} {\\bf P}(A_i|B) = \\frac{{\\bf P}(A_i \\cap B)}{{\\bf P}(B)} It turns out that we can use multiplication rule to calculate the nominator and use the total probability rule to calculate the denominator, thusly {\\bf P}(A_i|B) = \\frac{{\\bf P}(A_i){\\bf P}(B|A_i)}{\\sum_j{\\bf P}(A_j){\\bf P}(B|A_i)} {\\bf P}(A_i|B) = \\frac{{\\bf P}(A_i){\\bf P}(B|A_i)}{\\sum_j{\\bf P}(A_j){\\bf P}(B|A_i)} Lecture 3 Independence \u00b6 Independence of two events \u00b6 Motivation coin toss example: First toss is head or tail doesn't affect the probability of second toss is head. Occurance of A A privides no new information about B B : {\\bf P}(B|A) = {\\bf P}(B) {\\bf P}(B|A) = {\\bf P}(B) . Definition of independence \u00b6 Formal definition {\\bf P}(A \\cap B) = {\\bf P}(A) \\cdot {\\bf P}(B) {\\bf P}(A \\cap B) = {\\bf P}(A) \\cdot {\\bf P}(B) symmetric with respect to events. implies {\\bf P}(A|B) = {\\bf P}(A) {\\bf P}(A|B) = {\\bf P}(A) . Applies even if {\\bf P}(A) = 0 {\\bf P}(A) = 0 . Distinction between disjoin and independence Disjoin usually means dependent, because if one event happens we know the other event will not happen. Independence of event complements \u00b6 If A A and B B are independent, the A A and B^c B^c are independent. If A A and B B are independent, the A^c A^c and B^c B^c are independent. Conditioning independence \u00b6 Ordinary independence properties also apply to conditional independence. But independence doesn't imply conditional independence. Independence of a collection of events \u00b6 Intuition \"definition\": Information on some of the events doesn't change probability related to the remaining events. Formal definition pairwise independence \u00b6 independence V.S. pairwise independence \u00b6 Used two independence fair coin tosses as an example to show that pariwise independence isn't enough to show independence of collection of events. Reliability \u00b6 The king's sibling puzzle Monty Hall problem \u00b6 Lecture 4 \u00b6 Lecture 5 \u00b6 Definition of random variables \u00b6 A random variable (\"r.v.\") associates a value (a number) to every possible outcome. Mathematically, A function from sample space \\Omega \\Omega to the real numbers (discrete or continuous). We can have several random variables defined on the same sample space. (In Lecture 18, X_1, X_2, \\cdots X_1, X_2, \\cdots are independent random variables from the same distribution) A function of one or several random variables is also another random variable. A concrete example to understand the concept of a random variable is take a class of students, selecting students and measuring their body mass or height will give us the random variable W W and H H . The \"Body mass index\" is another random variable that is a function of the random variable W W and H H . meaning of X + Y X + Y random variable X+Y X+Y takes the value x+y x+y , when X X takes the value x x , and Y Y takes the value y y . Probability mass function \u00b6 Bernoulli and indicator random variables \u00b6 Bernoulli random variable X X : X = \\begin{cases} 1, & \\textrm{w.p. } p \\\\ 0, & \\textrm{w.p. } 1-p \\end{cases} X = \\begin{cases} 1, & \\textrm{w.p. } p \\\\ 0, & \\textrm{w.p. } 1-p \\end{cases} models a trial that results in success/failure, heads/tails, etc. indicator random variables of event A A : I_A = 1 I_A = 1 if A A occurs. connection between events and random variables. P(A) = P(I_A = 1) = P_{I_A}(1) P(A) = P(I_A = 1) = P_{I_A}(1) , P(A) P(A) : probability of event A happens P(I_A = 1) P(I_A = 1) : probablistic notition of indication R.V. when I_A I_A equal to 1 1 P_{I_A}(1) P_{I_A}(1) : PMF notation. Discrete uniform random variables \u00b6 Binomial random variables \u00b6 Experiment: toss a biased coin ( P(\\textrm{Head})=p P(\\textrm{Head})=p ) n n times and observe the result (number of heads) Geometric random variables \u00b6 Expectation \u00b6 The expected value rule \u00b6 Linearity of expectations \u00b6 Lecture 6 \u00b6 Lecture 7 \u00b6 Independence, variances, and binomial variance \u00b6 Lecture 8 \u00b6 Exponential random variables \u00b6 PDF: f_{X}(x) = \\begin{cases} \\lambda e^{-\\lambda x}, & x \\ge 0 \\\\ 0, & x < 0 \\end{cases} f_{X}(x) = \\begin{cases} \\lambda e^{-\\lambda x}, & x \\ge 0 \\\\ 0, & x < 0 \\end{cases} Probability of greater than a a , \\begin{align*} P(X \\ge a) &= \\displaystyle \\int_a^{+\\infty} \\lambda e^{-\\lambda x}dx \\\\ &= - \\displaystyle \\int_a^{+\\infty} de^{-\\lambda x} \\\\ &= -e^{-\\lambda x}\\Big|_a^{+\\infty} = -e^{-\\lambda \\cdot +\\infty} + e^{-\\lambda a} = e^{-\\lambda a} \\end{align*} \\begin{align*} P(X \\ge a) &= \\displaystyle \\int_a^{+\\infty} \\lambda e^{-\\lambda x}dx \\\\ &= - \\displaystyle \\int_a^{+\\infty} de^{-\\lambda x} \\\\ &= -e^{-\\lambda x}\\Big|_a^{+\\infty} = -e^{-\\lambda \\cdot +\\infty} + e^{-\\lambda a} = e^{-\\lambda a} \\end{align*} Expectation of exponential random variable {\\bf E}\\big[X\\big] = \\displaystyle \\int_0^{\\infty} x \\cdot \\lambda e^{-\\lambda x} dx = 1/\\lambda {\\bf E}\\big[X\\big] = \\displaystyle \\int_0^{\\infty} x \\cdot \\lambda e^{-\\lambda x} dx = 1/\\lambda Second moment of an exponential random variable {\\bf E}\\big[X^2 \\big] = \\displaystyle \\int_0^{\\infty} x^2 \\cdot \\lambda e^{-\\lambda x} dx = 2/\\lambda^2 {\\bf E}\\big[X^2 \\big] = \\displaystyle \\int_0^{\\infty} x^2 \\cdot \\lambda e^{-\\lambda x} dx = 2/\\lambda^2 Variance of an exponential random variable \\textbf{Var}(X) = {\\bf E}\\big[X^2 \\big] - \\Big({\\bf E}\\big[X\\big]\\Big)^2 = 1/\\lambda^2 \\textbf{Var}(X) = {\\bf E}\\big[X^2 \\big] - \\Big({\\bf E}\\big[X\\big]\\Big)^2 = 1/\\lambda^2 Lecture 9 \u00b6 Conditioning a continuous random variable on an event \u00b6 Memorylessness of the exponential PDF \u00b6 Total probability and expectation theorems \u00b6 Lecture 10 \u00b6 Total probability and total expectation theorems \u00b6 Follows from the discrete cases: Rules Discrete continous Total probability p_X(x) = \\displaystyle \\sum\\limits_{y} p_Y(y)p_{X \\mid Y}(x \\mid y) p_X(x) = \\displaystyle \\sum\\limits_{y} p_Y(y)p_{X \\mid Y}(x \\mid y) f_X(x) = \\displaystyle \\int_{-\\infty}^{+\\infty} f_Y(y) f_{X \\mid Y}(x \\mid y) dy f_X(x) = \\displaystyle \\int_{-\\infty}^{+\\infty} f_Y(y) f_{X \\mid Y}(x \\mid y) dy Conditional expectation {\\bf E}\\big[X \\mid Y = y \\big] = \\displaystyle \\sum\\limits_{x} x p_{X \\mid Y}(x \\mid y) {\\bf E}\\big[X \\mid Y = y \\big] = \\displaystyle \\sum\\limits_{x} x p_{X \\mid Y}(x \\mid y) {\\bf E}\\big[X \\mid Y = y \\big] = \\displaystyle {\\int_{-\\infty}^{+\\infty}} x f_{X \\mid Y}(x \\mid y) dx {\\bf E}\\big[X \\mid Y = y \\big] = \\displaystyle {\\int_{-\\infty}^{+\\infty}} x f_{X \\mid Y}(x \\mid y) dx Total expectation {\\bf E}\\big[X \\big] = \\displaystyle \\sum\\limits_{y} p_{Y}(y){\\bf E} \\big[X \\mid Y = y \\big] {\\bf E}\\big[X \\big] = \\displaystyle \\sum\\limits_{y} p_{Y}(y){\\bf E} \\big[X \\mid Y = y \\big] derivation hint: replace with the conditional expectatioin {\\bf E}\\big[X \\big] = \\displaystyle {\\int_{-\\infty}^{+\\infty}} f_{Y}(y){\\bf E}\\big[X \\mid Y = y \\big] dy {\\bf E}\\big[X \\big] = \\displaystyle {\\int_{-\\infty}^{+\\infty}} f_{Y}(y){\\bf E}\\big[X \\mid Y = y \\big] dy derivation hint: replace with the conditional expectatioin Expected value rule {\\bf E}\\big[ g(X) \\mid Y = y \\big] = \\displaystyle {\\sum\\limits_{x}} g(x) p_{X \\mid Y}(x \\mid y) {\\bf E}\\big[ g(X) \\mid Y = y \\big] = \\displaystyle {\\sum\\limits_{x}} g(x) p_{X \\mid Y}(x \\mid y) {\\bf E}\\big[ g(X) \\mid Y = y \\big] = \\displaystyle {\\int_{-\\infty}^{+\\infty}} g(x) f_{X \\mid Y}(x \\mid y) dx {\\bf E}\\big[ g(X) \\mid Y = y \\big] = \\displaystyle {\\int_{-\\infty}^{+\\infty}} g(x) f_{X \\mid Y}(x \\mid y) dx Solved problems (Lecture 8 - 10) \u00b6 10 Buffon's needle and Monte Carlo Simulation \u00b6 This problem is discussed in the text page 161, Example 3.11. Lecture 11 \u00b6 Lecture 12 \u00b6 Covariance \u00b6 Definition: \\textbf{cov}(X, Y) = {\\bf E}\\big[(X - {\\bf E}[X]) (Y - {\\bf E}[Y])\\big] \\textbf{cov}(X, Y) = {\\bf E}\\big[(X - {\\bf E}[X]) (Y - {\\bf E}[Y])\\big] Covariance tell use whether two r.v.s tend to move together, both to low or both to high Two random variables X X and Y Y are independent indicates: \\textbf{cov}(X, Y) = 0 \\textbf{cov}(X, Y) = 0 , but the converse it not true. Covariance properties \u00b6 Lecture 13 \u00b6 The conditional expectation as a random variable {\\bf E} \\big[X \\mid Y\\big] {\\bf E} \\big[X \\mid Y\\big] \u00b6 Given a function h(x) = x^2 h(x) = x^2 for all x x , and a random variable X X . what is h(X) h(X) ? h(X) h(X) is a function of a random variable X X . h(X) h(X) itself is a random variable that take value x^2 x^2 if X X happens to take the value x x . (you should really understand this statement, it is crucial to understand the concept of conditional expectatioin) For discrete case: {\\bf E}\\big[X \\mid Y = y \\big] = \\displaystyle \\sum\\limits_{x} x p_{X \\mid Y}(x \\mid y) {\\bf E}\\big[X \\mid Y = y \\big] = \\displaystyle \\sum\\limits_{x} x p_{X \\mid Y}(x \\mid y) , in which different vaule of y y will give us the different value of conditional expectation of X X . Then we can really treat {\\bf E}\\big[X \\mid Y = y \\big] {\\bf E}\\big[X \\mid Y = y \\big] as a function of y y , noted as g(y) g(y) . From the above reasoning, because Y Y is a random variable, g(Y) g(Y) is also a reandom variable, that takes the value of g(y) = {\\bf E}\\big[X \\mid Y = y \\big] g(y) = {\\bf E}\\big[X \\mid Y = y \\big] if Y Y happens to take the value y y . g(Y) = {\\bf E}\\big[X \\mid Y\\big] g(Y) = {\\bf E}\\big[X \\mid Y\\big] , is a function of Y Y a random variable has a distribution, mean, variance, etc. The law of iterated expectations \u00b6 By calculate {\\bf E}\\Big[{\\bf E}\\big[X \\mid Y\\big]\\Big] {\\bf E}\\Big[{\\bf E}\\big[X \\mid Y\\big]\\Big] , we could abtain that it is equal to {\\bf E}\\big[X\\big] {\\bf E}\\big[X\\big] The conditional variance as a random variable \\textbf{Var}(X \\mid Y = y) \\textbf{Var}(X \\mid Y = y) \u00b6 definition: \\textbf{Var}(X \\mid Y = y) = {\\bf E}\\Big[\\big(X - {\\bf E}\\big[X \\mid Y = y \\big]\\big)^2\\mid Y = y\\Big] \\textbf{Var}(X \\mid Y = y) = {\\bf E}\\Big[\\big(X - {\\bf E}\\big[X \\mid Y = y \\big]\\big)^2\\mid Y = y\\Big] \\textbf{Var}(X\\mid Y) \\textbf{Var}(X\\mid Y) is also a random variable, it takes the value \\textbf{Var}(X\\mid y) \\textbf{Var}(X\\mid y) when Y Y happens to take the value of y y . Total variance rule: \\textbf{Var}(X) = {\\bf E}\\Big[\\textbf{Var}(X \\mid Y)\\Big] + \\textbf{Var}\\Big({\\bf E}\\big[X\\mid Y\\big]\\Big) \\textbf{Var}(X) = {\\bf E}\\Big[\\textbf{Var}(X \\mid Y)\\Big] + \\textbf{Var}\\Big({\\bf E}\\big[X\\mid Y\\big]\\Big) Lecture 14 Introduction to Bayesian Inference \u00b6 The Bayesian inference framework \u00b6 Treat the unknown \\Theta \\Theta as a random variable. with prior p_{\\Theta} p_{\\Theta} or f_{\\Theta} f_{\\Theta} . With the observations X X , using Bayes rule to obtain the posterior probability of \\Theta \\Theta , P_{\\Theta|X}(\\cdot|X=x) P_{\\Theta|X}(\\cdot|X=x) . Once the posterior is available, the estimator can be obtained via maximize a posterioral probability (MAP) rule or leaset mean squares (LMS). The performance can be measured by \"probability of error\" or \"mean squared error\". Conditional probability of error and total probability of error \u00b6 The conditional probability of other estimators that made a mistake. One of the properties of MAP rule is the it guarantee smallest probability of error. The taotal probability of error is an abstract notion of error for MAP rule to estimator. It can be caculated using total probability theory. Discrete unknowns, discrete observation example (8. Exercise: Discrete unknowns) \u00b6 Let \\Theta_1 \\Theta_1 and \\Theta_2 \\Theta_2 be some unobserved Bernoulli random variables and let be an observation. Conditional on X = x X = x , the posterior joint PMF of \\Theta_1 \\Theta_1 and \\Theta_2 \\Theta_2 is given by p_{\\Theta _1,\\Theta _2\\mid X}(\\theta _1,\\theta _2\\mid x) = \\begin{cases} 0.26, & \\mbox{if } \\theta _1=0, \\theta _2=0, \\\\ 0.26, & \\mbox{if } \\theta _1=0, \\theta _2=1, \\\\ 0.21, & \\mbox{if } \\theta _1=1, \\theta _2=0, \\\\ 0.27, & \\mbox{if } \\theta _1=1, \\theta _2=1, \\\\ 0, & \\mbox{otherwise.} \\end{cases} p_{\\Theta _1,\\Theta _2\\mid X}(\\theta _1,\\theta _2\\mid x) = \\begin{cases} 0.26, & \\mbox{if } \\theta _1=0, \\theta _2=0, \\\\ 0.26, & \\mbox{if } \\theta _1=0, \\theta _2=1, \\\\ 0.21, & \\mbox{if } \\theta _1=1, \\theta _2=0, \\\\ 0.27, & \\mbox{if } \\theta _1=1, \\theta _2=1, \\\\ 0, & \\mbox{otherwise.} \\end{cases} What is the estimate of (\\Theta_1, \\Theta_2) (\\Theta_1, \\Theta_2) provided by MAP rule? (1, 1), because the p_{\\Theta _1,\\Theta _2\\mid X}(1, 1\\mid x) p_{\\Theta _1,\\Theta _2\\mid X}(1, 1\\mid x) is the maximum of all. What is the MAP estimate of \\Theta_1 \\Theta_1 based on X X , that is, the one that maximizes p_{\\Theta_1 \\mid X}(\\theta_1 \\mid x) p_{\\Theta_1 \\mid X}(\\theta_1 \\mid x) ? 0, from the marginal PMF, you can see that p_{\\Theta_1 \\mid X}(0 \\mid x) p_{\\Theta_1 \\mid X}(0 \\mid x) is maximized The moral of this problem is that an estimate of $ \\Theta_1 $ obtained by identifying the maximum of the join PMF of all unknown random variables can be different from the MAP estimate of \\Theta_1 \\Theta_1 from the marginal PMF. Discrete parameter, continuous observation \u00b6 Digital signal transmission continous parameter, continuous observation \u00b6 Analog signal transmission Lecture 15 Linear models with normal noise \u00b6 recognizing normal PDFs \u00b6 f_X(x) = c \\cdot e^{-(\\alpha x^2 + \\beta x + \\gamma)} f_X(x) = c \\cdot e^{-(\\alpha x^2 + \\beta x + \\gamma)} is a normal random variable with \\mu = -\\frac{\\beta}{2\\alpha} \\mu = -\\frac{\\beta}{2\\alpha} and variance \\frac{1}{2\\alpha} \\frac{1}{2\\alpha} The mean squared error \u00b6 The most important take away from this section is that for normal unknown and normal noise signal, X = \\Theta + W X = \\Theta + W , no matter which x_i x_i we observed, the mean squared error estimates {\\hat \\theta} {\\hat \\theta} are the same (the variance of \\Theta \\Theta ). In other words, the remaining uncertainty about \\theta \\theta after an observation is the same no matter what the observation is. The observation only determined the estimated mean of the random variable \\theta \\theta . (See the drawing at the end of the lecture) Measurement, estimate, and learning \u00b6 How to measure the gravitational attraction constant Lecture 16 Least mean square (LMS) estimation \u00b6 LMS estimation without any observations \u00b6 Given the prior p_{\\Theta}(\\theta) p_{\\Theta}(\\theta) unknow random variable \\Theta \\Theta , what's you realy want to estimate? You may interested in a point estimate. You may interested in find the estimator. With no observation, to minimize the mean squared error (MSE), {\\bf E}\\Big[\\big(\\Theta - \\hat \\theta \\big)^2\\Big] {\\bf E}\\Big[\\big(\\Theta - \\hat \\theta \\big)^2\\Big] , the value \\hat \\theta = {\\bf E}\\big[X\\big] \\hat \\theta = {\\bf E}\\big[X\\big] minimize the error. and the minimum error equal to \\textbf{Var}(\\Theta) \\textbf{Var}(\\Theta) Optimal mean squared error: {\\bf E}\\Big[\\big(\\Theta - {\\bf E}\\big[\\Theta \\big] \\big)^2\\Big] = \\textbf{Var}(\\Theta) {\\bf E}\\Big[\\big(\\Theta - {\\bf E}\\big[\\Theta \\big] \\big)^2\\Big] = \\textbf{Var}(\\Theta) . Because {\\bf E}\\big[\\Theta \\big] {\\bf E}\\big[\\Theta \\big] is the optimal value that minimize the MSE, if we replace \\hat \\theta \\hat \\theta with {\\bf E}\\big[\\Theta \\big] {\\bf E}\\big[\\Theta \\big] in the expression MSE, it coincidently match the definition of variance. You can alse use the {\\bf E}\\Big[\\big(\\Theta - \\hat \\theta \\big)^2\\Big] = \\textbf{Var}(\\Theta - \\hat \\theta) + \\Big({\\bf E}\\big[\\Theta - \\hat \\theta \\big]\\Big)^2 {\\bf E}\\Big[\\big(\\Theta - \\hat \\theta \\big)^2\\Big] = \\textbf{Var}(\\Theta - \\hat \\theta) + \\Big({\\bf E}\\big[\\Theta - \\hat \\theta \\big]\\Big)^2 to derive \\hat \\theta = {\\bf E}\\big[X\\big] \\hat \\theta = {\\bf E}\\big[X\\big] is the optimal value. LMS estimation; single unknown and observation \u00b6 Goal: interested in a point estimate \\hat \\theta \\hat \\theta of unknow random variable \\Theta \\Theta , with prior p_{\\Theta}(\\theta) p_{\\Theta}(\\theta) . (Given observation X X ; model p_{X\\mid \\Theta}(x\\mid \\theta) p_{X\\mid \\Theta}(x\\mid \\theta) ) We want to minimize the MSE, because this time we have the particular observation X = x X = x , we now live in a conditional universe, we need to minimize the conditional MSE, {\\bf E}\\Big[\\big(\\Theta - {\\bf E}\\big[\\Theta \\big] \\big)^2 \\mid X = x \\Big] {\\bf E}\\Big[\\big(\\Theta - {\\bf E}\\big[\\Theta \\big] \\big)^2 \\mid X = x \\Big] , the optimal value is \\hat \\theta = {\\bf E}\\big[\\Theta \\mid X = x\\big] \\hat \\theta = {\\bf E}\\big[\\Theta \\mid X = x\\big] By the \"happens to take\" reasoning and the iterated expection rule, we can achieve the conclusion: that \\hat \\Theta_{LMS} = {\\bf E}\\big[\\Theta \\mid X\\big] \\hat \\Theta_{LMS} = {\\bf E}\\big[\\Theta \\mid X\\big] minimize the MSE {\\bf E}\\Big[\\big(\\Theta - g(X) \\big)^2 \\Big] {\\bf E}\\Big[\\big(\\Theta - g(X) \\big)^2 \\Big] over all estimators \\hat \\Theta = g(X) \\hat \\Theta = g(X) . LMS performance evaluation \u00b6 MSE: {\\bf E}\\Big[\\big(\\Theta - {\\bf E}\\big[\\Theta \\mid X = x\\big] \\big)^2 \\mid X = x \\Big] = \\textbf{Var}(\\Theta \\mid X = x) {\\bf E}\\Big[\\big(\\Theta - {\\bf E}\\big[\\Theta \\mid X = x\\big] \\big)^2 \\mid X = x \\Big] = \\textbf{Var}(\\Theta \\mid X = x) Expected performance of the design: {\\bf E}\\Big[\\big(\\Theta - {\\bf E}\\big[\\Theta \\mid X\\big] \\big)^2 \\Big] = {\\bf E}\\Big[\\textbf{Var}(\\Theta \\mid X)\\Big] {\\bf E}\\Big[\\big(\\Theta - {\\bf E}\\big[\\Theta \\mid X\\big] \\big)^2 \\Big] = {\\bf E}\\Big[\\textbf{Var}(\\Theta \\mid X)\\Big] LMS relavant to estimation (not hypothesis testing) Same as MAP if the posterior is unimodal and symmetric around the mean. e.g. when the posterior is normal (the case in \"linear-normal\" models) The multidimensional case \u00b6 Lecture 17 Linear least mean squares (LLMS) estimation \u00b6 Lecture 18 Inequalities, convergence, and the Weak Law of Large Numbers \u00b6 The Weak Law of Large Numbers \u00b6 X_1, X_2, \\cdots, X_n X_1, X_2, \\cdots, X_n , i.i.d.; finite mean \\mu \\mu and variance \\sigma^2 \\sigma^2 Sample mean M_n M_n : it is not a constant, but a function of multiple random variables. You can understand it this way. In an experiment which you draw the exam score of student one by one, once you finish the first draw X_1 X_1 , the first sample is a fixed real number. But before your first draw, X_1 X_1 can be any score and it is a random variable. After you draw a total of n n scores, you have all the fixed value of X_1, X_2, \\cdots, X_n X_1, X_2, \\cdots, X_n , the sample mean is determined for this draw. However, abstractly, sample mean is a random variable, if you draw another n n example scores to get the sample mean, you get a different set of X_1, X_2, \\cdots, X_n X_1, X_2, \\cdots, X_n . The randomness comes from the randomness of different experiments, in each of the experiment, the sample X_i X_i is not random. {\\bf E}\\big[M_n\\big] =\\mu {\\bf E}\\big[M_n\\big] =\\mu . Two level of averaging. M_n M_n itself is averaging over n samples. Since these n samples are drawn randomly, taking the expectation of M_n M_n is averaging all possible sample means obtained through many (infinity) experiments. \\textbf{Var}(M_n) = \\frac{1}{n^2} \\cdot n \\cdot \\textbf{Var}(X_1) = \\frac{\\sigma^2}{n} \\textbf{Var}(M_n) = \\frac{1}{n^2} \\cdot n \\cdot \\textbf{Var}(X_1) = \\frac{\\sigma^2}{n} . Apply Chebyshev inequality: \\textbf{P}(|M_n - \\mu| \\ge \\epsilon) \\le \\frac{\\textbf{var}(M_n)}{\\epsilon^2} = \\frac{\\sigma^2}{n\\epsilon^2} \\rightarrow 0 \\textbf{P}(|M_n - \\mu| \\ge \\epsilon) \\le \\frac{\\textbf{var}(M_n)}{\\epsilon^2} = \\frac{\\sigma^2}{n\\epsilon^2} \\rightarrow 0 , as n \\rightarrow \\infty n \\rightarrow \\infty Weak Law of Large Numbers Weak Law of Large Numbers For \\epsilon > 0 \\epsilon > 0 , \\textbf{P}(|M_n - \\mu| \\ge \\epsilon) = \\textbf{P}\\Big(\\Big|\\frac{X_1 + X_2 + \\cdots + X_n}{n} - \\mu\\Big| \\ge \\epsilon \\Big) \\rightarrow 0 \\textbf{P}(|M_n - \\mu| \\ge \\epsilon) = \\textbf{P}\\Big(\\Big|\\frac{X_1 + X_2 + \\cdots + X_n}{n} - \\mu\\Big| \\ge \\epsilon \\Big) \\rightarrow 0 , as n \\rightarrow \\infty n \\rightarrow \\infty . Interpreting the WLLN \u00b6 One experiment many measurements X_i = \\mu + W_i X_i = \\mu + W_i , where W_i W_i is noise, \\textbf{E}[W_i] = 0 \\textbf{E}[W_i] = 0 , independent W_i W_i . Sample mean M_n M_n is unlikely to be far off from true mean \\mu \\mu Many independent repetitions of the same experiments event A A , with p = \\textbf{P}(A) p = \\textbf{P}(A) X_i X_i : indicator of event A A The sampel mean M_n M_n is the empirical frequency of event A A Application of WLLN - polling \u00b6 The probability of error greater than \\epsilon \\epsilon is smaller than a certain probability. You can use Chebyshev inequality and WLLN to estimate how many samples you need to fulfill a specific error probability requirement. Convergence in probability \u00b6 Definition: A sequence Y_n Y_n converges in probability to a number a a if For any \\epsilon > 0 \\epsilon > 0 , \\displaystyle \\lim_{n\\rightarrow\\infty}\\textbf{P}(|Y_n - a| \\ge \\epsilon) = 0 \\displaystyle \\lim_{n\\rightarrow\\infty}\\textbf{P}(|Y_n - a| \\ge \\epsilon) = 0 * Comparison between ordinary convergence and convergence in probability: Convergence in probability examples \u00b6 convergence in probability doesn't imply convergence of the expectations. How to find what value it converges to? Make an educated conjecture about the limit \\tau \\tau , write {\\bf P}(|Y_n - \\tau| \\ge \\epsilon) {\\bf P}(|Y_n - \\tau| \\ge \\epsilon) , and derive the value of it to where you can observe from the expression that the probability \\to 0 \\to 0 , when n \\to \\infty n \\to \\infty . Related topics \u00b6 Better bounds/approximations on tail probabilities Markov and Chebyshev inequalities Chernoff bound Central limit theorem Different types of convergence Convergence in probability Convergence \"with probability 1\" Strong law of large numbers Convergence of a sequence of distributions (CDFs) to a limiting CDF. Lecture 19 The Central Limit Theorem (CLT) \u00b6 Considering the sum of random variable S_n = X_1 + X_2, + \\cdots + X_n S_n = X_1 + X_2, + \\cdots + X_n , ( X_1, X_2, \\cdots, X_n X_1, X_2, \\cdots, X_n are i.i.d. with finite mean \\mu \\mu and variance \\sigma^2 \\sigma^2 ) Z_n = \\frac{S_n - n\\mu}{\\sqrt{n}\\sigma} Z_n = \\frac{S_n - n\\mu}{\\sqrt{n}\\sigma} , then we have the central limit Theorem, Central Limit Theorem: For every z z : \\displaystyle \\lim_{n \\to \\infty}\\textbf{P}(Z_n \\le z) = \\textbf{P}(Z \\le z) \\displaystyle \\lim_{n \\to \\infty}\\textbf{P}(Z_n \\le z) = \\textbf{P}(Z \\le z) What exactly does the CLT say? - Practice \u00b6 The practice of normal approximations: We have linear expression between S_n S_n and Z_n Z_n : S_n = \\sqrt n \\sigma Z_n + n\\mu S_n = \\sqrt n \\sigma Z_n + n\\mu , since Z_n Z_n can be treated as if it were normal, S_n S_n can be treated as if normal: N(n\\mu, n\\sigma^2) N(n\\mu, n\\sigma^2) Can we use the CLT when n is \"moderate\"? i.e. n = 30? Usually, yes. When the distribution of X X has common features with a normal distribution. symmetry and unimodality help Central Limit Theory examples \u00b6 \\textbf{P}(S_{\\color{red}n} \\le {\\color{red}a}) \\approx {\\color{red}b} \\textbf{P}(S_{\\color{red}n} \\le {\\color{red}a}) \\approx {\\color{red}b} Given two parameters, find the third. Package weight X_i X_i , i.i.d. exponential, \\lambda = 1/2 \\lambda = 1/2 , \\mu = \\sigma = 2 \\mu = \\sigma = 2 Load container with n = 100 n = 100 packages, what's the probability that the overall weight is heavier than 210? \\textbf{P}(S_n \\ge 210) = ? \\textbf{P}(S_n \\ge 210) = ? Load container with n = 100 n = 100 packages. Choose the \"capacity\" a so that \\textbf{P}(S_n \\ge a) \\approx 0.05 \\textbf{P}(S_n \\ge a) \\approx 0.05 Fix the capacity at 210, how large can n n be, so that \\textbf{P}(S_n \\ge 210) \\approx 0.05 \\textbf{P}(S_n \\ge 210) \\approx 0.05 Load container until wight exceeds 210, N N is the number of packages loaded, find \\textbf{P}(N > 100) \\textbf{P}(N > 100) Airline booking \u00b6 For any given flight, an airline tries to sell as many tickets as possible. Suppose that on average, of ticket holders fail to show up, all independent of one another. Knowing this, an airline will sell more tickets than there are seats available (i.e., overbook the flight) and hope that there is a sufficient number of ticket holders who do not show up, to compensate for its overbooking. Using the Central Limit Theorem, determine, the maximum number of tickets an airline can sell on a flight with 400 seats so that it can be approximately confident that all ticket holders who do show up will be able to board the plane. Use the de Moivre-Laplace -correction in your calculations. Hint: You may have to solve numerically a quadratic equation. Solution: Each ticket can either be used or not used by the the passager. It can be modeled with the random variable: X \\sim \\operatorname{Bern} \\left({p}\\right) X \\sim \\operatorname{Bern} \\left({p}\\right) , p=0.8 p=0.8 , then the total passergers presented at airport is S_n = X_1 + X_2 + \\cdots + X_n S_n = X_1 + X_2 + \\cdots + X_n , which is a Binomial: S_n \\sim \\operatorname{B} \\left({n, p}\\right) S_n \\sim \\operatorname{B} \\left({n, p}\\right) . For binomial random variable, \\mu = np = 0.8n, \\sigma^2 = np(1-p) = 0.16n \\mu = np = 0.8n, \\sigma^2 = np(1-p) = 0.16n . Our requirements is \\textbf{P}(S_n \\le 400) \\approx 0.99 \\textbf{P}(S_n \\le 400) \\approx 0.99 . Normalize S_n S_n in the probability and treat the normalized random variable Z_n Z_n as a normal distribution, refer to the normal table, and solve the number n n . \\begin{align*} \\textbf{P}(S_n \\le 400) &\\approx 0.99 \\\\ \\textbf{P}\\Big(\\frac{S_n - 0.8n}{0.4\\sqrt{n}} \\le \\frac{400.5 - 0.8n}{0.4\\sqrt{n}}\\Big) &\\approx 0.99 \\\\ \\textbf{P}\\Big(Z_n \\le \\frac{400.5 - 0.8n}{0.4\\sqrt{n}}\\Big) &\\approx 0.99 \\\\ \\Phi\\Big(\\frac{400.5 - 0.8n}{0.4\\sqrt{n}}\\Big) &\\approx 0.99 \\\\ \\frac{400.5 - 0.8n}{0.4\\sqrt{n}} &= 2.33 \\end{align*} \\begin{align*} \\textbf{P}(S_n \\le 400) &\\approx 0.99 \\\\ \\textbf{P}\\Big(\\frac{S_n - 0.8n}{0.4\\sqrt{n}} \\le \\frac{400.5 - 0.8n}{0.4\\sqrt{n}}\\Big) &\\approx 0.99 \\\\ \\textbf{P}\\Big(Z_n \\le \\frac{400.5 - 0.8n}{0.4\\sqrt{n}}\\Big) &\\approx 0.99 \\\\ \\Phi\\Big(\\frac{400.5 - 0.8n}{0.4\\sqrt{n}}\\Big) &\\approx 0.99 \\\\ \\frac{400.5 - 0.8n}{0.4\\sqrt{n}} &= 2.33 \\end{align*} Solve n n to obtain n = 475 n = 475 Normal approximation to the binomial \u00b6 Take X_i X_i as independent Bernuolli random variables: X \\sim \\operatorname{Bern} \\left({p}\\right) X \\sim \\operatorname{Bern} \\left({p}\\right) , 0 < p < 1 0 < p < 1 , then S_n = X_1 + X_2 + \\cdots + X_n S_n = X_1 + X_2 + \\cdots + X_n is Binomial: X \\sim \\operatorname{B} \\left({n, p}\\right) X \\sim \\operatorname{B} \\left({n, p}\\right) . Binomial random variable S_n S_n have \\mu = np \\mu = np , \\sigma^2=np(1-p) \\sigma^2=np(1-p) . According CLT, The normalized random variable \\frac{S_n-np}{\\sqrt{np(1-p)}} \\frac{S_n-np}{\\sqrt{np(1-p)}} is a standard normal. In order to find the \\textbf{P}(S_n \\le 21) \\textbf{P}(S_n \\le 21) (given n = 36 n = 36 , p = 0.5 p = 0.5 ), we use the equvilent of events to transform the probability \\textbf{P}(S_n \\le 21) \\textbf{P}(S_n \\le 21) into another probability about a normal random vairable \\textbf{P}\\Big(\\frac{S_n - 18}{3} \\le \\frac{21-28}{3}\\Big) = \\textbf{P}(Z_n \\le 1) = \\Phi(1) = 0.8413 \\textbf{P}\\Big(\\frac{S_n - 18}{3} \\le \\frac{21-28}{3}\\Big) = \\textbf{P}(Z_n \\le 1) = \\Phi(1) = 0.8413 . The 1/2 correction for integer random variables. To get a improved estimate of binomial from normal, we can take the middle point of two integers. Instead using \\textbf{P}(S_n \\le 21) \\textbf{P}(S_n \\le 21) or \\textbf{P}(S_n \\lt 22) \\textbf{P}(S_n \\lt 22) , we use \\textbf{P}(S_n \\lt 21.5) \\textbf{P}(S_n \\lt 21.5) which is very accurate De Moivre-Laplace CLT to the binomial. To estimate \\textbf{P}(S_n=19) \\textbf{P}(S_n=19) , we take \\textbf{P}(18.5 \\le S_n \\le 19.5) \\textbf{P}(18.5 \\le S_n \\le 19.5) and get an accurate estimate. Lecture 20 Introduction to classical statistics \u00b6 Overview of the classical statistical framework \u00b6 attributes Bayesian classical unkonwn \\Theta \\Theta is r.v. \\theta \\theta is a fixed value known p_\\Theta p_\\Theta prior distribution and samples X_i X_i only sample X_i X_i model p_{X \\mid \\Theta} p_{X \\mid \\Theta} , where observation is generated p_X(x; \\theta) p_X(x; \\theta) , \\theta \\theta is a real-valued parameter of the model, method use bayes rule to obtain p_{\\Theta \\mid X} p_{\\Theta \\mid X} , \\big(p_X = {\\int_{-\\infty}^{+\\infty}} p_\\Theta \\cdot p_{X \\mid \\Theta}d\\theta\\big) \\big(p_X = {\\int_{-\\infty}^{+\\infty}} p_\\Theta \\cdot p_{X \\mid \\Theta}d\\theta\\big) , then determind the estimate Design a estimator \\hat\\Theta \\hat\\Theta , to keep estimate error \\hat\\Theta - \\theta \\hat\\Theta - \\theta small estimator MAP, LMS ML Confidence intervals interpretation \u00b6 Every day, I try to estimate an unknown parameter using a fresh data set. I look at the data and then I use some formulas to calculate a 70% confidence interval, [\\hat\\Theta^-, \\hat\\Theta^+] [\\hat\\Theta^-, \\hat\\Theta^+] , based on the day's data. Classical statsistic interpretation \u00b6 If today I got the confidence interval [0.41, 0.47] [0.41, 0.47] , that doesn't mean there is 70\\% 70\\% probability that the true value will be inside [0.41, 0.47] [0.41, 0.47] . What the confidence interval is that 70% of those confidence intervals you obtained in all those days will include the true value. The \"confidence\" (or probability) can be think of \\frac{\\text{All CIs that include the true value}}{\\text{All CIs that include the true value + CIs that exclude the true value}} \\frac{\\text{All CIs that include the true value}}{\\text{All CIs that include the true value + CIs that exclude the true value}} . You cannot speak about a particular interval, and once you do that, there is no randomness anymore, the true value is either inside the interval or not, there are no other probabilities so to speak. Bayes' interpretation (Bayesian's Confidence Interval) \u00b6 Alternatively, I decided to use a Bayesian approach, by viewing the unknown parameter, denoted by \\Theta \\Theta , as a continuous random variable and assuming a prior PDF for \\Theta \\Theta . I observe a specific value x x , calculate the posterior f_{\\Theta}{X}(\\theta|x) f_{\\Theta}{X}(\\theta|x) , and find out that \\begin{align*} \\int _{0.41}^{0.47} f_{\\Theta |X}(\\theta \\, |\\, x)\\, d\\theta =0.70. \\end{align*} \\begin{align*} \\int _{0.41}^{0.47} f_{\\Theta |X}(\\theta \\, |\\, x)\\, d\\theta =0.70. \\end{align*} This time, I can say that there is a probability 70\\% 70\\% that the unknown parameter is inside the (Bayesian) confidence interval [0.41, 0.47] [0.41, 0.47] . Confidence intervals for the estimation of the mean \u00b6 I asked you to estimate the mean of i.i.d variables X_1, X_2, \\cdots, X_n X_1, X_2, \\cdots, X_n with true mean \\theta \\theta , and variance \\sigma^2 \\sigma^2 , and the estimate should achieve 95\\% 95\\% confidence interval. How you proceed with it? By Central Limit Theory, your estimate values \\hat\\Theta \\hat\\Theta is an normal distribution. By standardizing it, you get the standard normal Z_n = \\frac{\\hat\\Theta - \\theta}{\\sigma/\\sqrt{n}} Z_n = \\frac{\\hat\\Theta - \\theta}{\\sigma/\\sqrt{n}} . 95\\% 95\\% confidence interverl means standard normal is between symetric 95\\% 95\\% intervals [-b, b] [-b, b] . By looking up the normal table, b = 1.69 b = 1.69 corresponds to the probability 97.5\\% 97.5\\% , so \\textbf{P}(-1.69 \\le Z_n \\le 1.69) = 95\\% \\textbf{P}(-1.69 \\le Z_n \\le 1.69) = 95\\% thus \\textbf{P}\\big(\\frac{|\\hat\\Theta - \\theta|}{\\sigma/\\sqrt{n}} \\le 1.69\\big) = 95\\% \\textbf{P}\\big(\\frac{|\\hat\\Theta - \\theta|}{\\sigma/\\sqrt{n}} \\le 1.69\\big) = 95\\% , which can be rewrite as \\begin{align*} \\textbf{P}\\Big(\\hat\\Theta -\\frac{1.96\\sigma}{\\sqrt n} \\le \\theta \\le \\hat\\Theta +\\frac{1.96\\sigma}{\\sqrt n}\\Big) = 95\\%. \\end{align*} \\begin{align*} \\textbf{P}\\Big(\\hat\\Theta -\\frac{1.96\\sigma}{\\sqrt n} \\le \\theta \\le \\hat\\Theta +\\frac{1.96\\sigma}{\\sqrt n}\\Big) = 95\\%. \\end{align*} This is the way to construct the confidence interval. If you have the \\sigma \\sigma and n n , you have a concrete 95\\% 95\\% confidence interval to report. However, the \\sigma \\sigma is usually unknown, what you can do is to use the estimated \\hat\\Theta \\hat\\Theta to estimate the \\sigma \\sigma . Lecture 21 \u00b6 Lecture 22 \u00b6 Lecture 23 \u00b6 Lecture 24 \u00b6 Lecture 25 \u00b6 Lecture 26 \u00b6 Resources \u00b6 Distributtion Explorer by Justin Bois Tutorial 3b: Probability distributions and their stories Think Stats: Exploratory Data Analysis in Python Introduction to Probability Introduction to Probability and Statistics","title":"6.431 Probability"},{"location":"courses/6.431-probability/notes/#60416431-probability-the-science-of-uncertainty-and-data","text":"","title":"6.041/6.431 Probability - The Science of Uncertainty and Data"},{"location":"courses/6.431-probability/notes/#lecture-1-september-10-2018","text":"","title":"Lecture 1 (September 10, 2018)"},{"location":"courses/6.431-probability/notes/#probability-models","text":"To specify such a model, it includes 2 steps: Sample space: describe possible outcomes of an experiment Probability laws: describe beliefs about the likelihood of outcomes (or assign a probability to outcomes)","title":"Probability Models"},{"location":"courses/6.431-probability/notes/#sample-space","text":"list of possible outcomes must be mutually exclusive, collectively exhaustive, and at the right granularity.","title":"Sample space"},{"location":"courses/6.431-probability/notes/#probability-laws","text":"","title":"Probability laws"},{"location":"courses/6.431-probability/notes/#probability-axioms","text":"Motivation: Assign individual probability to a single dot is a challenge (in the dart example, each dot has zero probability), so we assign a probability to the subset of the sample space. Nonnegativity: {\\bf P}(A) \\geq 0 {\\bf P}(A) \\geq 0 Normalization: {\\bf P}(\\Omega) = 1 {\\bf P}(\\Omega) = 1 (Finite) Additivity: if A \\cap B = \\emptyset A \\cap B = \\emptyset then {\\bf P}(A \\cup B) = {\\bf P}(A) + {\\bf P}(B) {\\bf P}(A \\cup B) = {\\bf P}(A) + {\\bf P}(B) (need to be refined in future)","title":"Probability axioms"},{"location":"courses/6.431-probability/notes/#consequences-of-axioms-properties-of-probability-axioms","text":"{\\bf P}(A) \\leq 1 {\\bf P}(A) \\leq 1 {\\bf P}(\\emptyset) = 0 {\\bf P}(\\emptyset) = 0 {\\bf P}(A) + {\\bf P}(A^c) = 1 {\\bf P}(A) + {\\bf P}(A^c) = 1 A, B, \\text{and } C A, B, \\text{and } C are disjoint: {\\bf P}(A \\cup B \\cup C) = {\\bf P}(A) + {\\bf P}(B) + {\\bf P}(C) {\\bf P}(A \\cup B \\cup C) = {\\bf P}(A) + {\\bf P}(B) + {\\bf P}(C) A_1, \\cdots A_k, A_1, \\cdots A_k, are disjoint: {\\bf P}(A_1 \\cup \\cdots, \\cup A_k) = \\sum_{i - 1}^k{\\bf P}(A_i) {\\bf P}(A_1 \\cup \\cdots, \\cup A_k) = \\sum_{i - 1}^k{\\bf P}(A_i) If A \\subset B A \\subset B , then {\\bf P}(A) \\leq {\\bf P}(B) {\\bf P}(A) \\leq {\\bf P}(B) {\\bf P}(A \\cup B) = {\\bf P}(A) + {\\bf P}(B) - {\\bf P}(A \\cap B) {\\bf P}(A \\cup B) = {\\bf P}(A) + {\\bf P}(B) - {\\bf P}(A \\cap B) {\\bf P}(A \\cup B \\cup C) = {\\bf P}(A) + {\\bf P}(A^c \\cap B) + {\\bf P}(A^c \\cap B^c \\cap C) {\\bf P}(A \\cup B \\cup C) = {\\bf P}(A) + {\\bf P}(A^c \\cap B) + {\\bf P}(A^c \\cap B^c \\cap C) Union Bound: {\\bf P}(A \\cup B) \\leq {\\bf P}(A) + {\\bf P}(B) {\\bf P}(A \\cup B) \\leq {\\bf P}(A) + {\\bf P}(B)","title":"Consequences of axioms (properties of probability axioms)"},{"location":"courses/6.431-probability/notes/#probability-calculation-steps","text":"Specify the sample space Specify a probability law Identify an event of interest Calculate","title":"Probability calculation steps"},{"location":"courses/6.431-probability/notes/#countable-additivity","text":"The 3rd probability axiom previously mentioned can be extended to countable set (all integers), which means that the element in the set can be arranged in a sequence. This is contrary to the concept of uncountable set (i.e. a dart board, 2D plane). So you need to distinguish the following concepts: discrete set (countable set) continuous set (uncountable set, impossible to arrange in a sequence) finite set infinite set (possible to be arranged in a sequence) discrete (countable) finite set (i.e. {1, 2, 3}) discrete (countable) infinite set (i.e. all integers, all even integers, all old integers) continuous (uncountable) infinite set (i.e. \\{x | 0 \\leq x \\leq 1\\} \\{x | 0 \\leq x \\leq 1\\} ) Example: Sample space \\{1, 2, 3, \\cdots\\} \\{1, 2, 3, \\cdots\\} , given {\\bf P}(n) = \\frac{1}{2^n}, n = 1, 2, \\cdots {\\bf P}(n) = \\frac{1}{2^n}, n = 1, 2, \\cdots . Check the against the probability axioms that \\sum_{n = 1}^{\\infty}{\\bf P}(n) = 1 \\sum_{n = 1}^{\\infty}{\\bf P}(n) = 1 . What about the probability {\\bf P}(\\text{outcome is even}) {\\bf P}(\\text{outcome is even}) ? Using countable additivity axiom.","title":"Countable additivity"},{"location":"courses/6.431-probability/notes/#countable-additivity-axiom-refined-probability-axiom","text":"Note if A_1, A_2, A_3, \\cdot A_1, A_2, A_3, \\cdot is an infinite sequence of disjoint events, then {\\bf P}(A_1 \\cup A_2 \\cup A_3 \\cdots) = {\\bf P}(A_1) + {\\bf P}(A_2) + {\\bf P}(A_3) + \\cdots {\\bf P}(A_1 \\cup A_2 \\cup A_3 \\cdots) = {\\bf P}(A_1) + {\\bf P}(A_2) + {\\bf P}(A_3) + \\cdots To refine the 3rd probability axiom, additivity holds only for \"countable\" sequences of events. That means the additivity axiom must be a sequence , with finite or infinite elements. Note the following contradiction, the additivity axiom can not be applied to continuous sample space. A_1, A_2, A_3, \\cdots A_1, A_2, A_3, \\cdots are real coordinates, if you apply the additivity axiom, you will get: \\begin{align*} 1={\\bf P}(\\Omega ) &={\\bf P}\\big (\\{ A_1\\} \\cup \\{ A_2\\} \\cup \\{ A_3\\} \\cdots \\big )\\\\ &={\\bf P}(\\{ A_1\\} )+{\\bf P}(\\{ A_2\\} )+{\\bf P}(\\{ A_3\\} )+\\cdots \\\\ &= 0+0+0+\\cdots =0, \\end{align*} \\begin{align*} 1={\\bf P}(\\Omega ) &={\\bf P}\\big (\\{ A_1\\} \\cup \\{ A_2\\} \\cup \\{ A_3\\} \\cdots \\big )\\\\ &={\\bf P}(\\{ A_1\\} )+{\\bf P}(\\{ A_2\\} )+{\\bf P}(\\{ A_3\\} )+\\cdots \\\\ &= 0+0+0+\\cdots =0, \\end{align*} which is contradicts. This is because \"unit square\" is not a countable set.","title":"Countable Additivity Axiom (refined probability axiom)"},{"location":"courses/6.431-probability/notes/#interpretations-of-probability-theory","text":"frequency of events A? What is P(the president will be reelected)? Probability is often interpreted as: Description of beliefs Betting preference","title":"Interpretations of probability theory"},{"location":"courses/6.431-probability/notes/#lecture-2-conditioning-and-bayes-rule","text":"","title":"Lecture 2 Conditioning and Baye's rule"},{"location":"courses/6.431-probability/notes/#conditional-probability","text":"use new information to review a model Definition {\\bf P}(A|B) = {\\bf P}(A|B) = \"probability of A given that B occurred\" {\\bf P}(A|B) = \\frac{{\\bf P}(A \\cap B)}{{\\bf P}(B)} {\\bf P}(A|B) = \\frac{{\\bf P}(A \\cap B)}{{\\bf P}(B)} , defined only when {\\bf P}(B) > 0 {\\bf P}(B) > 0 Conditional probability share properties of ordinary probabilities (finite and infinite countable additivity) If A \\cap C = \\emptyset A \\cap C = \\emptyset , then {\\bf P}(A \\cup C|B) = P(A|B) + {\\bf P}(C|B) {\\bf P}(A \\cup C|B) = P(A|B) + {\\bf P}(C|B)","title":"Conditional probability"},{"location":"courses/6.431-probability/notes/#multiplication-rule","text":"{\\bf P}(A \\cap B) = {\\bf P}(A){\\bf P}(B|A) = {\\bf P}(B){\\bf P}(A|B) {\\bf P}(A \\cap B) = {\\bf P}(A){\\bf P}(B|A) = {\\bf P}(B){\\bf P}(A|B) {\\bf P}(A \\cap B \\cap C) = {\\bf P}(A){\\bf P}(B|A){\\bf P}(C|A \\cap B) {\\bf P}(A \\cap B \\cap C) = {\\bf P}(A){\\bf P}(B|A){\\bf P}(C|A \\cap B) It also applies to n events.","title":"Multiplication rule"},{"location":"courses/6.431-probability/notes/#total-probability-theorem-divide-and-conquer","text":"The settings: Partition of sample space into A_1, A_2, A_3 A_1, A_2, A_3 , have {\\bf P}(A_i) {\\bf P}(A_i) for every i i have {\\bf P}(B|A_i) {\\bf P}(B|A_i) for every i i {\\bf P}(B) = \\sum_{i} {\\bf P}(A_i){\\bf P}(B|A_i) {\\bf P}(B) = \\sum_{i} {\\bf P}(A_i){\\bf P}(B|A_i) It also applies to infinite countable sets according to the countable additivity axiom.","title":"Total probability theorem (divide and conquer)"},{"location":"courses/6.431-probability/notes/#bayes-rule","text":"The setting is the same as the total probability theorem. Partition of sample space into A_1, A_2, A_3 A_1, A_2, A_3 , have {\\bf P}(A_i) {\\bf P}(A_i) for every i i (initial believes) have {\\bf P}(B|A_i) {\\bf P}(B|A_i) for every i i revise \"believe\" given that B occurred: {\\bf P}(A_i|B) = \\frac{{\\bf P}(A_i \\cap B)}{{\\bf P}(B)} {\\bf P}(A_i|B) = \\frac{{\\bf P}(A_i \\cap B)}{{\\bf P}(B)} It turns out that we can use multiplication rule to calculate the nominator and use the total probability rule to calculate the denominator, thusly {\\bf P}(A_i|B) = \\frac{{\\bf P}(A_i){\\bf P}(B|A_i)}{\\sum_j{\\bf P}(A_j){\\bf P}(B|A_i)} {\\bf P}(A_i|B) = \\frac{{\\bf P}(A_i){\\bf P}(B|A_i)}{\\sum_j{\\bf P}(A_j){\\bf P}(B|A_i)}","title":"Baye's rule"},{"location":"courses/6.431-probability/notes/#lecture-3-independence","text":"","title":"Lecture 3 Independence"},{"location":"courses/6.431-probability/notes/#independence-of-two-events","text":"Motivation coin toss example: First toss is head or tail doesn't affect the probability of second toss is head. Occurance of A A privides no new information about B B : {\\bf P}(B|A) = {\\bf P}(B) {\\bf P}(B|A) = {\\bf P}(B) .","title":"Independence of two events"},{"location":"courses/6.431-probability/notes/#definition-of-independence","text":"Formal definition {\\bf P}(A \\cap B) = {\\bf P}(A) \\cdot {\\bf P}(B) {\\bf P}(A \\cap B) = {\\bf P}(A) \\cdot {\\bf P}(B) symmetric with respect to events. implies {\\bf P}(A|B) = {\\bf P}(A) {\\bf P}(A|B) = {\\bf P}(A) . Applies even if {\\bf P}(A) = 0 {\\bf P}(A) = 0 . Distinction between disjoin and independence Disjoin usually means dependent, because if one event happens we know the other event will not happen.","title":"Definition of independence"},{"location":"courses/6.431-probability/notes/#independence-of-event-complements","text":"If A A and B B are independent, the A A and B^c B^c are independent. If A A and B B are independent, the A^c A^c and B^c B^c are independent.","title":"Independence of event complements"},{"location":"courses/6.431-probability/notes/#conditioning-independence","text":"Ordinary independence properties also apply to conditional independence. But independence doesn't imply conditional independence.","title":"Conditioning independence"},{"location":"courses/6.431-probability/notes/#independence-of-a-collection-of-events","text":"Intuition \"definition\": Information on some of the events doesn't change probability related to the remaining events. Formal definition","title":"Independence of a collection of events"},{"location":"courses/6.431-probability/notes/#pairwise-independence","text":"","title":"pairwise independence"},{"location":"courses/6.431-probability/notes/#independence-vs-pairwise-independence","text":"Used two independence fair coin tosses as an example to show that pariwise independence isn't enough to show independence of collection of events.","title":"independence V.S. pairwise independence"},{"location":"courses/6.431-probability/notes/#reliability","text":"The king's sibling puzzle","title":"Reliability"},{"location":"courses/6.431-probability/notes/#monty-hall-problem","text":"","title":"Monty Hall problem"},{"location":"courses/6.431-probability/notes/#lecture-4","text":"","title":"Lecture 4"},{"location":"courses/6.431-probability/notes/#lecture-5","text":"","title":"Lecture 5"},{"location":"courses/6.431-probability/notes/#definition-of-random-variables","text":"A random variable (\"r.v.\") associates a value (a number) to every possible outcome. Mathematically, A function from sample space \\Omega \\Omega to the real numbers (discrete or continuous). We can have several random variables defined on the same sample space. (In Lecture 18, X_1, X_2, \\cdots X_1, X_2, \\cdots are independent random variables from the same distribution) A function of one or several random variables is also another random variable. A concrete example to understand the concept of a random variable is take a class of students, selecting students and measuring their body mass or height will give us the random variable W W and H H . The \"Body mass index\" is another random variable that is a function of the random variable W W and H H . meaning of X + Y X + Y random variable X+Y X+Y takes the value x+y x+y , when X X takes the value x x , and Y Y takes the value y y .","title":"Definition of random variables"},{"location":"courses/6.431-probability/notes/#probability-mass-function","text":"","title":"Probability mass function"},{"location":"courses/6.431-probability/notes/#bernoulli-and-indicator-random-variables","text":"Bernoulli random variable X X : X = \\begin{cases} 1, & \\textrm{w.p. } p \\\\ 0, & \\textrm{w.p. } 1-p \\end{cases} X = \\begin{cases} 1, & \\textrm{w.p. } p \\\\ 0, & \\textrm{w.p. } 1-p \\end{cases} models a trial that results in success/failure, heads/tails, etc. indicator random variables of event A A : I_A = 1 I_A = 1 if A A occurs. connection between events and random variables. P(A) = P(I_A = 1) = P_{I_A}(1) P(A) = P(I_A = 1) = P_{I_A}(1) , P(A) P(A) : probability of event A happens P(I_A = 1) P(I_A = 1) : probablistic notition of indication R.V. when I_A I_A equal to 1 1 P_{I_A}(1) P_{I_A}(1) : PMF notation.","title":"Bernoulli and indicator random variables"},{"location":"courses/6.431-probability/notes/#discrete-uniform-random-variables","text":"","title":"Discrete uniform random variables"},{"location":"courses/6.431-probability/notes/#binomial-random-variables","text":"Experiment: toss a biased coin ( P(\\textrm{Head})=p P(\\textrm{Head})=p ) n n times and observe the result (number of heads)","title":"Binomial random variables"},{"location":"courses/6.431-probability/notes/#geometric-random-variables","text":"","title":"Geometric random variables"},{"location":"courses/6.431-probability/notes/#expectation","text":"","title":"Expectation"},{"location":"courses/6.431-probability/notes/#the-expected-value-rule","text":"","title":"The expected value rule"},{"location":"courses/6.431-probability/notes/#linearity-of-expectations","text":"","title":"Linearity of expectations"},{"location":"courses/6.431-probability/notes/#lecture-6","text":"","title":"Lecture 6"},{"location":"courses/6.431-probability/notes/#lecture-7","text":"","title":"Lecture 7"},{"location":"courses/6.431-probability/notes/#independence-variances-and-binomial-variance","text":"","title":"Independence, variances, and binomial variance"},{"location":"courses/6.431-probability/notes/#lecture-8","text":"","title":"Lecture 8"},{"location":"courses/6.431-probability/notes/#exponential-random-variables","text":"PDF: f_{X}(x) = \\begin{cases} \\lambda e^{-\\lambda x}, & x \\ge 0 \\\\ 0, & x < 0 \\end{cases} f_{X}(x) = \\begin{cases} \\lambda e^{-\\lambda x}, & x \\ge 0 \\\\ 0, & x < 0 \\end{cases} Probability of greater than a a , \\begin{align*} P(X \\ge a) &= \\displaystyle \\int_a^{+\\infty} \\lambda e^{-\\lambda x}dx \\\\ &= - \\displaystyle \\int_a^{+\\infty} de^{-\\lambda x} \\\\ &= -e^{-\\lambda x}\\Big|_a^{+\\infty} = -e^{-\\lambda \\cdot +\\infty} + e^{-\\lambda a} = e^{-\\lambda a} \\end{align*} \\begin{align*} P(X \\ge a) &= \\displaystyle \\int_a^{+\\infty} \\lambda e^{-\\lambda x}dx \\\\ &= - \\displaystyle \\int_a^{+\\infty} de^{-\\lambda x} \\\\ &= -e^{-\\lambda x}\\Big|_a^{+\\infty} = -e^{-\\lambda \\cdot +\\infty} + e^{-\\lambda a} = e^{-\\lambda a} \\end{align*} Expectation of exponential random variable {\\bf E}\\big[X\\big] = \\displaystyle \\int_0^{\\infty} x \\cdot \\lambda e^{-\\lambda x} dx = 1/\\lambda {\\bf E}\\big[X\\big] = \\displaystyle \\int_0^{\\infty} x \\cdot \\lambda e^{-\\lambda x} dx = 1/\\lambda Second moment of an exponential random variable {\\bf E}\\big[X^2 \\big] = \\displaystyle \\int_0^{\\infty} x^2 \\cdot \\lambda e^{-\\lambda x} dx = 2/\\lambda^2 {\\bf E}\\big[X^2 \\big] = \\displaystyle \\int_0^{\\infty} x^2 \\cdot \\lambda e^{-\\lambda x} dx = 2/\\lambda^2 Variance of an exponential random variable \\textbf{Var}(X) = {\\bf E}\\big[X^2 \\big] - \\Big({\\bf E}\\big[X\\big]\\Big)^2 = 1/\\lambda^2 \\textbf{Var}(X) = {\\bf E}\\big[X^2 \\big] - \\Big({\\bf E}\\big[X\\big]\\Big)^2 = 1/\\lambda^2","title":"Exponential random variables"},{"location":"courses/6.431-probability/notes/#lecture-9","text":"","title":"Lecture 9"},{"location":"courses/6.431-probability/notes/#conditioning-a-continuous-random-variable-on-an-event","text":"","title":"Conditioning a continuous random variable on an event"},{"location":"courses/6.431-probability/notes/#memorylessness-of-the-exponential-pdf","text":"","title":"Memorylessness of the exponential PDF"},{"location":"courses/6.431-probability/notes/#total-probability-and-expectation-theorems","text":"","title":"Total probability and expectation theorems"},{"location":"courses/6.431-probability/notes/#lecture-10","text":"","title":"Lecture 10"},{"location":"courses/6.431-probability/notes/#total-probability-and-total-expectation-theorems","text":"Follows from the discrete cases: Rules Discrete continous Total probability p_X(x) = \\displaystyle \\sum\\limits_{y} p_Y(y)p_{X \\mid Y}(x \\mid y) p_X(x) = \\displaystyle \\sum\\limits_{y} p_Y(y)p_{X \\mid Y}(x \\mid y) f_X(x) = \\displaystyle \\int_{-\\infty}^{+\\infty} f_Y(y) f_{X \\mid Y}(x \\mid y) dy f_X(x) = \\displaystyle \\int_{-\\infty}^{+\\infty} f_Y(y) f_{X \\mid Y}(x \\mid y) dy Conditional expectation {\\bf E}\\big[X \\mid Y = y \\big] = \\displaystyle \\sum\\limits_{x} x p_{X \\mid Y}(x \\mid y) {\\bf E}\\big[X \\mid Y = y \\big] = \\displaystyle \\sum\\limits_{x} x p_{X \\mid Y}(x \\mid y) {\\bf E}\\big[X \\mid Y = y \\big] = \\displaystyle {\\int_{-\\infty}^{+\\infty}} x f_{X \\mid Y}(x \\mid y) dx {\\bf E}\\big[X \\mid Y = y \\big] = \\displaystyle {\\int_{-\\infty}^{+\\infty}} x f_{X \\mid Y}(x \\mid y) dx Total expectation {\\bf E}\\big[X \\big] = \\displaystyle \\sum\\limits_{y} p_{Y}(y){\\bf E} \\big[X \\mid Y = y \\big] {\\bf E}\\big[X \\big] = \\displaystyle \\sum\\limits_{y} p_{Y}(y){\\bf E} \\big[X \\mid Y = y \\big] derivation hint: replace with the conditional expectatioin {\\bf E}\\big[X \\big] = \\displaystyle {\\int_{-\\infty}^{+\\infty}} f_{Y}(y){\\bf E}\\big[X \\mid Y = y \\big] dy {\\bf E}\\big[X \\big] = \\displaystyle {\\int_{-\\infty}^{+\\infty}} f_{Y}(y){\\bf E}\\big[X \\mid Y = y \\big] dy derivation hint: replace with the conditional expectatioin Expected value rule {\\bf E}\\big[ g(X) \\mid Y = y \\big] = \\displaystyle {\\sum\\limits_{x}} g(x) p_{X \\mid Y}(x \\mid y) {\\bf E}\\big[ g(X) \\mid Y = y \\big] = \\displaystyle {\\sum\\limits_{x}} g(x) p_{X \\mid Y}(x \\mid y) {\\bf E}\\big[ g(X) \\mid Y = y \\big] = \\displaystyle {\\int_{-\\infty}^{+\\infty}} g(x) f_{X \\mid Y}(x \\mid y) dx {\\bf E}\\big[ g(X) \\mid Y = y \\big] = \\displaystyle {\\int_{-\\infty}^{+\\infty}} g(x) f_{X \\mid Y}(x \\mid y) dx","title":"Total probability and total expectation theorems"},{"location":"courses/6.431-probability/notes/#solved-problems-lecture-8-10","text":"","title":"Solved problems (Lecture 8 - 10)"},{"location":"courses/6.431-probability/notes/#10-buffons-needle-and-monte-carlo-simulation","text":"This problem is discussed in the text page 161, Example 3.11.","title":"10 Buffon's needle and Monte Carlo Simulation"},{"location":"courses/6.431-probability/notes/#lecture-11","text":"","title":"Lecture 11"},{"location":"courses/6.431-probability/notes/#lecture-12","text":"","title":"Lecture 12"},{"location":"courses/6.431-probability/notes/#covariance","text":"Definition: \\textbf{cov}(X, Y) = {\\bf E}\\big[(X - {\\bf E}[X]) (Y - {\\bf E}[Y])\\big] \\textbf{cov}(X, Y) = {\\bf E}\\big[(X - {\\bf E}[X]) (Y - {\\bf E}[Y])\\big] Covariance tell use whether two r.v.s tend to move together, both to low or both to high Two random variables X X and Y Y are independent indicates: \\textbf{cov}(X, Y) = 0 \\textbf{cov}(X, Y) = 0 , but the converse it not true.","title":"Covariance"},{"location":"courses/6.431-probability/notes/#covariance-properties","text":"","title":"Covariance properties"},{"location":"courses/6.431-probability/notes/#lecture-13","text":"","title":"Lecture 13"},{"location":"courses/6.431-probability/notes/#the-conditional-expectation-as-a-random-variable-bf-e-bigx-mid-ybigbf-e-bigx-mid-ybig","text":"Given a function h(x) = x^2 h(x) = x^2 for all x x , and a random variable X X . what is h(X) h(X) ? h(X) h(X) is a function of a random variable X X . h(X) h(X) itself is a random variable that take value x^2 x^2 if X X happens to take the value x x . (you should really understand this statement, it is crucial to understand the concept of conditional expectatioin) For discrete case: {\\bf E}\\big[X \\mid Y = y \\big] = \\displaystyle \\sum\\limits_{x} x p_{X \\mid Y}(x \\mid y) {\\bf E}\\big[X \\mid Y = y \\big] = \\displaystyle \\sum\\limits_{x} x p_{X \\mid Y}(x \\mid y) , in which different vaule of y y will give us the different value of conditional expectation of X X . Then we can really treat {\\bf E}\\big[X \\mid Y = y \\big] {\\bf E}\\big[X \\mid Y = y \\big] as a function of y y , noted as g(y) g(y) . From the above reasoning, because Y Y is a random variable, g(Y) g(Y) is also a reandom variable, that takes the value of g(y) = {\\bf E}\\big[X \\mid Y = y \\big] g(y) = {\\bf E}\\big[X \\mid Y = y \\big] if Y Y happens to take the value y y . g(Y) = {\\bf E}\\big[X \\mid Y\\big] g(Y) = {\\bf E}\\big[X \\mid Y\\big] , is a function of Y Y a random variable has a distribution, mean, variance, etc.","title":"The conditional expectation as a random variable {\\bf E} \\big[X \\mid Y\\big]{\\bf E} \\big[X \\mid Y\\big]"},{"location":"courses/6.431-probability/notes/#the-law-of-iterated-expectations","text":"By calculate {\\bf E}\\Big[{\\bf E}\\big[X \\mid Y\\big]\\Big] {\\bf E}\\Big[{\\bf E}\\big[X \\mid Y\\big]\\Big] , we could abtain that it is equal to {\\bf E}\\big[X\\big] {\\bf E}\\big[X\\big]","title":"The law of iterated expectations"},{"location":"courses/6.431-probability/notes/#the-conditional-variance-as-a-random-variable-textbfvarx-mid-y-ytextbfvarx-mid-y-y","text":"definition: \\textbf{Var}(X \\mid Y = y) = {\\bf E}\\Big[\\big(X - {\\bf E}\\big[X \\mid Y = y \\big]\\big)^2\\mid Y = y\\Big] \\textbf{Var}(X \\mid Y = y) = {\\bf E}\\Big[\\big(X - {\\bf E}\\big[X \\mid Y = y \\big]\\big)^2\\mid Y = y\\Big] \\textbf{Var}(X\\mid Y) \\textbf{Var}(X\\mid Y) is also a random variable, it takes the value \\textbf{Var}(X\\mid y) \\textbf{Var}(X\\mid y) when Y Y happens to take the value of y y . Total variance rule: \\textbf{Var}(X) = {\\bf E}\\Big[\\textbf{Var}(X \\mid Y)\\Big] + \\textbf{Var}\\Big({\\bf E}\\big[X\\mid Y\\big]\\Big) \\textbf{Var}(X) = {\\bf E}\\Big[\\textbf{Var}(X \\mid Y)\\Big] + \\textbf{Var}\\Big({\\bf E}\\big[X\\mid Y\\big]\\Big)","title":"The conditional variance as a random variable \\textbf{Var}(X \\mid Y = y)\\textbf{Var}(X \\mid Y = y)"},{"location":"courses/6.431-probability/notes/#lecture-14-introduction-to-bayesian-inference","text":"","title":"Lecture 14 Introduction to Bayesian Inference"},{"location":"courses/6.431-probability/notes/#the-bayesian-inference-framework","text":"Treat the unknown \\Theta \\Theta as a random variable. with prior p_{\\Theta} p_{\\Theta} or f_{\\Theta} f_{\\Theta} . With the observations X X , using Bayes rule to obtain the posterior probability of \\Theta \\Theta , P_{\\Theta|X}(\\cdot|X=x) P_{\\Theta|X}(\\cdot|X=x) . Once the posterior is available, the estimator can be obtained via maximize a posterioral probability (MAP) rule or leaset mean squares (LMS). The performance can be measured by \"probability of error\" or \"mean squared error\".","title":"The Bayesian inference framework"},{"location":"courses/6.431-probability/notes/#conditional-probability-of-error-and-total-probability-of-error","text":"The conditional probability of other estimators that made a mistake. One of the properties of MAP rule is the it guarantee smallest probability of error. The taotal probability of error is an abstract notion of error for MAP rule to estimator. It can be caculated using total probability theory.","title":"Conditional probability of error and total probability of error"},{"location":"courses/6.431-probability/notes/#discrete-unknowns-discrete-observation-example-8-exercise-discrete-unknowns","text":"Let \\Theta_1 \\Theta_1 and \\Theta_2 \\Theta_2 be some unobserved Bernoulli random variables and let be an observation. Conditional on X = x X = x , the posterior joint PMF of \\Theta_1 \\Theta_1 and \\Theta_2 \\Theta_2 is given by p_{\\Theta _1,\\Theta _2\\mid X}(\\theta _1,\\theta _2\\mid x) = \\begin{cases} 0.26, & \\mbox{if } \\theta _1=0, \\theta _2=0, \\\\ 0.26, & \\mbox{if } \\theta _1=0, \\theta _2=1, \\\\ 0.21, & \\mbox{if } \\theta _1=1, \\theta _2=0, \\\\ 0.27, & \\mbox{if } \\theta _1=1, \\theta _2=1, \\\\ 0, & \\mbox{otherwise.} \\end{cases} p_{\\Theta _1,\\Theta _2\\mid X}(\\theta _1,\\theta _2\\mid x) = \\begin{cases} 0.26, & \\mbox{if } \\theta _1=0, \\theta _2=0, \\\\ 0.26, & \\mbox{if } \\theta _1=0, \\theta _2=1, \\\\ 0.21, & \\mbox{if } \\theta _1=1, \\theta _2=0, \\\\ 0.27, & \\mbox{if } \\theta _1=1, \\theta _2=1, \\\\ 0, & \\mbox{otherwise.} \\end{cases} What is the estimate of (\\Theta_1, \\Theta_2) (\\Theta_1, \\Theta_2) provided by MAP rule? (1, 1), because the p_{\\Theta _1,\\Theta _2\\mid X}(1, 1\\mid x) p_{\\Theta _1,\\Theta _2\\mid X}(1, 1\\mid x) is the maximum of all. What is the MAP estimate of \\Theta_1 \\Theta_1 based on X X , that is, the one that maximizes p_{\\Theta_1 \\mid X}(\\theta_1 \\mid x) p_{\\Theta_1 \\mid X}(\\theta_1 \\mid x) ? 0, from the marginal PMF, you can see that p_{\\Theta_1 \\mid X}(0 \\mid x) p_{\\Theta_1 \\mid X}(0 \\mid x) is maximized The moral of this problem is that an estimate of $ \\Theta_1 $ obtained by identifying the maximum of the join PMF of all unknown random variables can be different from the MAP estimate of \\Theta_1 \\Theta_1 from the marginal PMF.","title":"Discrete unknowns, discrete observation example (8. Exercise: Discrete unknowns)"},{"location":"courses/6.431-probability/notes/#discrete-parameter-continuous-observation","text":"Digital signal transmission","title":"Discrete parameter, continuous observation"},{"location":"courses/6.431-probability/notes/#continous-parameter-continuous-observation","text":"Analog signal transmission","title":"continous parameter, continuous observation"},{"location":"courses/6.431-probability/notes/#lecture-15-linear-models-with-normal-noise","text":"","title":"Lecture 15 Linear models with normal noise"},{"location":"courses/6.431-probability/notes/#recognizing-normal-pdfs","text":"f_X(x) = c \\cdot e^{-(\\alpha x^2 + \\beta x + \\gamma)} f_X(x) = c \\cdot e^{-(\\alpha x^2 + \\beta x + \\gamma)} is a normal random variable with \\mu = -\\frac{\\beta}{2\\alpha} \\mu = -\\frac{\\beta}{2\\alpha} and variance \\frac{1}{2\\alpha} \\frac{1}{2\\alpha}","title":"recognizing normal PDFs"},{"location":"courses/6.431-probability/notes/#the-mean-squared-error","text":"The most important take away from this section is that for normal unknown and normal noise signal, X = \\Theta + W X = \\Theta + W , no matter which x_i x_i we observed, the mean squared error estimates {\\hat \\theta} {\\hat \\theta} are the same (the variance of \\Theta \\Theta ). In other words, the remaining uncertainty about \\theta \\theta after an observation is the same no matter what the observation is. The observation only determined the estimated mean of the random variable \\theta \\theta . (See the drawing at the end of the lecture)","title":"The mean squared error"},{"location":"courses/6.431-probability/notes/#measurement-estimate-and-learning","text":"How to measure the gravitational attraction constant","title":"Measurement, estimate, and learning"},{"location":"courses/6.431-probability/notes/#lecture-16-least-mean-square-lms-estimation","text":"","title":"Lecture 16 Least mean square (LMS) estimation"},{"location":"courses/6.431-probability/notes/#lms-estimation-without-any-observations","text":"Given the prior p_{\\Theta}(\\theta) p_{\\Theta}(\\theta) unknow random variable \\Theta \\Theta , what's you realy want to estimate? You may interested in a point estimate. You may interested in find the estimator. With no observation, to minimize the mean squared error (MSE), {\\bf E}\\Big[\\big(\\Theta - \\hat \\theta \\big)^2\\Big] {\\bf E}\\Big[\\big(\\Theta - \\hat \\theta \\big)^2\\Big] , the value \\hat \\theta = {\\bf E}\\big[X\\big] \\hat \\theta = {\\bf E}\\big[X\\big] minimize the error. and the minimum error equal to \\textbf{Var}(\\Theta) \\textbf{Var}(\\Theta) Optimal mean squared error: {\\bf E}\\Big[\\big(\\Theta - {\\bf E}\\big[\\Theta \\big] \\big)^2\\Big] = \\textbf{Var}(\\Theta) {\\bf E}\\Big[\\big(\\Theta - {\\bf E}\\big[\\Theta \\big] \\big)^2\\Big] = \\textbf{Var}(\\Theta) . Because {\\bf E}\\big[\\Theta \\big] {\\bf E}\\big[\\Theta \\big] is the optimal value that minimize the MSE, if we replace \\hat \\theta \\hat \\theta with {\\bf E}\\big[\\Theta \\big] {\\bf E}\\big[\\Theta \\big] in the expression MSE, it coincidently match the definition of variance. You can alse use the {\\bf E}\\Big[\\big(\\Theta - \\hat \\theta \\big)^2\\Big] = \\textbf{Var}(\\Theta - \\hat \\theta) + \\Big({\\bf E}\\big[\\Theta - \\hat \\theta \\big]\\Big)^2 {\\bf E}\\Big[\\big(\\Theta - \\hat \\theta \\big)^2\\Big] = \\textbf{Var}(\\Theta - \\hat \\theta) + \\Big({\\bf E}\\big[\\Theta - \\hat \\theta \\big]\\Big)^2 to derive \\hat \\theta = {\\bf E}\\big[X\\big] \\hat \\theta = {\\bf E}\\big[X\\big] is the optimal value.","title":"LMS estimation without any observations"},{"location":"courses/6.431-probability/notes/#lms-estimation-single-unknown-and-observation","text":"Goal: interested in a point estimate \\hat \\theta \\hat \\theta of unknow random variable \\Theta \\Theta , with prior p_{\\Theta}(\\theta) p_{\\Theta}(\\theta) . (Given observation X X ; model p_{X\\mid \\Theta}(x\\mid \\theta) p_{X\\mid \\Theta}(x\\mid \\theta) ) We want to minimize the MSE, because this time we have the particular observation X = x X = x , we now live in a conditional universe, we need to minimize the conditional MSE, {\\bf E}\\Big[\\big(\\Theta - {\\bf E}\\big[\\Theta \\big] \\big)^2 \\mid X = x \\Big] {\\bf E}\\Big[\\big(\\Theta - {\\bf E}\\big[\\Theta \\big] \\big)^2 \\mid X = x \\Big] , the optimal value is \\hat \\theta = {\\bf E}\\big[\\Theta \\mid X = x\\big] \\hat \\theta = {\\bf E}\\big[\\Theta \\mid X = x\\big] By the \"happens to take\" reasoning and the iterated expection rule, we can achieve the conclusion: that \\hat \\Theta_{LMS} = {\\bf E}\\big[\\Theta \\mid X\\big] \\hat \\Theta_{LMS} = {\\bf E}\\big[\\Theta \\mid X\\big] minimize the MSE {\\bf E}\\Big[\\big(\\Theta - g(X) \\big)^2 \\Big] {\\bf E}\\Big[\\big(\\Theta - g(X) \\big)^2 \\Big] over all estimators \\hat \\Theta = g(X) \\hat \\Theta = g(X) .","title":"LMS estimation; single unknown and observation"},{"location":"courses/6.431-probability/notes/#lms-performance-evaluation","text":"MSE: {\\bf E}\\Big[\\big(\\Theta - {\\bf E}\\big[\\Theta \\mid X = x\\big] \\big)^2 \\mid X = x \\Big] = \\textbf{Var}(\\Theta \\mid X = x) {\\bf E}\\Big[\\big(\\Theta - {\\bf E}\\big[\\Theta \\mid X = x\\big] \\big)^2 \\mid X = x \\Big] = \\textbf{Var}(\\Theta \\mid X = x) Expected performance of the design: {\\bf E}\\Big[\\big(\\Theta - {\\bf E}\\big[\\Theta \\mid X\\big] \\big)^2 \\Big] = {\\bf E}\\Big[\\textbf{Var}(\\Theta \\mid X)\\Big] {\\bf E}\\Big[\\big(\\Theta - {\\bf E}\\big[\\Theta \\mid X\\big] \\big)^2 \\Big] = {\\bf E}\\Big[\\textbf{Var}(\\Theta \\mid X)\\Big] LMS relavant to estimation (not hypothesis testing) Same as MAP if the posterior is unimodal and symmetric around the mean. e.g. when the posterior is normal (the case in \"linear-normal\" models)","title":"LMS performance evaluation"},{"location":"courses/6.431-probability/notes/#the-multidimensional-case","text":"","title":"The multidimensional case"},{"location":"courses/6.431-probability/notes/#lecture-17-linear-least-mean-squares-llms-estimation","text":"","title":"Lecture 17 Linear least mean squares (LLMS) estimation"},{"location":"courses/6.431-probability/notes/#lecture-18-inequalities-convergence-and-the-weak-law-of-large-numbers","text":"","title":"Lecture 18 Inequalities, convergence, and the Weak Law of Large Numbers"},{"location":"courses/6.431-probability/notes/#the-weak-law-of-large-numbers","text":"X_1, X_2, \\cdots, X_n X_1, X_2, \\cdots, X_n , i.i.d.; finite mean \\mu \\mu and variance \\sigma^2 \\sigma^2 Sample mean M_n M_n : it is not a constant, but a function of multiple random variables. You can understand it this way. In an experiment which you draw the exam score of student one by one, once you finish the first draw X_1 X_1 , the first sample is a fixed real number. But before your first draw, X_1 X_1 can be any score and it is a random variable. After you draw a total of n n scores, you have all the fixed value of X_1, X_2, \\cdots, X_n X_1, X_2, \\cdots, X_n , the sample mean is determined for this draw. However, abstractly, sample mean is a random variable, if you draw another n n example scores to get the sample mean, you get a different set of X_1, X_2, \\cdots, X_n X_1, X_2, \\cdots, X_n . The randomness comes from the randomness of different experiments, in each of the experiment, the sample X_i X_i is not random. {\\bf E}\\big[M_n\\big] =\\mu {\\bf E}\\big[M_n\\big] =\\mu . Two level of averaging. M_n M_n itself is averaging over n samples. Since these n samples are drawn randomly, taking the expectation of M_n M_n is averaging all possible sample means obtained through many (infinity) experiments. \\textbf{Var}(M_n) = \\frac{1}{n^2} \\cdot n \\cdot \\textbf{Var}(X_1) = \\frac{\\sigma^2}{n} \\textbf{Var}(M_n) = \\frac{1}{n^2} \\cdot n \\cdot \\textbf{Var}(X_1) = \\frac{\\sigma^2}{n} . Apply Chebyshev inequality: \\textbf{P}(|M_n - \\mu| \\ge \\epsilon) \\le \\frac{\\textbf{var}(M_n)}{\\epsilon^2} = \\frac{\\sigma^2}{n\\epsilon^2} \\rightarrow 0 \\textbf{P}(|M_n - \\mu| \\ge \\epsilon) \\le \\frac{\\textbf{var}(M_n)}{\\epsilon^2} = \\frac{\\sigma^2}{n\\epsilon^2} \\rightarrow 0 , as n \\rightarrow \\infty n \\rightarrow \\infty Weak Law of Large Numbers Weak Law of Large Numbers For \\epsilon > 0 \\epsilon > 0 , \\textbf{P}(|M_n - \\mu| \\ge \\epsilon) = \\textbf{P}\\Big(\\Big|\\frac{X_1 + X_2 + \\cdots + X_n}{n} - \\mu\\Big| \\ge \\epsilon \\Big) \\rightarrow 0 \\textbf{P}(|M_n - \\mu| \\ge \\epsilon) = \\textbf{P}\\Big(\\Big|\\frac{X_1 + X_2 + \\cdots + X_n}{n} - \\mu\\Big| \\ge \\epsilon \\Big) \\rightarrow 0 , as n \\rightarrow \\infty n \\rightarrow \\infty .","title":"The Weak Law of Large Numbers"},{"location":"courses/6.431-probability/notes/#interpreting-the-wlln","text":"One experiment many measurements X_i = \\mu + W_i X_i = \\mu + W_i , where W_i W_i is noise, \\textbf{E}[W_i] = 0 \\textbf{E}[W_i] = 0 , independent W_i W_i . Sample mean M_n M_n is unlikely to be far off from true mean \\mu \\mu Many independent repetitions of the same experiments event A A , with p = \\textbf{P}(A) p = \\textbf{P}(A) X_i X_i : indicator of event A A The sampel mean M_n M_n is the empirical frequency of event A A","title":"Interpreting the WLLN"},{"location":"courses/6.431-probability/notes/#application-of-wlln-polling","text":"The probability of error greater than \\epsilon \\epsilon is smaller than a certain probability. You can use Chebyshev inequality and WLLN to estimate how many samples you need to fulfill a specific error probability requirement.","title":"Application of WLLN - polling"},{"location":"courses/6.431-probability/notes/#convergence-in-probability","text":"Definition: A sequence Y_n Y_n converges in probability to a number a a if For any \\epsilon > 0 \\epsilon > 0 , \\displaystyle \\lim_{n\\rightarrow\\infty}\\textbf{P}(|Y_n - a| \\ge \\epsilon) = 0 \\displaystyle \\lim_{n\\rightarrow\\infty}\\textbf{P}(|Y_n - a| \\ge \\epsilon) = 0 * Comparison between ordinary convergence and convergence in probability:","title":"Convergence in probability"},{"location":"courses/6.431-probability/notes/#convergence-in-probability-examples","text":"convergence in probability doesn't imply convergence of the expectations. How to find what value it converges to? Make an educated conjecture about the limit \\tau \\tau , write {\\bf P}(|Y_n - \\tau| \\ge \\epsilon) {\\bf P}(|Y_n - \\tau| \\ge \\epsilon) , and derive the value of it to where you can observe from the expression that the probability \\to 0 \\to 0 , when n \\to \\infty n \\to \\infty .","title":"Convergence in probability examples"},{"location":"courses/6.431-probability/notes/#related-topics","text":"Better bounds/approximations on tail probabilities Markov and Chebyshev inequalities Chernoff bound Central limit theorem Different types of convergence Convergence in probability Convergence \"with probability 1\" Strong law of large numbers Convergence of a sequence of distributions (CDFs) to a limiting CDF.","title":"Related topics"},{"location":"courses/6.431-probability/notes/#lecture-19-the-central-limit-theorem-clt","text":"Considering the sum of random variable S_n = X_1 + X_2, + \\cdots + X_n S_n = X_1 + X_2, + \\cdots + X_n , ( X_1, X_2, \\cdots, X_n X_1, X_2, \\cdots, X_n are i.i.d. with finite mean \\mu \\mu and variance \\sigma^2 \\sigma^2 ) Z_n = \\frac{S_n - n\\mu}{\\sqrt{n}\\sigma} Z_n = \\frac{S_n - n\\mu}{\\sqrt{n}\\sigma} , then we have the central limit Theorem, Central Limit Theorem: For every z z : \\displaystyle \\lim_{n \\to \\infty}\\textbf{P}(Z_n \\le z) = \\textbf{P}(Z \\le z) \\displaystyle \\lim_{n \\to \\infty}\\textbf{P}(Z_n \\le z) = \\textbf{P}(Z \\le z)","title":"Lecture 19 The Central Limit Theorem (CLT)"},{"location":"courses/6.431-probability/notes/#what-exactly-does-the-clt-say-practice","text":"The practice of normal approximations: We have linear expression between S_n S_n and Z_n Z_n : S_n = \\sqrt n \\sigma Z_n + n\\mu S_n = \\sqrt n \\sigma Z_n + n\\mu , since Z_n Z_n can be treated as if it were normal, S_n S_n can be treated as if normal: N(n\\mu, n\\sigma^2) N(n\\mu, n\\sigma^2) Can we use the CLT when n is \"moderate\"? i.e. n = 30? Usually, yes. When the distribution of X X has common features with a normal distribution. symmetry and unimodality help","title":"What exactly does the CLT say? - Practice"},{"location":"courses/6.431-probability/notes/#central-limit-theory-examples","text":"\\textbf{P}(S_{\\color{red}n} \\le {\\color{red}a}) \\approx {\\color{red}b} \\textbf{P}(S_{\\color{red}n} \\le {\\color{red}a}) \\approx {\\color{red}b} Given two parameters, find the third. Package weight X_i X_i , i.i.d. exponential, \\lambda = 1/2 \\lambda = 1/2 , \\mu = \\sigma = 2 \\mu = \\sigma = 2 Load container with n = 100 n = 100 packages, what's the probability that the overall weight is heavier than 210? \\textbf{P}(S_n \\ge 210) = ? \\textbf{P}(S_n \\ge 210) = ? Load container with n = 100 n = 100 packages. Choose the \"capacity\" a so that \\textbf{P}(S_n \\ge a) \\approx 0.05 \\textbf{P}(S_n \\ge a) \\approx 0.05 Fix the capacity at 210, how large can n n be, so that \\textbf{P}(S_n \\ge 210) \\approx 0.05 \\textbf{P}(S_n \\ge 210) \\approx 0.05 Load container until wight exceeds 210, N N is the number of packages loaded, find \\textbf{P}(N > 100) \\textbf{P}(N > 100)","title":"Central Limit Theory examples"},{"location":"courses/6.431-probability/notes/#airline-booking","text":"For any given flight, an airline tries to sell as many tickets as possible. Suppose that on average, of ticket holders fail to show up, all independent of one another. Knowing this, an airline will sell more tickets than there are seats available (i.e., overbook the flight) and hope that there is a sufficient number of ticket holders who do not show up, to compensate for its overbooking. Using the Central Limit Theorem, determine, the maximum number of tickets an airline can sell on a flight with 400 seats so that it can be approximately confident that all ticket holders who do show up will be able to board the plane. Use the de Moivre-Laplace -correction in your calculations. Hint: You may have to solve numerically a quadratic equation. Solution: Each ticket can either be used or not used by the the passager. It can be modeled with the random variable: X \\sim \\operatorname{Bern} \\left({p}\\right) X \\sim \\operatorname{Bern} \\left({p}\\right) , p=0.8 p=0.8 , then the total passergers presented at airport is S_n = X_1 + X_2 + \\cdots + X_n S_n = X_1 + X_2 + \\cdots + X_n , which is a Binomial: S_n \\sim \\operatorname{B} \\left({n, p}\\right) S_n \\sim \\operatorname{B} \\left({n, p}\\right) . For binomial random variable, \\mu = np = 0.8n, \\sigma^2 = np(1-p) = 0.16n \\mu = np = 0.8n, \\sigma^2 = np(1-p) = 0.16n . Our requirements is \\textbf{P}(S_n \\le 400) \\approx 0.99 \\textbf{P}(S_n \\le 400) \\approx 0.99 . Normalize S_n S_n in the probability and treat the normalized random variable Z_n Z_n as a normal distribution, refer to the normal table, and solve the number n n . \\begin{align*} \\textbf{P}(S_n \\le 400) &\\approx 0.99 \\\\ \\textbf{P}\\Big(\\frac{S_n - 0.8n}{0.4\\sqrt{n}} \\le \\frac{400.5 - 0.8n}{0.4\\sqrt{n}}\\Big) &\\approx 0.99 \\\\ \\textbf{P}\\Big(Z_n \\le \\frac{400.5 - 0.8n}{0.4\\sqrt{n}}\\Big) &\\approx 0.99 \\\\ \\Phi\\Big(\\frac{400.5 - 0.8n}{0.4\\sqrt{n}}\\Big) &\\approx 0.99 \\\\ \\frac{400.5 - 0.8n}{0.4\\sqrt{n}} &= 2.33 \\end{align*} \\begin{align*} \\textbf{P}(S_n \\le 400) &\\approx 0.99 \\\\ \\textbf{P}\\Big(\\frac{S_n - 0.8n}{0.4\\sqrt{n}} \\le \\frac{400.5 - 0.8n}{0.4\\sqrt{n}}\\Big) &\\approx 0.99 \\\\ \\textbf{P}\\Big(Z_n \\le \\frac{400.5 - 0.8n}{0.4\\sqrt{n}}\\Big) &\\approx 0.99 \\\\ \\Phi\\Big(\\frac{400.5 - 0.8n}{0.4\\sqrt{n}}\\Big) &\\approx 0.99 \\\\ \\frac{400.5 - 0.8n}{0.4\\sqrt{n}} &= 2.33 \\end{align*} Solve n n to obtain n = 475 n = 475","title":"Airline booking"},{"location":"courses/6.431-probability/notes/#normal-approximation-to-the-binomial","text":"Take X_i X_i as independent Bernuolli random variables: X \\sim \\operatorname{Bern} \\left({p}\\right) X \\sim \\operatorname{Bern} \\left({p}\\right) , 0 < p < 1 0 < p < 1 , then S_n = X_1 + X_2 + \\cdots + X_n S_n = X_1 + X_2 + \\cdots + X_n is Binomial: X \\sim \\operatorname{B} \\left({n, p}\\right) X \\sim \\operatorname{B} \\left({n, p}\\right) . Binomial random variable S_n S_n have \\mu = np \\mu = np , \\sigma^2=np(1-p) \\sigma^2=np(1-p) . According CLT, The normalized random variable \\frac{S_n-np}{\\sqrt{np(1-p)}} \\frac{S_n-np}{\\sqrt{np(1-p)}} is a standard normal. In order to find the \\textbf{P}(S_n \\le 21) \\textbf{P}(S_n \\le 21) (given n = 36 n = 36 , p = 0.5 p = 0.5 ), we use the equvilent of events to transform the probability \\textbf{P}(S_n \\le 21) \\textbf{P}(S_n \\le 21) into another probability about a normal random vairable \\textbf{P}\\Big(\\frac{S_n - 18}{3} \\le \\frac{21-28}{3}\\Big) = \\textbf{P}(Z_n \\le 1) = \\Phi(1) = 0.8413 \\textbf{P}\\Big(\\frac{S_n - 18}{3} \\le \\frac{21-28}{3}\\Big) = \\textbf{P}(Z_n \\le 1) = \\Phi(1) = 0.8413 . The 1/2 correction for integer random variables. To get a improved estimate of binomial from normal, we can take the middle point of two integers. Instead using \\textbf{P}(S_n \\le 21) \\textbf{P}(S_n \\le 21) or \\textbf{P}(S_n \\lt 22) \\textbf{P}(S_n \\lt 22) , we use \\textbf{P}(S_n \\lt 21.5) \\textbf{P}(S_n \\lt 21.5) which is very accurate De Moivre-Laplace CLT to the binomial. To estimate \\textbf{P}(S_n=19) \\textbf{P}(S_n=19) , we take \\textbf{P}(18.5 \\le S_n \\le 19.5) \\textbf{P}(18.5 \\le S_n \\le 19.5) and get an accurate estimate.","title":"Normal approximation to the binomial"},{"location":"courses/6.431-probability/notes/#lecture-20-introduction-to-classical-statistics","text":"","title":"Lecture 20 Introduction to classical statistics"},{"location":"courses/6.431-probability/notes/#overview-of-the-classical-statistical-framework","text":"attributes Bayesian classical unkonwn \\Theta \\Theta is r.v. \\theta \\theta is a fixed value known p_\\Theta p_\\Theta prior distribution and samples X_i X_i only sample X_i X_i model p_{X \\mid \\Theta} p_{X \\mid \\Theta} , where observation is generated p_X(x; \\theta) p_X(x; \\theta) , \\theta \\theta is a real-valued parameter of the model, method use bayes rule to obtain p_{\\Theta \\mid X} p_{\\Theta \\mid X} , \\big(p_X = {\\int_{-\\infty}^{+\\infty}} p_\\Theta \\cdot p_{X \\mid \\Theta}d\\theta\\big) \\big(p_X = {\\int_{-\\infty}^{+\\infty}} p_\\Theta \\cdot p_{X \\mid \\Theta}d\\theta\\big) , then determind the estimate Design a estimator \\hat\\Theta \\hat\\Theta , to keep estimate error \\hat\\Theta - \\theta \\hat\\Theta - \\theta small estimator MAP, LMS ML","title":"Overview of the classical statistical framework"},{"location":"courses/6.431-probability/notes/#confidence-intervals-interpretation","text":"Every day, I try to estimate an unknown parameter using a fresh data set. I look at the data and then I use some formulas to calculate a 70% confidence interval, [\\hat\\Theta^-, \\hat\\Theta^+] [\\hat\\Theta^-, \\hat\\Theta^+] , based on the day's data.","title":"Confidence intervals interpretation"},{"location":"courses/6.431-probability/notes/#classical-statsistic-interpretation","text":"If today I got the confidence interval [0.41, 0.47] [0.41, 0.47] , that doesn't mean there is 70\\% 70\\% probability that the true value will be inside [0.41, 0.47] [0.41, 0.47] . What the confidence interval is that 70% of those confidence intervals you obtained in all those days will include the true value. The \"confidence\" (or probability) can be think of \\frac{\\text{All CIs that include the true value}}{\\text{All CIs that include the true value + CIs that exclude the true value}} \\frac{\\text{All CIs that include the true value}}{\\text{All CIs that include the true value + CIs that exclude the true value}} . You cannot speak about a particular interval, and once you do that, there is no randomness anymore, the true value is either inside the interval or not, there are no other probabilities so to speak.","title":"Classical statsistic interpretation"},{"location":"courses/6.431-probability/notes/#bayes-interpretation-bayesians-confidence-interval","text":"Alternatively, I decided to use a Bayesian approach, by viewing the unknown parameter, denoted by \\Theta \\Theta , as a continuous random variable and assuming a prior PDF for \\Theta \\Theta . I observe a specific value x x , calculate the posterior f_{\\Theta}{X}(\\theta|x) f_{\\Theta}{X}(\\theta|x) , and find out that \\begin{align*} \\int _{0.41}^{0.47} f_{\\Theta |X}(\\theta \\, |\\, x)\\, d\\theta =0.70. \\end{align*} \\begin{align*} \\int _{0.41}^{0.47} f_{\\Theta |X}(\\theta \\, |\\, x)\\, d\\theta =0.70. \\end{align*} This time, I can say that there is a probability 70\\% 70\\% that the unknown parameter is inside the (Bayesian) confidence interval [0.41, 0.47] [0.41, 0.47] .","title":"Bayes' interpretation (Bayesian's Confidence Interval)"},{"location":"courses/6.431-probability/notes/#confidence-intervals-for-the-estimation-of-the-mean","text":"I asked you to estimate the mean of i.i.d variables X_1, X_2, \\cdots, X_n X_1, X_2, \\cdots, X_n with true mean \\theta \\theta , and variance \\sigma^2 \\sigma^2 , and the estimate should achieve 95\\% 95\\% confidence interval. How you proceed with it? By Central Limit Theory, your estimate values \\hat\\Theta \\hat\\Theta is an normal distribution. By standardizing it, you get the standard normal Z_n = \\frac{\\hat\\Theta - \\theta}{\\sigma/\\sqrt{n}} Z_n = \\frac{\\hat\\Theta - \\theta}{\\sigma/\\sqrt{n}} . 95\\% 95\\% confidence interverl means standard normal is between symetric 95\\% 95\\% intervals [-b, b] [-b, b] . By looking up the normal table, b = 1.69 b = 1.69 corresponds to the probability 97.5\\% 97.5\\% , so \\textbf{P}(-1.69 \\le Z_n \\le 1.69) = 95\\% \\textbf{P}(-1.69 \\le Z_n \\le 1.69) = 95\\% thus \\textbf{P}\\big(\\frac{|\\hat\\Theta - \\theta|}{\\sigma/\\sqrt{n}} \\le 1.69\\big) = 95\\% \\textbf{P}\\big(\\frac{|\\hat\\Theta - \\theta|}{\\sigma/\\sqrt{n}} \\le 1.69\\big) = 95\\% , which can be rewrite as \\begin{align*} \\textbf{P}\\Big(\\hat\\Theta -\\frac{1.96\\sigma}{\\sqrt n} \\le \\theta \\le \\hat\\Theta +\\frac{1.96\\sigma}{\\sqrt n}\\Big) = 95\\%. \\end{align*} \\begin{align*} \\textbf{P}\\Big(\\hat\\Theta -\\frac{1.96\\sigma}{\\sqrt n} \\le \\theta \\le \\hat\\Theta +\\frac{1.96\\sigma}{\\sqrt n}\\Big) = 95\\%. \\end{align*} This is the way to construct the confidence interval. If you have the \\sigma \\sigma and n n , you have a concrete 95\\% 95\\% confidence interval to report. However, the \\sigma \\sigma is usually unknown, what you can do is to use the estimated \\hat\\Theta \\hat\\Theta to estimate the \\sigma \\sigma .","title":"Confidence intervals for the estimation of the mean"},{"location":"courses/6.431-probability/notes/#lecture-21","text":"","title":"Lecture 21"},{"location":"courses/6.431-probability/notes/#lecture-22","text":"","title":"Lecture 22"},{"location":"courses/6.431-probability/notes/#lecture-23","text":"","title":"Lecture 23"},{"location":"courses/6.431-probability/notes/#lecture-24","text":"","title":"Lecture 24"},{"location":"courses/6.431-probability/notes/#lecture-25","text":"","title":"Lecture 25"},{"location":"courses/6.431-probability/notes/#lecture-26","text":"","title":"Lecture 26"},{"location":"courses/6.431-probability/notes/#resources","text":"Distributtion Explorer by Justin Bois Tutorial 3b: Probability distributions and their stories Think Stats: Exploratory Data Analysis in Python Introduction to Probability Introduction to Probability and Statistics","title":"Resources"},{"location":"courses/9chap-algorithms/notes/","text":"9 Chapter Algorithms \u00b6 Lecture 1 \u4ecestrStr\u8c08\u9762\u8bd5\u6280\u5de7\u4e0eCoding Style \u00b6 # Problem Comment Date 1 Implement strStr() 08/07/2017, 2 Subsets No duplicates 08/07/2017, 3 Subsets II With duplicates 08/07/2017, 4 Permutations No duplicates 08/07/2017, 5 Permutations II with duplicates 08/07/2017, 6 Combinations from {1, ... n}, choose k of them 08/07/2017, 7 Combination Sum No duplicate, repeat selection allowed 08/09/2017, 8 Combination Sum II With duplicates, each can be select once 08/09/2017, 9 Combination Sum III {1, ..., 9} select k numbers and add to n, (See K Sum) 08/09/2017, 10 Combination Sum IV (Same as Backpack VI) 08/09/2017, 11 Letter Combinations of a Phone Number 08/07/2017, 12 Palindrome Partitioning 08/07/2017, 13 Palindrome Partitioning II C++\u7248\u672c\u5982\u4f55\u901a\u8fc7\u5927\u7684Input 08/07/2017, 14 Restore IP Address 08/07/2017, Notes \u00b6 strStr() problem \u4e0d\u7528\u90a3\u4e48\u7684\u590d\u6742\u7684\u7b97\u6cd5\uff0c\u4f46\u8981\u505a\u5230bug free\uff08\u68c0\u67e5corner case, base case, \u7a0b\u5e8f\u7f29\u8fdb\uff09\u3002 \u9762\u8bd5\u5b98 \u4ee3\u7801\u5de5\u6574 \uff08\u5f62\u6210\u81ea\u5df1\u7684style\uff09 coding\u4e60\u60ef\uff08corner test case\uff0cextreme case\u8003\u8651\u5230\u4e86\u6ca1\uff09 \u6c9f\u901a\u80fd\u529b\uff0c\u4ea4\u6d41\u969c\u788d\u6709\u5417 \u9762\u8bd5\u8003\u5bdf\u57fa\u672c\u529f \u7a0b\u5e8f\u98ce\u683c\uff08\u7f29\u8fdb\uff0c\u62ec\u53f7\uff0c\u53d8\u91cf\u540d\uff09 Coding\u4e60\u60ef\uff08\u5f02\u5e38\u68c0\u67e5\uff0c\u8fb9\u754c\u5904\u7406\uff09 \u6c9f\u901a\uff08\u8ba9\u9762\u8bd5\u5b98\u65f6\u523b\u660e\u767d\u4f60\u7684\u610f\u56fe\uff09 \u6d4b\u8bd5\uff08\u4e3b\u52a8\u5199\u51fa\u5408\u7406\u7684Testcase\uff09 \u5982\u4f55\u51c6\u5907\u9762\u8bd5 \u5982\u4f55\u4e00\u4e2a\u6708\u641e\u5b9a\u7b97\u6cd5\u9762\u8bd5\uff1f\u603b\u7ed3\u5f52\u7c7b\u76f8\u4f3c\u9898\u76ee \u627e\u51fa\u540c\u7c7b\u9898\u578b\u7684\u6a21\u677f\u7a0b\u5e8f \u62ffoffer\u56db\u5927\u6cd5\u5b9d \u6446\u6b63\u5fc3\u6001\uff0c\u5c3d\u53ef\u80fd\u591a\u5730\u5237\u9898\uff0c\u4e0d\u6025\u4e8e\u6c42\u6210\uff0c\u505a\u5b8c\u4e00\u7c7b\u518d\u6765\u4e0b\u4e00\u7c7b \u6109\u5feb\u4ea4\u6d41\uff0c \u7406\u89e3\u800c\u975e\u80cc\u8bf5\uff0c\u4e0d\u8bb0\u5177\u4f53\u5b9e\u73b0\u8bb0\u8fd9\u4e00\u7c7b\u9898\u901a\u7528\u601d\u7ef4\u65b9\u5f0f\u548c\u5206\u6790\u6280\u5de7\u3002 \u65f6\u95f4\u8981\u7528\u5728\u5200\u5203\u4e0a\uff0c\u4e0d\u8981\u628a\u65f6\u95f4\u6d6a\u8d39\u5728\u90a3\u4e9b\u57fa\u672c\u4e0d\u4f1a\u8003\u4f60\u53c8\u5f88\u5fc3\u865a\u7684\u5185\u5bb9\uff08\u6bd4\u5982KMP\uff0c\u7ea2\u9ed1\u6811\uff0cAVL\uff0cACM\u7ade\u8d5b\u9898\uff09 Lecture 2 \u4e8c\u5206\u641c\u7d22\u4e0e\u65cb\u8f6c\u6392\u5e8f\u6570\u7ec4 Binary Search & Rotated Sorted Array \u00b6 # Problem Comment Date 1 Classical Binary Search* 08/12/2017, 2 Search for a Range 08/12/2017, 3 Search Insert Position 08/12/2017, 4 Search in a Big Sorted Array 08/12/2017, 5 Search a 2D Matrix 08/12/2017, 6 Search a 2D Matrix II 08/15/2017, 7 First Bad Version 08/15/2017, 8 Find Peak Element Google, important, mit algorithm course. 08/15/2017, 9 Recover Rotated Sorted Array* 08/15/2017, 10 Find Minimum in Rotated Sorted Array 08/15/2017, 11 Find Minimum in Rotated Sorted Array II 08/15/2017, 12 Search in Rotated Sorted Array 08/15/2017, 13 Search in Rotated Sorted Array II 08/15/2017, 14 Median of Two Sorted Arrays 08/15/2017, 15 Rotate String* 08/15/2017, 16 Reverse Words in a String 08/15/2017, 17 Wood Cut* 08/15/2017, Notes \u00b6 Classical Binary Search Given an sorted integer array - nums, and an integer - target. Find the any/first/last position of target in nums, return -1 if target doesn\u2019t exist. Prototype: int binarySearch(int[] nums, int target) \u590d\u6742\u5ea6\u5206\u6790 T(n) = T(n/2) + O(1) = O(logn) \u7b97\u6cd5\u9762\u8bd5\u4e2d\u5982\u679c\u9700\u8981\u4f18\u5316O(n)\u7684\u65f6\u95f4\u590d\u6742\u5ea6\u90a3\u4e48\u53ea\u80fd\u662fO(logn)\u7684\u4e8c\u5206\u6cd5 Recursion or While-loop? bad for recursion: Linux have only 8MB stack, local variable takes much space for each level of recursion. for simple problem, use iteration instead of recursion. for more complex problem use recursion. ask interviewer. Binary Search\u6a21\u677f\u56db\u8981\u7d20 start + 1 < end start + (end - start) / 2 A[mid] == , < ,> A[start] A[end] ? target Find First Position of XXX VS. Find Last Position of XXX character of binary search the split depends on you need the first position of XXX or last position of XXX. See the code. Lecture 3 \u4e8c\u53c9\u6811\u95ee\u9898\u4e0e\u5206\u6cbb\u7b97\u6cd5 Binary Tree & Divide Conquer Algorithm \u00b6 # Problem Comment Date 1 Binary Tree Preorder Traversal stack, right push first 08/26/2017, 2 Maximum Depth of Binary Tree Recursion, same as find height of the tree 08/26/2017, 3 Balanced Binary Tree find height first 08/26/2017, 4 Lowest Common Ancestor of a Binary Tree Lowest Common Ancestor of a Binary Search Tree 08/26/2017, 5 Binary Tree Maximum Path Sum \"at least one node\" 08/26/2017, 6 Binary Tree Level Order Traversal BFS, DFS, queue 08/26/2017, 7 Binary Tree Level Order Traversal II call reverse() when done by Binary Tree Level Order Traversal 08/26/2017, 8 Binary Tree Zigzag Level Order Traversal use two stacks 08/26/2017, 9 Validate Binary Search Tree inorder traversal, LONG_MAX, LONG_MIN 08/26/2017, 10 Search Range in a Binary Search Tree* inorder traversal 08/26/2017, 11 Insert a Node in Binary Search Tree* recursion 08/26/2017, 12 Binary Search Tree Iterator use stack 08/26/2017, 13 Remove Node in Binary Search Tree* Problem Inorder Successor BST 08/26/2017, 14 Inorder Successor BST Successor of a node, special case: [2,1],p=1, [3,1,null,null,2], p=2. 08/26/2017, Notes \u00b6 \u6784\u9020binary tree \u552f\u4e00\u6216\u8005\u4e0d\u552f\u4e00 preorder + inorder postorder + inorder preorder + postorder \u4e0d\u552f\u4e00 divide conquer solve preorder traversal O(n^2) Max Path Sum root -> leaf root -> any node. any node -> any node \u5b58\u5728\u8d1f\u6570\u8282\u70b9\uff0c\u8fd9\u6761\u8def\u5f84\u81f3\u5c11\u5305\u542b\u4e00\u4e2a\u7ed3\u70b9 Lecture 4 \u52a8\u6001\u89c4\u5212 Dynamic Programming I \u00b6 # Problem Comment Date 1 Triangle 08/29/2017, 2 Minimum Path Sum 08/16/2017, 3 Unique Paths 08/16/2017, 4 Unique Paths II 08/16/2017, 5 Climbing Stairs 08/30/2017, 6 Jump Game 08/16/2017, 7 Jump Game II Maximum Subarray, Frog Jump 08/30/2017, Notes \u00b6 \u4ece\u9012\u5f52\u5230\u52a8\u5f52 - Triangle DFS - Traverse \u590d\u6742\u5ea6 O(2^n) n is the height (\u6bcf\u6b21\u6709\u4e24\u4e2a\u53ef\u9009\u8def\u5f84) DFS - Divide & Conquer \u590d\u6742\u5ea6 O(2^n) \u6bcf\u6761\u8def\u52b2\u6ca1\u6709\u4efb\u4f55\u6761\u4ef6\u8ba9\u5b83\u63d0\u524d\u8fd4\u56de \u901a\u8fc7\u753b\u56fe\u6765\u89c2\u5bdf\u5982\u4f55\u964d\u4f4e\u590d\u6742\u5ea6\uff08\u6709\u91cd\u590d\u8ba1\u7b97\uff09 Divide & Conquer + Memorization \u8bb0\u5fc6\u5316\u641c\u7d22\uff0c\u88c5\u4e86\u773c\u775b\uff0c\u77e5\u9053\u505c\u54ea Bottom-up \u52a8\u5f52\u7684\u56db\u4e2a\u6b65\u9aa4 \u72b6\u6001\u7684\u5b9a\u4e49 \u521d\u59cb\u5316\uff0c\u7ec8\u70b9\u5148\u6709\u503c \u5faa\u73af\u9012\u63a8\uff08\u4e0d\u662f\u9012\u5f52\uff09\u6c42\u89e3 \u6c42\u7ed3\u679c: \u8d77\u70b9 Top-down \u4e24\u79cd\u6ca1\u6709\u592a\u591a\u7684\u4f18\u7565\u4e4b\u5206\uff0c\u6839\u636e\u4e2a\u4eba\u4e60\u60ef\u7075\u6d3b\u8fd0\u7528 \u4ec0\u4e48\u6837\u7684\u9898\u9002\u5408\u4f7f\u7528\u52a8\u6001\u89c4\u5212\uff1f \u6ee1\u8db3\u4e09\u4e2a\u6761\u4ef6\u4e4b\u4e00\u5219\u6781\u6709\u53ef\u80fd\u4f7f\u7528\u52a8\u5f52\u6c42\u89e3 Maximum/Minimum Yes/No Count(*) (i.e. \u603b\u5171\u6709\u591a\u5c11\u4e2a\uff09 \u4e0d\u80fd\u7528\u52a8\u5f52 \u8981\u6c42\u51fa\u6240\u6709\u201c\u5177\u4f53\u201d\u7684\u65b9\u6848\u800c\u975e\u65b9\u6848\u201c\u4e2a\u6570\u201d http://www.lintcode.com/problem/palindromepartitioning/ \u8f93\u5165\u6570\u636e\u662f\u4e00\u4e2a\u201c\u96c6\u5408\u201d\u800c\u4e0d\u662f\u201c\u5e8f\u5217\u201d http://www.lintcode.com/problem/longest-consecutivesequence/ \u52a8\u6001\u89c4\u52124\u70b9\u8981\u7d20 \u72b6\u6001 State \u7075\u611f\uff0c\u521b\u9020\u529b\uff0c\u5b58\u50a8\u5c0f\u89c4\u6a21\u95ee\u9898\u7684\u7ed3\u679c \u65b9\u7a0b Function \u72b6\u6001\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u600e\u4e48\u901a\u8fc7\u5c0f\u7684\u72b6\u6001\uff0c\u6765\u7b97\u5927\u7684\u72b6\u6001 \u521d\u59cb\u5316 Intialization \u6700\u6781\u9650\u7684\u5c0f\u72b6\u6001\u662f\u4ec0\u4e48, \u8d77\u70b9 \u7b54\u6848 Answer \u6700\u5927\u7684\u90a3\u4e2a\u72b6\u6001\u662f\u4ec0\u4e48\uff0c\u7ec8\u70b9 \u9762\u8bd5\u6700\u5e38\u89c1\u7684\u56db\u79cd\u7c7b\u578b Matrix DP (15%) state: f[x][y] \u8868\u793a\u6211\u4ece\u8d77\u70b9\u8d70\u5230\u5750\u6807x,y\u2026\u2026 function: \u7814\u7a76\u8d70\u5230x,y\u8fd9\u4e2a\u70b9\u4e4b\u524d\u7684\u4e00\u6b65 intialize: \u8d77\u70b9 answer: \u7ec8\u70b9 Sequence DP (40%) state: f[i]\u8868\u793a\u201c\u524di\u201d\u4e2a\u4f4d\u7f6e/\u6570\u5b57/\u5b57\u6bcd,(\u4ee5\u7b2ci\u4e2a\u4e3a)... function: f[i] = f[j] \u2026 j \u662fi\u4e4b\u524d\u7684\u4e00\u4e2a\u4f4d\u7f6e intialize: f[0].. answer: f[n-1].. Two Sequences DP (40%) Others (5%) \u9762\u8bd5\u4e2d\u5e38\u7528\u52a8\u6001\u89c4\u5212 \u77e9\u9635\u52a8\u6001\u89c4\u5212\uff08\u5750\u6807\u76f8\u5173\uff09 \u5355\u5e8f\u5217\u52a8\u6001\u89c4\u5212\uff08\u4e0a\uff09 Lecture 5 \u52a8\u6001\u89c4\u5212 Dynamic Programming II \u00b6 # Problem Comment Date 1 Palindrome Partitioning II 08/16/2017, 2 Word Break 08/16/2017, 3 Longest Common Subsequence* 08/16/2017, 4 Longest Common Substring* 08/16/2017, 5 Edit Distance 08/16/2017, 6 Distinct Subsequences 08/16/2017, 7 Interleaving String 08/16/2017, Lecture 6 \u94fe\u8868 Linked List \u00b6 # Problem Comment Date 1 Remove Duplicates from Sorted List II 08/30/2017, 2 Reverse Linked List II 08/30/2017, 3 Merge Two Sorted Lists 08/30/2017, 4 Partition List 08/30/2017, 5 Sort List 08/30/2017, 6 Reorder List 08/30/2017, 7 Merge k Sorted Lists 08/30/2017, 8 Copy List with Random Pointer 08/30/2017, 9 Linked List Cycle 08/30/2017, 10 Convert Sorted List to Balanced Binary Search Tree 08/30/2017, 11 Convert Binary Tree to Doubly Linked List 08/30/2017, 12 Reverse List Nodes in k-Groups 08/30/2017, Notes \u00b6 Basic skills Insert a Node in Sorted List Remove a Node from Linked List Reverse a Linked List Merge Two Linked Lists Find the Middle of a Linked List Sorting quick sort, \u539f\u5730\u6392\u5e8f\uff0c\u4e0d\u7a33\u5b9a\u3002\u76f8\u5bf9\u987a\u5e8f\u4f1a\u6539\u53d8\uff0c\u6240\u4ee5\u4e0d\u7a33\u5b9a\u3002 \u7a33\u5b9a\u6392\u5e8f\uff0c \u7761\u7720\u6392\u5e8f Fast Slow Pointers Find the Middle of Linked List Remove Nth Node From End of List Linked List Cycle I, II Lecture 7 \u6570\u7ec4\u4e0e\u6570 Array & Numbers \u00b6 # Problem Comment Date 1 Recover Rotated Sorted Array* 08/30/2017, 2 Rotate String* 08/30/2017, 3 Reverse Words in a String 08/30/2017, 4 Merge Sorted Array 08/30/2017, 5 Merge Sorted Array II* 08/30/2017, 6 Median of Two Sorted Arrays 08/30/2017, 7 Maximum Subarray 08/30/2017, 8 Maximum Subarray II* Prefix sum, left, right 08/30/2017, 9 Maximum Subarray III* DP, from O(n 2k)O(n2k) to O(nk)O(nk) 08/30/2017, 10 Minimum Subarray* 08/30/2017, 11 Subarray Sum* Subarray Sum Equals K 08/30/2017, 12 Best Time to Buy and Sell Stock 08/30/2017, 13 Best Time to Buy and Sell Stock II 08/30/2017, 14 Best Time to Buy and Sell Stock III 08/30/2017, 15 Two Sum 08/30/2017, 16 3Sum 08/30/2017, 17 4Sum 08/30/2017, 18 K Sum* 08/30/2017, 19 Partition Array 08/30/2017, 20 Sort Colors 08/30/2017, 21 Sort Letters by Case 08/30/2017, 22 Interleaving Positive and Negative Numbers 08/30/2017, Notes \u00b6 \u4e09\u6b65\u7ffb\u8f6c\u6cd5 Recover Rotated Sorted Array Median of Two Sorted Arrays ( Important) find median ==> find kth largest take O(1) to reduce the problem to find the k/2 th element. Maximum Subarray == Best Time to Buy and Sell Stock (see the video at 53:00) \u524d\u7f00\u548c Lecture 8 \u6570\u636e\u7ed3\u6784 Data Structure \u00b6 # Comment Date \u9898\u76ee \u5907\u6ce8 \u65e5\u671f Min-Stack Implement a queue by two stacks Largest Rectangle in histogram \u9012\u589e\u6808 Max Tree \u9012\u51cf\u6808 Rehashing LRU Cache Longest Consecutive Sequence Subarray Sum Copy List with Random Pointer Anagrams Median Number Heapify Merge K sorted List Word Search II 08/30/2017, Notes \u00b6 Running time complexity for stack operation or other data structure talk average instead of worst case. think in the larger For the largest Rectangle in histogram problem, considering the concept of \u9012\u589e\u6808 \u5f3a\u5316\u73ed\u5185\u5bb9\uff1a\u9012\u589e\u6808\u5176\u4ed6\u5e94\u7528\uff0c\u8868\u8fbe\u5f0f\u503c\uff0c\u8868\u8fbe\u5f0f\u6811 Largest Rectangle in Histogram: \u601d\u8def\u6a21\u677f\uff1a\u201c\u8fd9\u79cd\u9898\u76ee\u7684\u76f8\u4f3c\u6027\u662f\u627e\u6570\u7ec4\u91cc\u9762\u4e00\u4e2a\u5143\u7d20\u5de6\u8fb9\u6216\u8005\u53f3\u8fb9\u7b2c\u4e00\u4e2a\u6bd4\u4ed6\u5c0f\u6216\u8005\u6bd4\u4ed6\u5927\u7684\u5143\u7d20\" \u5e76\u67e5\u96c6\uff0c\u4e24\u4e2a\u6307\u9488\uff0chashheap\uff0c\u52a8\u6001\u89c4\u5212\u4f18\u5316\u9898\u76ee Hash implementation: int hashfunc ( string key ){ int sum = 0 ; for ( int i = 0 ; i < key . length (); i ++ ) { sum = sum * 33 + ( int )( key . charAt ( i )); sum = sum % HASH_TABLE_SIZE ; } return sum } Collision open hashing closed hashing Lecture 9 \u56fe\u8bba\u4e0e\u641c\u7d22 Graph & Search \u00b6 \u9898\u76ee \u5907\u6ce8 \u65e5\u671f Clone graph Copy List with Random Pointer Find the connected Notes \u00b6 time for prepare interview easy: 5 min medium: 15 min hard: 30-40 min \u6700\u597d\u4e0d\u7528DFS\uff0c node\u6570\u91cf\u591a\u7684\u60c5\u51b5\u4e0b\uff0crecursion spaec\u6d88\u8017\u592a\u5927\uff0c Topological sorting inDegree outDegree \u4f55\u9002\u7528BFS\uff0c \u4ece\u4e00\u4e2a\u70b9\u51fa\u53d1\uff0c\u627e\u8fde\u901a\u5757 level by level, \u5bfb\u627e\u4e24node\u6700\u77ed\u8ddd\u79bb subset \u9690\u5f0f\u56fe\u641c\u7d22 VS \u663e\u5f0f\u56fe\u641c\u7d22 \u4f55\u65f6\u7528DFS\uff0c \u627e\u51fa\u6240\u6709\u7684\u7ed3\u679c\u800c\u4e0d\u662f\u627e\u51fa\u603b\u7ed3\u679c\u6570\u3002 Running time permutation: O(n! * n) Running time subset: O(2^n * n) \u641c\u7d22\u7c7b\u578b\uff1a \u663e\u5f0f\u56fe\u641c\u7d22 Matrix Graph \u9690\u5f0f\u56fe\u641c\u7d22 permutation subset combination \u5bf9\u5206\u5272\u7c7b\uff0c\u53ef\u4ee5\u628a\u95ee\u9898\u770b\u6210\u5206\u5272\u7ebf\u7684combination problem.","title":"Nine Chapter Algorithms"},{"location":"courses/9chap-algorithms/notes/#9-chapter-algorithms","text":"","title":"9 Chapter Algorithms"},{"location":"courses/9chap-algorithms/notes/#lecture-1-strstrcoding-style","text":"# Problem Comment Date 1 Implement strStr() 08/07/2017, 2 Subsets No duplicates 08/07/2017, 3 Subsets II With duplicates 08/07/2017, 4 Permutations No duplicates 08/07/2017, 5 Permutations II with duplicates 08/07/2017, 6 Combinations from {1, ... n}, choose k of them 08/07/2017, 7 Combination Sum No duplicate, repeat selection allowed 08/09/2017, 8 Combination Sum II With duplicates, each can be select once 08/09/2017, 9 Combination Sum III {1, ..., 9} select k numbers and add to n, (See K Sum) 08/09/2017, 10 Combination Sum IV (Same as Backpack VI) 08/09/2017, 11 Letter Combinations of a Phone Number 08/07/2017, 12 Palindrome Partitioning 08/07/2017, 13 Palindrome Partitioning II C++\u7248\u672c\u5982\u4f55\u901a\u8fc7\u5927\u7684Input 08/07/2017, 14 Restore IP Address 08/07/2017,","title":"Lecture 1 \u4ecestrStr\u8c08\u9762\u8bd5\u6280\u5de7\u4e0eCoding Style"},{"location":"courses/9chap-algorithms/notes/#notes","text":"strStr() problem \u4e0d\u7528\u90a3\u4e48\u7684\u590d\u6742\u7684\u7b97\u6cd5\uff0c\u4f46\u8981\u505a\u5230bug free\uff08\u68c0\u67e5corner case, base case, \u7a0b\u5e8f\u7f29\u8fdb\uff09\u3002 \u9762\u8bd5\u5b98 \u4ee3\u7801\u5de5\u6574 \uff08\u5f62\u6210\u81ea\u5df1\u7684style\uff09 coding\u4e60\u60ef\uff08corner test case\uff0cextreme case\u8003\u8651\u5230\u4e86\u6ca1\uff09 \u6c9f\u901a\u80fd\u529b\uff0c\u4ea4\u6d41\u969c\u788d\u6709\u5417 \u9762\u8bd5\u8003\u5bdf\u57fa\u672c\u529f \u7a0b\u5e8f\u98ce\u683c\uff08\u7f29\u8fdb\uff0c\u62ec\u53f7\uff0c\u53d8\u91cf\u540d\uff09 Coding\u4e60\u60ef\uff08\u5f02\u5e38\u68c0\u67e5\uff0c\u8fb9\u754c\u5904\u7406\uff09 \u6c9f\u901a\uff08\u8ba9\u9762\u8bd5\u5b98\u65f6\u523b\u660e\u767d\u4f60\u7684\u610f\u56fe\uff09 \u6d4b\u8bd5\uff08\u4e3b\u52a8\u5199\u51fa\u5408\u7406\u7684Testcase\uff09 \u5982\u4f55\u51c6\u5907\u9762\u8bd5 \u5982\u4f55\u4e00\u4e2a\u6708\u641e\u5b9a\u7b97\u6cd5\u9762\u8bd5\uff1f\u603b\u7ed3\u5f52\u7c7b\u76f8\u4f3c\u9898\u76ee \u627e\u51fa\u540c\u7c7b\u9898\u578b\u7684\u6a21\u677f\u7a0b\u5e8f \u62ffoffer\u56db\u5927\u6cd5\u5b9d \u6446\u6b63\u5fc3\u6001\uff0c\u5c3d\u53ef\u80fd\u591a\u5730\u5237\u9898\uff0c\u4e0d\u6025\u4e8e\u6c42\u6210\uff0c\u505a\u5b8c\u4e00\u7c7b\u518d\u6765\u4e0b\u4e00\u7c7b \u6109\u5feb\u4ea4\u6d41\uff0c \u7406\u89e3\u800c\u975e\u80cc\u8bf5\uff0c\u4e0d\u8bb0\u5177\u4f53\u5b9e\u73b0\u8bb0\u8fd9\u4e00\u7c7b\u9898\u901a\u7528\u601d\u7ef4\u65b9\u5f0f\u548c\u5206\u6790\u6280\u5de7\u3002 \u65f6\u95f4\u8981\u7528\u5728\u5200\u5203\u4e0a\uff0c\u4e0d\u8981\u628a\u65f6\u95f4\u6d6a\u8d39\u5728\u90a3\u4e9b\u57fa\u672c\u4e0d\u4f1a\u8003\u4f60\u53c8\u5f88\u5fc3\u865a\u7684\u5185\u5bb9\uff08\u6bd4\u5982KMP\uff0c\u7ea2\u9ed1\u6811\uff0cAVL\uff0cACM\u7ade\u8d5b\u9898\uff09","title":"Notes"},{"location":"courses/9chap-algorithms/notes/#lecture-2-binary-search-rotated-sorted-array","text":"# Problem Comment Date 1 Classical Binary Search* 08/12/2017, 2 Search for a Range 08/12/2017, 3 Search Insert Position 08/12/2017, 4 Search in a Big Sorted Array 08/12/2017, 5 Search a 2D Matrix 08/12/2017, 6 Search a 2D Matrix II 08/15/2017, 7 First Bad Version 08/15/2017, 8 Find Peak Element Google, important, mit algorithm course. 08/15/2017, 9 Recover Rotated Sorted Array* 08/15/2017, 10 Find Minimum in Rotated Sorted Array 08/15/2017, 11 Find Minimum in Rotated Sorted Array II 08/15/2017, 12 Search in Rotated Sorted Array 08/15/2017, 13 Search in Rotated Sorted Array II 08/15/2017, 14 Median of Two Sorted Arrays 08/15/2017, 15 Rotate String* 08/15/2017, 16 Reverse Words in a String 08/15/2017, 17 Wood Cut* 08/15/2017,","title":"Lecture 2 \u4e8c\u5206\u641c\u7d22\u4e0e\u65cb\u8f6c\u6392\u5e8f\u6570\u7ec4 Binary Search &amp; Rotated Sorted Array"},{"location":"courses/9chap-algorithms/notes/#notes_1","text":"Classical Binary Search Given an sorted integer array - nums, and an integer - target. Find the any/first/last position of target in nums, return -1 if target doesn\u2019t exist. Prototype: int binarySearch(int[] nums, int target) \u590d\u6742\u5ea6\u5206\u6790 T(n) = T(n/2) + O(1) = O(logn) \u7b97\u6cd5\u9762\u8bd5\u4e2d\u5982\u679c\u9700\u8981\u4f18\u5316O(n)\u7684\u65f6\u95f4\u590d\u6742\u5ea6\u90a3\u4e48\u53ea\u80fd\u662fO(logn)\u7684\u4e8c\u5206\u6cd5 Recursion or While-loop? bad for recursion: Linux have only 8MB stack, local variable takes much space for each level of recursion. for simple problem, use iteration instead of recursion. for more complex problem use recursion. ask interviewer. Binary Search\u6a21\u677f\u56db\u8981\u7d20 start + 1 < end start + (end - start) / 2 A[mid] == , < ,> A[start] A[end] ? target Find First Position of XXX VS. Find Last Position of XXX character of binary search the split depends on you need the first position of XXX or last position of XXX. See the code.","title":"Notes"},{"location":"courses/9chap-algorithms/notes/#lecture-3-binary-tree-divide-conquer-algorithm","text":"# Problem Comment Date 1 Binary Tree Preorder Traversal stack, right push first 08/26/2017, 2 Maximum Depth of Binary Tree Recursion, same as find height of the tree 08/26/2017, 3 Balanced Binary Tree find height first 08/26/2017, 4 Lowest Common Ancestor of a Binary Tree Lowest Common Ancestor of a Binary Search Tree 08/26/2017, 5 Binary Tree Maximum Path Sum \"at least one node\" 08/26/2017, 6 Binary Tree Level Order Traversal BFS, DFS, queue 08/26/2017, 7 Binary Tree Level Order Traversal II call reverse() when done by Binary Tree Level Order Traversal 08/26/2017, 8 Binary Tree Zigzag Level Order Traversal use two stacks 08/26/2017, 9 Validate Binary Search Tree inorder traversal, LONG_MAX, LONG_MIN 08/26/2017, 10 Search Range in a Binary Search Tree* inorder traversal 08/26/2017, 11 Insert a Node in Binary Search Tree* recursion 08/26/2017, 12 Binary Search Tree Iterator use stack 08/26/2017, 13 Remove Node in Binary Search Tree* Problem Inorder Successor BST 08/26/2017, 14 Inorder Successor BST Successor of a node, special case: [2,1],p=1, [3,1,null,null,2], p=2. 08/26/2017,","title":"Lecture 3 \u4e8c\u53c9\u6811\u95ee\u9898\u4e0e\u5206\u6cbb\u7b97\u6cd5 Binary Tree &amp; Divide Conquer Algorithm"},{"location":"courses/9chap-algorithms/notes/#notes_2","text":"\u6784\u9020binary tree \u552f\u4e00\u6216\u8005\u4e0d\u552f\u4e00 preorder + inorder postorder + inorder preorder + postorder \u4e0d\u552f\u4e00 divide conquer solve preorder traversal O(n^2) Max Path Sum root -> leaf root -> any node. any node -> any node \u5b58\u5728\u8d1f\u6570\u8282\u70b9\uff0c\u8fd9\u6761\u8def\u5f84\u81f3\u5c11\u5305\u542b\u4e00\u4e2a\u7ed3\u70b9","title":"Notes"},{"location":"courses/9chap-algorithms/notes/#lecture-4-dynamic-programming-i","text":"# Problem Comment Date 1 Triangle 08/29/2017, 2 Minimum Path Sum 08/16/2017, 3 Unique Paths 08/16/2017, 4 Unique Paths II 08/16/2017, 5 Climbing Stairs 08/30/2017, 6 Jump Game 08/16/2017, 7 Jump Game II Maximum Subarray, Frog Jump 08/30/2017,","title":"Lecture 4 \u52a8\u6001\u89c4\u5212 Dynamic Programming I"},{"location":"courses/9chap-algorithms/notes/#notes_3","text":"\u4ece\u9012\u5f52\u5230\u52a8\u5f52 - Triangle DFS - Traverse \u590d\u6742\u5ea6 O(2^n) n is the height (\u6bcf\u6b21\u6709\u4e24\u4e2a\u53ef\u9009\u8def\u5f84) DFS - Divide & Conquer \u590d\u6742\u5ea6 O(2^n) \u6bcf\u6761\u8def\u52b2\u6ca1\u6709\u4efb\u4f55\u6761\u4ef6\u8ba9\u5b83\u63d0\u524d\u8fd4\u56de \u901a\u8fc7\u753b\u56fe\u6765\u89c2\u5bdf\u5982\u4f55\u964d\u4f4e\u590d\u6742\u5ea6\uff08\u6709\u91cd\u590d\u8ba1\u7b97\uff09 Divide & Conquer + Memorization \u8bb0\u5fc6\u5316\u641c\u7d22\uff0c\u88c5\u4e86\u773c\u775b\uff0c\u77e5\u9053\u505c\u54ea Bottom-up \u52a8\u5f52\u7684\u56db\u4e2a\u6b65\u9aa4 \u72b6\u6001\u7684\u5b9a\u4e49 \u521d\u59cb\u5316\uff0c\u7ec8\u70b9\u5148\u6709\u503c \u5faa\u73af\u9012\u63a8\uff08\u4e0d\u662f\u9012\u5f52\uff09\u6c42\u89e3 \u6c42\u7ed3\u679c: \u8d77\u70b9 Top-down \u4e24\u79cd\u6ca1\u6709\u592a\u591a\u7684\u4f18\u7565\u4e4b\u5206\uff0c\u6839\u636e\u4e2a\u4eba\u4e60\u60ef\u7075\u6d3b\u8fd0\u7528 \u4ec0\u4e48\u6837\u7684\u9898\u9002\u5408\u4f7f\u7528\u52a8\u6001\u89c4\u5212\uff1f \u6ee1\u8db3\u4e09\u4e2a\u6761\u4ef6\u4e4b\u4e00\u5219\u6781\u6709\u53ef\u80fd\u4f7f\u7528\u52a8\u5f52\u6c42\u89e3 Maximum/Minimum Yes/No Count(*) (i.e. \u603b\u5171\u6709\u591a\u5c11\u4e2a\uff09 \u4e0d\u80fd\u7528\u52a8\u5f52 \u8981\u6c42\u51fa\u6240\u6709\u201c\u5177\u4f53\u201d\u7684\u65b9\u6848\u800c\u975e\u65b9\u6848\u201c\u4e2a\u6570\u201d http://www.lintcode.com/problem/palindromepartitioning/ \u8f93\u5165\u6570\u636e\u662f\u4e00\u4e2a\u201c\u96c6\u5408\u201d\u800c\u4e0d\u662f\u201c\u5e8f\u5217\u201d http://www.lintcode.com/problem/longest-consecutivesequence/ \u52a8\u6001\u89c4\u52124\u70b9\u8981\u7d20 \u72b6\u6001 State \u7075\u611f\uff0c\u521b\u9020\u529b\uff0c\u5b58\u50a8\u5c0f\u89c4\u6a21\u95ee\u9898\u7684\u7ed3\u679c \u65b9\u7a0b Function \u72b6\u6001\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u600e\u4e48\u901a\u8fc7\u5c0f\u7684\u72b6\u6001\uff0c\u6765\u7b97\u5927\u7684\u72b6\u6001 \u521d\u59cb\u5316 Intialization \u6700\u6781\u9650\u7684\u5c0f\u72b6\u6001\u662f\u4ec0\u4e48, \u8d77\u70b9 \u7b54\u6848 Answer \u6700\u5927\u7684\u90a3\u4e2a\u72b6\u6001\u662f\u4ec0\u4e48\uff0c\u7ec8\u70b9 \u9762\u8bd5\u6700\u5e38\u89c1\u7684\u56db\u79cd\u7c7b\u578b Matrix DP (15%) state: f[x][y] \u8868\u793a\u6211\u4ece\u8d77\u70b9\u8d70\u5230\u5750\u6807x,y\u2026\u2026 function: \u7814\u7a76\u8d70\u5230x,y\u8fd9\u4e2a\u70b9\u4e4b\u524d\u7684\u4e00\u6b65 intialize: \u8d77\u70b9 answer: \u7ec8\u70b9 Sequence DP (40%) state: f[i]\u8868\u793a\u201c\u524di\u201d\u4e2a\u4f4d\u7f6e/\u6570\u5b57/\u5b57\u6bcd,(\u4ee5\u7b2ci\u4e2a\u4e3a)... function: f[i] = f[j] \u2026 j \u662fi\u4e4b\u524d\u7684\u4e00\u4e2a\u4f4d\u7f6e intialize: f[0].. answer: f[n-1].. Two Sequences DP (40%) Others (5%) \u9762\u8bd5\u4e2d\u5e38\u7528\u52a8\u6001\u89c4\u5212 \u77e9\u9635\u52a8\u6001\u89c4\u5212\uff08\u5750\u6807\u76f8\u5173\uff09 \u5355\u5e8f\u5217\u52a8\u6001\u89c4\u5212\uff08\u4e0a\uff09","title":"Notes"},{"location":"courses/9chap-algorithms/notes/#lecture-5-dynamic-programming-ii","text":"# Problem Comment Date 1 Palindrome Partitioning II 08/16/2017, 2 Word Break 08/16/2017, 3 Longest Common Subsequence* 08/16/2017, 4 Longest Common Substring* 08/16/2017, 5 Edit Distance 08/16/2017, 6 Distinct Subsequences 08/16/2017, 7 Interleaving String 08/16/2017,","title":"Lecture 5 \u52a8\u6001\u89c4\u5212 Dynamic Programming II"},{"location":"courses/9chap-algorithms/notes/#lecture-6-linked-list","text":"# Problem Comment Date 1 Remove Duplicates from Sorted List II 08/30/2017, 2 Reverse Linked List II 08/30/2017, 3 Merge Two Sorted Lists 08/30/2017, 4 Partition List 08/30/2017, 5 Sort List 08/30/2017, 6 Reorder List 08/30/2017, 7 Merge k Sorted Lists 08/30/2017, 8 Copy List with Random Pointer 08/30/2017, 9 Linked List Cycle 08/30/2017, 10 Convert Sorted List to Balanced Binary Search Tree 08/30/2017, 11 Convert Binary Tree to Doubly Linked List 08/30/2017, 12 Reverse List Nodes in k-Groups 08/30/2017,","title":"Lecture 6 \u94fe\u8868 Linked List"},{"location":"courses/9chap-algorithms/notes/#notes_4","text":"Basic skills Insert a Node in Sorted List Remove a Node from Linked List Reverse a Linked List Merge Two Linked Lists Find the Middle of a Linked List Sorting quick sort, \u539f\u5730\u6392\u5e8f\uff0c\u4e0d\u7a33\u5b9a\u3002\u76f8\u5bf9\u987a\u5e8f\u4f1a\u6539\u53d8\uff0c\u6240\u4ee5\u4e0d\u7a33\u5b9a\u3002 \u7a33\u5b9a\u6392\u5e8f\uff0c \u7761\u7720\u6392\u5e8f Fast Slow Pointers Find the Middle of Linked List Remove Nth Node From End of List Linked List Cycle I, II","title":"Notes"},{"location":"courses/9chap-algorithms/notes/#lecture-7-array-numbers","text":"# Problem Comment Date 1 Recover Rotated Sorted Array* 08/30/2017, 2 Rotate String* 08/30/2017, 3 Reverse Words in a String 08/30/2017, 4 Merge Sorted Array 08/30/2017, 5 Merge Sorted Array II* 08/30/2017, 6 Median of Two Sorted Arrays 08/30/2017, 7 Maximum Subarray 08/30/2017, 8 Maximum Subarray II* Prefix sum, left, right 08/30/2017, 9 Maximum Subarray III* DP, from O(n 2k)O(n2k) to O(nk)O(nk) 08/30/2017, 10 Minimum Subarray* 08/30/2017, 11 Subarray Sum* Subarray Sum Equals K 08/30/2017, 12 Best Time to Buy and Sell Stock 08/30/2017, 13 Best Time to Buy and Sell Stock II 08/30/2017, 14 Best Time to Buy and Sell Stock III 08/30/2017, 15 Two Sum 08/30/2017, 16 3Sum 08/30/2017, 17 4Sum 08/30/2017, 18 K Sum* 08/30/2017, 19 Partition Array 08/30/2017, 20 Sort Colors 08/30/2017, 21 Sort Letters by Case 08/30/2017, 22 Interleaving Positive and Negative Numbers 08/30/2017,","title":"Lecture 7 \u6570\u7ec4\u4e0e\u6570 Array &amp; Numbers"},{"location":"courses/9chap-algorithms/notes/#notes_5","text":"\u4e09\u6b65\u7ffb\u8f6c\u6cd5 Recover Rotated Sorted Array Median of Two Sorted Arrays ( Important) find median ==> find kth largest take O(1) to reduce the problem to find the k/2 th element. Maximum Subarray == Best Time to Buy and Sell Stock (see the video at 53:00) \u524d\u7f00\u548c","title":"Notes"},{"location":"courses/9chap-algorithms/notes/#lecture-8-data-structure","text":"# Comment Date \u9898\u76ee \u5907\u6ce8 \u65e5\u671f Min-Stack Implement a queue by two stacks Largest Rectangle in histogram \u9012\u589e\u6808 Max Tree \u9012\u51cf\u6808 Rehashing LRU Cache Longest Consecutive Sequence Subarray Sum Copy List with Random Pointer Anagrams Median Number Heapify Merge K sorted List Word Search II 08/30/2017,","title":"Lecture 8 \u6570\u636e\u7ed3\u6784 Data Structure"},{"location":"courses/9chap-algorithms/notes/#notes_6","text":"Running time complexity for stack operation or other data structure talk average instead of worst case. think in the larger For the largest Rectangle in histogram problem, considering the concept of \u9012\u589e\u6808 \u5f3a\u5316\u73ed\u5185\u5bb9\uff1a\u9012\u589e\u6808\u5176\u4ed6\u5e94\u7528\uff0c\u8868\u8fbe\u5f0f\u503c\uff0c\u8868\u8fbe\u5f0f\u6811 Largest Rectangle in Histogram: \u601d\u8def\u6a21\u677f\uff1a\u201c\u8fd9\u79cd\u9898\u76ee\u7684\u76f8\u4f3c\u6027\u662f\u627e\u6570\u7ec4\u91cc\u9762\u4e00\u4e2a\u5143\u7d20\u5de6\u8fb9\u6216\u8005\u53f3\u8fb9\u7b2c\u4e00\u4e2a\u6bd4\u4ed6\u5c0f\u6216\u8005\u6bd4\u4ed6\u5927\u7684\u5143\u7d20\" \u5e76\u67e5\u96c6\uff0c\u4e24\u4e2a\u6307\u9488\uff0chashheap\uff0c\u52a8\u6001\u89c4\u5212\u4f18\u5316\u9898\u76ee Hash implementation: int hashfunc ( string key ){ int sum = 0 ; for ( int i = 0 ; i < key . length (); i ++ ) { sum = sum * 33 + ( int )( key . charAt ( i )); sum = sum % HASH_TABLE_SIZE ; } return sum } Collision open hashing closed hashing","title":"Notes"},{"location":"courses/9chap-algorithms/notes/#lecture-9-graph-search","text":"\u9898\u76ee \u5907\u6ce8 \u65e5\u671f Clone graph Copy List with Random Pointer Find the connected","title":"Lecture 9 \u56fe\u8bba\u4e0e\u641c\u7d22 Graph &amp; Search"},{"location":"courses/9chap-algorithms/notes/#notes_7","text":"time for prepare interview easy: 5 min medium: 15 min hard: 30-40 min \u6700\u597d\u4e0d\u7528DFS\uff0c node\u6570\u91cf\u591a\u7684\u60c5\u51b5\u4e0b\uff0crecursion spaec\u6d88\u8017\u592a\u5927\uff0c Topological sorting inDegree outDegree \u4f55\u9002\u7528BFS\uff0c \u4ece\u4e00\u4e2a\u70b9\u51fa\u53d1\uff0c\u627e\u8fde\u901a\u5757 level by level, \u5bfb\u627e\u4e24node\u6700\u77ed\u8ddd\u79bb subset \u9690\u5f0f\u56fe\u641c\u7d22 VS \u663e\u5f0f\u56fe\u641c\u7d22 \u4f55\u65f6\u7528DFS\uff0c \u627e\u51fa\u6240\u6709\u7684\u7ed3\u679c\u800c\u4e0d\u662f\u627e\u51fa\u603b\u7ed3\u679c\u6570\u3002 Running time permutation: O(n! * n) Running time subset: O(2^n * n) \u641c\u7d22\u7c7b\u578b\uff1a \u663e\u5f0f\u56fe\u641c\u7d22 Matrix Graph \u9690\u5f0f\u56fe\u641c\u7d22 permutation subset combination \u5bf9\u5206\u5272\u7c7b\uff0c\u53ef\u4ee5\u628a\u95ee\u9898\u770b\u6210\u5206\u5272\u7ebf\u7684combination problem.","title":"Notes"},{"location":"courses/9chap-dynamic-prog/notes/","text":"Nine Chapter Dynamic Programming \u00b6 Nine Chapter Dynamic Programming Course Lecture 1 Introduction to Dynamic Programming \u00b6 Problem Category Coin Change Unique Paths coordinate Jump Game \u52a8\u6001\u89c4\u5212\u9898\u76ee\u7279\u70b9: \u8ba1\u6570 \u6709\u591a\u5c11\u79cd\u65b9\u5f0f\u8d70\u5230\u53f3\u4e0b\u89d2 \u6709\u591a\u5c11\u79cd\u65b9\u6cd5\u9009\u51fak\u4e2a\u6570\u4f7f\u5f97\u548c\u662fSum \u6c42\u6700\u5927\u6700\u5c0f\u503c \u4ece\u5de6\u4e0a\u89d2\u8d70\u5230\u53f3\u4e0b\u89d2\u8def\u5f84\u7684\u6700\u5927\u6570\u5b57\u548c \u6700\u957f\u4e0a\u5347\u5b50\u5e8f\u5217\u957f\u5ea6 \u6c42\u5b58\u5728\u6027 \u53d6\u77f3\u5b50\u6e38\u620f\uff0c\u5148\u624b\u662f\u5426\u5fc5\u80dc \u80fd\u4e0d\u80fd\u9009\u51fak\u4e2a\u6570\u4f7f\u5f97\u548c\u662fSum \u72b6\u6001\u662f\u52a8\u6001\u89c4\u5212\u5b9a\u6d77\u795e\u9488\uff0c\u786e\u5b9a\u72b6\u6001\u9700\u8981\u4e24\u4e2a\u57fa\u672c\u610f\u8bc6\uff1a \u6700\u540e\u4e00\u6b65 \u5b50\u95ee\u9898 Four Ingredients for DP What's the state? start with the last step for the optimal solution decompose into subproblems Write the state transition? find the transition from subproblem relations Initial value and boundary conditions need to think careful in this step How can you compute the states? iteration directio forward computing Coin Change \u00b6 Imagine the last coin you can use and the minimum solution to found can be represented as f[amount] . It can be solved by solving the smaller problem first. we have f[amount] = min(f[amount], f[amount - last_coin] + 1) . The problem is we don't know which coin will be selected for the last one to reach the solution, so we have to iterate through the coins to check every one of them. We expecting to see two for loops in our code. DP 4 ingredient: size of the dp array f[amount + 1] initial state: f[0] = 0 , amount 0 can use 0 coin. subproblem: f[amount] = min(f[amount], f[amount - last\\_coin] + 1) . results: f[amount] DP solution class Solution { public : int coinChange ( vector < int >& coins , int amount ) { int n = coins . size (); int f [ amount + 1 ]; f [ 0 ] = 0 ; /* calculate the f[1], f[2], ... f[amount] */ for ( int i = 1 ; i <= amount ; i ++ ) { f [ i ] = INT_MAX ; /* small trick, set to invalid first */ for ( int j = 0 ; j < n ; j ++ ) { /* update states */ /* f[i] can select coins[j] && f[i - coins[j]] is possible && coins[j] is last coin */ if ( i >= coins [ j ] && f [ i - coins [ j ]] != INT_MAX && f [ i - coins [ j ]] + 1 < f [ i ]) { f [ i ] = f [ i - coins [ j ]] + 1 ; } } } return f [ amount ] == INT_MAX ? -1 : f [ amount ]; } }; Alternative Solution class Solution { public : int coinChange ( vector < int >& coins , int amount ) { int n = coins . size (); int f [ amount + 1 ]; // f[i] represent the minimum counts to make up i amount // use INT_MAX to represent impossible case. f [ 0 ] = 0 ; for ( int i = 1 ; i <= amount ; i ++ ) { f [ i ] = INT_MAX ; for ( int j = 0 ; j < n ; j ++ ) { if ( i >= coins [ j ] && f [ i - coins [ j ]] != INT_MAX ) { f [ i ] = min ( f [ i ], f [ i - coins [ j ]] + 1 ); } } } return f [ amount ] == INT_MAX ? -1 : f [ amount ]; } }; Unique Paths \u00b6 Solving smaller problem first than by accumulating the results from the smaller problems, we can solve the overall problem. Use a 2-d array to record the result the smaller problem, we know for the position f[i][j] = f[i - 1][j] + f[i][j - 1] , which means the summation of number of path from above and from left. The initial state is the first row and the first column are all equal to 1 . class Solution { public : /** * @param n, m: positive integer (1 <= n ,m <= 100) * @return an integer */ int uniquePaths ( int m , int n ) { int f [ m ][ n ] = { 0 }; for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { if ( i == 0 || j == 0 ) { f [ i ][ j ] = 1 ; } if ( i > 0 && j > 0 ) { f [ i ][ j ] = f [ i - 1 ][ j ] + f [ i ][ j - 1 ]; } } } return f [ m - 1 ][ n - 1 ]; } }; Jump Game \u00b6 Notice the problem statement \"Each element in the array represents your maximum jump length at that position.\" Solution 1 DP The problem have characteristics of the dynamic problem, that is it can be decomposed into smaller problem, the large problem can be solved using the result from solving smaller problems. We use a dp array f[i] to store whether it can jump from previous position to position i . if nums[i] + i >= j , it can also jump to position j . Looks like we are going to have a nested loop. the outer loop iterate to check whether can jump to each step j . inner loop check each step before j , namely the smaller size problem. class Solution { public : bool canJump ( vector < int >& nums ) { int n = nums . size (); if ( n == 0 ) return true ; int f [ n ]; f [ 0 ] = true ; /* initialize */ for ( int j = 1 ; j < n ; j ++ ) { f [ j ] = false ; for ( int i = 0 ; i < j ; i ++ ) { if ( f [ i ] && nums [ i ] + i >= j ) { f [ j ] = true ; break ; } } } return f [ n - 1 ]; } }; Solution 2 Greedy Use a variable cur_max to maintain the possible maximum jump position, if the current position is less than the maximum possible jump, return flase. class Solution { public : bool canJump ( vector < int >& nums ) { int n = nums . size (); if ( n == 0 ) return true ; int cur_max = nums [ 0 ]; /* maximum distance can reach from ith */ /* L.I.: current step (ith step) must <= cur_max jump */ for ( int i = 0 ; i < n ; i ++ ) { if ( i > cur_max ) /* must goes first */ return false ; if ( i + nums [ i ] > cur_max ) cur_max = i + nums [ i ]; } return true ; } }; Lecture 2 Dynamic Programming on Coordinates \u00b6 Problem Category Unique Paths II \u5750\u6807\u578b Paint House \u5e8f\u5217\u578b \uff0b \u72b6\u6001 Decode Ways \u5212\u5206\u578b Longest Increasing Continuous Subsequence \u5e8f\u5217\u578b / \u5212\u5206\u578b Minimum Path Sum \u5750\u6807\u578b Bomb Enemy \u5750\u6807\u578b Counting Bits \u5750\u6807\u578b \u5750\u6807\u578b\u52a8\u6001\u89c4\u5212 \u00b6 \u6700\u7b80\u5355\u7684\u52a8\u6001\u89c4\u5212\u7c7b\u578b \u7ed9\u5b9a\u4e00\u4e2a\u5e8f\u5217\u6216\u7f51\u683c \u9700\u8981\u627e\u5230\u5e8f\u5217\u4e2d\u67d0\u4e2a/\u4e9b\u5b50\u5e8f\u5217\u6216\u7f51\u683c\u4e2d\u7684\u67d0\u6761\u8def\u5f84 \u67d0\u79cd\u6027\u8d28\u6700\u5927/\u6700\u5c0f \u8ba1\u6570 \u5b58\u5728\u6027 \u52a8\u6001\u89c4\u5212\u65b9\u7a0b f[i] \u4e2d\u7684\u4e0b\u6807 i \u8868\u793a\u4ee5 a[i] \u4e3a\u7ed3\u5c3e\u7684\u6ee1\u8db3\u6761\u4ef6\u7684\u5b50\u5e8f\u5217\u7684\u6027\u8d28. f[i][j] \u4e2d\u7684 \u4e0b\u6807 i , j \u8868\u793a\u4ee5\u683c\u5b50 (i, j) \u4e3a\u7ed3\u5c3e\u7684\u6ee1\u8db3\u6761\u4ef6\u7684\u8def\u5f84\u7684\u6027\u8d28 \u6700\u5927\u503c/\u6700\u5c0f\u503c \u4e2a\u6570 \u662f\u5426\u5b58\u5728 \u5750\u6807\u578b\u52a8\u6001\u89c4\u5212\u7684\u521d\u59cb\u6761\u4ef6 f[0] \u5c31\u662f\u6307\u4ee5 a[0] \u4e3a\u7ed3\u5c3e\u7684\u5b50\u5e8f\u5217\u7684\u6027\u8d28 Unique Paths II \u00b6 C++ Naive DP class Solution { public : int uniquePathsWithObstacles ( vector < vector < int >>& obstacleGrid ) { int m = obstacleGrid . size (); int n = obstacleGrid [ 0 ]. size (); int i = 0 ; int j = 0 ; int flag = 0 ; vector < vector < int >> f ( m , vector < int > ( n )); if ( m == 0 || n == 0 ) { return 0 ; } if ( obstacleGrid [ 0 ][ 0 ] == 1 ) { return 0 ; } f [ 0 ][ 0 ] = 1 ; for ( i = 1 ; i < m ; i ++ ) { if ( obstacleGrid [ i ][ 0 ] == 1 ) { f [ i ][ 0 ] = 0 ; } else { f [ i ][ 0 ] = f [ i -1 ][ 0 ]; } } for ( j = 1 ; j < n ; j ++ ) { if ( obstacleGrid [ 0 ][ j ] == 1 ) { f [ 0 ][ j ] = 0 ; } else { f [ 0 ][ j ] = f [ 0 ][ j -1 ]; } } for ( i = 1 ; i < m ; i ++ ) { for ( j = 1 ; j < n ; j ++ ) { if ( obstacleGrid [ i ][ j ] == 1 ) { f [ i ][ j ] = 0 ; } else { f [ i ][ j ] = f [ i ][ j -1 ] + f [ i -1 ][ j ]; } } } return f [ m -1 ][ n -1 ]; } }; C++ Naive DP Refactored class Solution { public : int uniquePathsWithObstacles ( vector < vector < int >>& obstacleGrid ) { int m = obstacleGrid . size (); int n = m > 0 ? obstacleGrid [ 0 ]. size () : 0 ; if ( m == 0 && n == 0 ) { return 0 ; } vector < vector < int >> f ( m , vector < int > ( n , 0 )); f [ 0 ][ 0 ] = 1 ; if ( obstacleGrid [ 0 ][ 0 ] == 1 ) { return 0 ; } for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { if ( obstacleGrid [ i ][ j ] == 1 ) { f [ i ][ j ] = 0 ; } else { // whenever available if ( i > 0 ) { f [ i ][ j ] += f [ i - 1 ][ j ]; } if ( j > 0 ) { f [ i ][ j ] += f [ i ][ j - 1 ]; } } } } return f [ m - 1 ][ n - 1 ]; } }; Java O(n) space // Optimized using a rolling array or single row dp array instead of m x n. class Solution { public int uniquePathsWithObstacles ( int [][] a ) { int m = a . length ; int n = a [ 0 ] . length ; int dp [] = new int [ n ] ; dp [ 0 ] = 1 ; //T[i][j] = T[i-1][j] + T[i][j-1] for ( int i = 0 ; i < m ; i ++ ){ for ( int j = 0 ; j < n ; j ++ ){ if ( a [ i ][ j ] == 1 ){ dp [ j ] = 0 ; } else if ( j > 0 ){ dp [ j ] += dp [ j - 1 ] ; } } } return dp [ n - 1 ] ; } } Paint House \u00b6 DP last step: considering the optimal solution. The last paint must be one of the three colors and the paint cost is minimum. State: we can try to name f[i] the minimum cost of painting the first i houses, i = 0, 1, ... i-1 . However, we cannot know what color could be used for the last house. Change the state to: f[i][k] is the minium cost of painting the first i houses and the i-1 th element is painted as color k . This way, we can choose the last paint based on this piece of information. The state transition fomular: f[i][k] = f[i - 1][\\ell]_{\\ell != k} + costs[i - 1][k] f[i][k] = f[i - 1][\\ell]_{\\ell != k} + costs[i - 1][k] The result is minimum of the: f[i][0],\\ f[i][1],\\ f[i][2] f[i][0],\\ f[i][1],\\ f[i][2] class Solution { public : int minCost ( vector < vector < int >>& costs ) { int m = costs . size (); int f [ m + 1 ][ 3 ]; /* state: f[i][0] paint i house and the last (i - 1) house is red * /* initial value, paint 0 house cost 0 */ for ( int j = 0 ; j < 3 ; j ++ ) f [ 0 ][ j ] = 0 ; for ( int i = 1 ; i <= m ; i ++ ) { for ( int j = 0 ; j < 3 ; j ++ ) { // current color f [ i ][ j ] = INT_MAX ; for ( int k = 0 ; k < 3 ; k ++ ) { // painted color /* cannot paint same color for neighbor house */ if ( j == k ) continue ; // ith (index with i - 1) house is painted with color j if ( f [ i ][ j ] > f [ i - 1 ][ k ] + costs [ i - 1 ][ j ]) f [ i ][ j ] = f [ i - 1 ][ k ] + costs [ i - 1 ][ j ]; //f[i][j] = min(f[i - 1][k] + costs[i - 1][j], f[i][j]); } } } return min ( min ( f [ m ][ 0 ], f [ m ][ 1 ]), f [ m ][ 2 ]); } }; Decode Ways \u00b6 4 Ingredients Last step: A_0, A_1, A_2, ..., A_n-3, [A_n-2, A_n-1] A_0, A_1, A_2, ..., A_n-3, A_n-2, [A_n-1] State: f[n] : decode ways of first n letters. Smaller problem: f[n] = f[n - 1] + f[n - 2] | A_n-2,A_n-1 decodable. Init: f[0] = 1 Note Again the idea in this solution is to update the f[i][j] on demand, a technique that broadly used in 2-d coordinate based DP problems such as Bomb Enemy , Unique Paths II , and Minimum Path Sum . C++ DP class Solution { public : int numDecodings ( string s ) { int n = s . length (); int f [ n + 1 ] = { 0 }; if ( n == 0 ) return 0 ; f [ 0 ] = 1 ; for ( int i = 1 ; i <= n ; i ++ ) { int t = s [ i - 1 ] - '0' ; if ( t > 0 && t <= 9 ) { f [ i ] += f [ i - 1 ]; } if ( i > 1 ){ int q = ( s [ i - 2 ] - '0' ) * 10 + t ; if ( q >= 10 && q <= 26 ) { f [ i ] += f [ i - 2 ]; } } } return f [ n ]; } }; C++ DP O(1) class Solution { public : int numDecodings ( string s ) { if ( ! s . size () || s . front () == '0' ) return 0 ; // r2: decode ways of s[0, i-2] , r1: decode ways of s[0, i-1] int r1 = 1 , r2 = 1 ; // think it as a coordinate bases, not sequence based dp for ( int i = 1 ; i < s . size (); i ++ ) { // last char in s[0, i] is 0, cannot decode if ( s [ i ] == '0' ) r1 = 0 ; // two-digit letter, add r2 to r1, r2 get the previous r1 if ( s [ i - 1 ] == '1' || s [ i - 1 ] == '2' && s [ i ] <= '6' ) { r1 = r2 + r1 ; r2 = r1 - r2 ; } else { // one-digit letter, r2 get the previous r1 r2 = r1 ; } } return r1 ; } }; Longest Increasing Continuous Subsequence \u00b6 4 ingredients: Last step, last element a[n-1] could be in the result or not in the result. subproblem, suppose we have the LICS of the first n - 1 elements. represented as f[n-1] . base case and boundary condition, when no char in the string: f[0] = 1 . An empty string has LICS length of 1. order of calculation, calculate small index first. Not a leetcode The problem is not a leetcode problem, the original problem ask for sequences that could be increase or decrease. Using index in DP problems Avoid using both index i - 1 and i + 1 in a loop invariance, otherwise you'll have problems in keeping the loop invariance. Compare the following. Incorrect for ( int i = n - 1 ; i >= 0 ; i -- ) { f [ i ] = 1 ; if ( i < n - 1 && i > 0 && A [ i - 1 ] > A [ i ]) { f [ i ] = f [ i + 1 ] + 1 ; } res2 = max ( res2 , f [ i ]); } Correct for ( int i = n - 1 ; i >= 0 ; i -- ) { f [ i ] = 1 ; if ( i < n - 1 && A [ i ] > A [ i + 1 ]) { f [ i ] = f [ i + 1 ] + 1 ; } res2 = max ( res2 , f [ i ]); } Correct for ( int i = 0 ; i < n ; i ++ ) { f [ i ] = 1 ; if ( i > 0 && A [ i - 1 ] > A [ i ]) { f [ i ] = f [ i - 1 ] + 1 ; } res2 = max ( res2 , f [ i ]); } C++ DP solution class Solution { public : int longestIncreasingContinuousSubsequence ( vector < int >& A ) { // Write your code here int n = A . size (); int f [ n ]; int res1 = 0 ; int res2 = 0 ; for ( int i = 0 ; i < n ; i ++ ) { f [ i ] = 1 ; if ( i > 0 && A [ i - 1 ] < A [ i ]) { f [ i ] = f [ i - 1 ] + 1 ; } res1 = max ( res1 , f [ i ]); } for ( int i = n - 1 ; i >= 0 ; i -- ) { f [ i ] = 1 ; if ( i < n - 1 && A [ i ] > A [ i + 1 ]) { f [ i ] = f [ i + 1 ] + 1 ; } res2 = max ( res2 , f [ i ]); } return max ( res1 , res2 ); } }; Here we don't have to explicitly wirte f[i] = max(1, f[i - 1] + 1) , since we have the condition A[i - 1] < A[i] , we know the A[i] will be added to the f[i] , hence simply update f[i] = f[i - 1] + 1 directly. Minimum Path Sum \u00b6 This is coordinate based DP. We need to have f[i][j] to keep the minimum path sum to the grid[i][j] . Calculate f[i][j] from top to down and from left to right for each row. Space can be optimized. Notice we can do the init and first row and first column in nested loops, do not need use multiple for loops. C++ DP class Solution { public : int minPathSum ( vector < vector < int > > & grid ) { int m = grid . size (); int n = grid [ 0 ]. size (); if ( m == 0 || n == 0 ) return 0 ; int f [ m ][ n ]; for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { if ( i == 0 && j == 0 ) { f [ i ][ j ] = grid [ i ][ j ]; } if ( i == 0 && j > 0 ) { f [ i ][ j ] = f [ i ][ j - 1 ] + grid [ i ][ j ]; } if ( j == 0 && i > 0 ) { f [ i ][ j ] = f [ i - 1 ][ j ] + grid [ i ][ j ]; } if ( i > 0 && j > 0 ) { f [ i ][ j ] = min ( f [ i - 1 ][ j ], f [ i ][ j - 1 ]) + grid [ i ][ j ]; } } } return f [ m - 1 ][ n - 1 ]; } }; C++ DP Space O(n) class Solution { public : int minPathSum ( vector < vector < int > > & grid ) { int m = grid . size (); int n = grid [ 0 ]. size (); if ( m == 0 || n == 0 ) return 0 ; int f [ 2 ][ n ]; /* only two rows of status */ int prev = 1 ; int curr = 1 ; for ( int i = 0 ; i < m ; i ++ ) { // rolling the f array when move to a new row prev = curr ; curr = 1 - curr ; for ( int j = 0 ; j < n ; j ++ ) { f [ curr ][ j ] = grid [ i ][ j ]; if ( i == 0 && j > 0 ) { f [ i ][ j ] += f [ i ][ j - 1 ]; } if ( j == 0 && i > 0 ) { f [ curr ][ j ] += f [ prev ][ j ]; } if ( i > 0 && j > 0 ) { f [ curr ][ j ] += min ( f [ prev ][ j ], f [ curr ][ j - 1 ]); } } } return f [ curr ][ n - 1 ]; } }; Note \u5728\u5904\u7406\u4e8c\u7ef4\u5750\u6807\u578bDP\u95ee\u9898\u7684\u65f6\u5019\uff0c\u5bf9\u4e8e\u7b2c\u4e00\u884c\u6216\u8005\u7b2c\u4e00\u5217\u7684\u8ba1\u7b97\u6211\u4eec\u53ef\u4ee5\u628a\u5b83\u653e\u5230loop\u4e2d\u3002\u4f46\u662f\u8981\u52a0\u4e00\u4e2a\u6761\u4ef6\u3002 \u8fd9\u4e2a\u6761\u4ef6\u5c31\u662f if (i > 0) , \u8fd9\u4e2a\u6761\u4ef6\u4fdd\u8bc1\u4e86\u5728\u66f4\u65b0\u6570\u7ec4\u503c\u7684\u65f6\u5019\u4e0b\u6807\u4e0d\u4f1a\u8d8a\u754c\u3002\u5e76\u4e14\u5f88\u597d\u7684\u8be0\u91ca\u4e86\u6211\u4eec\u7684\u52a8\u673a\uff08\u5c31\u662f\u5bf9\u4e8e\u7b2c\u4e00\u884c\u7b2c\u4e00\u5217\u4ec5\u4ec5\u662f\u521d\u59cb\u5316)\u3002 \u5bf9\u4e8e1\u4e2d\u63d0\u5230\u7684trick\uff0c\u53e6\u5916\u4e00\u4e2a\u597d\u5904\u5c31\u662f\u5bf9\u4e8e\u7b2c\u4e00\u884c\u6216\u8005\u7b2c\u4e00\u5217\u6761\u4ef6\u5224\u65ad\u7684\u65f6\u5019\uff0c\u8fd9\u91cc\u8fd8\u6709\u4e00\u4e2a\u6280\u5de7\u5c31\u662f\u5148\u628a\u81ea\u8eab\u7684\u2018\u6027\u8d28\u2019\u6216\u8005\u2018\u503c\u2019\u52a0\u4e0a\uff0c \u5982\u679c\u6709\u4e0a\u4e00\u884c\u6211\u4eec\u518d\u53bb\u52a0\u4e0a\u4e00\u884c\u7684\u2018\u6027\u8d28\u2019\u6216\u2018\u503c\u2019\u3002\u8fd9\u91cc\u9700\u8981\u9006\u5411\u601d\u7ef4. \u5728\u591a\u79cd\u60c5\u51b5\u9700\u8981\u5206\u5f00\u8ba8\u8bba\u7684\u65f6\u5019\u6211\u4eec\u53ef\u4ee5\u5148\u2018\u4e00\u7968\u5426\u51b3\u2019\u6700\u660e\u663e\u60c5\u51b5\uff0c\u7136\u540e\u8ba8\u8bba\u5269\u4e0b\u7684\u60c5\u51b5\u3002 Bomb Enemy \u00b6 Breakdown the problem into smaller (simpler) problems. Take the special steps (i.e. different properties of the cell) as normal ones in the loop, deal with the special step when doing the calculation. class Solution { public : int maxKilledEnemies ( vector < vector < char >>& grid ) { int m = grid . size (); if ( m == 0 ) return 0 ; int n = grid [ 0 ]. size (); if ( n == 0 ) return 0 ; int f [ m ][ n ]; int res [ m ][ n ]; int ret = 0 ; for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { res [ i ][ j ] = 0 ; } } /* up */ for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { f [ i ][ j ] = 0 ; if ( grid [ i ][ j ] == 'W' ) { f [ i ][ j ] = 0 ; } else { if ( grid [ i ][ j ] == 'E' ) { f [ i ][ j ] = 1 ; } if ( i > 0 ) { f [ i ][ j ] += f [ i - 1 ][ j ]; } } res [ i ][ j ] += f [ i ][ j ]; } } /* down */ for ( int i = m - 1 ; i >= 0 ; i -- ) { for ( int j = 0 ; j < n ; j ++ ) { f [ i ][ j ] = 0 ; if ( grid [ i ][ j ] == 'W' ) { f [ i ][ j ] = 0 ; } else { if ( grid [ i ][ j ] == 'E' ) { f [ i ][ j ] = 1 ; } if ( i < m - 1 ) { f [ i ][ j ] += f [ i + 1 ][ j ]; } } res [ i ][ j ] += f [ i ][ j ]; } } /* left */ for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { f [ i ][ j ] = 0 ; if ( grid [ i ][ j ] == 'W' ) { f [ i ][ j ] = 0 ; } else { if ( grid [ i ][ j ] == 'E' ) { f [ i ][ j ] = 1 ; } if ( j > 0 ) { f [ i ][ j ] += f [ i ][ j - 1 ]; } } res [ i ][ j ] += f [ i ][ j ]; } } /* right */ for ( int i = 0 ; i < m ; i ++ ) { for ( int j = n - 1 ; j >= 0 ; j -- ) { f [ i ][ j ] = 0 ; if ( grid [ i ][ j ] == 'W' ) { f [ i ][ j ] = 0 ; } else { if ( grid [ i ][ j ] == 'E' ) { f [ i ][ j ] = 1 ; } if ( j < n - 1 ) { f [ i ][ j ] += f [ i ][ j + 1 ]; } } res [ i ][ j ] += f [ i ][ j ]; } } /* calculate resutls */ for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { if ( grid [ i ][ j ] == '0' ) if ( res [ i ][ j ] > ret ) ret = res [ i ][ j ]; } } return ret ; } }; Counting Bits \u00b6 Notice you can use the trick that i >> 1 == i / 2 to construct the subproblem i count 1 bits 1 1 2 1 3 2 6 2 12 2 25 3 50 3 class Solution { public : vector < int > countBits ( int num ) { vector < int > f ( num + 1 , 0 ); if ( num == 0 ) { return f ; } for ( int i = 1 ; i <= num ; i ++ ) { f [ i ] = f [ i >> 1 ] + ( i % 2 ); } return f ; } }; Lecture 3 Dynamic Programming on Sequences \u00b6 Problem Category Paint House II \u5e8f\u5217\u578b\uff0b\u72b6\u6001 House Robber \u5e8f\u5217\u578b\uff0b\u72b6\u6001 House Robber II \u5e8f\u5217\u578b\uff0b\u72b6\u6001 Best Time to Buy and Sell Stock Best Time to Buy and Sell Stock II Best Time to Buy and Sell Stock III \u5e8f\u5217\u578b\uff0b\u72b6\u6001 Best Time to Buy and Sell Stock IV \u5e8f\u5217\u578b\uff0b\u72b6\u6001 Longest Increasing Subsequence \u5e8f\u5217\u578b Russian Doll Envelopes \u5e8f\u5217\u578b\u52a8\u6001\u89c4\u5212 \u00b6 \u7ed9\u5b9a\u4e00\u4e2a\u5e8f\u5217 \u52a8\u6001\u89c4\u5212\u65b9\u7a0b f[i] \u4e2d\u7684\u4e0b\u6807 i \u8868\u793a\u524d i \u4e2a\u5143\u7d20 a[0], a[1], ..., a[i-1] \u7684\u67d0\u79cd\u6027\u8d28 \u5750\u6807\u578b\u7684 f[i] \u8868\u793a\u4ee5 a[i] \u4e3a\u7ed3\u5c3e\u7684\u67d0\u79cd\u6027\u8d28 \u521d\u59cb\u5316\u4e2d\uff0c f[0] \u8868\u793a\u7a7a\u5e8f\u5217\u7684\u6027\u8d28 \u5750\u6807\u578b\u52a8\u6001\u89c4\u5212\u7684\u521d\u59cb\u6761\u4ef6 f[0] \u5c31\u662f\u6307\u4ee5 a[0] \u4e3a\u7ed3\u5c3e\u7684\u5b50\u5e8f\u5217\u7684\u6027\u8d28 Paint House II \u00b6 the solution could be the same as the first version, but it is O(n\\cdot k^2) O(n\\cdot k^2) . how to make it O(n\\cdot k) O(n\\cdot k) ? By analyzing the state transition equation, we observed that we want to find a minimum value of a set of numbers except one for each house. Put it in English, the min cost to paint i - 1 th house with color k , we want to have the min cost of painting all previous i - 1 houses and the i-1 th house cannot be painted as color k . f [ i ][ 1 ] = min { f [ i -1 ][ 2 ] + cost [ i -1 ][ 1 ], f [ i -1 ][ 3 ] + cost [ i -1 ][ 1 ], ..., f [ i -1 ][ K ] + cost [ i -1 ][ 1 ]} f [ i ][ 2 ] = min { f [ i -1 ][ 1 ] + cost [ i -1 ][ 2 ], f [ i -1 ][ 3 ] + cost [ i -1 ][ 2 ], ..., f [ i -1 ][ K ] + cost [ i -1 ][ 2 ]} ... f [ i ][ K ] = min { f [ i -1 ][ 1 ] + cost [ i -1 ][ K ], f [ i -1 ][ 2 ] + cost [ i -1 ][ K ], ..., f [ i -1 ][ K -1 ] + cost [ i -1 ][ K ]} We could optimize the solution upon this. Basically, we can maintain the first two minimum value of the set f[i-1][1], f[i-1][2], f[i-1][3], ..., f[i-1][K] , min1 and min2 and their index j1 and j2 . There are two cases, the first case is the house i-1 is painted with the same color correspoinding to the minimum value. in this case, we cannot chose to paint it with the color corresponding to the minimum cost, we can update the state using the second minimum. The second case is that we paint the i-1th house with color other than the color corresponding to the minimum cost to pain, we can update using the minimum value. C++ O(nk) class Solution { public : /** * @param costs n x k cost matrix * @return an integer, the minimum cost to paint all houses */ int minCostII ( vector < vector < int >>& costs ) { int m = costs . size (); if ( m == 0 ) return 0 ; int k = costs [ 0 ]. size (); if ( k == 0 ) return 0 ; int f [ m + 1 ][ k ]; int min1 ; int min2 ; int j1 = 0 , j2 = 0 ; /* init, 0 house cost nothing. */ for ( int j = 0 ; j < k ; j ++ ) f [ 0 ][ j ] = 0 ; for ( int i = 1 ; i <= m ; i ++ ) { min1 = min2 = INT_MAX ; /* from all the colors, find the min1 and min2 */ for ( int j = 0 ; j < k ; j ++ ) { /* get the min1 and min2 first */ if ( f [ i - 1 ][ j ] < min1 ) { min2 = min1 ; j2 = j1 ; min1 = f [ i - 1 ][ j ]; j1 = j ; } else if ( f [ i - 1 ][ j ] < min2 ) { min2 = f [ i - 1 ][ j ]; j2 = j ; } } /* update the states based on the min1 and min2 */ for ( int j = 0 ; j < k ; j ++ ) { if ( j != j1 ) { f [ i ][ j ] = f [ i - 1 ][ j1 ] + costs [ i - 1 ][ j ]; } else { f [ i ][ j ] = f [ i - 1 ][ j2 ] + costs [ i - 1 ][ j ]; } } } int res = INT_MAX ; for ( int j = 0 ; j < k ; j ++ ) { if ( f [ m ][ j ] < res ) res = f [ m ][ j ]; } return res ; } }; C++ O(nk^2) class Solution { public : int minCostII ( vector < vector < int >>& costs ) { int n = costs . size (); int k = n > 0 ? costs [ 0 ]. size () : 0 ; if ( n == 0 && k == 0 ) return 0 ; if ( n == 1 && k == 1 ) return costs [ 0 ][ 0 ]; vector < vector < int >> f ( n + 1 , vector < int > ( k , 0 )); for ( int i = 0 ; i < k ; i ++ ) { f [ 0 ][ i ] = 0 ; // 0 cost for 0 paint } for ( int i = 1 ; i <= n ; i ++ ) { for ( int j = 0 ; j < k ; j ++ ) { f [ i ][ j ] = INT_MAX ; for ( int c = 0 ; c < k ; c ++ ) { if ( j == c ) { continue ; } if ( f [ i ][ j ] > f [ i - 1 ][ c ] + costs [ i - 1 ][ j ]) { f [ i ][ j ] = f [ i - 1 ][ c ] + costs [ i - 1 ][ j ]; } } } } int res = INT_MAX ; for ( int i = 0 ; i < k ; i ++ ) { if ( f [ n ][ i ] < res ) res = f [ n ][ i ]; } return res ; } }; House Robber \u00b6 Start with the last step. The last house could be either robbed or not. If the last house a[i-1] is robbed, then we cannot rob a[i-2] . we have f[i] = f[i-2] + a[i-1] . If the last house a[i-1] is not robbed, then we can rob a[i-2] . Alternatively, we can skip it to rob a[i-3] . We have f[i] = f[n-1] , in which this f[n-1] could not let us know whether a[i-2] is robbed or not. We add the state of a[i-1] to the state transition equation. Thus we have: f[i][0] represent \"the maximum money for robbing the first i houses and the last house hasn't been robbed.\" f[i][1] represent \"the maximum money for robbing the first i houses and the last house has been robbed.\" state transition equations: f[i][0] = max(f[i-1][0], f[i-1][1]) f[i][1] = f[i-1][0] + a[i-1] C++ class Solution { public : int rob ( vector < int >& nums ) { int n = nums . size (); if ( n == 0 ) return 0 ; int f [ n + 1 ][ 2 ] = { 0 }; for ( int i = 1 ; i <= n ; i ++ ) { f [ i ][ 0 ] = max ( f [ i - 1 ][ 0 ], f [ i - 1 ][ 1 ]); f [ i ][ 1 ] = f [ i - 1 ][ 0 ] + nums [ i - 1 ]; } return max ( f [ n ][ 0 ], f [ n ][ 1 ]); } }; C++ optimized class Solution { public : long long houseRobber ( vector < int > A ) { int n = A . size (); if ( n == 0 ) return 0 ; long long f [ n + 1 ]; /* init */ f [ 0 ] = 0 ; f [ 1 ] = A [ 0 ]; for ( int i = 2 ; i <= n ; i ++ ) { f [ i ] = max ( f [ i - 1 ], f [ i - 2 ] + A [ i - 1 ]); } return f [ n ]; } }; Solution 2 yes - first i days, robber last day, no - first i days, not robber at last day. class Solution { public : int rob ( vector < int >& nums ) { int n = nums . size (); if ( n == 0 ) return 0 ; int yes = 0 , no = 0 ; for ( int i = 1 ; i <= n ; i ++ ) { int tmp = no ; no = max ( yes , no ); yes = tmp + nums [ i - 1 ]; } return max ( yes , no ); } }; House Robber II \u00b6 Following House Robber , when we try to analyze the last step, the house i - 1 depends on the house i - 2 and the house 0 . How could we handle this? We could enumerate two cases, and reduce the probelm to House Rober house 0 is robbed and house i-1 is not. house i - 1 is robbed and house 0 is not. class Solution { public : int rob ( vector < int >& nums ) { int n = nums . size (); if ( n == 0 ) return 0 ; if ( n == 1 ) // edge case to deal with return nums [ 0 ]; vector < int > nums1 ( nums . begin () + 1 , nums . end ()); vector < int > nums2 ( nums . begin (), nums . end () - 1 ); return max ( rob_helper ( nums1 ), rob_helper ( nums2 )); } private : int rob_helper ( vector < int >& A ) { int n = A . size (); if ( n == 0 ) return 0 ; int f [ n + 1 ] = { 0 }; f [ 0 ] = 0 ; f [ 1 ] = A [ 0 ]; for ( int i = 2 ; i <= n ; i ++ ) { f [ i ] = max ( f [ i - 1 ], f [ i - 2 ] + A [ i - 1 ]); } return f [ n ]; } }; Best Time to Buy and Sell Stock \u00b6 Force youself to do it by only one pass. You'll find that you need two variables to record the minimum value currently found and the maximum profit currently found. class Solution { public : int maxProfit ( vector < int > & prices ) { int n = prices . size (); if ( n == 0 ) return 0 ; int min_price = INT_MAX ; int max_profit = 0 ; for ( int i = 0 ; i < n ; i ++ ) { if ( prices [ i ] < min_price ) { min_price = prices [ i ]; } else if ( prices [ i ] - min_price > max_profit ) { max_profit = prices [ i ] - min_price ; } } return max_profit ; } }; Best Time to Buy and Sell Stock II \u00b6 We can just buy and sell when ever the price is increased in a day. Once you can proof the day-by-day subproblem can form the solution. The solution becomes so easy. C++ class Solution { public : int maxProfit ( vector < int > & prices ) { int res = 0 ; for ( int i = 1 ; i < prices . size (); i ++ ) { if ( prices [ i - 1 ] < prices [ i ]) res += prices [ i ] - prices [ i - 1 ]; } return res ; } }; C++ Alternative class Solution { public : int maxProfit ( vector < int > & prices ) { int res = 0 ; for ( int i = 1 ; i < prices . size (); i ++ ) { res += max ( prices [ i ] - prices [ i - 1 ], 0 ) } return res ; } }; Best Time to Buy and Sell Stock III \u00b6 We can define 5 stages and write state transition equations based on it. class Solution { public : /** * @param prices: Given an integer array * @return: Maximum profit */ int maxProfit ( vector < int > & A ) { int n = A . size (); if ( n == 0 ) { return 0 ; } int f [ n + 1 ][ 6 ]; /* init */ f [ 0 ][ 1 ] = 0 ; f [ 0 ][ 2 ] = f [ 0 ][ 3 ] = f [ 0 ][ 4 ] = f [ 0 ][ 5 ] = INT_MIN ; for ( int i = 1 ; i <= n ; i ++ ) { // stage 1, 3, 5: // f[i][j] = max{f[i - 1][j], f[i - 1][j - 1] + A[i - 1] - A[i - 2]} for ( int j = 1 ; j <= 5 ; j += 2 ) { f [ i ][ j ] = f [ i - 1 ][ j ]; if ( i > 1 && j > 1 && f [ i - 1 ][ j - 1 ] != INT_MIN ) { f [ i ][ j ] = max ( f [ i ][ j ], f [ i - 1 ][ j - 1 ] + A [ i - 1 ] - A [ i - 2 ]); } } // stage 2, 4: // f[i][j] = max{f[i - 1][j] + A[i - 1] - A[i - 2], f[i - 1][j - 1], f[i - 1][j - 2] + A[i - 1] - A[i - 2]} for ( int j = 2 ; j < 5 ; j += 2 ) { f [ i ][ j ] = f [ i - 1 ][ j - 1 ]; if ( i > 1 && f [ i - 1 ][ j ] != INT_MIN ) { f [ i ][ j ] = max ( f [ i ][ j ], f [ i - 1 ][ j ] + A [ i - 1 ] - A [ i - 2 ]); } if ( i > 1 && j > 2 && f [ i - 1 ][ j - 2 ] != INT_MIN ) { f [ i ][ j ] = max ( f [ i ][ j ], f [ i - 1 ][ j - 2 ] + A [ i - 1 ] - A [ i - 2 ]); } } } return max ( f [ n ][ 1 ], max ( f [ n ][ 3 ], f [ n ][ 5 ])); } }; Best Time to Buy and Sell Stock IV \u00b6 If k is larger then n/2 , it is equivalent to the II. This is a generalized solution from Best Time to Buy and Sell Stock III . class Solution { public : int maxProfit ( int K , vector < int > & A ) { int n = A . size (); if ( n == 0 ) { return 0 ; } int f [ n + 1 ][ 2 * K + 1 + 1 ]; /* special case need to take care of */ if ( K > ( n / 2 )) { int res = 0 ; for ( int i = 0 ; i + 1 < n ; i ++ ) { if ( A [ i + 1 ] - A [ i ] > 0 ) { res += A [ i + 1 ] - A [ i ]; } } return res ; } /* init */ f [ 0 ][ 1 ] = 0 ; for ( int k = 2 ; k <= 2 * K + 1 ; k ++ ) { f [ 0 ][ k ] = INT_MIN ; } for ( int i = 1 ; i <= n ; i ++ ) { for ( int j = 1 ; j <= 2 * K + 1 ; j += 2 ) { f [ i ][ j ] = f [ i - 1 ][ j ]; if ( i > 1 && j > 1 && f [ i - 1 ][ j - 1 ] != INT_MIN ) { f [ i ][ j ] = max ( f [ i ][ j ], f [ i - 1 ][ j - 1 ] + A [ i - 1 ] - A [ i - 2 ]); } } for ( int j = 2 ; j < 2 * K + 1 ; j += 2 ) { f [ i ][ j ] = f [ i - 1 ][ j - 1 ]; if ( i > 1 && f [ i - 1 ][ j ] != INT_MIN ) { f [ i ][ j ] = max ( f [ i ][ j ], f [ i - 1 ][ j ] + A [ i - 1 ] - A [ i - 2 ]); } if ( i > 1 && j > 2 && f [ i - 1 ][ j - 2 ] != INT_MIN ) { f [ i ][ j ] = max ( f [ i ][ j ], f [ i - 1 ][ j - 2 ] + A [ i - 1 ] - A [ i - 2 ]); } } } int res = INT_MIN ; for ( int j = 1 ; j <= 2 * K + 1 ; j += 2 ) { if ( f [ n ][ j ] > res ) res = f [ n ][ j ]; } return res ; } }; Longest Increasing Subsequence \u00b6 Comparing to the problem Longest Increasing Continuous Subsequence . If the subsequence is not continuous. we have to enumerate each of the previous element before A[j] . Solution 2 DP with binary search O(nlogn) To reduce the complexity, we can try to find if there is any redundant work we have been done. or some how we could use some order information to avoid some of the calculation. Focus on the real meaning of longest Increasing Subsequence. In fact, you are looking for the smallest value before A[i] that leads to the longest Increasing Subsequence so far. use the state f[i] to record the LIS of the array A[0], ... A[i -1] . If we are at f[j], j > i , we are looking for the largest f[i] value that have the smallest A[i] . Solution 3 DP with binary search refactored In observing the fact that we can use extra space to keep the \"minimum elements see so far from nums that is the last element of LIS for the different length of such LISes\". Different from the regular DP solution, our extra space b is storing element from nums, and the element stored in b are not necessary in order. The index i of elements in b related to the length of a LIS whose last element is a[i] . specifically, i + 1 = length(LIS) . Solution 4 C++ using lower_bound We can use the lower_bound to replace the binary search routine in the above solution. C++ DP O(n^2) class Solution { public : int lengthOfLIS ( vector < int >& nums ) { int n = nums . size (); if ( n == 0 ) return 0 ; int res = 0 ; int f [ n ] = { 0 }; for ( int j = 0 ; j < n ; j ++ ) { /* case 1: a[j] is the subsequence */ f [ j ] = 1 ; /* case 2: LIS from a[0],...a[i] plus a[j] */ for ( int i = 0 ; i < j ; i ++ ) { if ( nums [ i ] < nums [ j ] && f [ i ] + 1 > f [ j ]) f [ j ] = f [ i ] + 1 ; } res = max ( res , f [ j ]); } return res ; } }; C++ DP with binary search O(nlogn) class Solution { public : int longestIncreasingSubsequence ( vector < int > nums ) { int n = nums . size (); if ( n == 0 ) return 0 ; //int f[n]; // b[i]: when f value is i, smallest a value (ending value) int b [ n + 1 ]; int top = 0 ; b [ 0 ] = INT_MIN ; // O(n) for ( int i = 0 ; i < n ; i ++ ) { // b[0] ~ b[top] // last value b[j] which is smaller than A[i] int start = 0 , end = top ; int mid ; int j ; // O(lgn) while ( start <= end ) { mid = start + ( end - start ) / 2 ; if ( b [ mid ] < nums [ i ]) { j = mid ; start = mid + 1 ; } else { end = mid - 1 ; } } // b[i]: length is j (f value is j), smallest ending value. // A[i] is after it. // f[i] = j + 1 // b[j + 1]: length is (j + 1), smallest ending value. // A[i] // B[j + 1] >= A[i] b [ j + 1 ] = nums [ i ]; if ( j + 1 > top ) { top = j + 1 ; } } // b[top] stores the smallest ending value for an LIS return top ; } }; DP with binary search refactored class Solution { public : int lengthOfLIS ( vector < int >& nums ) { vector < int > b ; for ( int i = 0 ; i < nums . size (); ++ i ) { int begin = 0 , end = b . size (); while ( begin != end ) { int mid = begin + ( end - begin ) / 2 ; if ( b [ mid ] < nums [ i ]) { begin = mid + 1 ; } else { end = mid ; } } if ( begin == b . size ()) // nums[i] greater than all element in b b . push_back ( nums [ i ]); else // begin point to next element no less than the target nums[i]. b [ begin ] = nums [ i ]; } return b . size (); } }; C++ using lower_bound class Solution { public : int lengthOfLIS ( vector < int >& nums ) { vector < int > b ; for ( auto a : nums ) { auto it = lower_bound ( b . begin (), b . end (), a ); if ( it == b . end ()) b . push_back ( a ); else * it = a ; } return b . size (); } }; Russian Doll Envelopes \u00b6 Sort the vector in ascending order and identify the subproblem, apply DP. notice the sort is ascending, not descending. For the O(nlogn) solution, think of how to optimize the longest increasing subsequence for one dimension when the envelopes are sorted properly. We use b[k] to record the smallest A[i] value that have length of k longest increasing subsequence. The reason behind this can be illustrated by the following example, when i = 1 , we can forget A[0] = 5 , because what ever LIS A[0] = 5 can contribute to the LIS, A[1] = 2 will be able to contribute. A[i] 5, 2, 3, 1, 4 f[i] 1, 1, 2, 2, 3 b[k] 5 (|k| = 1, i = 0) b[k] 2 (|k| = 1, i = 1) b[k] 2, 3 (|k| = 2, i = 2) b[k] 2, 1 (|k| = 2, i = 3) b[k] 2, 1, 4 (|k| = 2, i = 4) In iterating of A[i] we binary search to find the smaller value than A[i] in b[k] so far that this smaller value combined with A[i] will form a new LIS. To keep the loop invariant, we need to change b[k] by either modify the existing value in b[k] or add a new value to the end. C++ DP O(n^2) class Solution { public : int maxEnvelopes ( vector < vector < int >>& envelopes ) { int n = envelopes . size (); int f [ n ]; int res = 0 ; sort ( envelopes . begin (), envelopes . end (), []( vector < int >& a , vector < int >& b ){ if ( a [ 0 ] == b [ 0 ]) { return a [ 1 ] < b [ 1 ]; } else { return a [ 0 ] < b [ 0 ]; } }); for ( int i = 0 ; i < n ; ++ i ) { f [ i ] = 1 ; for ( int j = 0 ; j < i ; ++ j ) { if ( envelopes [ j ][ 0 ] < envelopes [ i ][ 0 ] && envelopes [ j ][ 0 ] < envelopes [ i ][ 0 ]) { f [ i ] = max ( f [ i ], f [ j ] + 1 ); } } res = max ( res , f [ i ]); } return res ; } }; C++ DP (nlogn) class Solution { public : int maxEnvelopes ( vector < pair < int , int >>& envelopes ) { int n = envelopes . size (); if ( n == 0 ) return 0 ; vector < int > f ; sort ( envelopes . begin (), envelopes . end (), []( const pair < int , int > a , const pair < int , int > b ) { if ( a . first == b . first ) { return a . second > b . second ; } else { return a . first < b . first ; } }); for ( int i = 0 ; i < n ; ++ i ) { int t = envelopes [ i ]. second ; int begin = 0 , end = f . size (); // f start with empty // when i = 0, the binary search will not happen, first value will // be added to f before binary search on it. while ( begin < end ) { int mid = begin + ( end - begin ) / 2 ; if ( f [ mid ] < t ) { begin = mid + 1 ; } else { end = mid ; } } if ( begin == f . size ()) { f . push_back ( t ); } else { f [ begin ] = t ; // index to f is related to the result } } return f . size (); } }; Lecture 4 \u00b6 Problem category Perfect Squares \u5212\u5206\u578b\uff0f\u5e8f\u5217\u578b Palindrome Partitioning II \u5212\u5206\u578b Copy Books \u5212\u5206\u578b Coins in A Line \u535a\u5f08\u578b Backpack \u80cc\u5305\u578b Backpack V \u80cc\u5305\u578b Backpack VI \u80cc\u5305\u578b Perfect Squares \u00b6 f[i] = \\min_{1 \\le j^2 \\le i}(f[i - j^2] + 1) f[i] = \\min_{1 \\le j^2 \\le i}(f[i - j^2] + 1) \u6574\u4f53\u601d\u60f3\u5c31\u662f\u679a\u4e3e j . \u672c\u9898\u5206\u6790\u65f6\u50cf\u5212\u5206\u578b\u4f46\u7a0b\u5e8f\u5199\u8d77\u6765\u50cf\u5e8f\u5217\u578b\u3002\u8fd8\u662f\u4ece\u6700\u540e\u4e00\u6b65\u5165\u624b\u3002\u8bbe\u6700\u540e\u4e00\u4e2a\u662f j^2 j^2 . \u6574\u4f53\u601d\u60f3\u5c31\u662f\u679a\u4e3e\u6700\u540e\u4e00\u6b65 j . \u6ce8\u610f\u521d\u59cb\u5316 f[i] . j*j \u53ef\u4ee5\u662f i , \u8fd9\u4e2a\u6b63\u597d\u5bf9\u5e94\u4e86\u6781\u7aef\u60c5\u51b5 \u5c31\u662f i \u53ea\u7531 j*j \u7ec4\u6210. if condition \u65e0\u9700\u68c0\u67e5 f[i - j*j] \u662f\u5426\u4e3a INT_MAX . \u56e0\u4e3a\u8fd9\u91cc\u6bd4 i \u5c0f\u7684 i - j*j \u603b\u662f\u53ef\u4ee5\u7531\u5b8c\u5168\u5e73\u65b9\u65701\u7ec4\u6210\u3002\u76f8\u6bd4\u8f83\u9898\u76ee Coin Change \u8fd9\u9053\u9898\uff0c\u8fd9\u4e00\u6b65\u53ef\u80fd\u6027\u68c0\u67e5\u5fc5\u4e0d\u53ef\u5c11\uff0c\u56e0\u4e3a\u5728 Coin Change \u4e2d\u67d0\u4e00\u9762\u503c\u4e0d\u4e00\u5b9a\u80fd\u88ab\u5151\u6362\u6210\u7ed9\u5b9a\u9762\u503c\u7684\u786c\u5e01\u3002\u6211\u4eec\u7528\u65e0\u7a77\u5927\u6765\u6807\u8bb0\u3002 C++ DP class Solution { public : int numSquares ( int n ) { if ( n == 0 ) return 0 ; int f [ n + 1 ] = { 0 }; f [ 0 ] = 0 ; for ( int i = 1 ; i <= n ; i ++ ) { f [ i ] = INT_MAX ; for ( int j = 1 ; j * j <= i ; j ++ ) { if ( f [ i - j * j ] + 1 < f [ i ]) { f [ i ] = f [ i - j * j ] + 1 ; } } } return f [ n ]; } }; C++ Python class Solution : def numSquares ( self , n : int ) -> int : if n == 0 : return 0 f = [ 0 ] * ( n + 1 ) for i in range ( 1 , n + 1 ): f [ i ] = float ( 'inf' ) for j in range ( 1 , int ( sqrt ( i )) + 1 ): if f [ i ] > f [ i - j * j ] + 1 : f [ i ] = f [ i - j * j ] + 1 return f [ n ] Palindrome Partitioning II \u00b6 Consider the last palindrome s[j, ..., i - 1] , f[i] represent the minimum number of partitions of first i characters. State transition equation: f[i] = \\min_{0 \\le j \\lt i}(f[j]+1 | (s[j],..., s[i-1]) \\text{ is palindrome}) f[i] = \\min_{0 \\le j \\lt i}(f[j]+1 | (s[j],..., s[i-1]) \\text{ is palindrome}) To know the substr s[j, ... i-1] is a palindrome or not, we can first find and record all the panlidrome substring in isPalin[i][j] , each element of which represents whether substr s[i, ..., j] is a palindrome or not. C++ DP with isPalin() class Solution { public : int minCut ( string s ) { int n = s . length (); if ( n == 0 ) return 0 ; int isPalin [ n ][ n ]; int f [ n + 1 ]; f [ 0 ] = 0 ; for ( int i = 0 ; i < n ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { isPalin [ i ][ j ] = 0 ; } } /* calculate palindrome */ for ( int t = 0 ; t < n ; t ++ ) { /* odd case */ int i = t ; int j = t ; while ( i >= 0 && j < n && s [ i ] == s [ j ]) { isPalin [ i ][ j ] = 1 ; i -- ; j ++ ; } /* even case */ i = t ; j = t + 1 ; while ( i >= 0 && j < n && s [ i ] == s [ j ]) { isPalin [ i ][ j ] = 1 ; i -- ; j ++ ; } } /* calculate the states */ for ( int i = 1 ; i <= n ; i ++ ) { f [ i ] = INT_MAX ; for ( int j = 0 ; j < i ; j ++ ) { if ( isPalin [ j ][ i - 1 ]) { f [ i ] = min ( f [ i ], f [ j ] + 1 ); } } } return f [ n ] - 1 ; } }; Java isPalin() public class Solution { /** * @param s a string * @ f[i], minmum cut of s[0],...s[i - 1] * e.g. * a a b a a * j=0 1 2 3 4 *i=0 a 1 1 0 * 1 a 1 0 * 2 b 1 0 * 3 a 1 0 * 4 a 1 * * when calculate isPalin[i][j], we need to know isPalin[i + 1][j + 1]. * Cannot calculate from i = 0, 1, ... have to go from i = n - 1, n - 2, ... 1, 0 * last step: s[j],...s[i - 1] * f[i] = f[j] + 1 | s[j], ... s[i - 1] is palindrome */ public int minCut ( String ss ) { char [] s = ss . toCharArray (); int n = s . length ; if ( n == 0 ) return 0 ; boolean [][] isPalin = new boolean [ n ][ n ] ; int [] f = new int [ n + 1 ] ; f [ 0 ] = - 1 ; for ( int i = 0 ; i < n ; i ++ ) { isPalin [ i ][ i ] = true ; if ( i + 1 < n ) { isPalin [ i ][ i + 1 ] = s [ i ] == s [ i + 1 ] ; } } for ( int i = n - 3 ; i >= 0 ; i -- ) { for ( int j = i + 2 ; j < n ; j ++ ) { isPalin [ i ][ j ] = isPalin [ i + 1 ][ j - 1 ] && s [ i ] == s [ j ] ; } } /* calculate the states */ for ( int i = 1 ; i <= n ; i ++ ) { f [ i ] = i - 1 ; for ( int j = 0 ; j < i ; j ++ ) { if ( isPalin [ j ][ i - 1 ] ) { f [ i ] = Math . min ( f [ i ] , f [ j ] + 1 ); } } } return f [ n ] ; } }; Warning \u8fd9\u4e2a\u65b9\u6cd5C++\u4e0d\u80fd\u901a\u8fc7Leetcode\u548clintcode\uff0c\u4e3b\u8981\u662f\u5bf9\u4e8e\u5f88\u957f\u7684string\uff0c isPalin \u4f1a\u5f88\u5927\u3002\u7c7b\u4f3c\u4e8e Best Time to Buy and Sell Stock IV \u4e2d\u5982\u679ck\u5f88\u5927\u7684\u65f6\u5019\u3002Java\u7248\u672c\u5219\u53ef\u4ee5\u3002 Note \u6280\u5de7\uff1a\u5728\u5faa\u73af\u4e2d\u9700\u8981\u67d0\u4e2a\u6027\u8d28\uff0c\u800c\u8fd9\u4e2a\u6027\u8d28\u53c8\u662f\u53ef\u4ee5\u5f88\u5bb9\u6613\u8ba1\u7b97\u548c\u8bb0\u5f55 (isPalin) \u6211\u4eec\u5c31\u91c7\u7528\u5148\u8ba1\u7b97\u6240\u6709\u5e76\u8bb0\u5f55\uff0c\u5728loop\u4e2d\u53bb\u76f4\u63a5\u8bbf\u95ee\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c\u5373\u53ef\u3002 Copy Books \u00b6 Coins in A Line \u00b6 \u8981\u6709\u5148\u624b\u201c\u5fc5\u80dc\u201d\u548c\u201c\u5fc5\u8d25\u201d\u7684\u6982\u5ff5\u3002\u7b80\u5355\u6765\u8bb2\u5c31\u662f\u5f53\u524d\u5148\u624b\u9762\u5bf9\u5f53\u524d\u5c40\u52bf\u5982\u679c\u6709\u4e00\u62db\u80fd\u4f7f\u4e0b\u8f6e\u5148\u624b\u5fc5\u8d25 \u90a3\u4e48\u5f53\u524d\u9009\u624b\u5c31\u5fc5\u80dc\u3002\u5982\u679c\u5f53\u524d\u9009\u624b\u9762\u5bf9\u5f53\u524d\u5c40\u52bf\u4e0b\u6240\u6709\u62db\u6570\u90fd\u4f1a\u4f7f\u5f97\u4e0b\u8f6e\u5148\u624b\u5fc5\u80dc\uff0c\u90a3\u4e48\u5f53\u524d\u9009\u624b\u5fc5\u8d25\u3002 f[i] \u8868\u793a\u5f53\u524d\u9009\u624b\u5fc5\u80dc(True)\u6216\u5fc5\u8d25(False). State transition equation: f[i] = f[i - 1] == \\text{false}\\ OR\\ f[i - 2] == \\text{false} f[i] = f[i - 1] == \\text{false}\\ OR\\ f[i - 2] == \\text{false} \u6b64\u9898\u6ce8\u610f\u521d\u59cb\u6761\u4ef6\u548c\u5fc5\u80dc\u548c\u5fc5\u8d25\u7684\u6982\u5ff5\u5c31\u4e0d\u4f1a\u6709\u4ec0\u4e48\u5dee\u9519\u3002 DB solution class Solution { public : bool firstWillWin ( int n ) { if ( n == 0 ) return false ; bool f [ n ]; f [ 0 ] = false ; for ( int i = 1 ; i <= 2 ; i ++ ) { f [ i ] = true ; } for ( int i = 3 ; i <= n ; i ++ ) { f [ i ] = ( f [ i - 1 ] == false ) || ( f [ i - 2 ] == false ); } return f [ n ]; } }; Math solution // You can actualy found the pattern if you have written several states. f[i] = i % 3 // 0 1 2 3 4 5 6 7 8 9 10 // F T T F T T F T T F T class Solution { public : bool firstWillWin ( int n ) { return n % 3 ; } }; Backpack \u00b6 Give n items with size A[i] and backpack size: M . Find the max total size that can fit in the backpack. We can reduce the size of f to two rolls. We can even reduce it to just one dimention. same technique can also be used in Backpack II and Backpack V. C++ DP class Solution { public : int backPack ( int m , vector < int > A ) { int n = A . size (); if ( n == 0 ) return 0 ; bool f [ n + 1 ][ m + 1 ]; /* init */ f [ 0 ][ 0 ] = true ; for ( int j = 1 ; j <= m ; j ++ ) { f [ 0 ][ j ] = false ; } for ( int i = 1 ; i <= n ; i ++ ) { for ( int j = 0 ; j <= m ; j ++ ) { f [ i ][ j ] = f [ i - 1 ][ j ]; /* must first init f[i][j] */ if ( j >= A [ i - 1 ]) { f [ i ][ j ] = f [ i ][ j ] || f [ i - 1 ][ j - A [ i - 1 ]]; //f[i][j] = f[i - 1][j] || f[i - 1][j - A[i - 1]]; /* this is wrong */ } } } for ( int j = m ; j >= 0 ; j -- ) { if ( f [ n ][ j ] == true ) return j ; } return 0 ; } }; C++ DP log(n) space class Solution { public : int backPack ( int m , vector < int > A ) { int n = A . size (); if ( n == 0 ) return 0 ; bool f [ m + 1 ]; /* init */ f [ 0 ] = true ; for ( int j = 1 ; j <= m ; j ++ ) { f [ j ] = false ; } for ( int i = 1 ; i <= n ; i ++ ) { for ( int j = m ; j >= 0 ; j -- ) { if ( j >= A [ i - 1 ]) { f [ j ] = f [ j ] || f [ j - A [ i - 1 ]]; } } } for ( int j = m ; j >= 0 ; j -- ) { if ( f [ j ] == true ) return j ; } return 0 ; } }; Backpack V \u00b6 This problem could follow the analysis from Backpack. Instead of memorize the boolean value whether first i items can fill the weight w . We record in f[i][w] the total number of possible fills of the weight w by first i items. We need to initialize the state f[i][j] first then to update it. C++ DP class Solution { public : int backPackV ( vector < int >& nums , int T ) { int n = nums . size (); if ( n == 0 ) { return 0 ; } int f [ n + 1 ][ T + 1 ]; f [ 0 ][ 0 ] = 1 ; for ( int j = 1 ; j <= T ; j ++ ) { f [ 0 ][ j ] = 0 ; } for ( int i = 1 ; i <= n ; i ++ ) { for ( int j = 0 ; j <= T ; j ++ ) { f [ i ][ j ] = f [ i - 1 ][ j ]; if ( j >= nums [ i - 1 ]) { f [ i ][ j ] += f [ i - 1 ][ j - nums [ i - 1 ]]; } } } return f [ n ][ T ]; } }; C++ DP log(n) space class Solution { public : int backPackV ( vector < int >& nums , int T ) { int n = nums . size (); if ( n == 0 ) { return 0 ; } int f [ T + 1 ]; f [ 0 ] = 1 ; for ( int j = 1 ; j <= T ; j ++ ) { f [ j ] = 0 ; } for ( int i = 1 ; i <= n ; i ++ ) { for ( int j = T ; j >= 0 ; j -- ) { //f[j] = f[j - A[i - 1]] ==> f'[j] if ( j >= nums [ i - 1 ]) { // f'[j] // cover old f[j] f [ j ] += f [ j - nums [ i - 1 ]]; } } } return f [ T ]; } }; Backpack VI \u00b6 \u8fd9\u9053\u9898\u7b49\u540c\u4e8eLeetcode\u91cc Combinations Sum IV \u8fd9\u91cc\u53ef\u4ee5\u968f\u4fbf\u53d6\uff0c\u4f3c\u4e4e\u9898\u76ee\u53d8\u5f97\u65e0\u6cd5\u4e0b\u624b\uff0c\u8003\u8651\u201c\u6700\u540e\u4e00\u6b65\u201d\u8fd9\u4e2a\u6280\u5de7\u4e0d\u80fd\u7528\u4e86\uff0c\u56e0\u4e3a\u6700\u540e\u4e00\u6b65\u53ef\u4ee5\u662f\u4efb\u610f\u4e00\u4e2a\u4e86\u3002 \u4f46\u4ecd\u7136\u53ef\u4ee5\u7528\u5b50\u95ee\u9898\u6765\u8003\u8651\u3002\u5148\u4e0d\u7ba1\u6700\u540e\u4e00\u6b65\u662f\u54ea\u4e00\u4e2a\uff0c\u6700\u540e\u4e00\u6b65\u4e4b\u524d\u7684\u76f8\u52a0\u7684\u603b\u548c\u4e00\u5b9a\u662f Target - x . \u8fd9\u6837\u5c31\u8f6c\u5316\u6210\u4e00\u4e2a\u5b50\u95ee\u9898\u53ef\u4ee5\u7528DP\u6765\u505a\u3002 \u5177\u4f53\u505a\u6cd5\u6211\u4eec\u53ef\u4ee5\u5bf9\u4e8e\u6bcf\u4e00\u4e2a\u5c0f\u4e8e\u201c\u603b\u627f\u91cd\u201d\u7684\u91cd\u91cf\u8fdb\u884c\u679a\u4e3e\u6700\u540e\u4e00\u6b65 x . \u53ef\u80fd\u7684 x \u662f A[0], ..., A[i - 1] \u4e2d\u4efb\u610f\u4e00\u4e2a. Must initialize f[i] = 0 . Because some of the state won't be updated (indicate they are not possible to be fill). C++ DP class Solution { public : int backPackVI ( vector < int >& nums , int T ) { int n = nums . size (); if ( n == 0 ) { return 0 ; } int f [ T + 1 ]; f [ 0 ] = 1 ; /* for each sub problem */ for ( int i = 1 ; i <= T ; i ++ ) { f [ i ] = 0 ; /* enumerate the last step */ for ( int j = 0 ; j < n ; j ++ ) { if ( i >= nums [ j ]) { f [ i ] += f [ i - nums [ j ]]; } } } return f [ T ]; } }; Output result // f[i]: \u5b58\u591a\u5c11\u79cd\u65b9\u5f0f // pi[i]: \u5982\u679c f[i] >= 1, \u6700\u540e\u4e00\u4e2a\u6570\u5b57\u53ef\u4ee5\u662fpi[i] class Solution { public : int backPackVI ( vector < int >& nums , int T ) { int n = nums . size (); if ( n == 0 ) { return 0 ; } int f [ T + 1 ]; /* pi[i]: \u5982\u679ci\u53ef\u62fc\u51fa(f[i] >= 1), \u6700\u540e\u4e00\u4e2a\u662fpi[i] */ int pi [ T + 1 ]; f [ 0 ] = 1 ; for ( int i = 1 ; i <= T ; i ++ ) { f [ i ] = 0 ; for ( int j = 0 ; j < n ; j ++ ) { if ( i >= nums [ j ]) { f [ i ] += f [ i - nums [ j ]]; /* \u6700\u540e\u4e00\u4e2a\u662fnums[j]\u7684\u53ef\u62fc\u51fai */ if ( f [ i - nums [ j ]] > 0 ) { /* \u7eaa\u5f55\u4e0b\u6765 */ pi [ i ] = nums [ j ]; } } } } if ( f [ T ] > 0 ) { int i = T ; cout << i << \"=\" << endl ; while ( i != 0 ) { // sum is i now; // last number is pi[i] // previuos sum is i - pi[i] cout << pi [ i ] << endl ; i -= pi [ i ]; } } return f [ T ]; } }; Summary Dynamic Programming Types \u00b6 \u5212\u5206\u578b\u52a8\u6001\u89c4\u5212\u7279\u70b9 \u00b6 \u7ed9\u5b9a\u957f\u5ea6\u4e3aN\u7684\u5e8f\u5217\u6216\u5b57\u7b26\u4e32\uff0c\u8981\u6c42\u5212\u5206\u6210\u82e5\u5e72\u6bb5 \u6bb5\u6570\u4e0d\u9650\uff0c\u6216\u6307\u5b9aK\u6bb5 \u6bcf\u4e00\u6bb5\u6ee1\u8db3\u4e00\u5b9a\u7684\u6027\u8d28 \u7c7b\u4f3c\u4e8e\u5e8f\u5217\u578b\u52a8\u6001\u89c4\u5212\uff0c\u4f46\u662f\u901a\u5e38\u8981\u52a0\u4e0a\u6bb5\u6570\u4fe1\u606f \u7c7b\u4f3c\u4e8e\u5e8f\u5217\u578b\u52a8\u6001\u89c4\u5212\uff0c\u4f46\u662f\u901a\u5e38\u8981\u52a0\u4e0a\u6bb5\u6570\u4fe1\u606f \u4e00\u822c\u7528 f[i][j] \u8bb0\u5f55\u524di\u4e2a\u5143\u7d20(\u5143\u7d200~i-1)\u5206\u6210j\u6bb5\u7684\u6027\u8d28\uff0c\u5982\u6700\u5c0f\u4ee3\u4ef7 \u628a\u72b6\u6001 f[i] \u6216 f[i][j] \u521d\u59cb\u5316\u4e3a\u6781\u5927\u503c\u6216\u6781\u5c0f\u503c\u3002\u4e00\u65b9\u9762\uff0c\u8fd9\u6837\u6709\u5229\u4e8e\u5728\u66f4\u65b0\u72b6\u6001\u65f6\u8ba1\u7b97\u6700 \u5927\u6216\u6700\u5c0f\u503c\uff1b\u53e6\u4e00\u65b9\u9762\uff0c\u5728\u66f4\u65b0\u72b6\u6001\u662f\u53ef\u4ee5\u901a\u8fc7\u67e5\u770b\u5b50\u72b6\u6001\u662f\u5426\u4e3a\u6781\u503c\u6765\u5224\u65ad\u5f53\u524d\u4e00\u6b65\u66f4\u65b0\u662f\u5426\u6709\u610f\u4e49\u3002 \u5178\u578b\u9898\u76ee\uff1a1. Perfect Squares, \u5168\u90e8\u72b6\u6001\u90fd\u6709\u610f\u4e49\uff0c\u53ef\u7701\u7565\u68c0\u67e5\u30022. Coin Change \u6709\u4e9b \u6570\u989d\u662f\u65e0\u6cd5\u7528\u7ed9\u5b9a\u9762\u503c\u7684\u786c\u5e01\u5151\u6362\uff0c\u66f4\u65b0\u524d\u9700\u68c0\u67e5\u72b6\u6001\u662f\u5426\u4e3a\u6781\u81f4\u4ee5\u5224\u65ad\u5f53\u524d\u66f4\u65b0\u662f\u5426\u6709\u610f\u4e49\u3002 \u5212\u5206\u6027\u52a8\u6001\u89c4\u5212\u603b\u7ed3 \u8981\u6c42\u5c06\u4e00\u4e2a\u5e8f\u5217\u6216\u5b57\u7b26\u4e32\u5212\u5206\u6210\u82e5\u5e72\u6ee1\u8db3\u8981\u6c42\u7684\u7247\u6bb5 \u89e3\u51b3\u65b9\u6cd5\uff1a\u6700\u540e\u4e00\u6b65/\u6700\u540e\u4e00\u6bb5 \u679a\u4e3e\u6700\u540e\u4e00\u6bb5\u7684\u8d77\u70b9 \u5982\u679c\u9898\u76ee\u4e0d\u6307\u5b9a\u6bb5\u6570\uff0c\u7528 f[i] \u8868\u793a\u524d i \u4e2a\u5143\u7d20\u5206\u6bb5\u540e\u7684\u53ef\u884c\u6027/\u6700\u503c\uff0c\u53ef\u884c\u6027\uff0c\u65b9\u5f0f\u6570\uff1a Perfect Squares, Palindrome Partition II \u5982\u679c\u9898\u76ee\u6307\u5b9a\u6bb5\u6570\uff0c\u7528 f[i][j]\u8868\u793a\u524d i \u4e2a\u5143\u7d20\u5206\u6210 j \u6bb5\u540e\u7684\u53ef\u884c\u6027/\u6700\u503c\uff0c\u53ef\u884c\u6027\uff0c\u65b9\u5f0f\u6570\uff1aCopy Books \u80cc\u5305\u578b\u52a8\u6001\u89c4\u5212\u7c7b\u578b \u5e03\u5c14\u503c \u8ba1\u6570 \u6700\u503c \u5355\u4e00\u7269\u54c1 \u65e0\u9650\u591a\u7269\u54c1 \u80cc\u5305\u578b\u52a8\u6001\u89c4\u5212\u7a7a\u95f4\u4f18\u5316 \u6eda\u52a8\u6570\u7ec4 \u5355\u4e2a\u6570\u7ec4 \u80cc\u5305\u578b\u52a8\u6001\u89c4\u5212\u603b\u7ed3 \u80cc\u5305\u95ee\u9898\u7684\u6570\u7ec4\u5927\u5c0f\u4e0e\u603b\u627f\u91cd\u6709\u5173 Backpack \u53ef\u884c\u6027\u80cc\u5305 \u8981\u6c42\u4e0d\u8d85\u8fc7Target\u65f6\u80fd\u62fc\u51fa\u7684\u6700\u5927\u91cd\u91cf \u8bb0\u5f55\u524di\u4e2a\u7269\u54c1\u80fd\u4e0d\u80fd\u62fc\u51fa\u91cd\u91cfw Backpack V, Backpack VI, \u8ba1\u6570\u578b\u80cc\u5305 \u8981\u6c42\u6709\u591a\u5c11\u79cd\u65b9\u5f0f\u62fc\u51fa\u91cd\u91cfTarget Backpack V\uff1a\u8bb0\u5f55\u524di\u4e2a\u7269\u54c1\u6709\u591a\u5c11\u79cd\u65b9\u5f0f\u62fc\u51fa\u91cd\u91cfw Backpack VI\uff1a\u8bb0\u5f55\u6709\u591a\u5c11\u79cd\u65b9\u5f0f(\u53ef\u91cd\u590d)\u62fc\u51fa\u91cd\u91cfw \u5173\u952e\u70b9 \u6700\u540e\u4e00\u6b65 \u6700\u540e\u4e00\u4e2a\u80cc\u5305\u5185\u7684\u7269\u54c1\u662f\u54ea\u4e2a \u6700\u540e\u4e00\u4e2a\u7269\u54c1\u6709\u6ca1\u6709\u8fdb\u80cc\u5305 Note \u6ce8\u610f\u5728loop\u4e2d\u8981\u5148\u521d\u59cb\u5316 f[i][j] , \u7136\u540e\u518d\u53bb\u66f4\u65b0\uff0c\u5426\u8005\u5982\u679cif\u8bed\u53e5\u5224\u65adfalse\u7684\u65f6\u5019\u662f\u6ca1\u6cd5\u66f4\u65b0 f[i][j] \u7684\u3002\u4f8b\u5982\u9898\u76ee\uff1aBackpack \u548c Backpack V Lecture 5 \u00b6 Problem Category Backpack II \u80cc\u5305\u578b Backpack III \u80cc\u5305\u578b Coins in A Line III \u535a\u5f08\u578b Longest Palindromic Subsequence \u533a\u95f4\u578b Burst Balloons \u533a\u95f4\u578b Scramble String \u533a\u95f4\u578b Backpack II \u00b6 Given n items with size A[i] and value V[i] , and a backpack with size m . What's the maximum value can you put into the backpack? Example Given 4 items with size [2, 3, 5, 7] and value [1, 5, 2, 4] , and a backpack with size 10. The maximum value is 9. \u601d\u8003\u65b9\u5f0f\u4efb\u7136\u662f\u4ece\u6700\u540e\u4e00\u4e2a\u7269\u54c1\u9009\u8fd8\u662f\u4e0d\u9009\uff0c\u53ea\u662f\u6211\u4eec\u73b0\u5728\u8003\u8651\u7684\u662f\u4ef7\u503c\u3002\u6b64\u65f6\u72b6\u6001\u5c31\u4e0d\u80fd\u662f\u53ef\u884c\u6027 Backpack \u6216\u8005\u591a\u5c11\u79cd\u4e86 Backpack V , \u6211\u4eec\u8981\u7eaa\u5f55\u603b\u4ef7\u503c\u3002 State: f[i][w] represent the value of the first i items that weight w . State transition equation: f[i][w] = max(f[i - 1][w],\\ f[i - 1][w - A[i - 1]] + V[i - 1] | w \u2265 A[i-1] \\text{\u4e14}\\ f[i-1][w-A[i-1]] \\neq -1) f[i][w] = max(f[i - 1][w],\\ f[i - 1][w - A[i - 1]] + V[i - 1] | w \u2265 A[i-1] \\text{\u4e14}\\ f[i-1][w-A[i-1]] \\neq -1) Initialization: f[0][0] = 0 , f[0][1] = -1, ... f[0][w] = -1 . -1 \u4ee3\u8868\u4e0d\u80fd\u88ab\u62fc\u51fa\u3002 C++ DP O(n^2) space class Solution { public : int backPackII ( vector < int > A , vector < int > V , int m ) { int n = A . szie (); if ( n == 0 ) return 0 ; int f [ n + 1 ][ m + 1 ]; f [ 0 ][ 0 ] = 0 ; for ( int j = 1 ; j < m ; ++ j ) { f [ 0 ][ j ] = -1 ; } for ( int i = 1 ; i <= n ; ++ i ) { for ( int j = 1 ; j <= m ; ++ j ) { f [ i ][ j ] = f [ i - 1 ][ j ]; if ( f [ i - 1 ][ j - A [ i - 1 ]] != -1 && j >= A [ i - 1 ]) { f [ i ][ j ] = max ( f [ i ][ j ], f [ i - 1 ][ j - A [ i - 1 ]] + V [ i - 1 ]); } } } int res = 0 ; for ( int j = 0 ; j <= m ; ++ j ) { if ( f [ n ][ j ] != -1 && f [ n ][ j ] > res ) { res = f [ n ][ j ]; } } return res ; } }; C++ DP O(n) space class Solution { public : int backPackII ( vector < int > A , vector < int > V , int m ) { int n = A . szie (); if ( n == 0 ) return 0 ; int f [ m + 1 ]; f [ 0 ] = 0 ; for ( int j = 1 ; j < m ; ++ j ) { f [ 0 ][ j ] = -1 ; } for ( int i = 1 ; i <= n ; ++ i ) { for ( int j = m ; j >= 0 ; -- j ) { if ( f [ j - A [ i - 1 ]] != -1 && j >= A [ i - 1 ]) { f [ j ] = max ( f [ j ], f [ j - A [ i - 1 ]] + V [ i - 1 ]); } } } int res = 0 ; for ( int j = 0 ; j <= m ; ++ j ) { if ( f [ j ] != -1 && f [ j ] > res ) { res = f [ j ]; } } return res ; } }; Backpack III \u00b6 Given n kind of items with size A[i] and value V[i] ( each item has an infinite number available) and a backpack with size m . What's the maximum value can you put into the backpack? Example Given 4 items with size [2, 3, 5, 7] and value [1, 5, 2, 4] , and a backpack with size 10. The maximum value is 15. We can follow the \"last step\" principle, assume the last item in and not in the backpack. But when we do this, we notice that the last one item could also be in the previous because we have infinite times of the item available. We cannot proceed. Instead of thinking the \"last one\" in the final answer, we can think the \"last one\" that has been selected. This paradigm shift enables use to tackle the problem more cleverly. We can enumerate the last type (not the last one in the final result) of items we can select. State: f[i][w] : \u524d i \u79cd \u7269\u54c1\u80fd\u591f\u62fc\u6210\u91cd\u91cf\u4e3a w \u7684\u6700\u5927\u4ef7\u503c Transition equation: f[i][w] = \\max_{k \\ge 0}{f[i-1[w], f[i - 1][j - k A[i -1]] + k V[i-1]} f[i][w] = \\max_{k \\ge 0}{f[i-1[w], f[i - 1][j - k A[i -1]] + k V[i-1]} class Solution { public : int backPackIII ( vector < int >& A , vector < int >& V , int m ) { int n = A . size (); if ( n == 0 ) { return 0 ; } int f [ m + 1 ]; /* init */ f [ 0 ] = 0 ; for ( int j = 1 ; j <= m ; j ++ ) { f [ j ] = -1 ; } for ( int i = 1 ; i <= n ; i ++ ) { for ( int j = 0 ; j <= m ; j ++ ) { if ( f [ j - A [ i - 1 ]] != -1 && j >= A [ i - 1 ]) { f [ j ] = max ( f [ j ], f [ j - A [ i - 1 ]] + V [ i - 1 ]); } } } int res = 0 ; for ( int j = 0 ; j <= m ; j ++ ) { if ( f [ j ] != -1 && f [ j ] > res ) { res = f [ j ]; } } return res ; } }; Note Notice the solution is identical to the Backpack II except one line (line 18), but you should notice there is a long way from Backpack II to Backpack III . See the detailed explaination of this problem in Dynamic Programming Coins in A Line III \u00b6 There are n coins in a line. Two players take turns to take a coin from one of the ends of the line until there are no more coins left. The player with the larger amount of money wins. Could you please decide the first player will win or lose? \u8fd9\u662f\u4e00\u4e2a\u535a\u5f08\u95ee\u9898\u3002\u770b\u4e0a\u53bb\u5f88\u96be\u5165\u624b\u3002\u8981\u5bf9\u9898\u76ee\u8ba4\u771f\u5ba1\u89c6\u3002\u6211\u5f00\u59cb\u7684\u65f6\u5019\u5c31\u6ca1\u6709\u61c2\u5f97\u9898\u610f\u3002\u4e0b\u9762\u662f\u6211\u7684\u9519\u8bef\u5206\u6790\u3002 /***************** WRONG *************************** * \u5f53\u524d\u5c40\u9762\uff1aa[i], ... a[j] * Alice = A, Bob = B, * Sa = A - B; * \u5148\u624b: Alice A + a[i](a[j]) * Bob = B, * \u4e0b\u4e00\u8f6e\uff1aa[i+1], ... a[j] * Alice = A' = A + a[i], Bob = B' = B, * Sb = B - A'; * \u5148\u624b\uff1aBob B + a[i + 1] (a[j - 1]) * \u540e\u624b\uff1aAlice: A'; * ... * * *** Sb = B - A - a[i] * *** Sb = -Sa - a[i] * * **************** WRONG ****************************/ \u6ce8\u610f\u8fd9\u9053\u9898\u95ee\u7684\u662f\u5148\u624b\u662f\u5426\u4f1a\u8d62\u3002\u6240\u4ee5\u5e94\u8be5\u4ee5\u201c\u5148\u624b\u201d\u8fd9\u4e2a\u5173\u952e\u8bcd\u51fa\u53d1\u3002\u5c31\u662f\u7b2c\u4e00\u6b65\u3002\u8fd9\u4e00\u70b9\u660e\u786e\u4e86\uff0c \u518d\u91cd\u65b0\u5206\u6790\uff0c\u95ee\u9898\u8fce\u5203\u800c\u89e3\u3002 /* \u6ce8\u610f\uff1a\u6211\u4eec\u7684\u9898\u610f\u662f\u95ee\u5148\u624b\u662f\u5426\u5fc5\u80dc\u3002\u5219\u5f53\u8003\u8651\u5148\u624b\u51fa\u624b\u4e4b\u524d\u7684\u72b6\u6001 * * \u5f53\u524d\u5c40\u9762\uff1aa[0], ... a[n-1] * Alice = A = 0, Bob = B = 0, * delta: Sa = A - B = 0; * \u5148\u624b: Alice \u53d6 a[i](\u6216\u8005a[j]), \u76ee\u6807\u662f\u6700\u5927\u5316A - B * A = a[i] * B = 0, * \u5728\u63a5\u4e0b\u6765\u7684\u6bcf\u8f6e\u4e2d\uff0cBob\u5c3d\u6700\u5927\u52aa\u529b\u53bb\u6700\u5927\u5316B - A * \u6700\u7ec8\uff1aAlice \u83b7\u5f97 A + a[i], * Bob \u83b7\u5f97 B, * \u6ce8\u610f\uff1a\u6700\u7ec8\u7ed3\u679c\u6307\u7684\u662f\u201c\u5fc5\u80dc\u201d\u6216\u201c\u5fc5\u8d25\u201d\u7684\u6cd5\u5219\u8d70\u5230\u6700\u540e. * \u5fc5\u80dc\u5fc5\u8d25\u6982\u5ff5\uff1a\u5148\u624b\u5728\u5f53\u524d\u5c40\u9762\u6709\u4e00\u79cd\u53ef\u80fd\u6027\u201c\u5fc5\u80dc\u201d\u5219\u201c\u5fc5\u80dc\u201d\uff0c\u6ca1\u6709\u4e00\u79cd\"\u5fc5\u80dc\"\u53ef\u80fd\u6027\u4fbf\u201c\u5fc5\u8d25\u201d\u3002 * \u6211\u4eec\u60f3\u77e5\u9053\u7684\u662fSa = A + a[i] - B = -Sb + a[i]\u662f\u5426\u4f1a\u5927\u4e8e0 * * \u6211\u4eec\u53d1\u73b0 Sa = -Sb + a[i] * \u8fd9\u662f\u4e2a\u5b50\u95ee\u9898\u3002\u5b50\u95ee\u9898\u662f\u76f8\u5bf9\u4e8e\u5148\u624bAlice\u6765\u8bb2\u7684\uff0c\u5b50\u95ee\u9898\u53ef\u4ee5\u770b\u505a\u6bcf\u6b21\u8f6e\u5230Alice\u7684\u5c40\u9762(\u5148\u624b), Alice \u7684\u95ee\u9898\u89c4\u6a21\u90fd\u53d8\u5c0f\u4e86\u3002 * State: f[i][j] = Alice\u9762\u5bf9\u5c40\u9762a[i], ... a[j]\u65f6\u80fd\u5f97\u5230\u7684\u6700\u5927\u7684\u4e8e\u5bf9\u624b\u7684\u6570\u503c\u5dee\u3002 * Equation: f[i][j] = max(a[i] - f[i+1][j], a[j] - f[i][j-1]) * Init: f[0][0] = 0; * f[i][i] = a[i]; * \u533a\u95f4\u578b\uff0c\u5199code\u5e94\u8be5\u662f\u679a\u4e3e\u957f\u5ea6 */ \u6ce8\u610f18-20\u884c\u679a\u4e3e\u957f\u5ea6\u7684\u65b9\u6cd5\u3002\u8fb9\u754c\u60c5\u51b5\u548c\u4e0b\u6807\u8ba1\u7b97\u4e0d\u80fd\u5f04\u9519\u3002\u540c\u6837\u7684code\u5728 Longest Palindromic Subsequence \u4e5f\u7528\u5230\u3002 \u65f6\u95f4\u590d\u6742\u5ea6 O(n^2) O(n^2) , \u7a7a\u95f4\u590d\u6742\u5ea6 O(n^2) O(n^2) class Solution { public : bool firstWillWin ( vector < int > & values ) { int n = values . size (); if ( n == 0 ) return true ; int f [ n ][ n ]; f [ 0 ][ 0 ] = 0 ; int j ; /* init */ for ( int i = 1 ; i < n ; i ++ ) { f [ i ][ i ] = values [ i ]; } /* enumerate length */ for ( int len = 2 ; len <= n ; len ++ ) { for ( int i = 0 ; i <= n - len ; i ++ ) { j = i + len - 1 ; f [ i ][ j ] = max ( values [ i ] - f [ i + 1 ][ j ], values [ j ] - f [ i ][ j -1 ]); } } return f [ 0 ][ n -1 ] >= 0 ; } }; Longest Palindromic Subsequence \u00b6 \u533a\u95f4\u578b\u52a8\u6001\u89c4\u5212\uff0c\u5b50\u95ee\u9898\u662f\u53bb\u5934\u53bb\u5c3e\u4e4b\u540e\u89c4\u6a21\u4fbf\u5c0f\u3002\u5199\u51fa\u72b6\u6001\u540e\u5173\u952e\u5728\u4e8e\u521d\u59cb\u5316\uff0c\u521d\u59cb\u5316\u7684\u662f\u5bf9\u89d2\u7ebf\u3002 \u8fd9\u9053\u9898\u7684\u72b6\u6001\u8981\u76f8\u5bf9\u597d\u60f3\uff0c\u4f46\u662f\u521d\u59cb\u5316\u548c\u7ed3\u679c\u76f8\u5bf9\u590d\u6742\u3002 * last step: a[i] == a[j] * state: f[i][j], LPS in string s[i], ... s[j] * equation: f[i][j] = max(f[i+1][j], f[i][j - 1], f[i-1][j-1]|s[i] == s[j]); * init: f[0][0] = 0, f[i][i] = 1; * if (s[i] == s[i + 1]) * f[i][i+1] = 2; * result: max(f[i][j]) C++ DP public class Solution { public int longestPalindromeSubseq ( string s ) { int n = s . length (); if ( n == 0 ) return 0 ; if ( n == 1 ) return 1 ; int f [ n ][ n ]; int i , j , len ; f [ 0 ][ 0 ] = 0 ; for ( i = 1 ; i < n ; i ++ ) { f [ i ][ i ] = 1 ; } for ( i = 0 ; i < n - 1 ; i ++ ) { if ( s [ i ] == s [ i + 1 ]) { f [ i ][ i + 1 ] = 2 ; } else { f [ i ][ i + 1 ] = 1 ; } } /* enumerate the len */ for ( len = 3 ; len <= n ; len ++ ) { for ( i = 0 ; i <= n - len ; i ++ ) { j = i + len - 1 ; /* init */ f [ i ][ j ] = f [ i + 1 ][ j ]; // remove left if ( f [ i ][ j - 1 ] > f [ i ][ j ]) { // remove right f [ i ][ j ] = f [ i ][ j - 1 ]; } if ( s [ i ] == s [ j ] && f [ i + 1 ][ j - 1 ] + 2 > f [ i ][ j ]) { f [ i ][ j ] = f [ i + 1 ][ j - 1 ] + 2 ; } } } int res = 0 ; for ( i = 0 ; i < n ; i ++ ) { for ( j = i ; j < n ; j ++ ) { if ( f [ i ][ j ] > res ) { res = f [ i ][ j ]; } } } return res ; } } Java memoization public class Solution { int [][] f = null ; char [] s = null ; public void compute () { if ( f [ i ][ j ] != - 1 ) return ; if ( i == j ) { f [ i ][ j ] = ( s [ i ] == s [ j ] ) ? 2 : 1 ; return ; } compute ( i , j - 1 ); compute ( i + 1 , j ); compute ( i + 1 , j - 1 ); f [ i ][ j ] = Math . max ( f [ i ][ j - 1 ] , f [ i + 1 ][ j ] ); if ( s [ i ] == s [ j ] ) { f [ i ][ j ] = Math . max ( f [ i ][ j ] , f [ i + 1 ][ j - 1 ] + 2 ); } } public int longestPalindromeSubseq ( String ss ) { s = ss . toCharArry (); int n = s . length ; if ( n == 0 ) { return 0 ; } f = new int [ n ][ n ] ; for ( int i = 0 ; i < n ; ++ i ) { for ( int j = i ; j < n ; ++ j ) { f [ i ][ j ] = - 1 ; } } compute ( 0 , n 1 ); return f [ 0 ][ n - 1 ] ; } } Note \u5212\u5206\u578b\u52a8\u6001\u89c4\u5212\u679a\u4e3e\u7684\u662f\u957f\u5ea6\u800c\u4e0d\u662f\u4e0b\u6807\u3002 Burst Balloons \u00b6 This is a hard problem. you should very carefully to analyze the problem and write down the equation. Here is my first attempt, which made a mistake. You should play closely attention to the boundary of the for loop and the state transition equaltion. The time complexity is O(n^3) O(n^3) , and space complexity is O(n^3) O(n^3) class Solution { public : /** * @param nums a list of integer * @return an integer, maximum coins */ int maxCoins ( vector < int >& nums ) { int n = nums . size (); if ( n == 1 ) { return nums [ 0 ]; } nums . insert ( nums . begin (), 1 ); nums . push_back ( 1 ); int f [ n + 2 ][ n + 2 ]; int i , j , k , len ; /* f[i][j] = maxi<k<j{f[i][k] + f[k][j] + a[i] * a[k] * a[j]} */ for ( i = 0 ; i < n + 1 ; i ++ ) { f [ i ][ i + 1 ] = 0 ; } for ( len = 3 ; len <= n + 2 ; len ++ ) { /*\u8fb9\u754c\uff1a\u957f\u5ea6\u679a\u4e3e\u5230n+2 */ for ( i = 0 ; i <= n - len + 2 ; i ++ ) { /*\u8fb9\u754c\uff1ai\u662f\u8981\u4ece0\u5f00\u59cb\uff0cj\u5230n+1, len = j - i + 1*/ j = i + len - 1 ; f [ i ][ j ] = 0 ; for ( k = i + 1 ; k < j ; k ++ ) { /*\u8fb9\u754c: k\u662f\u8981burst\u7684\u6c14\u7403\uff0c\u6211\u4eec\u8bf4f[i][j]\u662f\u4e0d\u5305\u62ecnums[i]\u548cnums[j]\u7684maxCoins*/ f [ i ][ j ] = max ( f [ i ][ j ], f [ i ][ k ] + f [ k ][ j ] + nums [ i ] * nums [ k ] * nums [ j ]); } } } return f [ 0 ][ n + 1 ]; } }; Scramble String \u00b6 Here is the analysis \u8fd9\u91cc\u65f6\u95f4\u590d\u6742\u5ea6\u8fbe\u5230\u4e86 O(n^4) O(n^4) , \u7a7a\u95f4\u590d\u6742\u5ea6\u901a\u8fc7\u964d\u7ef4\u5904\u7406\u4e3a O(n^3) O(n^3) . /* --------------------- * | s1 | s2 | S * --------------------- * --------------------- * | t1 | t2 | T * --------------------- * Last step: s1 <==> t1 && s2 <==> t2 OR s1 <==> t2 && S2 <==> t1 * Subproblem: s1 < S * state: f[i][j][h][k] \u4ee3\u8868\u662f\u5426S\u4e2d\u4ecei\u5230j\u7684\u5b57\u7b26\u662f\u7531T\u4e2d\u4eceh\u5230k\u7684\u5b57\u7b26scramble * \u56e0\u4e3a\u957f\u5ea6\u4e00\u6837\uff0c\u6240\u4ee5\u6211\u4eec\u53ef\u4ee5\u5bf9\u72b6\u6001\u8fdb\u884c\u964d\u7ef4\u5904\u7406\u3002 * state: f[i][j][k]: \u4ee3\u8868 s[i], ... s[i + k - 1] \u662f\u5426\u662f t[j], ... t[j + k - * 1] scramble string. * equation: f[i][j][k] = (f[i][j][w] && f[i + w][j + w] || f[i][j + k - w][w] && f[i + w][j][k - w]) for all 0 < w < k * init: f[0][0][0] = ? (\u4e0d\u91cd\u8981) * if (s[i] == t[j] && k == 1) { * f[i][j][1] = true; * } * */ class Solution { public : /* * * f[i][j][k] = OR_{0 < w < k}(f[i][j][w] && f[i+w][j+w][k-w]) OR (f[i][j+w][w] && f[i+w][j][) */ bool isScramble ( string & s1 , string & s2 ) { int m = s1 . length (); int n = s2 . length (); if ( m != n ) return false ; if ( m == 0 ) return true ; int f [ n ][ n ][ n + 1 ]; int i , j , k , w ; /* init */ for ( i = 0 ; i < n ; i ++ ) { for ( j = 0 ; j < n ; j ++ ) { f [ i ][ j ][ 1 ] = s1 [ i ] == s2 [ j ]; } } for ( k = 2 ; k <= n ; k ++ ) { for ( i = 0 ; i <= n - k ; i ++ ) { for ( j = 0 ; j <= n - k ; j ++ ) { f [ i ][ j ][ k ] = false ; //enumerate partition position (s1's length) for ( w = 1 ; w < k ; w ++ ) { // case 1: no swap match if ( f [ i ][ j ][ w ] && f [ i + w ][ j + w ][ k - w ]) { f [ i ][ j ][ k ] = true ; break ; } // case 2: swap match if ( f [ i ][ j + k - w ][ w ] && f [ i + w ][ j ][ k - w ]) { f [ i ][ j ][ k ] = true ; break ; } } } } } return f [ 0 ][ 0 ][ n ]; } }; \u533a\u95f4\u578b\u52a8\u6001\u89c4\u5212\u603b\u7ed3 \u00b6 \u7ed9\u5b9a\u4e00\u4e2a\u5e8f\u5217/\u5b57\u7b26\u4e32\uff0c\u8fdb\u884c\u4e00\u4e9b\u64cd\u4f5c \u6700\u540e\u4e00\u6b65\u4f1a\u5c06\u5e8f\u5217/\u5b57\u7b26\u4e32\u53bb\u5934/\u53bb\u5c3e \u5269\u4e0b\u7684\u4f1a\u662f\u4e00\u4e2a\u533a\u95f4 [i, j] \u72b6\u6001\u81ea\u7136\u5b9a\u4e49\u4e3a f[i][j] \uff0c\u8868\u793a\u9762\u5bf9\u5b50\u5e8f\u5217 [i, \u2026, j] \u65f6\u7684\u6700\u4f18\u6027\u8d28 \u5199\u7a0b\u5e8f\u65f6\u6ce8\u610f\u8981\u679a\u4e3e\u533a\u95f4\u957f\u5ea6 Lecture 6 \u00b6 Problem Category Longest Common Subsequence Interleaving String Edit Distance Distinct Subsequences Regular Expression Matching Wildcard Matching Ones and Zeroes Longest Common Subsequence \u00b6 State: f[i][j] , LCS of the first i chars from A and the first j chars from B. \u5b9a\u4e49\u72b6\u6001\u662f\u4e00\u5b9a\u8981\u6ce8\u610f\u4e0b\u6807\u7684\u610f\u4e49\u3002\u5728\u8fd9\u91cc\u6211\u4eec\u6307\u7684\u662f\u524di\u4e2a\u5b57\u7b26\u548c\u524dj\u4e2a\u5b57\u7b26 notice we can initialize the base case inside the loop. class Solution { public : int longestCommonSubsequence ( string A , string B ) { int m = A . length (); int n = B . length (); if ( m == 0 || n == 0 ) { return 0 ; } int f [ m + 1 ][ n + 1 ]; for ( int i = 0 ; i <= m ; ++ i ) { for ( int j = 0 ; j <= n ; ++ j ) { if ( i == 0 || j == 0 ) { f [ i ][ j ] = 0 ; continue ; } f [ i ][ j ] = max ( f [ i - 1 ][ j ], f [ i ][ j - 1 ]); if ( A [ i - 1 ] == B [ j - 1 ]) { f [ i ][ j ] = max ( f [ i ][ j ], f [ i - 1 ][ j - 1 ] + 1 ); } } } return f [ m ][ n ]; } }; Interleaving String \u00b6 Consider the where the last char in s3 came from, either from s1 or s2. state: f[i][j] , whether the first i from s1 and the first j from s2 is interleaving of the first i + j from s3 . equation: f[i][j] = (f[i][j - 1] && s[i + j - 1] == s2[j - 1]) || (f[i - 1][j] && s[i + j - 1] == s1[i - 1]) we may attempt to have 3d matrix for the DP, but we can reduce the state representation by noticing the index of s3 can be derived from index of s1 and s2 . /* * * last step: last s3[k - 1] == s1[m - 1] || s3[k - 1] == s2[n - 1] * state: f[i][j][k]: s3[0],..s[k - 1] is interleaving of s1[0], ... s[i - 1] and s2[0], .. s[j - 1] * equation 1: f[i][j][k] = (f[i][j - 1][k - 1] && s3[k - 1] == s1[i - 1]) * || (f[i - 1][j][k - 1] && s3[k - 1] == s2[j - 1]) * equation 2: f[i][j] = (f[i][j - 1] && s[i + j - 1] == s2[j - 1]) * || (f[i - 1][j] && s[i + j - 1] == s1[i - 1]) * init: f[0][0] = 1; * f[0][0] = false if k > 0 */ class Solution { public : bool isInterleave ( string s1 , string s2 , string s3 ) { int m = s1 . length (); int n = s2 . length (); int k = s3 . length (); if ( m + n != k ) return false ; f [ m + 1 ][ n + 1 ]; for ( int i = 0 ; i <= m ; ++ i ) { for ( int j = 0 ; j <= n ; ++ j ) { if ( i == 0 || j == 0 ) { f [ i ][ j ] = false ; continue ; } f [ i ][ j ] = false ; if ( i > 0 && s1 [ i - 1 ] == s3 [ i + j - 1 ]) { f [ i ][ j ] |= f [ i - 1 ][ j ]; } if ( j > 0 && s2 [ j - 1 ] == s3 [ i + j - 1 ]) { f [ i ][ j ] |= f [ i ][ j - 1 ]; } } } return f [ m ][ n ]; } }; Edit Distance \u00b6 Notice the state update is not straightforward, you need to have the correct setting first and then write down the state transition equation. We define the state as follows f[i][j] is the edit distance to make the first i chars in A and first j chars in B the same. To make A and B the same, their last step need to be the same. There are three editing operations: Insert a char at the end of A , so that A[-1] == B[-1] . The state should be updated as f[i][j] = f[i][j - 1] + 1 . This assignment operation purely mean that I can get f[i][j] from subproblem result. It doesn't indicate any of ther iteration of the string. You cannot have it like f[i][j] = f[i - 1][j - 1] + 1 . Delete a char at the end of A , so that A[-1] == B[-1] . The state update should be f[i][j] = f[i - 1][j] + 1 . Replace a char at the end of A , so that A[-1] == B[-1] . The state update should be f[i][j] = f[i - 1][j - 1] + 1 . No operation is needed, the last chars of the arrays are the same, such as A[-1] == B[-1] . The state update should be f[i][j] = f[i - 1][j - 1] . notice the state represent the edit distance , don't try to correlate it to the index to the string after the edit , no string is changed while calculating the edit distance f[i][j] . C++ DP Space O(mn) class Solution { public : /* * * State: f[i][j] is the minimum number of steps to edit w1 to w2. * f[i][j] = f[i][j - 1] + 1; insert * = f[i - 1][j] + 1; delete * = f[i - 1][j - 1] + 1; replace * = f[i - 1][j - 1]; no opration needed */ int minDistance ( string word1 , string word2 ) { int m = word1 . length (); int n = word2 . length (); int f [ m + 1 ][ n + 1 ]; f [ 0 ][ 0 ] = 0 ; int i , j ; for ( i = 0 ; i <= m ; i ++ ) { for ( j = 0 ; j <= n ; j ++ ) { /* init */ if ( i == 0 ) { f [ i ][ j ] = j ; continue ; } if ( j == 0 ) { f [ i ][ j ] = i ; continue ; } // delete // insert f [ i ][ j ] = min ( f [ i - 1 ][ j ] + 1 , f [ i ][ j - 1 ] + 1 ); // replace f [ i ][ j ] = min ( f [ i ][ j ], f [ i - 1 ][ j - 1 ] + 1 ); // no operation if ( word1 [ i - 1 ] == word2 [ j - 1 ]) { f [ i ][ j ] = min ( f [ i ][ j ], f [ i - 1 ][ j - 1 ]); } } } return f [ m ][ n ]; }; }; C++ space O(n) class Solution { public : /* * * State: f[i][j] is the minimum number of steps to convert w1 to w2. (len(w1) < len(w2)) * f[i][j] = f[i][j - 1] + 1; insert * = f[i - 1][j] + 1; delete * = f[i - 1][j - 1] + 1; replace * = f[i - 1][j - 1]; no opration needed * */ int minDistance ( string word1 , string word2 ) { int m = word1 . length (); int n = word2 . length (); int f [ m + 1 ][ n + 1 ]; f [ 0 ][ 0 ] = 0 ; int i , j ; int prev = -1 ; int curr = 0 ; for ( i = 0 ; i <= m ; i ++ ) { prev = curr ; curr = 1 - curr ; for ( j = 0 ; j <= n ; j ++ ) { /* init */ if ( i == 0 ) { f [ curr ][ j ] = j ; continue ; } if ( j == 0 ) { f [ curr ][ j ] = i ; continue ; } // delete // insert f [ curr ][ j ] = min ( f [ prev ][ j ] + 1 , f [ curr ][ j - 1 ] + 1 ); // replace f [ curr ][ j ] = min ( f [ curr ][ j ], f [ prev ][ j - 1 ] + 1 ); // no operation if ( word1 [ i - 1 ] == word2 [ j - 1 ]) { f [ curr ][ j ] = min ( f [ curr ][ j ], f [ prev ][ j - 1 ]); } } } return f [ curr ][ n ]; }; }; Note When using the prev and curr to index the 2d array, don't have to strictly follow the meaning of it. What you just need to make sure is the two row is rolling and not going to overwrite a useful value. Notice the update of the \"rolling index\" is in between the two for loops. It has nothing to do with index j . Distinct Subsequence \u00b6 Last step, t[n-1] match to s[m-1] or doesn't match. State: f[i][j] is the number of subsequence for t[0:j-2] in s[0:i-2] state transition: f[i][j] = f[i - 1][j - 1]|s[i - 1] == t[j - 1] + f[i - 1][j] class Solution { public : /* * * Last step: S[m - 1] in the * S[0], ... S[m - 2], S[m - 1] * T[0], ... T[n - 2], T[n - 1] * State: f[i][j]: # of subsequence for the first j chars in T have in the first i chars in S * Equation: f[i][j] = f[i - 1][j - 1]|S[i - 1] == T[j - 1] + f[i - 1][j] * 1. S[m - 1] == T[n - 1] * 1. f[i - 1][j - 1] * 2. f[i - 1][j] * 2. S[m - 1] != T[n - 1] * 1. f[i - 1][j] * Init: f[0][0] = 1; * f[0][i] = 0; * f[i][0] = 1; */ int numDistinct ( string & S , string & T ) { int m = S . length (); int n = T . length (); int f [ m + 1 ][ n + 1 ]; for ( int i = 0 ; i <= m ; i ++ ) { for ( int j = 0 ; j <= n ; j ++ ) { if ( i == 0 && j == 0 ) { f [ i ][ j ] = 1 ; continue ; } if ( i == 0 ) { f [ i ][ j ] = 0 ; continue ; } if ( j == 0 ) { f [ i ][ j ] = 1 ; continue ; } f [ i ][ j ] = f [ i - 1 ][ j ]; if ( S [ i - 1 ] == T [ j - 1 ]) { f [ i ][ j ] += f [ i - 1 ][ j - 1 ]; } } } return f [ m ][ n ]; } }; Regular Expression Matching \u00b6 C++ DP Solution class Solution { public : /* * * Last step (last char to match in the strings): * p[j - 1] != s[i - 1] ==> f[i][j] = false; * p[j - 1] == s[i - 1] ==> f[i][j] = f[i - 1][j - 1]; * p[j - 1] == '.' ==> f[i][j] = f[i - 1][j - 1] * * p[j - 1] == '*', * case 1 p[j - 2] == '.' ==> f[i][j] = f[i - 1][j]; ??? why not f[i][j] = true; * p[j - 2] != '.' * p[j - 2] == s[i - 1] ==> f[i][j] = f[i'][j - 2] | i' is the length not equal to p[j - 2]. * which one? / * \\ * case 2 p[j - 2] == s[i - 1] ==> f[i][j] = f[i - 1][j] * p[j - 2] != s[i - 1] ==> f[i][j] = f[i][j - 2]; * * We see that case 1 and case 2 could be combined and put inside one if statement. * State: f[i][j]: the first i chars form s match regex the first j chars from p * Equation: f[i][j] = * Init: f[0][0] = 0; * f[0][j] = ture; if p[j - 1] == '*' * f[i][0] = false; */ bool isMatch ( const char * s , const char * p ) { int i , j ; int m = 0 ; int n = 0 ; while ( s [ m ] != '\\0' ) { m ++ ; } while ( p [ n ] != '\\0' ) { n ++ ; } int f [ m + 1 ][ n + 1 ]; for ( i = 0 ; i <= m ; i ++ ) { for ( j = 0 ; j <= n ; j ++ ) { if ( i == 0 && j == 0 ) { f [ i ][ j ] = true ; continue ; } if ( j == 0 ) { f [ i ][ j ] = false ; continue ; } f [ i ][ j ] = false ; if ( p [ j - 1 ] == '*' ) { // matched one char at end of s, s=[----]a, p=[-----]a* if ( i > 0 && j > 1 && ( p [ j - 2 ] == '.' || p [ j - 2 ] == s [ i - 1 ])) { f [ i ][ j ] |= f [ i - 1 ][ j ]; //use whole p to match s[0, ... i - 2] } // don't care the a* or .*, // match previous chars s=[-----a], p=[------]a* if ( j > 1 ) { f [ i ][ j ] |= f [ i ][ j - 2 ]; } } else if ( i > 0 && ( p [ j - 1 ] == '.' || p [ j - 1 ] == s [ i - 1 ])) { f [ i ][ j ] = f [ i - 1 ][ j - 1 ]; } } } return f [ m ][ n ]; } }; C++ recursive solution // The idea is to deal with each case one by one. class Solution { public : bool isMatch ( string s , string p ) { /* base cases when none left or only one char left */ if ( p . empty ()) return s . empty (); if ( p . length () == 1 ) { return ( s . length () == 1 && ( s [ 0 ] == p [ 0 ] || p [ 0 ] == '.' )); } if ( p [ 1 ] != '*' ) { if ( s . empty ()) return false ; return ( s [ 0 ] == p [ 0 ] || p [ 0 ] == '.' ) && isMatch ( s . substr ( 1 ), p . substr ( 1 )); } /* p[1] == '*' */ while ( ! s . empty () && ( s [ 0 ] == p [ 0 ] || p [ 0 ] == '.' )) { /* consider the case: s=\"aaaabcd\", p=\".*b*cd\"*/ if ( isMatch ( s , p . substr ( 2 ))) return true ; s = s . substr ( 1 ); } /* consider the case: s=\"aaaaa\", p=\"a*\" and case s=\"aaaaa\", p=\"a*b\" */ return isMatch ( s , p . substr ( 2 )); /* how comes? */ } }; Wildcard Matching \u00b6 \u4e0d\u9700\u8981\u77e5\u9053' '\u6700\u591a\u80fd\u5339\u914d\u51e0\u4e2a\u5b57\u7b26\u3002\u5728\u5f53\u524dstep\u53ea\u9700\u8003\u8651\u7528' '\u53bb\u5339\u914d\u4e00\u4e2a\u5b57\u7b26\u6216\u8005\u5b8c\u5168\u4e0d\u5339\u914d\u5c31\u8986\u76d6\u4e86 \u6240\u6709\u7684\u60c5\u51b5\u3002\u81f3\u4e8e\u6700\u7ec8\u53ef\u4ee5\u5339\u914d\u51e0\u4e2a\uff0c\u662f\u5728\u591a\u4e2astep\u4e2d\u7684\u4fe1\u606f\u3002\u5f53\u524dstep\u5e76\u4e0d\u9700\u8981\u5173\u5fc3. When p[j - 1] == '*' , since we don't know ' ' match how many chars in s? In the first solution, I used a third loop to check. We can think it in this way, for p[j - 1] == '*' , we can use ' ' to match the trailing character or not to use '*' to match previously. the we have f[i][j] = f[i - 1][j] | f[i][j - 1] . C++ O(mnk) class Solution { public : /* * * last step: * p[j - 1] == '?' * f[i][j] = f[i - 1][j - 1] * s[i - 1] == p[j - 1] * f[i][j] = f[i - 1][j - 1] * p[j - 1] == '*', ('*' may match k of trailng characters from s, but we don't know how many) * f[i][j] = f[i][j - 1] OR f[i - 1][j - 1] OR f[i - 2][j - 1], .. f[i - k][j - 1] * State: f[i][j]: the first i chars from s match the first j letters from p * Equation: * Init: f[0][0] = true; * f[i][0] = false; * f[0][j] = true; if (p[j - 1] == \"*\" && j == 1) * calculate by DP: * f[0][j] = false; if (j > 1) */ bool isMatch ( const char * s , const char * p ) { int m = 0 ; int n = 0 ; int i , j , k ; while ( s [ m ] != '\\0' ) { m ++ ; } while ( p [ n ] != '\\0' ) { n ++ ; } int f [ m + 1 ][ n + 1 ]; for ( i = 0 ; i <= m ; i ++ ) { for ( j = 0 ; j <= n ; j ++ ) { if ( i == 0 && j == 0 ) { f [ i ][ j ] = true ; continue ; } if ( j == 0 ) { f [ i ][ j ] = false ; continue ; } f [ i ][ j ] = false ; if ( p [ j - 1 ] == '*' ) { // k == i indicate the special case: f[0][1] (s=\"\", p=\"*\") for ( k = 0 ; k <= i ; k ++ ) { f [ i ][ j ] |= f [ i - k ][ j - 1 ]; } } else { if ( i > 0 && ( p [ j - 1 ] == '?' || s [ i - 1 ] == p [ j - 1 ])) { f [ i ][ j ] = f [ i - 1 ][ j - 1 ]; } } } } return f [ m ][ n ]; } }; C++ O(mn) class Solution { public : bool isMatch ( const char * s , const char * p ) { // write your code here int m = 0 ; int n = 0 ; int i , j , k ; while ( s [ m ] != '\\0' ) { m ++ ; } while ( p [ n ] != '\\0' ) { n ++ ; } int f [ m + 1 ][ n + 1 ]; for ( i = 0 ; i <= m ; i ++ ) { for ( j = 0 ; j <= n ; j ++ ) { if ( i == 0 && j == 0 ) { f [ i ][ j ] = true ; continue ; } if ( j == 0 ) { f [ i ][ j ] = false ; continue ; } f [ i ][ j ] = false ; if ( p [ j - 1 ] == '*' ) { f [ i ][ j ] = f [ i ][ j - 1 ]; // ignore the '*', match nothing if ( i > 0 ) { f [ i ][ j ] |= f [ i - 1 ][ j ]; // match the trailing char from s and continue } } else { if ( i > 0 && ( p [ j - 1 ] == '?' || s [ i - 1 ] == p [ j - 1 ])) { f [ i ][ j ] = f [ i - 1 ][ j - 1 ]; } } } } return f [ m ][ n ]; } }; Ones and Zeroes \u00b6 This is essentially a backpack problem see the backpack problem. The key to solve the problem is taking the last item or do not take the last item. \u6280\u5de7\uff1a\u80cc\u5305\u95ee\u9898\u7684\u201c\u6700\u540e\u4e00\u6b65\u201d\u662f\u6307\u6700\u540e\u4e00\u4e2a\u201c\u7269\u54c1\u201d\u5728\u7ed3\u679c\u4e2d\u6216\u8005\u4e0d\u518d\u7ed3\u679c\u4e2d. C++ DP O(mnk) class Solution { public : /* * * \u80cc\u5305\u95ee\u9898\uff0c\u6700\u540e\u4e00\u4e2a\u8fdb\u201c\u80cc\u5305\u201d\uff0c\u6700\u540e\u4e00\u4e2a\u4e0d\u8fdb\u3002 * State: f[i][j][k]: maximum of k strs can be formed by i zeros and j ones. * Equation: f[i][j][k] = max(f[i][j][k - 1], f[i - a][j - b][k - 1] + 1| i > a and j > b) * Init: f[i][j][0] = 0 * */ int findMaxForm ( vector < string >& strs , int m , int n ) { int len = strs . size (); int cnt0 [ len ]; int cnt1 [ len ]; int i , j , k ; int f [ m + 1 ][ n + 1 ][ len + 1 ]; /* count all the zeros and ones */ for ( k = 0 ; k < len ; k ++ ) { cnt0 [ k ] = 0 ; cnt1 [ k ] = 0 ; for ( i = 0 ; i < strs [ k ]. length (); i ++ ) { if ( strs [ k ][ i ] == '0' ) { cnt0 [ k ] ++ ; } else { cnt1 [ k ] ++ ; } } } for ( i = 0 ; i <= m ; i ++ ) { for ( j = 0 ; j <= n ; j ++ ) { f [ i ][ j ][ 0 ] = 0 ; } } /* k started from 1 */ for ( k = 1 ; k <= len ; k ++ ) { for ( i = 0 ; i <= m ; i ++ ) { for ( j = 0 ; j <= n ; j ++ ) { f [ i ][ j ][ k ] = f [ i ][ j ][ k - 1 ]; if ( i >= cnt0 [ k - 1 ] && j >= cnt1 [ k - 1 ]) { f [ i ][ j ][ k ] = max ( f [ i ][ j ][ k ], f [ i - cnt0 [ k - 1 ]][ j - cnt1 [ k - 1 ]][ k - 1 ] + 1 ); } } } } return f [ m ][ n ][ len ]; } }; C++ DP O(nk) class Solution { public : int findMaxForm ( vector < string >& strs , int m , int n ) { int len = strs . size (); int cnt0 [ len ]; int cnt1 [ len ]; int i , j , k ; /* count all the zeros and ones */ for ( k = 0 ; k < len ; k ++ ) { cnt0 [ k ] = 0 ; cnt1 [ k ] = 0 ; for ( i = 0 ; i < strs [ k ]. length (); i ++ ) { if ( strs [ k ][ i ] == '0' ) { cnt0 [ k ] ++ ; } else { cnt1 [ k ] ++ ; } } } int f [ m + 1 ][ n + 1 ][ len + 1 ]; int prev , curr = 0 ; for ( i = 0 ; i <= m ; i ++ ) { for ( j = 0 ; j <= n ; j ++ ) { f [ i ][ j ][ curr ] = 0 ; } } /* k started from 1 */ for ( k = 1 ; k <= len ; k ++ ) { prev = curr ; curr = 1 - curr ; for ( i = 0 ; i <= m ; i ++ ) { for ( j = 0 ; j <= n ; j ++ ) { f [ i ][ j ][ curr ] = f [ i ][ j ][ prev ]; if ( i >= cnt0 [ k - 1 ] && j >= cnt1 [ k - 1 ]) { f [ i ][ j ][ curr ] = max ( f [ i ][ j ][ curr ], f [ i - cnt0 [ k - 1 ]][ j - cnt1 [ k - 1 ]][ prev ] + 1 ); } } } } return f [ m ][ n ][ curr ]; } }; \u53cc\u5e8f\u5217\u578b\u52a8\u6001\u89c4\u5212\u603b\u7ed3 \u00b6 \u4e24\u4e2a\u4e00\u7ef4\u5e8f\u5217/\u5b57\u7b26\u4e32 \u7a81\u7834\u53e3 \u4e32A\u548c\u4e32B\u7684\u6700\u540e\u4e00\u4e2a\u5b57\u7b26\u662f\u5426\u5339\u914d \u662f\u5426\u9700\u8981\u4e32A/\u4e32B\u7684\u6700\u540e\u4e00\u4e2a\u5b57\u7b26 \u7f29\u51cf\u95ee\u9898\u89c4\u6a21 \u6570\u7ec4\u4e0b\u6807\u8868\u793a\u5e8f\u5217 A \u524d i \u4e2a\uff0c\u5e8f\u5217 B \u524d j \u4e2a: f[i][j] \u521d\u59cb\u6761\u4ef6\u548c\u8fb9\u754c\u60c5\u51b5 \u7a7a\u4e32\u5982\u4f55\u5904\u7406 \u8ba1\u6570\u578b(\u60c5\u51b51+\u60c5\u51b52+\u2026)\u4ee5\u53ca\u6700\u503c\u578b(min/max{\u60c5\u51b51\uff0c\u60c5\u51b52\uff0c\u2026}) \u5339\u914d\u7684\u60c5\u51b5\u4e0b\u52ff\u5fd8+1(\u64cd\u4f5c\u6570\u591a1\u6b21\uff0c\u5339\u914d\u957f\u5ea6\u591a1) Lecture 7 \u00b6 Problem Category Minimum Adjustment Cost K Sum \u80cc\u5305\u578b Longest Increasing Subsequence \u5e8f\u5217\u578b K Edit Distance \u53cc\u5e8f\u5217 + Trie Frog Jump \u5750\u6807 + \u72b6\u6001 Maximal Square \u5750\u6807 Maximal Rectangle Minimum Adjustment Cost \u00b6 The key is how to come up the last step and the induction. The key is to come up a sensable modle of the problem. See the analysis in the code comments. The hard part of the problem is how to prove the the range: 1 <= B[i] <= 100 , so that we can create the state array f[n + 1][100 + 1] The initial value of this DP problem is not obvious. You have to pay attention to the initial condition of this problem, it different from the provious DP problems. class Solution { public : /** * \u6280\u5de7\uff1a\u8981\u770b\u4f5c\u628aA\u6570\u7ec4\u901a\u8fc7\u53d8\u5316\u53d8\u6210B\u6570\u7ec4\u3002\u6700\u540e\u4e00\u6b65\u8003\u8651\u628aA[i]\u53d8\u6210B[i]. * \u8fd9\u6837\u6bd4\u5728A\u7684\u57fa\u7840\u4e0a\u53d8\u5316\u66f4\u5bb9\u6613\u8003\u8651\u9012\u63a8\u5173\u7cfb * Last step: change A[i] to B[i], result[i] = result[i - 1] + abs(B[i] - A[i]) * State: f[i][j]: the cost of changing the first i element in A, * and the last elemnet A[i - 1] changed is changed to j (j == B[i]) * Induction: A[i - 1] --> j, A[i - 2] --> k, |j - k| < target ==> j - target <= k <= j + target * Equation: f[i][j] = min_{j - target <= k <= j + target, 1 <= k <= 100}(f[i - 1][k] + abs(j - A[i])) * Init: f[0][0] = ? * f[1][k] = abs(k - A[0]) */ int MinAdjustmentCost ( vector < int > A , int target ) { int n = A . size (); int f [ n + 1 ][ 100 + 1 ]; int i , j , k ; for ( j = 1 ; j <= 100 ; j ++ ) { f [ 1 ][ j ] = abs ( j - A [ 0 ]); } for ( i = 2 ; i <= n ; i ++ ) { for ( j = 1 ; j <= 100 ; j ++ ) { f [ i ][ j ] = INT_MAX ; for ( k = j - target ; k <= j + target ; k ++ ) { if ( k < 1 || k > 100 ) { continue ; } f [ i ][ j ] = min ( f [ i ][ j ], f [ i - 1 ][ k ] + abs ( j - A [ i - 1 ])); } } } int res = INT_MAX ; for ( j = 1 ; j <= 100 ; j ++ ) { res = min ( res , f [ n ][ j ]); } return res ; } }; Note The space complexity is O(100n) O(100n) . The time complexity is O(1002n) O(1002n) . K Sum \u00b6 This is a backpack problem which is very similar to the problem Backpack VI \u80cc\u5305\u95ee\u9898\u7684\u8bdd\uff0c\u603b\u627f\u91cd\u8981\u8fdb\u72b6\u6001\uff0c\u4e5f\u5c31\u662f\u8bf4 target \u8981\u8fdb\u72b6\u6001\u3002\u5176\u4ed6\u8fd8\u6709\u4ec0\u4e48\u8981\u8fdb\u72b6\u6001\u5462\uff1f\u8fd9\u5c31\u8981\u6c42\u8ba4\u771f\u5206\u6790\u9898\u76ee\u3002 \u5bf9\u4e8e\u8fd9\u79cd\u7c7b\u4f3c\u6c42\u4e0d\u653e\u56de\u7684\u7ec4\u5408\u6570\u7684\u8fc7\u7a0b\uff0c\u4e0d\u8981\u5c1d\u8bd5\u53bb\u7528\u5faa\u73af\u6765\u51b3\u5b9a\u67d0\u4e00\u4e2a\u5143\u7d20\u662f\u5426\u88ab\u9009\u4e2d\u3002\u800c\u662f\u8981\u7ed3\u5408 induction\u7684\u60f3\u6cd5\uff0c\u4ece\u67d0\u4e2a\u7279\u6b8a\u7684\u6b65\u9aa4\u7740\u773c\uff0c\u5728\u8fd9\u4e2a\u6b65\u9aa4\u4e2d\u8003\u8651\u9009\u4e2d\u6216\u8005\u4e0d\u9009\u67d0\u4e2a\u5143\u7d20\uff08\u672c\u8eab\u5e26\u6709\u5faa\u73af\u7684\u610f\u601d\uff0c \u4efb\u4f55\u4e00\u4e2a\u5143\u7d20\u53ea\u80fd\u88ab\u9009\u4e2d\u4e00\u6b21\uff0c\u548c\u4e0d\u9009\u4e00\u6b21\uff09 \u4e0b\u9762\u4ee3\u7801\u6ce8\u91ca\u4e2d\u7ed9\u51fa\u4e86\u6211\u6700\u521d\u7684\u9519\u8bef\u5206\u6790\uff0c\u968f\u540e\u53c8\u7ed9\u51fa\u4e86\u6b63\u786e\u7684\u5206\u6790\u3002\u5bf9\u6bd4\u4e8c\u8005\u4e0d\u540c\uff0c\u4e89\u53d6\u4ee5\u540e\u5c11\u72af\u7c7b\u4f3c\u9519\u8bef\u3002 \u5173\u952e\u9700\u8981\u6ce8\u610f\u7684\u662f\u521d\u59cb\u5316\u3002\u5148\u628a f[0][q][s] \u5168\u90e8\u521d\u59cb\u5316\u4e3a0\uff0c\u7136\u540e\u518d\u521d\u59cb\u5316 f[0][0][0] \u3002\u8fd9\u4e24\u4e2a\u6b65\u9aa4\u4e0d\u80fd\u98a0\u5012\u987a\u5e8f\uff0c \u6ce8\u610f\u4e5f\u53ef\u4ee5\u5728for loop\u4e2d\u521d\u59cb\u5316f[0][0][0]\u3002 Time complexity O(n \\cdot k \\cdot \\text{target}) O(n \\cdot k \\cdot \\text{target}) , space complexity O(n \\cdot k \\cdot \\text{target}) O(n \\cdot k \\cdot \\text{target}) . We can use rolling array to reduce the space complexity to O(k \\cdot \\text{target}) O(k \\cdot \\text{target}) . C++ DP class Solution { public : /** * This is a backpack problem, * Last step: A[i] is selected or not selected. * State: f[i][j]: total solution of i element can sum up to j. * Equation: f[i][j] = f[i - 1][j] + f[i - 1][j - A[i - 1]]|j > A[i - 1] for all i == k. * Equation: f[i][j] = sum(f[i - 1][j - A[k]] | 0 <= k <= i - 1, j > A[k]). * Init: f[0][0] = 1; // ? * f[0][j] = 0 * ******************** WRONG ********************************* * ******************** CORRECT ******************************* * Last step: A[i - 1] is selected or not selected. * If selected, we should select k - 1 numbers from A[0], .. A[n - 2] that sum to target - A[i - 1]. * If not selected, we should select k number from A[0], .. A[n - 2] that sum to target. * State: f[i][k][s]: total solution of selecting k element from first i element that sum up to j. * Equation: f[i][k][s] = f[i - 1][k][s] + f[i - 1][k - 1][s - A[i - 1]]|s > A[i - 1]. * Init: f[0][0][0] = 1; // ? * f[0][k][s] = 0 * */ int kSum ( vector < int > A , int k , int target ) { // wirte your code here int n = A . size (); int f [ n + 1 ][ k + 1 ][ target + 1 ]; int i , q , s ; // init for ( q = 0 ; q <= k ; q ++ ) { for ( s = 0 ; s <= target ; s ++ ) { f [ 0 ][ q ][ s ] = 0 ; } } f [ 0 ][ 0 ][ 0 ] = 1 ; for ( i = 1 ; i <= n ; i ++ ) { for ( q = 0 ; q <= k ; q ++ ) { for ( s = 0 ; s <= target ; s ++ ) { // not select A[i - 1] f [ i ][ q ][ s ] = f [ i - 1 ][ q ][ s ]; // select A[i - 1] if ( q >= 1 && s >= A [ i - 1 ]) { f [ i ][ q ][ s ] += f [ i - 1 ][ q - 1 ][ s - A [ i - 1 ]]; } } } } return f [ n ][ k ][ target ]; } }; C++ DP space optimized class Solution { public : int kSum ( vector < int > A , int k , int target ) { // wirte your code here int n = A . size (); int f [ n + 1 ][ k + 1 ][ target + 1 ]; int i , q , s ; int curr = 0 , prev = 0 ; // init for ( q = 0 ; q <= k ; q ++ ) { for ( s = 0 ; s <= target ; s ++ ) { f [ curr ][ q ][ s ] = 0 ; } } f [ curr ][ 0 ][ 0 ] = 1 ; for ( i = 1 ; i <= n ; i ++ ) { prev = curr ; curr = 1 - curr ; for ( q = 0 ; q <= k ; q ++ ) { for ( s = 0 ; s <= target ; s ++ ) { // not select A[i - 1] f [ curr ][ q ][ s ] = f [ prev ][ q ][ s ]; // select A[i - 1] if ( q >= 1 && s >= A [ i - 1 ]) { f [ curr ][ q ][ s ] += f [ prev ][ q - 1 ][ s - A [ i - 1 ]]; } } } } return f [ curr ][ k ][ target ]; } }; Longest Increasing Subsequence \u00b6 K Edit Distance \u00b6 Frog Jump \u00b6 You could use naive solution to iterate all the possible cases and use memoization to speed up. In that case, you can also use binary search to located a stone in the array which help to improve the complexity. But the best solution is to use a map to keep all the possible steps for a stone. C++ map to set class Solution { public : bool canCross ( vector < int >& stones ) { int n = stones . size (); unordered_map < int , unordered_set < int >> hmap ; for ( int i = 0 ; i < stones . size (); i ++ ) { hmap [ stones [ i ]] = unordered_set < int > (); } hmap [ stones [ 0 ]]. insert ( 0 ); for ( int i = 0 ; i < stones . size (); i ++ ) { for ( int k : hmap [ stones [ i ]]) { for ( int s = k - 1 ; s <= k + 1 ; s ++ ) { if ( s > 0 && hmap . find ( stones [ i ] + s ) != hmap . end ()) { hmap [ stones [ i ] + s ]. insert ( s ); } } } } return hmap [ stones [ n - 1 ]]. size () > 0 ; } }; 221. Maximal Square \u00b6 85. Maximal Rectangle \u00b6","title":"Nine Chapter Dynamic Programming"},{"location":"courses/9chap-dynamic-prog/notes/#nine-chapter-dynamic-programming","text":"Nine Chapter Dynamic Programming Course","title":"Nine Chapter Dynamic Programming"},{"location":"courses/9chap-dynamic-prog/notes/#lecture-1-introduction-to-dynamic-programming","text":"Problem Category Coin Change Unique Paths coordinate Jump Game \u52a8\u6001\u89c4\u5212\u9898\u76ee\u7279\u70b9: \u8ba1\u6570 \u6709\u591a\u5c11\u79cd\u65b9\u5f0f\u8d70\u5230\u53f3\u4e0b\u89d2 \u6709\u591a\u5c11\u79cd\u65b9\u6cd5\u9009\u51fak\u4e2a\u6570\u4f7f\u5f97\u548c\u662fSum \u6c42\u6700\u5927\u6700\u5c0f\u503c \u4ece\u5de6\u4e0a\u89d2\u8d70\u5230\u53f3\u4e0b\u89d2\u8def\u5f84\u7684\u6700\u5927\u6570\u5b57\u548c \u6700\u957f\u4e0a\u5347\u5b50\u5e8f\u5217\u957f\u5ea6 \u6c42\u5b58\u5728\u6027 \u53d6\u77f3\u5b50\u6e38\u620f\uff0c\u5148\u624b\u662f\u5426\u5fc5\u80dc \u80fd\u4e0d\u80fd\u9009\u51fak\u4e2a\u6570\u4f7f\u5f97\u548c\u662fSum \u72b6\u6001\u662f\u52a8\u6001\u89c4\u5212\u5b9a\u6d77\u795e\u9488\uff0c\u786e\u5b9a\u72b6\u6001\u9700\u8981\u4e24\u4e2a\u57fa\u672c\u610f\u8bc6\uff1a \u6700\u540e\u4e00\u6b65 \u5b50\u95ee\u9898 Four Ingredients for DP What's the state? start with the last step for the optimal solution decompose into subproblems Write the state transition? find the transition from subproblem relations Initial value and boundary conditions need to think careful in this step How can you compute the states? iteration directio forward computing","title":"Lecture 1 Introduction to Dynamic Programming"},{"location":"courses/9chap-dynamic-prog/notes/#coin-change","text":"Imagine the last coin you can use and the minimum solution to found can be represented as f[amount] . It can be solved by solving the smaller problem first. we have f[amount] = min(f[amount], f[amount - last_coin] + 1) . The problem is we don't know which coin will be selected for the last one to reach the solution, so we have to iterate through the coins to check every one of them. We expecting to see two for loops in our code. DP 4 ingredient: size of the dp array f[amount + 1] initial state: f[0] = 0 , amount 0 can use 0 coin. subproblem: f[amount] = min(f[amount], f[amount - last\\_coin] + 1) . results: f[amount] DP solution class Solution { public : int coinChange ( vector < int >& coins , int amount ) { int n = coins . size (); int f [ amount + 1 ]; f [ 0 ] = 0 ; /* calculate the f[1], f[2], ... f[amount] */ for ( int i = 1 ; i <= amount ; i ++ ) { f [ i ] = INT_MAX ; /* small trick, set to invalid first */ for ( int j = 0 ; j < n ; j ++ ) { /* update states */ /* f[i] can select coins[j] && f[i - coins[j]] is possible && coins[j] is last coin */ if ( i >= coins [ j ] && f [ i - coins [ j ]] != INT_MAX && f [ i - coins [ j ]] + 1 < f [ i ]) { f [ i ] = f [ i - coins [ j ]] + 1 ; } } } return f [ amount ] == INT_MAX ? -1 : f [ amount ]; } }; Alternative Solution class Solution { public : int coinChange ( vector < int >& coins , int amount ) { int n = coins . size (); int f [ amount + 1 ]; // f[i] represent the minimum counts to make up i amount // use INT_MAX to represent impossible case. f [ 0 ] = 0 ; for ( int i = 1 ; i <= amount ; i ++ ) { f [ i ] = INT_MAX ; for ( int j = 0 ; j < n ; j ++ ) { if ( i >= coins [ j ] && f [ i - coins [ j ]] != INT_MAX ) { f [ i ] = min ( f [ i ], f [ i - coins [ j ]] + 1 ); } } } return f [ amount ] == INT_MAX ? -1 : f [ amount ]; } };","title":"Coin Change"},{"location":"courses/9chap-dynamic-prog/notes/#unique-paths","text":"Solving smaller problem first than by accumulating the results from the smaller problems, we can solve the overall problem. Use a 2-d array to record the result the smaller problem, we know for the position f[i][j] = f[i - 1][j] + f[i][j - 1] , which means the summation of number of path from above and from left. The initial state is the first row and the first column are all equal to 1 . class Solution { public : /** * @param n, m: positive integer (1 <= n ,m <= 100) * @return an integer */ int uniquePaths ( int m , int n ) { int f [ m ][ n ] = { 0 }; for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { if ( i == 0 || j == 0 ) { f [ i ][ j ] = 1 ; } if ( i > 0 && j > 0 ) { f [ i ][ j ] = f [ i - 1 ][ j ] + f [ i ][ j - 1 ]; } } } return f [ m - 1 ][ n - 1 ]; } };","title":"Unique Paths"},{"location":"courses/9chap-dynamic-prog/notes/#jump-game","text":"Notice the problem statement \"Each element in the array represents your maximum jump length at that position.\" Solution 1 DP The problem have characteristics of the dynamic problem, that is it can be decomposed into smaller problem, the large problem can be solved using the result from solving smaller problems. We use a dp array f[i] to store whether it can jump from previous position to position i . if nums[i] + i >= j , it can also jump to position j . Looks like we are going to have a nested loop. the outer loop iterate to check whether can jump to each step j . inner loop check each step before j , namely the smaller size problem. class Solution { public : bool canJump ( vector < int >& nums ) { int n = nums . size (); if ( n == 0 ) return true ; int f [ n ]; f [ 0 ] = true ; /* initialize */ for ( int j = 1 ; j < n ; j ++ ) { f [ j ] = false ; for ( int i = 0 ; i < j ; i ++ ) { if ( f [ i ] && nums [ i ] + i >= j ) { f [ j ] = true ; break ; } } } return f [ n - 1 ]; } }; Solution 2 Greedy Use a variable cur_max to maintain the possible maximum jump position, if the current position is less than the maximum possible jump, return flase. class Solution { public : bool canJump ( vector < int >& nums ) { int n = nums . size (); if ( n == 0 ) return true ; int cur_max = nums [ 0 ]; /* maximum distance can reach from ith */ /* L.I.: current step (ith step) must <= cur_max jump */ for ( int i = 0 ; i < n ; i ++ ) { if ( i > cur_max ) /* must goes first */ return false ; if ( i + nums [ i ] > cur_max ) cur_max = i + nums [ i ]; } return true ; } };","title":"Jump Game"},{"location":"courses/9chap-dynamic-prog/notes/#lecture-2-dynamic-programming-on-coordinates","text":"Problem Category Unique Paths II \u5750\u6807\u578b Paint House \u5e8f\u5217\u578b \uff0b \u72b6\u6001 Decode Ways \u5212\u5206\u578b Longest Increasing Continuous Subsequence \u5e8f\u5217\u578b / \u5212\u5206\u578b Minimum Path Sum \u5750\u6807\u578b Bomb Enemy \u5750\u6807\u578b Counting Bits \u5750\u6807\u578b","title":"Lecture 2 Dynamic Programming on Coordinates"},{"location":"courses/9chap-dynamic-prog/notes/#_1","text":"\u6700\u7b80\u5355\u7684\u52a8\u6001\u89c4\u5212\u7c7b\u578b \u7ed9\u5b9a\u4e00\u4e2a\u5e8f\u5217\u6216\u7f51\u683c \u9700\u8981\u627e\u5230\u5e8f\u5217\u4e2d\u67d0\u4e2a/\u4e9b\u5b50\u5e8f\u5217\u6216\u7f51\u683c\u4e2d\u7684\u67d0\u6761\u8def\u5f84 \u67d0\u79cd\u6027\u8d28\u6700\u5927/\u6700\u5c0f \u8ba1\u6570 \u5b58\u5728\u6027 \u52a8\u6001\u89c4\u5212\u65b9\u7a0b f[i] \u4e2d\u7684\u4e0b\u6807 i \u8868\u793a\u4ee5 a[i] \u4e3a\u7ed3\u5c3e\u7684\u6ee1\u8db3\u6761\u4ef6\u7684\u5b50\u5e8f\u5217\u7684\u6027\u8d28. f[i][j] \u4e2d\u7684 \u4e0b\u6807 i , j \u8868\u793a\u4ee5\u683c\u5b50 (i, j) \u4e3a\u7ed3\u5c3e\u7684\u6ee1\u8db3\u6761\u4ef6\u7684\u8def\u5f84\u7684\u6027\u8d28 \u6700\u5927\u503c/\u6700\u5c0f\u503c \u4e2a\u6570 \u662f\u5426\u5b58\u5728 \u5750\u6807\u578b\u52a8\u6001\u89c4\u5212\u7684\u521d\u59cb\u6761\u4ef6 f[0] \u5c31\u662f\u6307\u4ee5 a[0] \u4e3a\u7ed3\u5c3e\u7684\u5b50\u5e8f\u5217\u7684\u6027\u8d28","title":"\u5750\u6807\u578b\u52a8\u6001\u89c4\u5212"},{"location":"courses/9chap-dynamic-prog/notes/#unique-paths-ii","text":"C++ Naive DP class Solution { public : int uniquePathsWithObstacles ( vector < vector < int >>& obstacleGrid ) { int m = obstacleGrid . size (); int n = obstacleGrid [ 0 ]. size (); int i = 0 ; int j = 0 ; int flag = 0 ; vector < vector < int >> f ( m , vector < int > ( n )); if ( m == 0 || n == 0 ) { return 0 ; } if ( obstacleGrid [ 0 ][ 0 ] == 1 ) { return 0 ; } f [ 0 ][ 0 ] = 1 ; for ( i = 1 ; i < m ; i ++ ) { if ( obstacleGrid [ i ][ 0 ] == 1 ) { f [ i ][ 0 ] = 0 ; } else { f [ i ][ 0 ] = f [ i -1 ][ 0 ]; } } for ( j = 1 ; j < n ; j ++ ) { if ( obstacleGrid [ 0 ][ j ] == 1 ) { f [ 0 ][ j ] = 0 ; } else { f [ 0 ][ j ] = f [ 0 ][ j -1 ]; } } for ( i = 1 ; i < m ; i ++ ) { for ( j = 1 ; j < n ; j ++ ) { if ( obstacleGrid [ i ][ j ] == 1 ) { f [ i ][ j ] = 0 ; } else { f [ i ][ j ] = f [ i ][ j -1 ] + f [ i -1 ][ j ]; } } } return f [ m -1 ][ n -1 ]; } }; C++ Naive DP Refactored class Solution { public : int uniquePathsWithObstacles ( vector < vector < int >>& obstacleGrid ) { int m = obstacleGrid . size (); int n = m > 0 ? obstacleGrid [ 0 ]. size () : 0 ; if ( m == 0 && n == 0 ) { return 0 ; } vector < vector < int >> f ( m , vector < int > ( n , 0 )); f [ 0 ][ 0 ] = 1 ; if ( obstacleGrid [ 0 ][ 0 ] == 1 ) { return 0 ; } for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { if ( obstacleGrid [ i ][ j ] == 1 ) { f [ i ][ j ] = 0 ; } else { // whenever available if ( i > 0 ) { f [ i ][ j ] += f [ i - 1 ][ j ]; } if ( j > 0 ) { f [ i ][ j ] += f [ i ][ j - 1 ]; } } } } return f [ m - 1 ][ n - 1 ]; } }; Java O(n) space // Optimized using a rolling array or single row dp array instead of m x n. class Solution { public int uniquePathsWithObstacles ( int [][] a ) { int m = a . length ; int n = a [ 0 ] . length ; int dp [] = new int [ n ] ; dp [ 0 ] = 1 ; //T[i][j] = T[i-1][j] + T[i][j-1] for ( int i = 0 ; i < m ; i ++ ){ for ( int j = 0 ; j < n ; j ++ ){ if ( a [ i ][ j ] == 1 ){ dp [ j ] = 0 ; } else if ( j > 0 ){ dp [ j ] += dp [ j - 1 ] ; } } } return dp [ n - 1 ] ; } }","title":"Unique Paths II"},{"location":"courses/9chap-dynamic-prog/notes/#paint-house","text":"DP last step: considering the optimal solution. The last paint must be one of the three colors and the paint cost is minimum. State: we can try to name f[i] the minimum cost of painting the first i houses, i = 0, 1, ... i-1 . However, we cannot know what color could be used for the last house. Change the state to: f[i][k] is the minium cost of painting the first i houses and the i-1 th element is painted as color k . This way, we can choose the last paint based on this piece of information. The state transition fomular: f[i][k] = f[i - 1][\\ell]_{\\ell != k} + costs[i - 1][k] f[i][k] = f[i - 1][\\ell]_{\\ell != k} + costs[i - 1][k] The result is minimum of the: f[i][0],\\ f[i][1],\\ f[i][2] f[i][0],\\ f[i][1],\\ f[i][2] class Solution { public : int minCost ( vector < vector < int >>& costs ) { int m = costs . size (); int f [ m + 1 ][ 3 ]; /* state: f[i][0] paint i house and the last (i - 1) house is red * /* initial value, paint 0 house cost 0 */ for ( int j = 0 ; j < 3 ; j ++ ) f [ 0 ][ j ] = 0 ; for ( int i = 1 ; i <= m ; i ++ ) { for ( int j = 0 ; j < 3 ; j ++ ) { // current color f [ i ][ j ] = INT_MAX ; for ( int k = 0 ; k < 3 ; k ++ ) { // painted color /* cannot paint same color for neighbor house */ if ( j == k ) continue ; // ith (index with i - 1) house is painted with color j if ( f [ i ][ j ] > f [ i - 1 ][ k ] + costs [ i - 1 ][ j ]) f [ i ][ j ] = f [ i - 1 ][ k ] + costs [ i - 1 ][ j ]; //f[i][j] = min(f[i - 1][k] + costs[i - 1][j], f[i][j]); } } } return min ( min ( f [ m ][ 0 ], f [ m ][ 1 ]), f [ m ][ 2 ]); } };","title":"Paint House"},{"location":"courses/9chap-dynamic-prog/notes/#decode-ways","text":"4 Ingredients Last step: A_0, A_1, A_2, ..., A_n-3, [A_n-2, A_n-1] A_0, A_1, A_2, ..., A_n-3, A_n-2, [A_n-1] State: f[n] : decode ways of first n letters. Smaller problem: f[n] = f[n - 1] + f[n - 2] | A_n-2,A_n-1 decodable. Init: f[0] = 1 Note Again the idea in this solution is to update the f[i][j] on demand, a technique that broadly used in 2-d coordinate based DP problems such as Bomb Enemy , Unique Paths II , and Minimum Path Sum . C++ DP class Solution { public : int numDecodings ( string s ) { int n = s . length (); int f [ n + 1 ] = { 0 }; if ( n == 0 ) return 0 ; f [ 0 ] = 1 ; for ( int i = 1 ; i <= n ; i ++ ) { int t = s [ i - 1 ] - '0' ; if ( t > 0 && t <= 9 ) { f [ i ] += f [ i - 1 ]; } if ( i > 1 ){ int q = ( s [ i - 2 ] - '0' ) * 10 + t ; if ( q >= 10 && q <= 26 ) { f [ i ] += f [ i - 2 ]; } } } return f [ n ]; } }; C++ DP O(1) class Solution { public : int numDecodings ( string s ) { if ( ! s . size () || s . front () == '0' ) return 0 ; // r2: decode ways of s[0, i-2] , r1: decode ways of s[0, i-1] int r1 = 1 , r2 = 1 ; // think it as a coordinate bases, not sequence based dp for ( int i = 1 ; i < s . size (); i ++ ) { // last char in s[0, i] is 0, cannot decode if ( s [ i ] == '0' ) r1 = 0 ; // two-digit letter, add r2 to r1, r2 get the previous r1 if ( s [ i - 1 ] == '1' || s [ i - 1 ] == '2' && s [ i ] <= '6' ) { r1 = r2 + r1 ; r2 = r1 - r2 ; } else { // one-digit letter, r2 get the previous r1 r2 = r1 ; } } return r1 ; } };","title":"Decode Ways"},{"location":"courses/9chap-dynamic-prog/notes/#longest-increasing-continuous-subsequence","text":"4 ingredients: Last step, last element a[n-1] could be in the result or not in the result. subproblem, suppose we have the LICS of the first n - 1 elements. represented as f[n-1] . base case and boundary condition, when no char in the string: f[0] = 1 . An empty string has LICS length of 1. order of calculation, calculate small index first. Not a leetcode The problem is not a leetcode problem, the original problem ask for sequences that could be increase or decrease. Using index in DP problems Avoid using both index i - 1 and i + 1 in a loop invariance, otherwise you'll have problems in keeping the loop invariance. Compare the following. Incorrect for ( int i = n - 1 ; i >= 0 ; i -- ) { f [ i ] = 1 ; if ( i < n - 1 && i > 0 && A [ i - 1 ] > A [ i ]) { f [ i ] = f [ i + 1 ] + 1 ; } res2 = max ( res2 , f [ i ]); } Correct for ( int i = n - 1 ; i >= 0 ; i -- ) { f [ i ] = 1 ; if ( i < n - 1 && A [ i ] > A [ i + 1 ]) { f [ i ] = f [ i + 1 ] + 1 ; } res2 = max ( res2 , f [ i ]); } Correct for ( int i = 0 ; i < n ; i ++ ) { f [ i ] = 1 ; if ( i > 0 && A [ i - 1 ] > A [ i ]) { f [ i ] = f [ i - 1 ] + 1 ; } res2 = max ( res2 , f [ i ]); } C++ DP solution class Solution { public : int longestIncreasingContinuousSubsequence ( vector < int >& A ) { // Write your code here int n = A . size (); int f [ n ]; int res1 = 0 ; int res2 = 0 ; for ( int i = 0 ; i < n ; i ++ ) { f [ i ] = 1 ; if ( i > 0 && A [ i - 1 ] < A [ i ]) { f [ i ] = f [ i - 1 ] + 1 ; } res1 = max ( res1 , f [ i ]); } for ( int i = n - 1 ; i >= 0 ; i -- ) { f [ i ] = 1 ; if ( i < n - 1 && A [ i ] > A [ i + 1 ]) { f [ i ] = f [ i + 1 ] + 1 ; } res2 = max ( res2 , f [ i ]); } return max ( res1 , res2 ); } }; Here we don't have to explicitly wirte f[i] = max(1, f[i - 1] + 1) , since we have the condition A[i - 1] < A[i] , we know the A[i] will be added to the f[i] , hence simply update f[i] = f[i - 1] + 1 directly.","title":"Longest Increasing Continuous Subsequence"},{"location":"courses/9chap-dynamic-prog/notes/#minimum-path-sum","text":"This is coordinate based DP. We need to have f[i][j] to keep the minimum path sum to the grid[i][j] . Calculate f[i][j] from top to down and from left to right for each row. Space can be optimized. Notice we can do the init and first row and first column in nested loops, do not need use multiple for loops. C++ DP class Solution { public : int minPathSum ( vector < vector < int > > & grid ) { int m = grid . size (); int n = grid [ 0 ]. size (); if ( m == 0 || n == 0 ) return 0 ; int f [ m ][ n ]; for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { if ( i == 0 && j == 0 ) { f [ i ][ j ] = grid [ i ][ j ]; } if ( i == 0 && j > 0 ) { f [ i ][ j ] = f [ i ][ j - 1 ] + grid [ i ][ j ]; } if ( j == 0 && i > 0 ) { f [ i ][ j ] = f [ i - 1 ][ j ] + grid [ i ][ j ]; } if ( i > 0 && j > 0 ) { f [ i ][ j ] = min ( f [ i - 1 ][ j ], f [ i ][ j - 1 ]) + grid [ i ][ j ]; } } } return f [ m - 1 ][ n - 1 ]; } }; C++ DP Space O(n) class Solution { public : int minPathSum ( vector < vector < int > > & grid ) { int m = grid . size (); int n = grid [ 0 ]. size (); if ( m == 0 || n == 0 ) return 0 ; int f [ 2 ][ n ]; /* only two rows of status */ int prev = 1 ; int curr = 1 ; for ( int i = 0 ; i < m ; i ++ ) { // rolling the f array when move to a new row prev = curr ; curr = 1 - curr ; for ( int j = 0 ; j < n ; j ++ ) { f [ curr ][ j ] = grid [ i ][ j ]; if ( i == 0 && j > 0 ) { f [ i ][ j ] += f [ i ][ j - 1 ]; } if ( j == 0 && i > 0 ) { f [ curr ][ j ] += f [ prev ][ j ]; } if ( i > 0 && j > 0 ) { f [ curr ][ j ] += min ( f [ prev ][ j ], f [ curr ][ j - 1 ]); } } } return f [ curr ][ n - 1 ]; } }; Note \u5728\u5904\u7406\u4e8c\u7ef4\u5750\u6807\u578bDP\u95ee\u9898\u7684\u65f6\u5019\uff0c\u5bf9\u4e8e\u7b2c\u4e00\u884c\u6216\u8005\u7b2c\u4e00\u5217\u7684\u8ba1\u7b97\u6211\u4eec\u53ef\u4ee5\u628a\u5b83\u653e\u5230loop\u4e2d\u3002\u4f46\u662f\u8981\u52a0\u4e00\u4e2a\u6761\u4ef6\u3002 \u8fd9\u4e2a\u6761\u4ef6\u5c31\u662f if (i > 0) , \u8fd9\u4e2a\u6761\u4ef6\u4fdd\u8bc1\u4e86\u5728\u66f4\u65b0\u6570\u7ec4\u503c\u7684\u65f6\u5019\u4e0b\u6807\u4e0d\u4f1a\u8d8a\u754c\u3002\u5e76\u4e14\u5f88\u597d\u7684\u8be0\u91ca\u4e86\u6211\u4eec\u7684\u52a8\u673a\uff08\u5c31\u662f\u5bf9\u4e8e\u7b2c\u4e00\u884c\u7b2c\u4e00\u5217\u4ec5\u4ec5\u662f\u521d\u59cb\u5316)\u3002 \u5bf9\u4e8e1\u4e2d\u63d0\u5230\u7684trick\uff0c\u53e6\u5916\u4e00\u4e2a\u597d\u5904\u5c31\u662f\u5bf9\u4e8e\u7b2c\u4e00\u884c\u6216\u8005\u7b2c\u4e00\u5217\u6761\u4ef6\u5224\u65ad\u7684\u65f6\u5019\uff0c\u8fd9\u91cc\u8fd8\u6709\u4e00\u4e2a\u6280\u5de7\u5c31\u662f\u5148\u628a\u81ea\u8eab\u7684\u2018\u6027\u8d28\u2019\u6216\u8005\u2018\u503c\u2019\u52a0\u4e0a\uff0c \u5982\u679c\u6709\u4e0a\u4e00\u884c\u6211\u4eec\u518d\u53bb\u52a0\u4e0a\u4e00\u884c\u7684\u2018\u6027\u8d28\u2019\u6216\u2018\u503c\u2019\u3002\u8fd9\u91cc\u9700\u8981\u9006\u5411\u601d\u7ef4. \u5728\u591a\u79cd\u60c5\u51b5\u9700\u8981\u5206\u5f00\u8ba8\u8bba\u7684\u65f6\u5019\u6211\u4eec\u53ef\u4ee5\u5148\u2018\u4e00\u7968\u5426\u51b3\u2019\u6700\u660e\u663e\u60c5\u51b5\uff0c\u7136\u540e\u8ba8\u8bba\u5269\u4e0b\u7684\u60c5\u51b5\u3002","title":"Minimum Path Sum"},{"location":"courses/9chap-dynamic-prog/notes/#bomb-enemy","text":"Breakdown the problem into smaller (simpler) problems. Take the special steps (i.e. different properties of the cell) as normal ones in the loop, deal with the special step when doing the calculation. class Solution { public : int maxKilledEnemies ( vector < vector < char >>& grid ) { int m = grid . size (); if ( m == 0 ) return 0 ; int n = grid [ 0 ]. size (); if ( n == 0 ) return 0 ; int f [ m ][ n ]; int res [ m ][ n ]; int ret = 0 ; for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { res [ i ][ j ] = 0 ; } } /* up */ for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { f [ i ][ j ] = 0 ; if ( grid [ i ][ j ] == 'W' ) { f [ i ][ j ] = 0 ; } else { if ( grid [ i ][ j ] == 'E' ) { f [ i ][ j ] = 1 ; } if ( i > 0 ) { f [ i ][ j ] += f [ i - 1 ][ j ]; } } res [ i ][ j ] += f [ i ][ j ]; } } /* down */ for ( int i = m - 1 ; i >= 0 ; i -- ) { for ( int j = 0 ; j < n ; j ++ ) { f [ i ][ j ] = 0 ; if ( grid [ i ][ j ] == 'W' ) { f [ i ][ j ] = 0 ; } else { if ( grid [ i ][ j ] == 'E' ) { f [ i ][ j ] = 1 ; } if ( i < m - 1 ) { f [ i ][ j ] += f [ i + 1 ][ j ]; } } res [ i ][ j ] += f [ i ][ j ]; } } /* left */ for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { f [ i ][ j ] = 0 ; if ( grid [ i ][ j ] == 'W' ) { f [ i ][ j ] = 0 ; } else { if ( grid [ i ][ j ] == 'E' ) { f [ i ][ j ] = 1 ; } if ( j > 0 ) { f [ i ][ j ] += f [ i ][ j - 1 ]; } } res [ i ][ j ] += f [ i ][ j ]; } } /* right */ for ( int i = 0 ; i < m ; i ++ ) { for ( int j = n - 1 ; j >= 0 ; j -- ) { f [ i ][ j ] = 0 ; if ( grid [ i ][ j ] == 'W' ) { f [ i ][ j ] = 0 ; } else { if ( grid [ i ][ j ] == 'E' ) { f [ i ][ j ] = 1 ; } if ( j < n - 1 ) { f [ i ][ j ] += f [ i ][ j + 1 ]; } } res [ i ][ j ] += f [ i ][ j ]; } } /* calculate resutls */ for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { if ( grid [ i ][ j ] == '0' ) if ( res [ i ][ j ] > ret ) ret = res [ i ][ j ]; } } return ret ; } };","title":"Bomb Enemy"},{"location":"courses/9chap-dynamic-prog/notes/#counting-bits","text":"Notice you can use the trick that i >> 1 == i / 2 to construct the subproblem i count 1 bits 1 1 2 1 3 2 6 2 12 2 25 3 50 3 class Solution { public : vector < int > countBits ( int num ) { vector < int > f ( num + 1 , 0 ); if ( num == 0 ) { return f ; } for ( int i = 1 ; i <= num ; i ++ ) { f [ i ] = f [ i >> 1 ] + ( i % 2 ); } return f ; } };","title":"Counting Bits"},{"location":"courses/9chap-dynamic-prog/notes/#lecture-3-dynamic-programming-on-sequences","text":"Problem Category Paint House II \u5e8f\u5217\u578b\uff0b\u72b6\u6001 House Robber \u5e8f\u5217\u578b\uff0b\u72b6\u6001 House Robber II \u5e8f\u5217\u578b\uff0b\u72b6\u6001 Best Time to Buy and Sell Stock Best Time to Buy and Sell Stock II Best Time to Buy and Sell Stock III \u5e8f\u5217\u578b\uff0b\u72b6\u6001 Best Time to Buy and Sell Stock IV \u5e8f\u5217\u578b\uff0b\u72b6\u6001 Longest Increasing Subsequence \u5e8f\u5217\u578b Russian Doll Envelopes","title":"Lecture 3 Dynamic Programming on Sequences"},{"location":"courses/9chap-dynamic-prog/notes/#_2","text":"\u7ed9\u5b9a\u4e00\u4e2a\u5e8f\u5217 \u52a8\u6001\u89c4\u5212\u65b9\u7a0b f[i] \u4e2d\u7684\u4e0b\u6807 i \u8868\u793a\u524d i \u4e2a\u5143\u7d20 a[0], a[1], ..., a[i-1] \u7684\u67d0\u79cd\u6027\u8d28 \u5750\u6807\u578b\u7684 f[i] \u8868\u793a\u4ee5 a[i] \u4e3a\u7ed3\u5c3e\u7684\u67d0\u79cd\u6027\u8d28 \u521d\u59cb\u5316\u4e2d\uff0c f[0] \u8868\u793a\u7a7a\u5e8f\u5217\u7684\u6027\u8d28 \u5750\u6807\u578b\u52a8\u6001\u89c4\u5212\u7684\u521d\u59cb\u6761\u4ef6 f[0] \u5c31\u662f\u6307\u4ee5 a[0] \u4e3a\u7ed3\u5c3e\u7684\u5b50\u5e8f\u5217\u7684\u6027\u8d28","title":"\u5e8f\u5217\u578b\u52a8\u6001\u89c4\u5212"},{"location":"courses/9chap-dynamic-prog/notes/#paint-house-ii","text":"the solution could be the same as the first version, but it is O(n\\cdot k^2) O(n\\cdot k^2) . how to make it O(n\\cdot k) O(n\\cdot k) ? By analyzing the state transition equation, we observed that we want to find a minimum value of a set of numbers except one for each house. Put it in English, the min cost to paint i - 1 th house with color k , we want to have the min cost of painting all previous i - 1 houses and the i-1 th house cannot be painted as color k . f [ i ][ 1 ] = min { f [ i -1 ][ 2 ] + cost [ i -1 ][ 1 ], f [ i -1 ][ 3 ] + cost [ i -1 ][ 1 ], ..., f [ i -1 ][ K ] + cost [ i -1 ][ 1 ]} f [ i ][ 2 ] = min { f [ i -1 ][ 1 ] + cost [ i -1 ][ 2 ], f [ i -1 ][ 3 ] + cost [ i -1 ][ 2 ], ..., f [ i -1 ][ K ] + cost [ i -1 ][ 2 ]} ... f [ i ][ K ] = min { f [ i -1 ][ 1 ] + cost [ i -1 ][ K ], f [ i -1 ][ 2 ] + cost [ i -1 ][ K ], ..., f [ i -1 ][ K -1 ] + cost [ i -1 ][ K ]} We could optimize the solution upon this. Basically, we can maintain the first two minimum value of the set f[i-1][1], f[i-1][2], f[i-1][3], ..., f[i-1][K] , min1 and min2 and their index j1 and j2 . There are two cases, the first case is the house i-1 is painted with the same color correspoinding to the minimum value. in this case, we cannot chose to paint it with the color corresponding to the minimum cost, we can update the state using the second minimum. The second case is that we paint the i-1th house with color other than the color corresponding to the minimum cost to pain, we can update using the minimum value. C++ O(nk) class Solution { public : /** * @param costs n x k cost matrix * @return an integer, the minimum cost to paint all houses */ int minCostII ( vector < vector < int >>& costs ) { int m = costs . size (); if ( m == 0 ) return 0 ; int k = costs [ 0 ]. size (); if ( k == 0 ) return 0 ; int f [ m + 1 ][ k ]; int min1 ; int min2 ; int j1 = 0 , j2 = 0 ; /* init, 0 house cost nothing. */ for ( int j = 0 ; j < k ; j ++ ) f [ 0 ][ j ] = 0 ; for ( int i = 1 ; i <= m ; i ++ ) { min1 = min2 = INT_MAX ; /* from all the colors, find the min1 and min2 */ for ( int j = 0 ; j < k ; j ++ ) { /* get the min1 and min2 first */ if ( f [ i - 1 ][ j ] < min1 ) { min2 = min1 ; j2 = j1 ; min1 = f [ i - 1 ][ j ]; j1 = j ; } else if ( f [ i - 1 ][ j ] < min2 ) { min2 = f [ i - 1 ][ j ]; j2 = j ; } } /* update the states based on the min1 and min2 */ for ( int j = 0 ; j < k ; j ++ ) { if ( j != j1 ) { f [ i ][ j ] = f [ i - 1 ][ j1 ] + costs [ i - 1 ][ j ]; } else { f [ i ][ j ] = f [ i - 1 ][ j2 ] + costs [ i - 1 ][ j ]; } } } int res = INT_MAX ; for ( int j = 0 ; j < k ; j ++ ) { if ( f [ m ][ j ] < res ) res = f [ m ][ j ]; } return res ; } }; C++ O(nk^2) class Solution { public : int minCostII ( vector < vector < int >>& costs ) { int n = costs . size (); int k = n > 0 ? costs [ 0 ]. size () : 0 ; if ( n == 0 && k == 0 ) return 0 ; if ( n == 1 && k == 1 ) return costs [ 0 ][ 0 ]; vector < vector < int >> f ( n + 1 , vector < int > ( k , 0 )); for ( int i = 0 ; i < k ; i ++ ) { f [ 0 ][ i ] = 0 ; // 0 cost for 0 paint } for ( int i = 1 ; i <= n ; i ++ ) { for ( int j = 0 ; j < k ; j ++ ) { f [ i ][ j ] = INT_MAX ; for ( int c = 0 ; c < k ; c ++ ) { if ( j == c ) { continue ; } if ( f [ i ][ j ] > f [ i - 1 ][ c ] + costs [ i - 1 ][ j ]) { f [ i ][ j ] = f [ i - 1 ][ c ] + costs [ i - 1 ][ j ]; } } } } int res = INT_MAX ; for ( int i = 0 ; i < k ; i ++ ) { if ( f [ n ][ i ] < res ) res = f [ n ][ i ]; } return res ; } };","title":"Paint House II"},{"location":"courses/9chap-dynamic-prog/notes/#house-robber","text":"Start with the last step. The last house could be either robbed or not. If the last house a[i-1] is robbed, then we cannot rob a[i-2] . we have f[i] = f[i-2] + a[i-1] . If the last house a[i-1] is not robbed, then we can rob a[i-2] . Alternatively, we can skip it to rob a[i-3] . We have f[i] = f[n-1] , in which this f[n-1] could not let us know whether a[i-2] is robbed or not. We add the state of a[i-1] to the state transition equation. Thus we have: f[i][0] represent \"the maximum money for robbing the first i houses and the last house hasn't been robbed.\" f[i][1] represent \"the maximum money for robbing the first i houses and the last house has been robbed.\" state transition equations: f[i][0] = max(f[i-1][0], f[i-1][1]) f[i][1] = f[i-1][0] + a[i-1] C++ class Solution { public : int rob ( vector < int >& nums ) { int n = nums . size (); if ( n == 0 ) return 0 ; int f [ n + 1 ][ 2 ] = { 0 }; for ( int i = 1 ; i <= n ; i ++ ) { f [ i ][ 0 ] = max ( f [ i - 1 ][ 0 ], f [ i - 1 ][ 1 ]); f [ i ][ 1 ] = f [ i - 1 ][ 0 ] + nums [ i - 1 ]; } return max ( f [ n ][ 0 ], f [ n ][ 1 ]); } }; C++ optimized class Solution { public : long long houseRobber ( vector < int > A ) { int n = A . size (); if ( n == 0 ) return 0 ; long long f [ n + 1 ]; /* init */ f [ 0 ] = 0 ; f [ 1 ] = A [ 0 ]; for ( int i = 2 ; i <= n ; i ++ ) { f [ i ] = max ( f [ i - 1 ], f [ i - 2 ] + A [ i - 1 ]); } return f [ n ]; } }; Solution 2 yes - first i days, robber last day, no - first i days, not robber at last day. class Solution { public : int rob ( vector < int >& nums ) { int n = nums . size (); if ( n == 0 ) return 0 ; int yes = 0 , no = 0 ; for ( int i = 1 ; i <= n ; i ++ ) { int tmp = no ; no = max ( yes , no ); yes = tmp + nums [ i - 1 ]; } return max ( yes , no ); } };","title":"House Robber"},{"location":"courses/9chap-dynamic-prog/notes/#house-robber-ii","text":"Following House Robber , when we try to analyze the last step, the house i - 1 depends on the house i - 2 and the house 0 . How could we handle this? We could enumerate two cases, and reduce the probelm to House Rober house 0 is robbed and house i-1 is not. house i - 1 is robbed and house 0 is not. class Solution { public : int rob ( vector < int >& nums ) { int n = nums . size (); if ( n == 0 ) return 0 ; if ( n == 1 ) // edge case to deal with return nums [ 0 ]; vector < int > nums1 ( nums . begin () + 1 , nums . end ()); vector < int > nums2 ( nums . begin (), nums . end () - 1 ); return max ( rob_helper ( nums1 ), rob_helper ( nums2 )); } private : int rob_helper ( vector < int >& A ) { int n = A . size (); if ( n == 0 ) return 0 ; int f [ n + 1 ] = { 0 }; f [ 0 ] = 0 ; f [ 1 ] = A [ 0 ]; for ( int i = 2 ; i <= n ; i ++ ) { f [ i ] = max ( f [ i - 1 ], f [ i - 2 ] + A [ i - 1 ]); } return f [ n ]; } };","title":"House Robber II"},{"location":"courses/9chap-dynamic-prog/notes/#best-time-to-buy-and-sell-stock","text":"Force youself to do it by only one pass. You'll find that you need two variables to record the minimum value currently found and the maximum profit currently found. class Solution { public : int maxProfit ( vector < int > & prices ) { int n = prices . size (); if ( n == 0 ) return 0 ; int min_price = INT_MAX ; int max_profit = 0 ; for ( int i = 0 ; i < n ; i ++ ) { if ( prices [ i ] < min_price ) { min_price = prices [ i ]; } else if ( prices [ i ] - min_price > max_profit ) { max_profit = prices [ i ] - min_price ; } } return max_profit ; } };","title":"Best Time to Buy and Sell Stock"},{"location":"courses/9chap-dynamic-prog/notes/#best-time-to-buy-and-sell-stock-ii","text":"We can just buy and sell when ever the price is increased in a day. Once you can proof the day-by-day subproblem can form the solution. The solution becomes so easy. C++ class Solution { public : int maxProfit ( vector < int > & prices ) { int res = 0 ; for ( int i = 1 ; i < prices . size (); i ++ ) { if ( prices [ i - 1 ] < prices [ i ]) res += prices [ i ] - prices [ i - 1 ]; } return res ; } }; C++ Alternative class Solution { public : int maxProfit ( vector < int > & prices ) { int res = 0 ; for ( int i = 1 ; i < prices . size (); i ++ ) { res += max ( prices [ i ] - prices [ i - 1 ], 0 ) } return res ; } };","title":"Best Time to Buy and Sell Stock II"},{"location":"courses/9chap-dynamic-prog/notes/#best-time-to-buy-and-sell-stock-iii","text":"We can define 5 stages and write state transition equations based on it. class Solution { public : /** * @param prices: Given an integer array * @return: Maximum profit */ int maxProfit ( vector < int > & A ) { int n = A . size (); if ( n == 0 ) { return 0 ; } int f [ n + 1 ][ 6 ]; /* init */ f [ 0 ][ 1 ] = 0 ; f [ 0 ][ 2 ] = f [ 0 ][ 3 ] = f [ 0 ][ 4 ] = f [ 0 ][ 5 ] = INT_MIN ; for ( int i = 1 ; i <= n ; i ++ ) { // stage 1, 3, 5: // f[i][j] = max{f[i - 1][j], f[i - 1][j - 1] + A[i - 1] - A[i - 2]} for ( int j = 1 ; j <= 5 ; j += 2 ) { f [ i ][ j ] = f [ i - 1 ][ j ]; if ( i > 1 && j > 1 && f [ i - 1 ][ j - 1 ] != INT_MIN ) { f [ i ][ j ] = max ( f [ i ][ j ], f [ i - 1 ][ j - 1 ] + A [ i - 1 ] - A [ i - 2 ]); } } // stage 2, 4: // f[i][j] = max{f[i - 1][j] + A[i - 1] - A[i - 2], f[i - 1][j - 1], f[i - 1][j - 2] + A[i - 1] - A[i - 2]} for ( int j = 2 ; j < 5 ; j += 2 ) { f [ i ][ j ] = f [ i - 1 ][ j - 1 ]; if ( i > 1 && f [ i - 1 ][ j ] != INT_MIN ) { f [ i ][ j ] = max ( f [ i ][ j ], f [ i - 1 ][ j ] + A [ i - 1 ] - A [ i - 2 ]); } if ( i > 1 && j > 2 && f [ i - 1 ][ j - 2 ] != INT_MIN ) { f [ i ][ j ] = max ( f [ i ][ j ], f [ i - 1 ][ j - 2 ] + A [ i - 1 ] - A [ i - 2 ]); } } } return max ( f [ n ][ 1 ], max ( f [ n ][ 3 ], f [ n ][ 5 ])); } };","title":"Best Time to Buy and Sell Stock III"},{"location":"courses/9chap-dynamic-prog/notes/#best-time-to-buy-and-sell-stock-iv","text":"If k is larger then n/2 , it is equivalent to the II. This is a generalized solution from Best Time to Buy and Sell Stock III . class Solution { public : int maxProfit ( int K , vector < int > & A ) { int n = A . size (); if ( n == 0 ) { return 0 ; } int f [ n + 1 ][ 2 * K + 1 + 1 ]; /* special case need to take care of */ if ( K > ( n / 2 )) { int res = 0 ; for ( int i = 0 ; i + 1 < n ; i ++ ) { if ( A [ i + 1 ] - A [ i ] > 0 ) { res += A [ i + 1 ] - A [ i ]; } } return res ; } /* init */ f [ 0 ][ 1 ] = 0 ; for ( int k = 2 ; k <= 2 * K + 1 ; k ++ ) { f [ 0 ][ k ] = INT_MIN ; } for ( int i = 1 ; i <= n ; i ++ ) { for ( int j = 1 ; j <= 2 * K + 1 ; j += 2 ) { f [ i ][ j ] = f [ i - 1 ][ j ]; if ( i > 1 && j > 1 && f [ i - 1 ][ j - 1 ] != INT_MIN ) { f [ i ][ j ] = max ( f [ i ][ j ], f [ i - 1 ][ j - 1 ] + A [ i - 1 ] - A [ i - 2 ]); } } for ( int j = 2 ; j < 2 * K + 1 ; j += 2 ) { f [ i ][ j ] = f [ i - 1 ][ j - 1 ]; if ( i > 1 && f [ i - 1 ][ j ] != INT_MIN ) { f [ i ][ j ] = max ( f [ i ][ j ], f [ i - 1 ][ j ] + A [ i - 1 ] - A [ i - 2 ]); } if ( i > 1 && j > 2 && f [ i - 1 ][ j - 2 ] != INT_MIN ) { f [ i ][ j ] = max ( f [ i ][ j ], f [ i - 1 ][ j - 2 ] + A [ i - 1 ] - A [ i - 2 ]); } } } int res = INT_MIN ; for ( int j = 1 ; j <= 2 * K + 1 ; j += 2 ) { if ( f [ n ][ j ] > res ) res = f [ n ][ j ]; } return res ; } };","title":"Best Time to Buy and Sell Stock IV"},{"location":"courses/9chap-dynamic-prog/notes/#longest-increasing-subsequence","text":"Comparing to the problem Longest Increasing Continuous Subsequence . If the subsequence is not continuous. we have to enumerate each of the previous element before A[j] . Solution 2 DP with binary search O(nlogn) To reduce the complexity, we can try to find if there is any redundant work we have been done. or some how we could use some order information to avoid some of the calculation. Focus on the real meaning of longest Increasing Subsequence. In fact, you are looking for the smallest value before A[i] that leads to the longest Increasing Subsequence so far. use the state f[i] to record the LIS of the array A[0], ... A[i -1] . If we are at f[j], j > i , we are looking for the largest f[i] value that have the smallest A[i] . Solution 3 DP with binary search refactored In observing the fact that we can use extra space to keep the \"minimum elements see so far from nums that is the last element of LIS for the different length of such LISes\". Different from the regular DP solution, our extra space b is storing element from nums, and the element stored in b are not necessary in order. The index i of elements in b related to the length of a LIS whose last element is a[i] . specifically, i + 1 = length(LIS) . Solution 4 C++ using lower_bound We can use the lower_bound to replace the binary search routine in the above solution. C++ DP O(n^2) class Solution { public : int lengthOfLIS ( vector < int >& nums ) { int n = nums . size (); if ( n == 0 ) return 0 ; int res = 0 ; int f [ n ] = { 0 }; for ( int j = 0 ; j < n ; j ++ ) { /* case 1: a[j] is the subsequence */ f [ j ] = 1 ; /* case 2: LIS from a[0],...a[i] plus a[j] */ for ( int i = 0 ; i < j ; i ++ ) { if ( nums [ i ] < nums [ j ] && f [ i ] + 1 > f [ j ]) f [ j ] = f [ i ] + 1 ; } res = max ( res , f [ j ]); } return res ; } }; C++ DP with binary search O(nlogn) class Solution { public : int longestIncreasingSubsequence ( vector < int > nums ) { int n = nums . size (); if ( n == 0 ) return 0 ; //int f[n]; // b[i]: when f value is i, smallest a value (ending value) int b [ n + 1 ]; int top = 0 ; b [ 0 ] = INT_MIN ; // O(n) for ( int i = 0 ; i < n ; i ++ ) { // b[0] ~ b[top] // last value b[j] which is smaller than A[i] int start = 0 , end = top ; int mid ; int j ; // O(lgn) while ( start <= end ) { mid = start + ( end - start ) / 2 ; if ( b [ mid ] < nums [ i ]) { j = mid ; start = mid + 1 ; } else { end = mid - 1 ; } } // b[i]: length is j (f value is j), smallest ending value. // A[i] is after it. // f[i] = j + 1 // b[j + 1]: length is (j + 1), smallest ending value. // A[i] // B[j + 1] >= A[i] b [ j + 1 ] = nums [ i ]; if ( j + 1 > top ) { top = j + 1 ; } } // b[top] stores the smallest ending value for an LIS return top ; } }; DP with binary search refactored class Solution { public : int lengthOfLIS ( vector < int >& nums ) { vector < int > b ; for ( int i = 0 ; i < nums . size (); ++ i ) { int begin = 0 , end = b . size (); while ( begin != end ) { int mid = begin + ( end - begin ) / 2 ; if ( b [ mid ] < nums [ i ]) { begin = mid + 1 ; } else { end = mid ; } } if ( begin == b . size ()) // nums[i] greater than all element in b b . push_back ( nums [ i ]); else // begin point to next element no less than the target nums[i]. b [ begin ] = nums [ i ]; } return b . size (); } }; C++ using lower_bound class Solution { public : int lengthOfLIS ( vector < int >& nums ) { vector < int > b ; for ( auto a : nums ) { auto it = lower_bound ( b . begin (), b . end (), a ); if ( it == b . end ()) b . push_back ( a ); else * it = a ; } return b . size (); } };","title":"Longest Increasing Subsequence"},{"location":"courses/9chap-dynamic-prog/notes/#russian-doll-envelopes","text":"Sort the vector in ascending order and identify the subproblem, apply DP. notice the sort is ascending, not descending. For the O(nlogn) solution, think of how to optimize the longest increasing subsequence for one dimension when the envelopes are sorted properly. We use b[k] to record the smallest A[i] value that have length of k longest increasing subsequence. The reason behind this can be illustrated by the following example, when i = 1 , we can forget A[0] = 5 , because what ever LIS A[0] = 5 can contribute to the LIS, A[1] = 2 will be able to contribute. A[i] 5, 2, 3, 1, 4 f[i] 1, 1, 2, 2, 3 b[k] 5 (|k| = 1, i = 0) b[k] 2 (|k| = 1, i = 1) b[k] 2, 3 (|k| = 2, i = 2) b[k] 2, 1 (|k| = 2, i = 3) b[k] 2, 1, 4 (|k| = 2, i = 4) In iterating of A[i] we binary search to find the smaller value than A[i] in b[k] so far that this smaller value combined with A[i] will form a new LIS. To keep the loop invariant, we need to change b[k] by either modify the existing value in b[k] or add a new value to the end. C++ DP O(n^2) class Solution { public : int maxEnvelopes ( vector < vector < int >>& envelopes ) { int n = envelopes . size (); int f [ n ]; int res = 0 ; sort ( envelopes . begin (), envelopes . end (), []( vector < int >& a , vector < int >& b ){ if ( a [ 0 ] == b [ 0 ]) { return a [ 1 ] < b [ 1 ]; } else { return a [ 0 ] < b [ 0 ]; } }); for ( int i = 0 ; i < n ; ++ i ) { f [ i ] = 1 ; for ( int j = 0 ; j < i ; ++ j ) { if ( envelopes [ j ][ 0 ] < envelopes [ i ][ 0 ] && envelopes [ j ][ 0 ] < envelopes [ i ][ 0 ]) { f [ i ] = max ( f [ i ], f [ j ] + 1 ); } } res = max ( res , f [ i ]); } return res ; } }; C++ DP (nlogn) class Solution { public : int maxEnvelopes ( vector < pair < int , int >>& envelopes ) { int n = envelopes . size (); if ( n == 0 ) return 0 ; vector < int > f ; sort ( envelopes . begin (), envelopes . end (), []( const pair < int , int > a , const pair < int , int > b ) { if ( a . first == b . first ) { return a . second > b . second ; } else { return a . first < b . first ; } }); for ( int i = 0 ; i < n ; ++ i ) { int t = envelopes [ i ]. second ; int begin = 0 , end = f . size (); // f start with empty // when i = 0, the binary search will not happen, first value will // be added to f before binary search on it. while ( begin < end ) { int mid = begin + ( end - begin ) / 2 ; if ( f [ mid ] < t ) { begin = mid + 1 ; } else { end = mid ; } } if ( begin == f . size ()) { f . push_back ( t ); } else { f [ begin ] = t ; // index to f is related to the result } } return f . size (); } };","title":"Russian Doll Envelopes"},{"location":"courses/9chap-dynamic-prog/notes/#lecture-4","text":"Problem category Perfect Squares \u5212\u5206\u578b\uff0f\u5e8f\u5217\u578b Palindrome Partitioning II \u5212\u5206\u578b Copy Books \u5212\u5206\u578b Coins in A Line \u535a\u5f08\u578b Backpack \u80cc\u5305\u578b Backpack V \u80cc\u5305\u578b Backpack VI \u80cc\u5305\u578b","title":"Lecture 4"},{"location":"courses/9chap-dynamic-prog/notes/#perfect-squares","text":"f[i] = \\min_{1 \\le j^2 \\le i}(f[i - j^2] + 1) f[i] = \\min_{1 \\le j^2 \\le i}(f[i - j^2] + 1) \u6574\u4f53\u601d\u60f3\u5c31\u662f\u679a\u4e3e j . \u672c\u9898\u5206\u6790\u65f6\u50cf\u5212\u5206\u578b\u4f46\u7a0b\u5e8f\u5199\u8d77\u6765\u50cf\u5e8f\u5217\u578b\u3002\u8fd8\u662f\u4ece\u6700\u540e\u4e00\u6b65\u5165\u624b\u3002\u8bbe\u6700\u540e\u4e00\u4e2a\u662f j^2 j^2 . \u6574\u4f53\u601d\u60f3\u5c31\u662f\u679a\u4e3e\u6700\u540e\u4e00\u6b65 j . \u6ce8\u610f\u521d\u59cb\u5316 f[i] . j*j \u53ef\u4ee5\u662f i , \u8fd9\u4e2a\u6b63\u597d\u5bf9\u5e94\u4e86\u6781\u7aef\u60c5\u51b5 \u5c31\u662f i \u53ea\u7531 j*j \u7ec4\u6210. if condition \u65e0\u9700\u68c0\u67e5 f[i - j*j] \u662f\u5426\u4e3a INT_MAX . \u56e0\u4e3a\u8fd9\u91cc\u6bd4 i \u5c0f\u7684 i - j*j \u603b\u662f\u53ef\u4ee5\u7531\u5b8c\u5168\u5e73\u65b9\u65701\u7ec4\u6210\u3002\u76f8\u6bd4\u8f83\u9898\u76ee Coin Change \u8fd9\u9053\u9898\uff0c\u8fd9\u4e00\u6b65\u53ef\u80fd\u6027\u68c0\u67e5\u5fc5\u4e0d\u53ef\u5c11\uff0c\u56e0\u4e3a\u5728 Coin Change \u4e2d\u67d0\u4e00\u9762\u503c\u4e0d\u4e00\u5b9a\u80fd\u88ab\u5151\u6362\u6210\u7ed9\u5b9a\u9762\u503c\u7684\u786c\u5e01\u3002\u6211\u4eec\u7528\u65e0\u7a77\u5927\u6765\u6807\u8bb0\u3002 C++ DP class Solution { public : int numSquares ( int n ) { if ( n == 0 ) return 0 ; int f [ n + 1 ] = { 0 }; f [ 0 ] = 0 ; for ( int i = 1 ; i <= n ; i ++ ) { f [ i ] = INT_MAX ; for ( int j = 1 ; j * j <= i ; j ++ ) { if ( f [ i - j * j ] + 1 < f [ i ]) { f [ i ] = f [ i - j * j ] + 1 ; } } } return f [ n ]; } }; C++ Python class Solution : def numSquares ( self , n : int ) -> int : if n == 0 : return 0 f = [ 0 ] * ( n + 1 ) for i in range ( 1 , n + 1 ): f [ i ] = float ( 'inf' ) for j in range ( 1 , int ( sqrt ( i )) + 1 ): if f [ i ] > f [ i - j * j ] + 1 : f [ i ] = f [ i - j * j ] + 1 return f [ n ]","title":"Perfect Squares"},{"location":"courses/9chap-dynamic-prog/notes/#palindrome-partitioning-ii","text":"Consider the last palindrome s[j, ..., i - 1] , f[i] represent the minimum number of partitions of first i characters. State transition equation: f[i] = \\min_{0 \\le j \\lt i}(f[j]+1 | (s[j],..., s[i-1]) \\text{ is palindrome}) f[i] = \\min_{0 \\le j \\lt i}(f[j]+1 | (s[j],..., s[i-1]) \\text{ is palindrome}) To know the substr s[j, ... i-1] is a palindrome or not, we can first find and record all the panlidrome substring in isPalin[i][j] , each element of which represents whether substr s[i, ..., j] is a palindrome or not. C++ DP with isPalin() class Solution { public : int minCut ( string s ) { int n = s . length (); if ( n == 0 ) return 0 ; int isPalin [ n ][ n ]; int f [ n + 1 ]; f [ 0 ] = 0 ; for ( int i = 0 ; i < n ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { isPalin [ i ][ j ] = 0 ; } } /* calculate palindrome */ for ( int t = 0 ; t < n ; t ++ ) { /* odd case */ int i = t ; int j = t ; while ( i >= 0 && j < n && s [ i ] == s [ j ]) { isPalin [ i ][ j ] = 1 ; i -- ; j ++ ; } /* even case */ i = t ; j = t + 1 ; while ( i >= 0 && j < n && s [ i ] == s [ j ]) { isPalin [ i ][ j ] = 1 ; i -- ; j ++ ; } } /* calculate the states */ for ( int i = 1 ; i <= n ; i ++ ) { f [ i ] = INT_MAX ; for ( int j = 0 ; j < i ; j ++ ) { if ( isPalin [ j ][ i - 1 ]) { f [ i ] = min ( f [ i ], f [ j ] + 1 ); } } } return f [ n ] - 1 ; } }; Java isPalin() public class Solution { /** * @param s a string * @ f[i], minmum cut of s[0],...s[i - 1] * e.g. * a a b a a * j=0 1 2 3 4 *i=0 a 1 1 0 * 1 a 1 0 * 2 b 1 0 * 3 a 1 0 * 4 a 1 * * when calculate isPalin[i][j], we need to know isPalin[i + 1][j + 1]. * Cannot calculate from i = 0, 1, ... have to go from i = n - 1, n - 2, ... 1, 0 * last step: s[j],...s[i - 1] * f[i] = f[j] + 1 | s[j], ... s[i - 1] is palindrome */ public int minCut ( String ss ) { char [] s = ss . toCharArray (); int n = s . length ; if ( n == 0 ) return 0 ; boolean [][] isPalin = new boolean [ n ][ n ] ; int [] f = new int [ n + 1 ] ; f [ 0 ] = - 1 ; for ( int i = 0 ; i < n ; i ++ ) { isPalin [ i ][ i ] = true ; if ( i + 1 < n ) { isPalin [ i ][ i + 1 ] = s [ i ] == s [ i + 1 ] ; } } for ( int i = n - 3 ; i >= 0 ; i -- ) { for ( int j = i + 2 ; j < n ; j ++ ) { isPalin [ i ][ j ] = isPalin [ i + 1 ][ j - 1 ] && s [ i ] == s [ j ] ; } } /* calculate the states */ for ( int i = 1 ; i <= n ; i ++ ) { f [ i ] = i - 1 ; for ( int j = 0 ; j < i ; j ++ ) { if ( isPalin [ j ][ i - 1 ] ) { f [ i ] = Math . min ( f [ i ] , f [ j ] + 1 ); } } } return f [ n ] ; } }; Warning \u8fd9\u4e2a\u65b9\u6cd5C++\u4e0d\u80fd\u901a\u8fc7Leetcode\u548clintcode\uff0c\u4e3b\u8981\u662f\u5bf9\u4e8e\u5f88\u957f\u7684string\uff0c isPalin \u4f1a\u5f88\u5927\u3002\u7c7b\u4f3c\u4e8e Best Time to Buy and Sell Stock IV \u4e2d\u5982\u679ck\u5f88\u5927\u7684\u65f6\u5019\u3002Java\u7248\u672c\u5219\u53ef\u4ee5\u3002 Note \u6280\u5de7\uff1a\u5728\u5faa\u73af\u4e2d\u9700\u8981\u67d0\u4e2a\u6027\u8d28\uff0c\u800c\u8fd9\u4e2a\u6027\u8d28\u53c8\u662f\u53ef\u4ee5\u5f88\u5bb9\u6613\u8ba1\u7b97\u548c\u8bb0\u5f55 (isPalin) \u6211\u4eec\u5c31\u91c7\u7528\u5148\u8ba1\u7b97\u6240\u6709\u5e76\u8bb0\u5f55\uff0c\u5728loop\u4e2d\u53bb\u76f4\u63a5\u8bbf\u95ee\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c\u5373\u53ef\u3002","title":"Palindrome Partitioning II"},{"location":"courses/9chap-dynamic-prog/notes/#copy-books","text":"","title":"Copy Books"},{"location":"courses/9chap-dynamic-prog/notes/#coins-in-a-line","text":"\u8981\u6709\u5148\u624b\u201c\u5fc5\u80dc\u201d\u548c\u201c\u5fc5\u8d25\u201d\u7684\u6982\u5ff5\u3002\u7b80\u5355\u6765\u8bb2\u5c31\u662f\u5f53\u524d\u5148\u624b\u9762\u5bf9\u5f53\u524d\u5c40\u52bf\u5982\u679c\u6709\u4e00\u62db\u80fd\u4f7f\u4e0b\u8f6e\u5148\u624b\u5fc5\u8d25 \u90a3\u4e48\u5f53\u524d\u9009\u624b\u5c31\u5fc5\u80dc\u3002\u5982\u679c\u5f53\u524d\u9009\u624b\u9762\u5bf9\u5f53\u524d\u5c40\u52bf\u4e0b\u6240\u6709\u62db\u6570\u90fd\u4f1a\u4f7f\u5f97\u4e0b\u8f6e\u5148\u624b\u5fc5\u80dc\uff0c\u90a3\u4e48\u5f53\u524d\u9009\u624b\u5fc5\u8d25\u3002 f[i] \u8868\u793a\u5f53\u524d\u9009\u624b\u5fc5\u80dc(True)\u6216\u5fc5\u8d25(False). State transition equation: f[i] = f[i - 1] == \\text{false}\\ OR\\ f[i - 2] == \\text{false} f[i] = f[i - 1] == \\text{false}\\ OR\\ f[i - 2] == \\text{false} \u6b64\u9898\u6ce8\u610f\u521d\u59cb\u6761\u4ef6\u548c\u5fc5\u80dc\u548c\u5fc5\u8d25\u7684\u6982\u5ff5\u5c31\u4e0d\u4f1a\u6709\u4ec0\u4e48\u5dee\u9519\u3002 DB solution class Solution { public : bool firstWillWin ( int n ) { if ( n == 0 ) return false ; bool f [ n ]; f [ 0 ] = false ; for ( int i = 1 ; i <= 2 ; i ++ ) { f [ i ] = true ; } for ( int i = 3 ; i <= n ; i ++ ) { f [ i ] = ( f [ i - 1 ] == false ) || ( f [ i - 2 ] == false ); } return f [ n ]; } }; Math solution // You can actualy found the pattern if you have written several states. f[i] = i % 3 // 0 1 2 3 4 5 6 7 8 9 10 // F T T F T T F T T F T class Solution { public : bool firstWillWin ( int n ) { return n % 3 ; } };","title":"Coins in A Line"},{"location":"courses/9chap-dynamic-prog/notes/#backpack","text":"Give n items with size A[i] and backpack size: M . Find the max total size that can fit in the backpack. We can reduce the size of f to two rolls. We can even reduce it to just one dimention. same technique can also be used in Backpack II and Backpack V. C++ DP class Solution { public : int backPack ( int m , vector < int > A ) { int n = A . size (); if ( n == 0 ) return 0 ; bool f [ n + 1 ][ m + 1 ]; /* init */ f [ 0 ][ 0 ] = true ; for ( int j = 1 ; j <= m ; j ++ ) { f [ 0 ][ j ] = false ; } for ( int i = 1 ; i <= n ; i ++ ) { for ( int j = 0 ; j <= m ; j ++ ) { f [ i ][ j ] = f [ i - 1 ][ j ]; /* must first init f[i][j] */ if ( j >= A [ i - 1 ]) { f [ i ][ j ] = f [ i ][ j ] || f [ i - 1 ][ j - A [ i - 1 ]]; //f[i][j] = f[i - 1][j] || f[i - 1][j - A[i - 1]]; /* this is wrong */ } } } for ( int j = m ; j >= 0 ; j -- ) { if ( f [ n ][ j ] == true ) return j ; } return 0 ; } }; C++ DP log(n) space class Solution { public : int backPack ( int m , vector < int > A ) { int n = A . size (); if ( n == 0 ) return 0 ; bool f [ m + 1 ]; /* init */ f [ 0 ] = true ; for ( int j = 1 ; j <= m ; j ++ ) { f [ j ] = false ; } for ( int i = 1 ; i <= n ; i ++ ) { for ( int j = m ; j >= 0 ; j -- ) { if ( j >= A [ i - 1 ]) { f [ j ] = f [ j ] || f [ j - A [ i - 1 ]]; } } } for ( int j = m ; j >= 0 ; j -- ) { if ( f [ j ] == true ) return j ; } return 0 ; } };","title":"Backpack"},{"location":"courses/9chap-dynamic-prog/notes/#backpack-v","text":"This problem could follow the analysis from Backpack. Instead of memorize the boolean value whether first i items can fill the weight w . We record in f[i][w] the total number of possible fills of the weight w by first i items. We need to initialize the state f[i][j] first then to update it. C++ DP class Solution { public : int backPackV ( vector < int >& nums , int T ) { int n = nums . size (); if ( n == 0 ) { return 0 ; } int f [ n + 1 ][ T + 1 ]; f [ 0 ][ 0 ] = 1 ; for ( int j = 1 ; j <= T ; j ++ ) { f [ 0 ][ j ] = 0 ; } for ( int i = 1 ; i <= n ; i ++ ) { for ( int j = 0 ; j <= T ; j ++ ) { f [ i ][ j ] = f [ i - 1 ][ j ]; if ( j >= nums [ i - 1 ]) { f [ i ][ j ] += f [ i - 1 ][ j - nums [ i - 1 ]]; } } } return f [ n ][ T ]; } }; C++ DP log(n) space class Solution { public : int backPackV ( vector < int >& nums , int T ) { int n = nums . size (); if ( n == 0 ) { return 0 ; } int f [ T + 1 ]; f [ 0 ] = 1 ; for ( int j = 1 ; j <= T ; j ++ ) { f [ j ] = 0 ; } for ( int i = 1 ; i <= n ; i ++ ) { for ( int j = T ; j >= 0 ; j -- ) { //f[j] = f[j - A[i - 1]] ==> f'[j] if ( j >= nums [ i - 1 ]) { // f'[j] // cover old f[j] f [ j ] += f [ j - nums [ i - 1 ]]; } } } return f [ T ]; } };","title":"Backpack V"},{"location":"courses/9chap-dynamic-prog/notes/#backpack-vi","text":"\u8fd9\u9053\u9898\u7b49\u540c\u4e8eLeetcode\u91cc Combinations Sum IV \u8fd9\u91cc\u53ef\u4ee5\u968f\u4fbf\u53d6\uff0c\u4f3c\u4e4e\u9898\u76ee\u53d8\u5f97\u65e0\u6cd5\u4e0b\u624b\uff0c\u8003\u8651\u201c\u6700\u540e\u4e00\u6b65\u201d\u8fd9\u4e2a\u6280\u5de7\u4e0d\u80fd\u7528\u4e86\uff0c\u56e0\u4e3a\u6700\u540e\u4e00\u6b65\u53ef\u4ee5\u662f\u4efb\u610f\u4e00\u4e2a\u4e86\u3002 \u4f46\u4ecd\u7136\u53ef\u4ee5\u7528\u5b50\u95ee\u9898\u6765\u8003\u8651\u3002\u5148\u4e0d\u7ba1\u6700\u540e\u4e00\u6b65\u662f\u54ea\u4e00\u4e2a\uff0c\u6700\u540e\u4e00\u6b65\u4e4b\u524d\u7684\u76f8\u52a0\u7684\u603b\u548c\u4e00\u5b9a\u662f Target - x . \u8fd9\u6837\u5c31\u8f6c\u5316\u6210\u4e00\u4e2a\u5b50\u95ee\u9898\u53ef\u4ee5\u7528DP\u6765\u505a\u3002 \u5177\u4f53\u505a\u6cd5\u6211\u4eec\u53ef\u4ee5\u5bf9\u4e8e\u6bcf\u4e00\u4e2a\u5c0f\u4e8e\u201c\u603b\u627f\u91cd\u201d\u7684\u91cd\u91cf\u8fdb\u884c\u679a\u4e3e\u6700\u540e\u4e00\u6b65 x . \u53ef\u80fd\u7684 x \u662f A[0], ..., A[i - 1] \u4e2d\u4efb\u610f\u4e00\u4e2a. Must initialize f[i] = 0 . Because some of the state won't be updated (indicate they are not possible to be fill). C++ DP class Solution { public : int backPackVI ( vector < int >& nums , int T ) { int n = nums . size (); if ( n == 0 ) { return 0 ; } int f [ T + 1 ]; f [ 0 ] = 1 ; /* for each sub problem */ for ( int i = 1 ; i <= T ; i ++ ) { f [ i ] = 0 ; /* enumerate the last step */ for ( int j = 0 ; j < n ; j ++ ) { if ( i >= nums [ j ]) { f [ i ] += f [ i - nums [ j ]]; } } } return f [ T ]; } }; Output result // f[i]: \u5b58\u591a\u5c11\u79cd\u65b9\u5f0f // pi[i]: \u5982\u679c f[i] >= 1, \u6700\u540e\u4e00\u4e2a\u6570\u5b57\u53ef\u4ee5\u662fpi[i] class Solution { public : int backPackVI ( vector < int >& nums , int T ) { int n = nums . size (); if ( n == 0 ) { return 0 ; } int f [ T + 1 ]; /* pi[i]: \u5982\u679ci\u53ef\u62fc\u51fa(f[i] >= 1), \u6700\u540e\u4e00\u4e2a\u662fpi[i] */ int pi [ T + 1 ]; f [ 0 ] = 1 ; for ( int i = 1 ; i <= T ; i ++ ) { f [ i ] = 0 ; for ( int j = 0 ; j < n ; j ++ ) { if ( i >= nums [ j ]) { f [ i ] += f [ i - nums [ j ]]; /* \u6700\u540e\u4e00\u4e2a\u662fnums[j]\u7684\u53ef\u62fc\u51fai */ if ( f [ i - nums [ j ]] > 0 ) { /* \u7eaa\u5f55\u4e0b\u6765 */ pi [ i ] = nums [ j ]; } } } } if ( f [ T ] > 0 ) { int i = T ; cout << i << \"=\" << endl ; while ( i != 0 ) { // sum is i now; // last number is pi[i] // previuos sum is i - pi[i] cout << pi [ i ] << endl ; i -= pi [ i ]; } } return f [ T ]; } };","title":"Backpack VI"},{"location":"courses/9chap-dynamic-prog/notes/#summary-dynamic-programming-types","text":"","title":"Summary Dynamic Programming Types"},{"location":"courses/9chap-dynamic-prog/notes/#_3","text":"\u7ed9\u5b9a\u957f\u5ea6\u4e3aN\u7684\u5e8f\u5217\u6216\u5b57\u7b26\u4e32\uff0c\u8981\u6c42\u5212\u5206\u6210\u82e5\u5e72\u6bb5 \u6bb5\u6570\u4e0d\u9650\uff0c\u6216\u6307\u5b9aK\u6bb5 \u6bcf\u4e00\u6bb5\u6ee1\u8db3\u4e00\u5b9a\u7684\u6027\u8d28 \u7c7b\u4f3c\u4e8e\u5e8f\u5217\u578b\u52a8\u6001\u89c4\u5212\uff0c\u4f46\u662f\u901a\u5e38\u8981\u52a0\u4e0a\u6bb5\u6570\u4fe1\u606f \u7c7b\u4f3c\u4e8e\u5e8f\u5217\u578b\u52a8\u6001\u89c4\u5212\uff0c\u4f46\u662f\u901a\u5e38\u8981\u52a0\u4e0a\u6bb5\u6570\u4fe1\u606f \u4e00\u822c\u7528 f[i][j] \u8bb0\u5f55\u524di\u4e2a\u5143\u7d20(\u5143\u7d200~i-1)\u5206\u6210j\u6bb5\u7684\u6027\u8d28\uff0c\u5982\u6700\u5c0f\u4ee3\u4ef7 \u628a\u72b6\u6001 f[i] \u6216 f[i][j] \u521d\u59cb\u5316\u4e3a\u6781\u5927\u503c\u6216\u6781\u5c0f\u503c\u3002\u4e00\u65b9\u9762\uff0c\u8fd9\u6837\u6709\u5229\u4e8e\u5728\u66f4\u65b0\u72b6\u6001\u65f6\u8ba1\u7b97\u6700 \u5927\u6216\u6700\u5c0f\u503c\uff1b\u53e6\u4e00\u65b9\u9762\uff0c\u5728\u66f4\u65b0\u72b6\u6001\u662f\u53ef\u4ee5\u901a\u8fc7\u67e5\u770b\u5b50\u72b6\u6001\u662f\u5426\u4e3a\u6781\u503c\u6765\u5224\u65ad\u5f53\u524d\u4e00\u6b65\u66f4\u65b0\u662f\u5426\u6709\u610f\u4e49\u3002 \u5178\u578b\u9898\u76ee\uff1a1. Perfect Squares, \u5168\u90e8\u72b6\u6001\u90fd\u6709\u610f\u4e49\uff0c\u53ef\u7701\u7565\u68c0\u67e5\u30022. Coin Change \u6709\u4e9b \u6570\u989d\u662f\u65e0\u6cd5\u7528\u7ed9\u5b9a\u9762\u503c\u7684\u786c\u5e01\u5151\u6362\uff0c\u66f4\u65b0\u524d\u9700\u68c0\u67e5\u72b6\u6001\u662f\u5426\u4e3a\u6781\u81f4\u4ee5\u5224\u65ad\u5f53\u524d\u66f4\u65b0\u662f\u5426\u6709\u610f\u4e49\u3002 \u5212\u5206\u6027\u52a8\u6001\u89c4\u5212\u603b\u7ed3 \u8981\u6c42\u5c06\u4e00\u4e2a\u5e8f\u5217\u6216\u5b57\u7b26\u4e32\u5212\u5206\u6210\u82e5\u5e72\u6ee1\u8db3\u8981\u6c42\u7684\u7247\u6bb5 \u89e3\u51b3\u65b9\u6cd5\uff1a\u6700\u540e\u4e00\u6b65/\u6700\u540e\u4e00\u6bb5 \u679a\u4e3e\u6700\u540e\u4e00\u6bb5\u7684\u8d77\u70b9 \u5982\u679c\u9898\u76ee\u4e0d\u6307\u5b9a\u6bb5\u6570\uff0c\u7528 f[i] \u8868\u793a\u524d i \u4e2a\u5143\u7d20\u5206\u6bb5\u540e\u7684\u53ef\u884c\u6027/\u6700\u503c\uff0c\u53ef\u884c\u6027\uff0c\u65b9\u5f0f\u6570\uff1a Perfect Squares, Palindrome Partition II \u5982\u679c\u9898\u76ee\u6307\u5b9a\u6bb5\u6570\uff0c\u7528 f[i][j]\u8868\u793a\u524d i \u4e2a\u5143\u7d20\u5206\u6210 j \u6bb5\u540e\u7684\u53ef\u884c\u6027/\u6700\u503c\uff0c\u53ef\u884c\u6027\uff0c\u65b9\u5f0f\u6570\uff1aCopy Books \u80cc\u5305\u578b\u52a8\u6001\u89c4\u5212\u7c7b\u578b \u5e03\u5c14\u503c \u8ba1\u6570 \u6700\u503c \u5355\u4e00\u7269\u54c1 \u65e0\u9650\u591a\u7269\u54c1 \u80cc\u5305\u578b\u52a8\u6001\u89c4\u5212\u7a7a\u95f4\u4f18\u5316 \u6eda\u52a8\u6570\u7ec4 \u5355\u4e2a\u6570\u7ec4 \u80cc\u5305\u578b\u52a8\u6001\u89c4\u5212\u603b\u7ed3 \u80cc\u5305\u95ee\u9898\u7684\u6570\u7ec4\u5927\u5c0f\u4e0e\u603b\u627f\u91cd\u6709\u5173 Backpack \u53ef\u884c\u6027\u80cc\u5305 \u8981\u6c42\u4e0d\u8d85\u8fc7Target\u65f6\u80fd\u62fc\u51fa\u7684\u6700\u5927\u91cd\u91cf \u8bb0\u5f55\u524di\u4e2a\u7269\u54c1\u80fd\u4e0d\u80fd\u62fc\u51fa\u91cd\u91cfw Backpack V, Backpack VI, \u8ba1\u6570\u578b\u80cc\u5305 \u8981\u6c42\u6709\u591a\u5c11\u79cd\u65b9\u5f0f\u62fc\u51fa\u91cd\u91cfTarget Backpack V\uff1a\u8bb0\u5f55\u524di\u4e2a\u7269\u54c1\u6709\u591a\u5c11\u79cd\u65b9\u5f0f\u62fc\u51fa\u91cd\u91cfw Backpack VI\uff1a\u8bb0\u5f55\u6709\u591a\u5c11\u79cd\u65b9\u5f0f(\u53ef\u91cd\u590d)\u62fc\u51fa\u91cd\u91cfw \u5173\u952e\u70b9 \u6700\u540e\u4e00\u6b65 \u6700\u540e\u4e00\u4e2a\u80cc\u5305\u5185\u7684\u7269\u54c1\u662f\u54ea\u4e2a \u6700\u540e\u4e00\u4e2a\u7269\u54c1\u6709\u6ca1\u6709\u8fdb\u80cc\u5305 Note \u6ce8\u610f\u5728loop\u4e2d\u8981\u5148\u521d\u59cb\u5316 f[i][j] , \u7136\u540e\u518d\u53bb\u66f4\u65b0\uff0c\u5426\u8005\u5982\u679cif\u8bed\u53e5\u5224\u65adfalse\u7684\u65f6\u5019\u662f\u6ca1\u6cd5\u66f4\u65b0 f[i][j] \u7684\u3002\u4f8b\u5982\u9898\u76ee\uff1aBackpack \u548c Backpack V","title":"\u5212\u5206\u578b\u52a8\u6001\u89c4\u5212\u7279\u70b9"},{"location":"courses/9chap-dynamic-prog/notes/#lecture-5","text":"Problem Category Backpack II \u80cc\u5305\u578b Backpack III \u80cc\u5305\u578b Coins in A Line III \u535a\u5f08\u578b Longest Palindromic Subsequence \u533a\u95f4\u578b Burst Balloons \u533a\u95f4\u578b Scramble String \u533a\u95f4\u578b","title":"Lecture 5"},{"location":"courses/9chap-dynamic-prog/notes/#backpack-ii","text":"Given n items with size A[i] and value V[i] , and a backpack with size m . What's the maximum value can you put into the backpack? Example Given 4 items with size [2, 3, 5, 7] and value [1, 5, 2, 4] , and a backpack with size 10. The maximum value is 9. \u601d\u8003\u65b9\u5f0f\u4efb\u7136\u662f\u4ece\u6700\u540e\u4e00\u4e2a\u7269\u54c1\u9009\u8fd8\u662f\u4e0d\u9009\uff0c\u53ea\u662f\u6211\u4eec\u73b0\u5728\u8003\u8651\u7684\u662f\u4ef7\u503c\u3002\u6b64\u65f6\u72b6\u6001\u5c31\u4e0d\u80fd\u662f\u53ef\u884c\u6027 Backpack \u6216\u8005\u591a\u5c11\u79cd\u4e86 Backpack V , \u6211\u4eec\u8981\u7eaa\u5f55\u603b\u4ef7\u503c\u3002 State: f[i][w] represent the value of the first i items that weight w . State transition equation: f[i][w] = max(f[i - 1][w],\\ f[i - 1][w - A[i - 1]] + V[i - 1] | w \u2265 A[i-1] \\text{\u4e14}\\ f[i-1][w-A[i-1]] \\neq -1) f[i][w] = max(f[i - 1][w],\\ f[i - 1][w - A[i - 1]] + V[i - 1] | w \u2265 A[i-1] \\text{\u4e14}\\ f[i-1][w-A[i-1]] \\neq -1) Initialization: f[0][0] = 0 , f[0][1] = -1, ... f[0][w] = -1 . -1 \u4ee3\u8868\u4e0d\u80fd\u88ab\u62fc\u51fa\u3002 C++ DP O(n^2) space class Solution { public : int backPackII ( vector < int > A , vector < int > V , int m ) { int n = A . szie (); if ( n == 0 ) return 0 ; int f [ n + 1 ][ m + 1 ]; f [ 0 ][ 0 ] = 0 ; for ( int j = 1 ; j < m ; ++ j ) { f [ 0 ][ j ] = -1 ; } for ( int i = 1 ; i <= n ; ++ i ) { for ( int j = 1 ; j <= m ; ++ j ) { f [ i ][ j ] = f [ i - 1 ][ j ]; if ( f [ i - 1 ][ j - A [ i - 1 ]] != -1 && j >= A [ i - 1 ]) { f [ i ][ j ] = max ( f [ i ][ j ], f [ i - 1 ][ j - A [ i - 1 ]] + V [ i - 1 ]); } } } int res = 0 ; for ( int j = 0 ; j <= m ; ++ j ) { if ( f [ n ][ j ] != -1 && f [ n ][ j ] > res ) { res = f [ n ][ j ]; } } return res ; } }; C++ DP O(n) space class Solution { public : int backPackII ( vector < int > A , vector < int > V , int m ) { int n = A . szie (); if ( n == 0 ) return 0 ; int f [ m + 1 ]; f [ 0 ] = 0 ; for ( int j = 1 ; j < m ; ++ j ) { f [ 0 ][ j ] = -1 ; } for ( int i = 1 ; i <= n ; ++ i ) { for ( int j = m ; j >= 0 ; -- j ) { if ( f [ j - A [ i - 1 ]] != -1 && j >= A [ i - 1 ]) { f [ j ] = max ( f [ j ], f [ j - A [ i - 1 ]] + V [ i - 1 ]); } } } int res = 0 ; for ( int j = 0 ; j <= m ; ++ j ) { if ( f [ j ] != -1 && f [ j ] > res ) { res = f [ j ]; } } return res ; } };","title":"Backpack II"},{"location":"courses/9chap-dynamic-prog/notes/#backpack-iii","text":"Given n kind of items with size A[i] and value V[i] ( each item has an infinite number available) and a backpack with size m . What's the maximum value can you put into the backpack? Example Given 4 items with size [2, 3, 5, 7] and value [1, 5, 2, 4] , and a backpack with size 10. The maximum value is 15. We can follow the \"last step\" principle, assume the last item in and not in the backpack. But when we do this, we notice that the last one item could also be in the previous because we have infinite times of the item available. We cannot proceed. Instead of thinking the \"last one\" in the final answer, we can think the \"last one\" that has been selected. This paradigm shift enables use to tackle the problem more cleverly. We can enumerate the last type (not the last one in the final result) of items we can select. State: f[i][w] : \u524d i \u79cd \u7269\u54c1\u80fd\u591f\u62fc\u6210\u91cd\u91cf\u4e3a w \u7684\u6700\u5927\u4ef7\u503c Transition equation: f[i][w] = \\max_{k \\ge 0}{f[i-1[w], f[i - 1][j - k A[i -1]] + k V[i-1]} f[i][w] = \\max_{k \\ge 0}{f[i-1[w], f[i - 1][j - k A[i -1]] + k V[i-1]} class Solution { public : int backPackIII ( vector < int >& A , vector < int >& V , int m ) { int n = A . size (); if ( n == 0 ) { return 0 ; } int f [ m + 1 ]; /* init */ f [ 0 ] = 0 ; for ( int j = 1 ; j <= m ; j ++ ) { f [ j ] = -1 ; } for ( int i = 1 ; i <= n ; i ++ ) { for ( int j = 0 ; j <= m ; j ++ ) { if ( f [ j - A [ i - 1 ]] != -1 && j >= A [ i - 1 ]) { f [ j ] = max ( f [ j ], f [ j - A [ i - 1 ]] + V [ i - 1 ]); } } } int res = 0 ; for ( int j = 0 ; j <= m ; j ++ ) { if ( f [ j ] != -1 && f [ j ] > res ) { res = f [ j ]; } } return res ; } }; Note Notice the solution is identical to the Backpack II except one line (line 18), but you should notice there is a long way from Backpack II to Backpack III . See the detailed explaination of this problem in Dynamic Programming","title":"Backpack III"},{"location":"courses/9chap-dynamic-prog/notes/#coins-in-a-line-iii","text":"There are n coins in a line. Two players take turns to take a coin from one of the ends of the line until there are no more coins left. The player with the larger amount of money wins. Could you please decide the first player will win or lose? \u8fd9\u662f\u4e00\u4e2a\u535a\u5f08\u95ee\u9898\u3002\u770b\u4e0a\u53bb\u5f88\u96be\u5165\u624b\u3002\u8981\u5bf9\u9898\u76ee\u8ba4\u771f\u5ba1\u89c6\u3002\u6211\u5f00\u59cb\u7684\u65f6\u5019\u5c31\u6ca1\u6709\u61c2\u5f97\u9898\u610f\u3002\u4e0b\u9762\u662f\u6211\u7684\u9519\u8bef\u5206\u6790\u3002 /***************** WRONG *************************** * \u5f53\u524d\u5c40\u9762\uff1aa[i], ... a[j] * Alice = A, Bob = B, * Sa = A - B; * \u5148\u624b: Alice A + a[i](a[j]) * Bob = B, * \u4e0b\u4e00\u8f6e\uff1aa[i+1], ... a[j] * Alice = A' = A + a[i], Bob = B' = B, * Sb = B - A'; * \u5148\u624b\uff1aBob B + a[i + 1] (a[j - 1]) * \u540e\u624b\uff1aAlice: A'; * ... * * *** Sb = B - A - a[i] * *** Sb = -Sa - a[i] * * **************** WRONG ****************************/ \u6ce8\u610f\u8fd9\u9053\u9898\u95ee\u7684\u662f\u5148\u624b\u662f\u5426\u4f1a\u8d62\u3002\u6240\u4ee5\u5e94\u8be5\u4ee5\u201c\u5148\u624b\u201d\u8fd9\u4e2a\u5173\u952e\u8bcd\u51fa\u53d1\u3002\u5c31\u662f\u7b2c\u4e00\u6b65\u3002\u8fd9\u4e00\u70b9\u660e\u786e\u4e86\uff0c \u518d\u91cd\u65b0\u5206\u6790\uff0c\u95ee\u9898\u8fce\u5203\u800c\u89e3\u3002 /* \u6ce8\u610f\uff1a\u6211\u4eec\u7684\u9898\u610f\u662f\u95ee\u5148\u624b\u662f\u5426\u5fc5\u80dc\u3002\u5219\u5f53\u8003\u8651\u5148\u624b\u51fa\u624b\u4e4b\u524d\u7684\u72b6\u6001 * * \u5f53\u524d\u5c40\u9762\uff1aa[0], ... a[n-1] * Alice = A = 0, Bob = B = 0, * delta: Sa = A - B = 0; * \u5148\u624b: Alice \u53d6 a[i](\u6216\u8005a[j]), \u76ee\u6807\u662f\u6700\u5927\u5316A - B * A = a[i] * B = 0, * \u5728\u63a5\u4e0b\u6765\u7684\u6bcf\u8f6e\u4e2d\uff0cBob\u5c3d\u6700\u5927\u52aa\u529b\u53bb\u6700\u5927\u5316B - A * \u6700\u7ec8\uff1aAlice \u83b7\u5f97 A + a[i], * Bob \u83b7\u5f97 B, * \u6ce8\u610f\uff1a\u6700\u7ec8\u7ed3\u679c\u6307\u7684\u662f\u201c\u5fc5\u80dc\u201d\u6216\u201c\u5fc5\u8d25\u201d\u7684\u6cd5\u5219\u8d70\u5230\u6700\u540e. * \u5fc5\u80dc\u5fc5\u8d25\u6982\u5ff5\uff1a\u5148\u624b\u5728\u5f53\u524d\u5c40\u9762\u6709\u4e00\u79cd\u53ef\u80fd\u6027\u201c\u5fc5\u80dc\u201d\u5219\u201c\u5fc5\u80dc\u201d\uff0c\u6ca1\u6709\u4e00\u79cd\"\u5fc5\u80dc\"\u53ef\u80fd\u6027\u4fbf\u201c\u5fc5\u8d25\u201d\u3002 * \u6211\u4eec\u60f3\u77e5\u9053\u7684\u662fSa = A + a[i] - B = -Sb + a[i]\u662f\u5426\u4f1a\u5927\u4e8e0 * * \u6211\u4eec\u53d1\u73b0 Sa = -Sb + a[i] * \u8fd9\u662f\u4e2a\u5b50\u95ee\u9898\u3002\u5b50\u95ee\u9898\u662f\u76f8\u5bf9\u4e8e\u5148\u624bAlice\u6765\u8bb2\u7684\uff0c\u5b50\u95ee\u9898\u53ef\u4ee5\u770b\u505a\u6bcf\u6b21\u8f6e\u5230Alice\u7684\u5c40\u9762(\u5148\u624b), Alice \u7684\u95ee\u9898\u89c4\u6a21\u90fd\u53d8\u5c0f\u4e86\u3002 * State: f[i][j] = Alice\u9762\u5bf9\u5c40\u9762a[i], ... a[j]\u65f6\u80fd\u5f97\u5230\u7684\u6700\u5927\u7684\u4e8e\u5bf9\u624b\u7684\u6570\u503c\u5dee\u3002 * Equation: f[i][j] = max(a[i] - f[i+1][j], a[j] - f[i][j-1]) * Init: f[0][0] = 0; * f[i][i] = a[i]; * \u533a\u95f4\u578b\uff0c\u5199code\u5e94\u8be5\u662f\u679a\u4e3e\u957f\u5ea6 */ \u6ce8\u610f18-20\u884c\u679a\u4e3e\u957f\u5ea6\u7684\u65b9\u6cd5\u3002\u8fb9\u754c\u60c5\u51b5\u548c\u4e0b\u6807\u8ba1\u7b97\u4e0d\u80fd\u5f04\u9519\u3002\u540c\u6837\u7684code\u5728 Longest Palindromic Subsequence \u4e5f\u7528\u5230\u3002 \u65f6\u95f4\u590d\u6742\u5ea6 O(n^2) O(n^2) , \u7a7a\u95f4\u590d\u6742\u5ea6 O(n^2) O(n^2) class Solution { public : bool firstWillWin ( vector < int > & values ) { int n = values . size (); if ( n == 0 ) return true ; int f [ n ][ n ]; f [ 0 ][ 0 ] = 0 ; int j ; /* init */ for ( int i = 1 ; i < n ; i ++ ) { f [ i ][ i ] = values [ i ]; } /* enumerate length */ for ( int len = 2 ; len <= n ; len ++ ) { for ( int i = 0 ; i <= n - len ; i ++ ) { j = i + len - 1 ; f [ i ][ j ] = max ( values [ i ] - f [ i + 1 ][ j ], values [ j ] - f [ i ][ j -1 ]); } } return f [ 0 ][ n -1 ] >= 0 ; } };","title":"Coins in A Line III"},{"location":"courses/9chap-dynamic-prog/notes/#longest-palindromic-subsequence","text":"\u533a\u95f4\u578b\u52a8\u6001\u89c4\u5212\uff0c\u5b50\u95ee\u9898\u662f\u53bb\u5934\u53bb\u5c3e\u4e4b\u540e\u89c4\u6a21\u4fbf\u5c0f\u3002\u5199\u51fa\u72b6\u6001\u540e\u5173\u952e\u5728\u4e8e\u521d\u59cb\u5316\uff0c\u521d\u59cb\u5316\u7684\u662f\u5bf9\u89d2\u7ebf\u3002 \u8fd9\u9053\u9898\u7684\u72b6\u6001\u8981\u76f8\u5bf9\u597d\u60f3\uff0c\u4f46\u662f\u521d\u59cb\u5316\u548c\u7ed3\u679c\u76f8\u5bf9\u590d\u6742\u3002 * last step: a[i] == a[j] * state: f[i][j], LPS in string s[i], ... s[j] * equation: f[i][j] = max(f[i+1][j], f[i][j - 1], f[i-1][j-1]|s[i] == s[j]); * init: f[0][0] = 0, f[i][i] = 1; * if (s[i] == s[i + 1]) * f[i][i+1] = 2; * result: max(f[i][j]) C++ DP public class Solution { public int longestPalindromeSubseq ( string s ) { int n = s . length (); if ( n == 0 ) return 0 ; if ( n == 1 ) return 1 ; int f [ n ][ n ]; int i , j , len ; f [ 0 ][ 0 ] = 0 ; for ( i = 1 ; i < n ; i ++ ) { f [ i ][ i ] = 1 ; } for ( i = 0 ; i < n - 1 ; i ++ ) { if ( s [ i ] == s [ i + 1 ]) { f [ i ][ i + 1 ] = 2 ; } else { f [ i ][ i + 1 ] = 1 ; } } /* enumerate the len */ for ( len = 3 ; len <= n ; len ++ ) { for ( i = 0 ; i <= n - len ; i ++ ) { j = i + len - 1 ; /* init */ f [ i ][ j ] = f [ i + 1 ][ j ]; // remove left if ( f [ i ][ j - 1 ] > f [ i ][ j ]) { // remove right f [ i ][ j ] = f [ i ][ j - 1 ]; } if ( s [ i ] == s [ j ] && f [ i + 1 ][ j - 1 ] + 2 > f [ i ][ j ]) { f [ i ][ j ] = f [ i + 1 ][ j - 1 ] + 2 ; } } } int res = 0 ; for ( i = 0 ; i < n ; i ++ ) { for ( j = i ; j < n ; j ++ ) { if ( f [ i ][ j ] > res ) { res = f [ i ][ j ]; } } } return res ; } } Java memoization public class Solution { int [][] f = null ; char [] s = null ; public void compute () { if ( f [ i ][ j ] != - 1 ) return ; if ( i == j ) { f [ i ][ j ] = ( s [ i ] == s [ j ] ) ? 2 : 1 ; return ; } compute ( i , j - 1 ); compute ( i + 1 , j ); compute ( i + 1 , j - 1 ); f [ i ][ j ] = Math . max ( f [ i ][ j - 1 ] , f [ i + 1 ][ j ] ); if ( s [ i ] == s [ j ] ) { f [ i ][ j ] = Math . max ( f [ i ][ j ] , f [ i + 1 ][ j - 1 ] + 2 ); } } public int longestPalindromeSubseq ( String ss ) { s = ss . toCharArry (); int n = s . length ; if ( n == 0 ) { return 0 ; } f = new int [ n ][ n ] ; for ( int i = 0 ; i < n ; ++ i ) { for ( int j = i ; j < n ; ++ j ) { f [ i ][ j ] = - 1 ; } } compute ( 0 , n 1 ); return f [ 0 ][ n - 1 ] ; } } Note \u5212\u5206\u578b\u52a8\u6001\u89c4\u5212\u679a\u4e3e\u7684\u662f\u957f\u5ea6\u800c\u4e0d\u662f\u4e0b\u6807\u3002","title":"Longest Palindromic Subsequence"},{"location":"courses/9chap-dynamic-prog/notes/#burst-balloons","text":"This is a hard problem. you should very carefully to analyze the problem and write down the equation. Here is my first attempt, which made a mistake. You should play closely attention to the boundary of the for loop and the state transition equaltion. The time complexity is O(n^3) O(n^3) , and space complexity is O(n^3) O(n^3) class Solution { public : /** * @param nums a list of integer * @return an integer, maximum coins */ int maxCoins ( vector < int >& nums ) { int n = nums . size (); if ( n == 1 ) { return nums [ 0 ]; } nums . insert ( nums . begin (), 1 ); nums . push_back ( 1 ); int f [ n + 2 ][ n + 2 ]; int i , j , k , len ; /* f[i][j] = maxi<k<j{f[i][k] + f[k][j] + a[i] * a[k] * a[j]} */ for ( i = 0 ; i < n + 1 ; i ++ ) { f [ i ][ i + 1 ] = 0 ; } for ( len = 3 ; len <= n + 2 ; len ++ ) { /*\u8fb9\u754c\uff1a\u957f\u5ea6\u679a\u4e3e\u5230n+2 */ for ( i = 0 ; i <= n - len + 2 ; i ++ ) { /*\u8fb9\u754c\uff1ai\u662f\u8981\u4ece0\u5f00\u59cb\uff0cj\u5230n+1, len = j - i + 1*/ j = i + len - 1 ; f [ i ][ j ] = 0 ; for ( k = i + 1 ; k < j ; k ++ ) { /*\u8fb9\u754c: k\u662f\u8981burst\u7684\u6c14\u7403\uff0c\u6211\u4eec\u8bf4f[i][j]\u662f\u4e0d\u5305\u62ecnums[i]\u548cnums[j]\u7684maxCoins*/ f [ i ][ j ] = max ( f [ i ][ j ], f [ i ][ k ] + f [ k ][ j ] + nums [ i ] * nums [ k ] * nums [ j ]); } } } return f [ 0 ][ n + 1 ]; } };","title":"Burst Balloons"},{"location":"courses/9chap-dynamic-prog/notes/#scramble-string","text":"Here is the analysis \u8fd9\u91cc\u65f6\u95f4\u590d\u6742\u5ea6\u8fbe\u5230\u4e86 O(n^4) O(n^4) , \u7a7a\u95f4\u590d\u6742\u5ea6\u901a\u8fc7\u964d\u7ef4\u5904\u7406\u4e3a O(n^3) O(n^3) . /* --------------------- * | s1 | s2 | S * --------------------- * --------------------- * | t1 | t2 | T * --------------------- * Last step: s1 <==> t1 && s2 <==> t2 OR s1 <==> t2 && S2 <==> t1 * Subproblem: s1 < S * state: f[i][j][h][k] \u4ee3\u8868\u662f\u5426S\u4e2d\u4ecei\u5230j\u7684\u5b57\u7b26\u662f\u7531T\u4e2d\u4eceh\u5230k\u7684\u5b57\u7b26scramble * \u56e0\u4e3a\u957f\u5ea6\u4e00\u6837\uff0c\u6240\u4ee5\u6211\u4eec\u53ef\u4ee5\u5bf9\u72b6\u6001\u8fdb\u884c\u964d\u7ef4\u5904\u7406\u3002 * state: f[i][j][k]: \u4ee3\u8868 s[i], ... s[i + k - 1] \u662f\u5426\u662f t[j], ... t[j + k - * 1] scramble string. * equation: f[i][j][k] = (f[i][j][w] && f[i + w][j + w] || f[i][j + k - w][w] && f[i + w][j][k - w]) for all 0 < w < k * init: f[0][0][0] = ? (\u4e0d\u91cd\u8981) * if (s[i] == t[j] && k == 1) { * f[i][j][1] = true; * } * */ class Solution { public : /* * * f[i][j][k] = OR_{0 < w < k}(f[i][j][w] && f[i+w][j+w][k-w]) OR (f[i][j+w][w] && f[i+w][j][) */ bool isScramble ( string & s1 , string & s2 ) { int m = s1 . length (); int n = s2 . length (); if ( m != n ) return false ; if ( m == 0 ) return true ; int f [ n ][ n ][ n + 1 ]; int i , j , k , w ; /* init */ for ( i = 0 ; i < n ; i ++ ) { for ( j = 0 ; j < n ; j ++ ) { f [ i ][ j ][ 1 ] = s1 [ i ] == s2 [ j ]; } } for ( k = 2 ; k <= n ; k ++ ) { for ( i = 0 ; i <= n - k ; i ++ ) { for ( j = 0 ; j <= n - k ; j ++ ) { f [ i ][ j ][ k ] = false ; //enumerate partition position (s1's length) for ( w = 1 ; w < k ; w ++ ) { // case 1: no swap match if ( f [ i ][ j ][ w ] && f [ i + w ][ j + w ][ k - w ]) { f [ i ][ j ][ k ] = true ; break ; } // case 2: swap match if ( f [ i ][ j + k - w ][ w ] && f [ i + w ][ j ][ k - w ]) { f [ i ][ j ][ k ] = true ; break ; } } } } } return f [ 0 ][ 0 ][ n ]; } };","title":"Scramble String"},{"location":"courses/9chap-dynamic-prog/notes/#_4","text":"\u7ed9\u5b9a\u4e00\u4e2a\u5e8f\u5217/\u5b57\u7b26\u4e32\uff0c\u8fdb\u884c\u4e00\u4e9b\u64cd\u4f5c \u6700\u540e\u4e00\u6b65\u4f1a\u5c06\u5e8f\u5217/\u5b57\u7b26\u4e32\u53bb\u5934/\u53bb\u5c3e \u5269\u4e0b\u7684\u4f1a\u662f\u4e00\u4e2a\u533a\u95f4 [i, j] \u72b6\u6001\u81ea\u7136\u5b9a\u4e49\u4e3a f[i][j] \uff0c\u8868\u793a\u9762\u5bf9\u5b50\u5e8f\u5217 [i, \u2026, j] \u65f6\u7684\u6700\u4f18\u6027\u8d28 \u5199\u7a0b\u5e8f\u65f6\u6ce8\u610f\u8981\u679a\u4e3e\u533a\u95f4\u957f\u5ea6","title":"\u533a\u95f4\u578b\u52a8\u6001\u89c4\u5212\u603b\u7ed3"},{"location":"courses/9chap-dynamic-prog/notes/#lecture-6","text":"Problem Category Longest Common Subsequence Interleaving String Edit Distance Distinct Subsequences Regular Expression Matching Wildcard Matching Ones and Zeroes","title":"Lecture 6"},{"location":"courses/9chap-dynamic-prog/notes/#longest-common-subsequence","text":"State: f[i][j] , LCS of the first i chars from A and the first j chars from B. \u5b9a\u4e49\u72b6\u6001\u662f\u4e00\u5b9a\u8981\u6ce8\u610f\u4e0b\u6807\u7684\u610f\u4e49\u3002\u5728\u8fd9\u91cc\u6211\u4eec\u6307\u7684\u662f\u524di\u4e2a\u5b57\u7b26\u548c\u524dj\u4e2a\u5b57\u7b26 notice we can initialize the base case inside the loop. class Solution { public : int longestCommonSubsequence ( string A , string B ) { int m = A . length (); int n = B . length (); if ( m == 0 || n == 0 ) { return 0 ; } int f [ m + 1 ][ n + 1 ]; for ( int i = 0 ; i <= m ; ++ i ) { for ( int j = 0 ; j <= n ; ++ j ) { if ( i == 0 || j == 0 ) { f [ i ][ j ] = 0 ; continue ; } f [ i ][ j ] = max ( f [ i - 1 ][ j ], f [ i ][ j - 1 ]); if ( A [ i - 1 ] == B [ j - 1 ]) { f [ i ][ j ] = max ( f [ i ][ j ], f [ i - 1 ][ j - 1 ] + 1 ); } } } return f [ m ][ n ]; } };","title":"Longest Common Subsequence"},{"location":"courses/9chap-dynamic-prog/notes/#interleaving-string","text":"Consider the where the last char in s3 came from, either from s1 or s2. state: f[i][j] , whether the first i from s1 and the first j from s2 is interleaving of the first i + j from s3 . equation: f[i][j] = (f[i][j - 1] && s[i + j - 1] == s2[j - 1]) || (f[i - 1][j] && s[i + j - 1] == s1[i - 1]) we may attempt to have 3d matrix for the DP, but we can reduce the state representation by noticing the index of s3 can be derived from index of s1 and s2 . /* * * last step: last s3[k - 1] == s1[m - 1] || s3[k - 1] == s2[n - 1] * state: f[i][j][k]: s3[0],..s[k - 1] is interleaving of s1[0], ... s[i - 1] and s2[0], .. s[j - 1] * equation 1: f[i][j][k] = (f[i][j - 1][k - 1] && s3[k - 1] == s1[i - 1]) * || (f[i - 1][j][k - 1] && s3[k - 1] == s2[j - 1]) * equation 2: f[i][j] = (f[i][j - 1] && s[i + j - 1] == s2[j - 1]) * || (f[i - 1][j] && s[i + j - 1] == s1[i - 1]) * init: f[0][0] = 1; * f[0][0] = false if k > 0 */ class Solution { public : bool isInterleave ( string s1 , string s2 , string s3 ) { int m = s1 . length (); int n = s2 . length (); int k = s3 . length (); if ( m + n != k ) return false ; f [ m + 1 ][ n + 1 ]; for ( int i = 0 ; i <= m ; ++ i ) { for ( int j = 0 ; j <= n ; ++ j ) { if ( i == 0 || j == 0 ) { f [ i ][ j ] = false ; continue ; } f [ i ][ j ] = false ; if ( i > 0 && s1 [ i - 1 ] == s3 [ i + j - 1 ]) { f [ i ][ j ] |= f [ i - 1 ][ j ]; } if ( j > 0 && s2 [ j - 1 ] == s3 [ i + j - 1 ]) { f [ i ][ j ] |= f [ i ][ j - 1 ]; } } } return f [ m ][ n ]; } };","title":"Interleaving String"},{"location":"courses/9chap-dynamic-prog/notes/#edit-distance","text":"Notice the state update is not straightforward, you need to have the correct setting first and then write down the state transition equation. We define the state as follows f[i][j] is the edit distance to make the first i chars in A and first j chars in B the same. To make A and B the same, their last step need to be the same. There are three editing operations: Insert a char at the end of A , so that A[-1] == B[-1] . The state should be updated as f[i][j] = f[i][j - 1] + 1 . This assignment operation purely mean that I can get f[i][j] from subproblem result. It doesn't indicate any of ther iteration of the string. You cannot have it like f[i][j] = f[i - 1][j - 1] + 1 . Delete a char at the end of A , so that A[-1] == B[-1] . The state update should be f[i][j] = f[i - 1][j] + 1 . Replace a char at the end of A , so that A[-1] == B[-1] . The state update should be f[i][j] = f[i - 1][j - 1] + 1 . No operation is needed, the last chars of the arrays are the same, such as A[-1] == B[-1] . The state update should be f[i][j] = f[i - 1][j - 1] . notice the state represent the edit distance , don't try to correlate it to the index to the string after the edit , no string is changed while calculating the edit distance f[i][j] . C++ DP Space O(mn) class Solution { public : /* * * State: f[i][j] is the minimum number of steps to edit w1 to w2. * f[i][j] = f[i][j - 1] + 1; insert * = f[i - 1][j] + 1; delete * = f[i - 1][j - 1] + 1; replace * = f[i - 1][j - 1]; no opration needed */ int minDistance ( string word1 , string word2 ) { int m = word1 . length (); int n = word2 . length (); int f [ m + 1 ][ n + 1 ]; f [ 0 ][ 0 ] = 0 ; int i , j ; for ( i = 0 ; i <= m ; i ++ ) { for ( j = 0 ; j <= n ; j ++ ) { /* init */ if ( i == 0 ) { f [ i ][ j ] = j ; continue ; } if ( j == 0 ) { f [ i ][ j ] = i ; continue ; } // delete // insert f [ i ][ j ] = min ( f [ i - 1 ][ j ] + 1 , f [ i ][ j - 1 ] + 1 ); // replace f [ i ][ j ] = min ( f [ i ][ j ], f [ i - 1 ][ j - 1 ] + 1 ); // no operation if ( word1 [ i - 1 ] == word2 [ j - 1 ]) { f [ i ][ j ] = min ( f [ i ][ j ], f [ i - 1 ][ j - 1 ]); } } } return f [ m ][ n ]; }; }; C++ space O(n) class Solution { public : /* * * State: f[i][j] is the minimum number of steps to convert w1 to w2. (len(w1) < len(w2)) * f[i][j] = f[i][j - 1] + 1; insert * = f[i - 1][j] + 1; delete * = f[i - 1][j - 1] + 1; replace * = f[i - 1][j - 1]; no opration needed * */ int minDistance ( string word1 , string word2 ) { int m = word1 . length (); int n = word2 . length (); int f [ m + 1 ][ n + 1 ]; f [ 0 ][ 0 ] = 0 ; int i , j ; int prev = -1 ; int curr = 0 ; for ( i = 0 ; i <= m ; i ++ ) { prev = curr ; curr = 1 - curr ; for ( j = 0 ; j <= n ; j ++ ) { /* init */ if ( i == 0 ) { f [ curr ][ j ] = j ; continue ; } if ( j == 0 ) { f [ curr ][ j ] = i ; continue ; } // delete // insert f [ curr ][ j ] = min ( f [ prev ][ j ] + 1 , f [ curr ][ j - 1 ] + 1 ); // replace f [ curr ][ j ] = min ( f [ curr ][ j ], f [ prev ][ j - 1 ] + 1 ); // no operation if ( word1 [ i - 1 ] == word2 [ j - 1 ]) { f [ curr ][ j ] = min ( f [ curr ][ j ], f [ prev ][ j - 1 ]); } } } return f [ curr ][ n ]; }; }; Note When using the prev and curr to index the 2d array, don't have to strictly follow the meaning of it. What you just need to make sure is the two row is rolling and not going to overwrite a useful value. Notice the update of the \"rolling index\" is in between the two for loops. It has nothing to do with index j .","title":"Edit Distance"},{"location":"courses/9chap-dynamic-prog/notes/#distinct-subsequence","text":"Last step, t[n-1] match to s[m-1] or doesn't match. State: f[i][j] is the number of subsequence for t[0:j-2] in s[0:i-2] state transition: f[i][j] = f[i - 1][j - 1]|s[i - 1] == t[j - 1] + f[i - 1][j] class Solution { public : /* * * Last step: S[m - 1] in the * S[0], ... S[m - 2], S[m - 1] * T[0], ... T[n - 2], T[n - 1] * State: f[i][j]: # of subsequence for the first j chars in T have in the first i chars in S * Equation: f[i][j] = f[i - 1][j - 1]|S[i - 1] == T[j - 1] + f[i - 1][j] * 1. S[m - 1] == T[n - 1] * 1. f[i - 1][j - 1] * 2. f[i - 1][j] * 2. S[m - 1] != T[n - 1] * 1. f[i - 1][j] * Init: f[0][0] = 1; * f[0][i] = 0; * f[i][0] = 1; */ int numDistinct ( string & S , string & T ) { int m = S . length (); int n = T . length (); int f [ m + 1 ][ n + 1 ]; for ( int i = 0 ; i <= m ; i ++ ) { for ( int j = 0 ; j <= n ; j ++ ) { if ( i == 0 && j == 0 ) { f [ i ][ j ] = 1 ; continue ; } if ( i == 0 ) { f [ i ][ j ] = 0 ; continue ; } if ( j == 0 ) { f [ i ][ j ] = 1 ; continue ; } f [ i ][ j ] = f [ i - 1 ][ j ]; if ( S [ i - 1 ] == T [ j - 1 ]) { f [ i ][ j ] += f [ i - 1 ][ j - 1 ]; } } } return f [ m ][ n ]; } };","title":"Distinct Subsequence"},{"location":"courses/9chap-dynamic-prog/notes/#regular-expression-matching","text":"C++ DP Solution class Solution { public : /* * * Last step (last char to match in the strings): * p[j - 1] != s[i - 1] ==> f[i][j] = false; * p[j - 1] == s[i - 1] ==> f[i][j] = f[i - 1][j - 1]; * p[j - 1] == '.' ==> f[i][j] = f[i - 1][j - 1] * * p[j - 1] == '*', * case 1 p[j - 2] == '.' ==> f[i][j] = f[i - 1][j]; ??? why not f[i][j] = true; * p[j - 2] != '.' * p[j - 2] == s[i - 1] ==> f[i][j] = f[i'][j - 2] | i' is the length not equal to p[j - 2]. * which one? / * \\ * case 2 p[j - 2] == s[i - 1] ==> f[i][j] = f[i - 1][j] * p[j - 2] != s[i - 1] ==> f[i][j] = f[i][j - 2]; * * We see that case 1 and case 2 could be combined and put inside one if statement. * State: f[i][j]: the first i chars form s match regex the first j chars from p * Equation: f[i][j] = * Init: f[0][0] = 0; * f[0][j] = ture; if p[j - 1] == '*' * f[i][0] = false; */ bool isMatch ( const char * s , const char * p ) { int i , j ; int m = 0 ; int n = 0 ; while ( s [ m ] != '\\0' ) { m ++ ; } while ( p [ n ] != '\\0' ) { n ++ ; } int f [ m + 1 ][ n + 1 ]; for ( i = 0 ; i <= m ; i ++ ) { for ( j = 0 ; j <= n ; j ++ ) { if ( i == 0 && j == 0 ) { f [ i ][ j ] = true ; continue ; } if ( j == 0 ) { f [ i ][ j ] = false ; continue ; } f [ i ][ j ] = false ; if ( p [ j - 1 ] == '*' ) { // matched one char at end of s, s=[----]a, p=[-----]a* if ( i > 0 && j > 1 && ( p [ j - 2 ] == '.' || p [ j - 2 ] == s [ i - 1 ])) { f [ i ][ j ] |= f [ i - 1 ][ j ]; //use whole p to match s[0, ... i - 2] } // don't care the a* or .*, // match previous chars s=[-----a], p=[------]a* if ( j > 1 ) { f [ i ][ j ] |= f [ i ][ j - 2 ]; } } else if ( i > 0 && ( p [ j - 1 ] == '.' || p [ j - 1 ] == s [ i - 1 ])) { f [ i ][ j ] = f [ i - 1 ][ j - 1 ]; } } } return f [ m ][ n ]; } }; C++ recursive solution // The idea is to deal with each case one by one. class Solution { public : bool isMatch ( string s , string p ) { /* base cases when none left or only one char left */ if ( p . empty ()) return s . empty (); if ( p . length () == 1 ) { return ( s . length () == 1 && ( s [ 0 ] == p [ 0 ] || p [ 0 ] == '.' )); } if ( p [ 1 ] != '*' ) { if ( s . empty ()) return false ; return ( s [ 0 ] == p [ 0 ] || p [ 0 ] == '.' ) && isMatch ( s . substr ( 1 ), p . substr ( 1 )); } /* p[1] == '*' */ while ( ! s . empty () && ( s [ 0 ] == p [ 0 ] || p [ 0 ] == '.' )) { /* consider the case: s=\"aaaabcd\", p=\".*b*cd\"*/ if ( isMatch ( s , p . substr ( 2 ))) return true ; s = s . substr ( 1 ); } /* consider the case: s=\"aaaaa\", p=\"a*\" and case s=\"aaaaa\", p=\"a*b\" */ return isMatch ( s , p . substr ( 2 )); /* how comes? */ } };","title":"Regular Expression Matching"},{"location":"courses/9chap-dynamic-prog/notes/#wildcard-matching","text":"\u4e0d\u9700\u8981\u77e5\u9053' '\u6700\u591a\u80fd\u5339\u914d\u51e0\u4e2a\u5b57\u7b26\u3002\u5728\u5f53\u524dstep\u53ea\u9700\u8003\u8651\u7528' '\u53bb\u5339\u914d\u4e00\u4e2a\u5b57\u7b26\u6216\u8005\u5b8c\u5168\u4e0d\u5339\u914d\u5c31\u8986\u76d6\u4e86 \u6240\u6709\u7684\u60c5\u51b5\u3002\u81f3\u4e8e\u6700\u7ec8\u53ef\u4ee5\u5339\u914d\u51e0\u4e2a\uff0c\u662f\u5728\u591a\u4e2astep\u4e2d\u7684\u4fe1\u606f\u3002\u5f53\u524dstep\u5e76\u4e0d\u9700\u8981\u5173\u5fc3. When p[j - 1] == '*' , since we don't know ' ' match how many chars in s? In the first solution, I used a third loop to check. We can think it in this way, for p[j - 1] == '*' , we can use ' ' to match the trailing character or not to use '*' to match previously. the we have f[i][j] = f[i - 1][j] | f[i][j - 1] . C++ O(mnk) class Solution { public : /* * * last step: * p[j - 1] == '?' * f[i][j] = f[i - 1][j - 1] * s[i - 1] == p[j - 1] * f[i][j] = f[i - 1][j - 1] * p[j - 1] == '*', ('*' may match k of trailng characters from s, but we don't know how many) * f[i][j] = f[i][j - 1] OR f[i - 1][j - 1] OR f[i - 2][j - 1], .. f[i - k][j - 1] * State: f[i][j]: the first i chars from s match the first j letters from p * Equation: * Init: f[0][0] = true; * f[i][0] = false; * f[0][j] = true; if (p[j - 1] == \"*\" && j == 1) * calculate by DP: * f[0][j] = false; if (j > 1) */ bool isMatch ( const char * s , const char * p ) { int m = 0 ; int n = 0 ; int i , j , k ; while ( s [ m ] != '\\0' ) { m ++ ; } while ( p [ n ] != '\\0' ) { n ++ ; } int f [ m + 1 ][ n + 1 ]; for ( i = 0 ; i <= m ; i ++ ) { for ( j = 0 ; j <= n ; j ++ ) { if ( i == 0 && j == 0 ) { f [ i ][ j ] = true ; continue ; } if ( j == 0 ) { f [ i ][ j ] = false ; continue ; } f [ i ][ j ] = false ; if ( p [ j - 1 ] == '*' ) { // k == i indicate the special case: f[0][1] (s=\"\", p=\"*\") for ( k = 0 ; k <= i ; k ++ ) { f [ i ][ j ] |= f [ i - k ][ j - 1 ]; } } else { if ( i > 0 && ( p [ j - 1 ] == '?' || s [ i - 1 ] == p [ j - 1 ])) { f [ i ][ j ] = f [ i - 1 ][ j - 1 ]; } } } } return f [ m ][ n ]; } }; C++ O(mn) class Solution { public : bool isMatch ( const char * s , const char * p ) { // write your code here int m = 0 ; int n = 0 ; int i , j , k ; while ( s [ m ] != '\\0' ) { m ++ ; } while ( p [ n ] != '\\0' ) { n ++ ; } int f [ m + 1 ][ n + 1 ]; for ( i = 0 ; i <= m ; i ++ ) { for ( j = 0 ; j <= n ; j ++ ) { if ( i == 0 && j == 0 ) { f [ i ][ j ] = true ; continue ; } if ( j == 0 ) { f [ i ][ j ] = false ; continue ; } f [ i ][ j ] = false ; if ( p [ j - 1 ] == '*' ) { f [ i ][ j ] = f [ i ][ j - 1 ]; // ignore the '*', match nothing if ( i > 0 ) { f [ i ][ j ] |= f [ i - 1 ][ j ]; // match the trailing char from s and continue } } else { if ( i > 0 && ( p [ j - 1 ] == '?' || s [ i - 1 ] == p [ j - 1 ])) { f [ i ][ j ] = f [ i - 1 ][ j - 1 ]; } } } } return f [ m ][ n ]; } };","title":"Wildcard Matching"},{"location":"courses/9chap-dynamic-prog/notes/#ones-and-zeroes","text":"This is essentially a backpack problem see the backpack problem. The key to solve the problem is taking the last item or do not take the last item. \u6280\u5de7\uff1a\u80cc\u5305\u95ee\u9898\u7684\u201c\u6700\u540e\u4e00\u6b65\u201d\u662f\u6307\u6700\u540e\u4e00\u4e2a\u201c\u7269\u54c1\u201d\u5728\u7ed3\u679c\u4e2d\u6216\u8005\u4e0d\u518d\u7ed3\u679c\u4e2d. C++ DP O(mnk) class Solution { public : /* * * \u80cc\u5305\u95ee\u9898\uff0c\u6700\u540e\u4e00\u4e2a\u8fdb\u201c\u80cc\u5305\u201d\uff0c\u6700\u540e\u4e00\u4e2a\u4e0d\u8fdb\u3002 * State: f[i][j][k]: maximum of k strs can be formed by i zeros and j ones. * Equation: f[i][j][k] = max(f[i][j][k - 1], f[i - a][j - b][k - 1] + 1| i > a and j > b) * Init: f[i][j][0] = 0 * */ int findMaxForm ( vector < string >& strs , int m , int n ) { int len = strs . size (); int cnt0 [ len ]; int cnt1 [ len ]; int i , j , k ; int f [ m + 1 ][ n + 1 ][ len + 1 ]; /* count all the zeros and ones */ for ( k = 0 ; k < len ; k ++ ) { cnt0 [ k ] = 0 ; cnt1 [ k ] = 0 ; for ( i = 0 ; i < strs [ k ]. length (); i ++ ) { if ( strs [ k ][ i ] == '0' ) { cnt0 [ k ] ++ ; } else { cnt1 [ k ] ++ ; } } } for ( i = 0 ; i <= m ; i ++ ) { for ( j = 0 ; j <= n ; j ++ ) { f [ i ][ j ][ 0 ] = 0 ; } } /* k started from 1 */ for ( k = 1 ; k <= len ; k ++ ) { for ( i = 0 ; i <= m ; i ++ ) { for ( j = 0 ; j <= n ; j ++ ) { f [ i ][ j ][ k ] = f [ i ][ j ][ k - 1 ]; if ( i >= cnt0 [ k - 1 ] && j >= cnt1 [ k - 1 ]) { f [ i ][ j ][ k ] = max ( f [ i ][ j ][ k ], f [ i - cnt0 [ k - 1 ]][ j - cnt1 [ k - 1 ]][ k - 1 ] + 1 ); } } } } return f [ m ][ n ][ len ]; } }; C++ DP O(nk) class Solution { public : int findMaxForm ( vector < string >& strs , int m , int n ) { int len = strs . size (); int cnt0 [ len ]; int cnt1 [ len ]; int i , j , k ; /* count all the zeros and ones */ for ( k = 0 ; k < len ; k ++ ) { cnt0 [ k ] = 0 ; cnt1 [ k ] = 0 ; for ( i = 0 ; i < strs [ k ]. length (); i ++ ) { if ( strs [ k ][ i ] == '0' ) { cnt0 [ k ] ++ ; } else { cnt1 [ k ] ++ ; } } } int f [ m + 1 ][ n + 1 ][ len + 1 ]; int prev , curr = 0 ; for ( i = 0 ; i <= m ; i ++ ) { for ( j = 0 ; j <= n ; j ++ ) { f [ i ][ j ][ curr ] = 0 ; } } /* k started from 1 */ for ( k = 1 ; k <= len ; k ++ ) { prev = curr ; curr = 1 - curr ; for ( i = 0 ; i <= m ; i ++ ) { for ( j = 0 ; j <= n ; j ++ ) { f [ i ][ j ][ curr ] = f [ i ][ j ][ prev ]; if ( i >= cnt0 [ k - 1 ] && j >= cnt1 [ k - 1 ]) { f [ i ][ j ][ curr ] = max ( f [ i ][ j ][ curr ], f [ i - cnt0 [ k - 1 ]][ j - cnt1 [ k - 1 ]][ prev ] + 1 ); } } } } return f [ m ][ n ][ curr ]; } };","title":"Ones and Zeroes"},{"location":"courses/9chap-dynamic-prog/notes/#_5","text":"\u4e24\u4e2a\u4e00\u7ef4\u5e8f\u5217/\u5b57\u7b26\u4e32 \u7a81\u7834\u53e3 \u4e32A\u548c\u4e32B\u7684\u6700\u540e\u4e00\u4e2a\u5b57\u7b26\u662f\u5426\u5339\u914d \u662f\u5426\u9700\u8981\u4e32A/\u4e32B\u7684\u6700\u540e\u4e00\u4e2a\u5b57\u7b26 \u7f29\u51cf\u95ee\u9898\u89c4\u6a21 \u6570\u7ec4\u4e0b\u6807\u8868\u793a\u5e8f\u5217 A \u524d i \u4e2a\uff0c\u5e8f\u5217 B \u524d j \u4e2a: f[i][j] \u521d\u59cb\u6761\u4ef6\u548c\u8fb9\u754c\u60c5\u51b5 \u7a7a\u4e32\u5982\u4f55\u5904\u7406 \u8ba1\u6570\u578b(\u60c5\u51b51+\u60c5\u51b52+\u2026)\u4ee5\u53ca\u6700\u503c\u578b(min/max{\u60c5\u51b51\uff0c\u60c5\u51b52\uff0c\u2026}) \u5339\u914d\u7684\u60c5\u51b5\u4e0b\u52ff\u5fd8+1(\u64cd\u4f5c\u6570\u591a1\u6b21\uff0c\u5339\u914d\u957f\u5ea6\u591a1)","title":"\u53cc\u5e8f\u5217\u578b\u52a8\u6001\u89c4\u5212\u603b\u7ed3"},{"location":"courses/9chap-dynamic-prog/notes/#lecture-7","text":"Problem Category Minimum Adjustment Cost K Sum \u80cc\u5305\u578b Longest Increasing Subsequence \u5e8f\u5217\u578b K Edit Distance \u53cc\u5e8f\u5217 + Trie Frog Jump \u5750\u6807 + \u72b6\u6001 Maximal Square \u5750\u6807 Maximal Rectangle","title":"Lecture 7"},{"location":"courses/9chap-dynamic-prog/notes/#minimum-adjustment-cost","text":"The key is how to come up the last step and the induction. The key is to come up a sensable modle of the problem. See the analysis in the code comments. The hard part of the problem is how to prove the the range: 1 <= B[i] <= 100 , so that we can create the state array f[n + 1][100 + 1] The initial value of this DP problem is not obvious. You have to pay attention to the initial condition of this problem, it different from the provious DP problems. class Solution { public : /** * \u6280\u5de7\uff1a\u8981\u770b\u4f5c\u628aA\u6570\u7ec4\u901a\u8fc7\u53d8\u5316\u53d8\u6210B\u6570\u7ec4\u3002\u6700\u540e\u4e00\u6b65\u8003\u8651\u628aA[i]\u53d8\u6210B[i]. * \u8fd9\u6837\u6bd4\u5728A\u7684\u57fa\u7840\u4e0a\u53d8\u5316\u66f4\u5bb9\u6613\u8003\u8651\u9012\u63a8\u5173\u7cfb * Last step: change A[i] to B[i], result[i] = result[i - 1] + abs(B[i] - A[i]) * State: f[i][j]: the cost of changing the first i element in A, * and the last elemnet A[i - 1] changed is changed to j (j == B[i]) * Induction: A[i - 1] --> j, A[i - 2] --> k, |j - k| < target ==> j - target <= k <= j + target * Equation: f[i][j] = min_{j - target <= k <= j + target, 1 <= k <= 100}(f[i - 1][k] + abs(j - A[i])) * Init: f[0][0] = ? * f[1][k] = abs(k - A[0]) */ int MinAdjustmentCost ( vector < int > A , int target ) { int n = A . size (); int f [ n + 1 ][ 100 + 1 ]; int i , j , k ; for ( j = 1 ; j <= 100 ; j ++ ) { f [ 1 ][ j ] = abs ( j - A [ 0 ]); } for ( i = 2 ; i <= n ; i ++ ) { for ( j = 1 ; j <= 100 ; j ++ ) { f [ i ][ j ] = INT_MAX ; for ( k = j - target ; k <= j + target ; k ++ ) { if ( k < 1 || k > 100 ) { continue ; } f [ i ][ j ] = min ( f [ i ][ j ], f [ i - 1 ][ k ] + abs ( j - A [ i - 1 ])); } } } int res = INT_MAX ; for ( j = 1 ; j <= 100 ; j ++ ) { res = min ( res , f [ n ][ j ]); } return res ; } }; Note The space complexity is O(100n) O(100n) . The time complexity is O(1002n) O(1002n) .","title":"Minimum Adjustment Cost"},{"location":"courses/9chap-dynamic-prog/notes/#k-sum","text":"This is a backpack problem which is very similar to the problem Backpack VI \u80cc\u5305\u95ee\u9898\u7684\u8bdd\uff0c\u603b\u627f\u91cd\u8981\u8fdb\u72b6\u6001\uff0c\u4e5f\u5c31\u662f\u8bf4 target \u8981\u8fdb\u72b6\u6001\u3002\u5176\u4ed6\u8fd8\u6709\u4ec0\u4e48\u8981\u8fdb\u72b6\u6001\u5462\uff1f\u8fd9\u5c31\u8981\u6c42\u8ba4\u771f\u5206\u6790\u9898\u76ee\u3002 \u5bf9\u4e8e\u8fd9\u79cd\u7c7b\u4f3c\u6c42\u4e0d\u653e\u56de\u7684\u7ec4\u5408\u6570\u7684\u8fc7\u7a0b\uff0c\u4e0d\u8981\u5c1d\u8bd5\u53bb\u7528\u5faa\u73af\u6765\u51b3\u5b9a\u67d0\u4e00\u4e2a\u5143\u7d20\u662f\u5426\u88ab\u9009\u4e2d\u3002\u800c\u662f\u8981\u7ed3\u5408 induction\u7684\u60f3\u6cd5\uff0c\u4ece\u67d0\u4e2a\u7279\u6b8a\u7684\u6b65\u9aa4\u7740\u773c\uff0c\u5728\u8fd9\u4e2a\u6b65\u9aa4\u4e2d\u8003\u8651\u9009\u4e2d\u6216\u8005\u4e0d\u9009\u67d0\u4e2a\u5143\u7d20\uff08\u672c\u8eab\u5e26\u6709\u5faa\u73af\u7684\u610f\u601d\uff0c \u4efb\u4f55\u4e00\u4e2a\u5143\u7d20\u53ea\u80fd\u88ab\u9009\u4e2d\u4e00\u6b21\uff0c\u548c\u4e0d\u9009\u4e00\u6b21\uff09 \u4e0b\u9762\u4ee3\u7801\u6ce8\u91ca\u4e2d\u7ed9\u51fa\u4e86\u6211\u6700\u521d\u7684\u9519\u8bef\u5206\u6790\uff0c\u968f\u540e\u53c8\u7ed9\u51fa\u4e86\u6b63\u786e\u7684\u5206\u6790\u3002\u5bf9\u6bd4\u4e8c\u8005\u4e0d\u540c\uff0c\u4e89\u53d6\u4ee5\u540e\u5c11\u72af\u7c7b\u4f3c\u9519\u8bef\u3002 \u5173\u952e\u9700\u8981\u6ce8\u610f\u7684\u662f\u521d\u59cb\u5316\u3002\u5148\u628a f[0][q][s] \u5168\u90e8\u521d\u59cb\u5316\u4e3a0\uff0c\u7136\u540e\u518d\u521d\u59cb\u5316 f[0][0][0] \u3002\u8fd9\u4e24\u4e2a\u6b65\u9aa4\u4e0d\u80fd\u98a0\u5012\u987a\u5e8f\uff0c \u6ce8\u610f\u4e5f\u53ef\u4ee5\u5728for loop\u4e2d\u521d\u59cb\u5316f[0][0][0]\u3002 Time complexity O(n \\cdot k \\cdot \\text{target}) O(n \\cdot k \\cdot \\text{target}) , space complexity O(n \\cdot k \\cdot \\text{target}) O(n \\cdot k \\cdot \\text{target}) . We can use rolling array to reduce the space complexity to O(k \\cdot \\text{target}) O(k \\cdot \\text{target}) . C++ DP class Solution { public : /** * This is a backpack problem, * Last step: A[i] is selected or not selected. * State: f[i][j]: total solution of i element can sum up to j. * Equation: f[i][j] = f[i - 1][j] + f[i - 1][j - A[i - 1]]|j > A[i - 1] for all i == k. * Equation: f[i][j] = sum(f[i - 1][j - A[k]] | 0 <= k <= i - 1, j > A[k]). * Init: f[0][0] = 1; // ? * f[0][j] = 0 * ******************** WRONG ********************************* * ******************** CORRECT ******************************* * Last step: A[i - 1] is selected or not selected. * If selected, we should select k - 1 numbers from A[0], .. A[n - 2] that sum to target - A[i - 1]. * If not selected, we should select k number from A[0], .. A[n - 2] that sum to target. * State: f[i][k][s]: total solution of selecting k element from first i element that sum up to j. * Equation: f[i][k][s] = f[i - 1][k][s] + f[i - 1][k - 1][s - A[i - 1]]|s > A[i - 1]. * Init: f[0][0][0] = 1; // ? * f[0][k][s] = 0 * */ int kSum ( vector < int > A , int k , int target ) { // wirte your code here int n = A . size (); int f [ n + 1 ][ k + 1 ][ target + 1 ]; int i , q , s ; // init for ( q = 0 ; q <= k ; q ++ ) { for ( s = 0 ; s <= target ; s ++ ) { f [ 0 ][ q ][ s ] = 0 ; } } f [ 0 ][ 0 ][ 0 ] = 1 ; for ( i = 1 ; i <= n ; i ++ ) { for ( q = 0 ; q <= k ; q ++ ) { for ( s = 0 ; s <= target ; s ++ ) { // not select A[i - 1] f [ i ][ q ][ s ] = f [ i - 1 ][ q ][ s ]; // select A[i - 1] if ( q >= 1 && s >= A [ i - 1 ]) { f [ i ][ q ][ s ] += f [ i - 1 ][ q - 1 ][ s - A [ i - 1 ]]; } } } } return f [ n ][ k ][ target ]; } }; C++ DP space optimized class Solution { public : int kSum ( vector < int > A , int k , int target ) { // wirte your code here int n = A . size (); int f [ n + 1 ][ k + 1 ][ target + 1 ]; int i , q , s ; int curr = 0 , prev = 0 ; // init for ( q = 0 ; q <= k ; q ++ ) { for ( s = 0 ; s <= target ; s ++ ) { f [ curr ][ q ][ s ] = 0 ; } } f [ curr ][ 0 ][ 0 ] = 1 ; for ( i = 1 ; i <= n ; i ++ ) { prev = curr ; curr = 1 - curr ; for ( q = 0 ; q <= k ; q ++ ) { for ( s = 0 ; s <= target ; s ++ ) { // not select A[i - 1] f [ curr ][ q ][ s ] = f [ prev ][ q ][ s ]; // select A[i - 1] if ( q >= 1 && s >= A [ i - 1 ]) { f [ curr ][ q ][ s ] += f [ prev ][ q - 1 ][ s - A [ i - 1 ]]; } } } } return f [ curr ][ k ][ target ]; } };","title":"K Sum"},{"location":"courses/9chap-dynamic-prog/notes/#longest-increasing-subsequence_1","text":"","title":"Longest Increasing Subsequence"},{"location":"courses/9chap-dynamic-prog/notes/#k-edit-distance","text":"","title":"K Edit Distance"},{"location":"courses/9chap-dynamic-prog/notes/#frog-jump","text":"You could use naive solution to iterate all the possible cases and use memoization to speed up. In that case, you can also use binary search to located a stone in the array which help to improve the complexity. But the best solution is to use a map to keep all the possible steps for a stone. C++ map to set class Solution { public : bool canCross ( vector < int >& stones ) { int n = stones . size (); unordered_map < int , unordered_set < int >> hmap ; for ( int i = 0 ; i < stones . size (); i ++ ) { hmap [ stones [ i ]] = unordered_set < int > (); } hmap [ stones [ 0 ]]. insert ( 0 ); for ( int i = 0 ; i < stones . size (); i ++ ) { for ( int k : hmap [ stones [ i ]]) { for ( int s = k - 1 ; s <= k + 1 ; s ++ ) { if ( s > 0 && hmap . find ( stones [ i ] + s ) != hmap . end ()) { hmap [ stones [ i ] + s ]. insert ( s ); } } } } return hmap [ stones [ n - 1 ]]. size () > 0 ; } };","title":"Frog Jump"},{"location":"courses/9chap-dynamic-prog/notes/#221-maximal-square","text":"","title":"221. Maximal Square"},{"location":"courses/9chap-dynamic-prog/notes/#85-maximal-rectangle","text":"","title":"85. Maximal Rectangle"},{"location":"courses/9chap-system-design/notes/","text":"Nine Chapter System Design \u00b6 Detailed notes see the slides. Here are materials missing form the slides. Lecture 1 \u00b6 Design Netflix \u00b6 Excerpt from Mobile Needs A Four-Tier Engagement Platform How the four tiers handle the load: A client tier accounts for the unique attributes of different devices. This presentation layer insulates the unique capabilities of each app and device \u2014 desktop or mobile, browser or dedicated app \u2014 from the services that back-end applications deliver. This boundary allows developers to create back-end services like flight status and shipment notification independent of the mobile app that will consume them. Creating this clear boundary drives productivity for your developers without an onerous maintenance challenge; it will be critical for a fluid business partner network. A delivery tier handles special middle- and last-mile challenges. This element uses intelligence from the client layer to determine the optimal way to deliver contextually appropriate content. The delivery tier accomplishes this by using over-the-wire content transformation \u2014 as opposed to protocol-based conversions at the next aggregation layer \u2014 and leveraging edge-of-network cache functionality for increasingly dynamic data. CDNs such as those provided by Akamai, along with delivery optimization solutions like Instart Logic, application delivery controllers like Riverbed Stingray, and on-premises in-memory database caches, fulfill this today. An aggregation tier integrates internal and external services and transforms data. This API layer has two brokerage roles, providing discoverability between app requests and services and bidirectional translation between client requests and back-end or third-party services. This makes composing the underlying data and services easier and enables relatively simple real-time translation to the appropriate data format. The service composition becomes more dynamic with the addition of business intelligence, analytics, and role-based access, which occurs in this tier. A services tier spans internally and externally provisioned data and functionality. This final architectural element dynamically composes data and business processes through a set of continuously deployable services. This tier provides data to the layers above without concern for how that data is consumed; the other layers can exist behind the corporate firewall or externally \u2014 or both! This allows for the ultimate flexibility in the consumption and dynamic composition of services, whether leveraged by apps or by the evolving partner ecosystem. Reference \u00b6 Netflix\u2019s Viewing Data Mobile Needs A Four-Tier Engagement Platform NGINX Search Results for: netflix Netflix Tech Blog Lecture 2 Database systems \u00b6 Design user system \u00b6 Scenario Unique Key","title":"Nine Chapter System Design"},{"location":"courses/9chap-system-design/notes/#nine-chapter-system-design","text":"Detailed notes see the slides. Here are materials missing form the slides.","title":"Nine Chapter System Design"},{"location":"courses/9chap-system-design/notes/#lecture-1","text":"","title":"Lecture 1"},{"location":"courses/9chap-system-design/notes/#design-netflix","text":"Excerpt from Mobile Needs A Four-Tier Engagement Platform How the four tiers handle the load: A client tier accounts for the unique attributes of different devices. This presentation layer insulates the unique capabilities of each app and device \u2014 desktop or mobile, browser or dedicated app \u2014 from the services that back-end applications deliver. This boundary allows developers to create back-end services like flight status and shipment notification independent of the mobile app that will consume them. Creating this clear boundary drives productivity for your developers without an onerous maintenance challenge; it will be critical for a fluid business partner network. A delivery tier handles special middle- and last-mile challenges. This element uses intelligence from the client layer to determine the optimal way to deliver contextually appropriate content. The delivery tier accomplishes this by using over-the-wire content transformation \u2014 as opposed to protocol-based conversions at the next aggregation layer \u2014 and leveraging edge-of-network cache functionality for increasingly dynamic data. CDNs such as those provided by Akamai, along with delivery optimization solutions like Instart Logic, application delivery controllers like Riverbed Stingray, and on-premises in-memory database caches, fulfill this today. An aggregation tier integrates internal and external services and transforms data. This API layer has two brokerage roles, providing discoverability between app requests and services and bidirectional translation between client requests and back-end or third-party services. This makes composing the underlying data and services easier and enables relatively simple real-time translation to the appropriate data format. The service composition becomes more dynamic with the addition of business intelligence, analytics, and role-based access, which occurs in this tier. A services tier spans internally and externally provisioned data and functionality. This final architectural element dynamically composes data and business processes through a set of continuously deployable services. This tier provides data to the layers above without concern for how that data is consumed; the other layers can exist behind the corporate firewall or externally \u2014 or both! This allows for the ultimate flexibility in the consumption and dynamic composition of services, whether leveraged by apps or by the evolving partner ecosystem.","title":"Design Netflix"},{"location":"courses/9chap-system-design/notes/#reference","text":"Netflix\u2019s Viewing Data Mobile Needs A Four-Tier Engagement Platform NGINX Search Results for: netflix Netflix Tech Blog","title":"Reference"},{"location":"courses/9chap-system-design/notes/#lecture-2-database-systems","text":"","title":"Lecture 2 Database systems"},{"location":"courses/9chap-system-design/notes/#design-user-system","text":"Scenario Unique Key","title":"Design user system"},{"location":"courses/applied-scrum-for-agile/notes/","text":"Applied Scrum for Agile Project Management \u00b6 Week 3 Scrum Team Formation \u00b6 3.1.0 Prep for Scrum Team Formation \u00b6 founding document: a charter. Project Objectives Project Stakeholders all impacted people Project Constraints standard, interface, etc. Project Risks Definition of Done agree on how the work is closed. As you go through this lesson think: How many roles do you have in your project? How are you organized? As individual teams? Do you have support teams? Who defines work? What's the process? Who has the final say to \"accept\" work? Are they on the team? How might a Scrum team have advantages or disadvantages in achieving fast, innovative solutions? Scrum Team Members Product Owner (Variations) Business Rep Architect Owner Scrum Master (Variations) Project manager Junior PM Business Analyst Development Team Member (Variations) Developer Business Analyst Tech Writer Architect Support 3.1.2 Scrum Team Formation Summary Points \u00b6 The key document that sets up the team is the Charter. This includes: Project Objectives - what the sponsors and/or customers expect from this project Stakeholders - who \"has a stake\" from sponsors to customers and why. Includes technical, security, business, and operational stakeholders Constraints - what must the project also do or do not in accomplishing the objective, such as standards, interfaces, and dependencies Risks - what are major risks (internal and external), this includes business, technical, political, social, environmental Definition of Done - the agreement among stakeholders of how work is closed by the Product Owner and supporting stakeholders Once the Team Charter is in-place, the team should assemble based on the skills needed to do the work: Product Owner - person responsible for managing the Product Backlog; can only be one person; Scrum Master - person responsible for facilitating and keeping the team on track; leads Sprint Planning and Retrospectives Development Team Member - person on the team who takes responsibility for the team's success in completing the Sprint objectives These are the general types of team members on a Scrum Team, however, we know that many times we need variations based on our organization. The following are typical variations Product Owners variations Types Business Representative - from the business line using the product, or subject matter expert (SME) for customer Technical Expert - from the systems engineering or technology group of the customer, has experience with building for business lines Sponsor - the executive, director, or product manager for a business line (internal or external); focuses on marketing and feature analysis Business Owner - the owner of the business selling the product; often a combination of other types of representatives Level of Availability Dedicated - always on the team and available to the team; focusing on the backlog refinement whenever not supporting other team members On-Call - available when needed at all times; however, may have limited time outside of team support for backlog refinement (e.g. other duties) Matrixed - works on multiple products or projects, balancing time based on their own direction or the direction of departmental goals Minimal - available only for Sprint ceremonies and minimal other times Absent - available with long lead times for set consultation hours; cannot predict if/when they will be avialable Scrum Master variations Types of facilitation Facilitator - facilitates planning meetings, daily scrums, coordination with stakeholders (requirements, etc.), and retrospectives Project Manager - works as facilitator, as well as managing human resources; and is responsible for reporting and project success Junior Project Manager - works as facilitator and is responsible for reporting and project success for the Scrum team Business Analyst - works as facilitator and also provides supports for Product Owner and Development Team with requirements elaboration Types of Availability * Dedicated - soles works on the Scrum team * Split - works across multiple Scrum teams (can be the same or different projects or product lines) * Rotating Team Member - can be a rotating member of the Development Team who acts as the Scrum Master for a Sprint * Matrixed - can be part of a department with responsibilities to the department, Program, Program Management Office, etc. * Minimal / Absent - can be only available for running ceremonies and on-call for other facilitation Development Team Member variations Types of representation Generalizing Specialist - a team member that can perform any role as needed, but is trained in a certain skill set (usually development) Developer (Hard/Software) - a technical team member who focuses primarily on building the product to specifications Business Analyst / Tester - an analyst who defines and checks the work being developed by the team meets the Product Owner's intent Technical Writer - an analyst who provides support for other team members in capturing notes, metadata, and communicating work Architect - an experienced team member in a technical or business domains that serves as subject matter expert (SME) Support Team - a team member that provides enabling technology, such as work tracking software, builds, deployments, machining, etc. Types of Availability Dedicated - soles works on the Scrum team Split - works across multiple Scrum teams (can be the same or different projects or product lines) Matrixed - can be part of a department with responsibilities to the department, other projects or product lines, or Centers of Excellence On-Call - is available as needed, when needed by the team to provide surge support or expert guidance Absent - available with long lead times for set consultation hours; cannot predict if/when they will be available There are many advantages to having fully dedicated team members. Most often the objections for fully dedicating team members arise when considering the Product Owner and the Scrum Master. It appears as if these team members have a lot of \"downtime\" during the actual Spring Execution phase. This is hardly the case: Product Owners need to use time between ceremonies to liaison with Stakeholders Product Owners need to research market changes (Political, Environmental, Social, Technological) to validate product features Product Owners need to work with team members and provide quick feedback to keep the Development Team running as fast as possible Scrum Masters should to facilitate meetings, especially with stakeholders - this can take a long time to research and prepare! Scrum Masters should to help guide requirements, development, and testing based on best practices Scrum Masters should to help escalate issues outside the team; and address real-world obstacles that get in the team's way Scrum Masters should always be analyzing the data of the team's performance to identify continuous improvement opportunities Scrum Master is often having to \"prep\" the stakeholders and retrain where necessary the Product Owner and Development Team How does this compare with your organization? Do you have \"professional facilitators\" who can ensure productive meetings? What about dedicated product owners to answer questions and make calls so development keeps on-track? 3.1.4 Scrum Team Formation References \u00b6 Scrum Alliance - www.scrumalliance.org Agile Atlas Learn About Scrum Scrum.org What Is Scrum Scrum Guide Note that the real-world modifications and adjustments are based on the real-world experiences of the Instructor, Team, and their colleagues who all deserve credit for the production of this material. 3.2.0 Prep for Three Part User Story \u00b6 In this next lesson you'll be taken through the process of writing a good User Story, and it's three parts: Value Statement Assumptions Acceptance Criteria These elements are essential for any story because often what is captured is only one or two parts at most. The Value Statement guides development with a simple formula: As a ...[Who]... I want... [What Functionality]... in order to... [Why It's Important]. This framework is easy enough to read and learn, but hard to master. Using this framework enables inquiry-based elaboration by the team (asking good questions), and offers a means of empathizing with the user [Who] may not be able to fully communicate their needs. The Assumptions constrain the development based on additional considerations for the specific User Story, Theme, or Product. This is also the portion of the User Story that captures any non-functional requirements for implementation. By having a well specified set of assumptions the Development Team knows the bounds of the solution space. The Acceptance Criteria set the goal for the product component being built by stating what tests must be passed. These tests are logical and/or numerical in nature for clearly achievable goals of the user story. Often through elaborating Acceptance Criteria the Development Team can better assess the true needs and assumptions for implementing a given story. Critical thinking point - think about how your requirements are currently captured. Are they one line? A hiearchy of detailing constraints? How do you know the priority, reasoning, and means of validating your requirements are met? 3.2.2 Three-Part User Story Summary Points \u00b6 There are three parts to a User Story: Value Statement Assumptions Acceptance Criteria These work together to for the complete \"User Story.\" It is NOT just the Value Statement. The proper means of creating a Value Statement is up for debate, but usually the statements are as follows: As a...[Who]...I want to...[What Functionality Desired]...in order to...[Why It's Important]. The \"Who\" is the user, either directly using or consuming outputs from the product the Development Team is building. It's important that if this user is not clearly defined already in the Project Charter then they are added by the Product Owner, with consensus by the Stakeholders. This ensures there's a clear understanding when say the term \"End User\" is put into the [Who] part of the Value Statement. Usually \"End User\" is too vague. It should be a descriptive title such as \"Research Librarian\" or \"Fighter Co-Pilot.\" The \"What\" should be the MOST important aspect of the component being built. Often because product features serve multiple users and needs or wants of users the User Stories can seem to compete with one another. When multiple types of value or outputs are created by a new product feature or component, make sure that the one that is put in the Value Statement is the priority. Priorities matter because a product that does everything for everyone in the end serves no one well. Most importantly the \"What\" or \"Functionality Desired\" should be stated in only business terms. Examples: I want to Login Using My User Name and Password --- WRONG! I want to access my account --- RIGHT! The \"Why It's Important\" is a critical failing of most User Stories. This is true for experienced and beginning Scrum teams. The easiest trap is to simply restate the \"What\" in another way. Such as \"I want to Login to access my account.\" As stated above, the proper \"What\" in this statement is \"access my account.\" So what's the \"Why?\" It could be: To check notifications To assign work To check the rankings of my fantasy football team The key is that it's important and the MOST important \"Why\" of the feature or function. Others that are less important belong in the Assumptions. Additional key points: Assumptions are a bucket for everything else Captures less important value created by the User Story Captures detailing of Why the user story is important Identifies constraints from preceding or proceeding tasks, work, components, etc. Identifies all the standards, influences, reference architectures, etc. Captures other reasons \"Why\" this story might be important Can limit the Acceptance Criteria and the Value Statement - not just the Value Statement The only information that is not needed to be listed here, are standard procedures captured in the Definition of Done. Acceptance Criteria are NOT restatements of the Value Statement Should clearly define the primary use cases for testing Must specify any performance or loading that the product increment should meet Must be comprehensive in detailing all tests that will be run to close the story Finally it's important to note that all User Stories should be modular because they capture business functionality . If written correctly these shouldn't ever be in conflict because of technical dependencies. User Stories are owned by the Product Owner when in the Product Backlog, and then once they are moved into the Sprint Backlog they are owned by the Development Team. The Development Team can update these stories with comments, but cannot change the Value Statement, Assumption, or Acceptance Criteria without the Product Owner and Development Team Members' consent. All User Stories are governed by the Project's Definition of Done. In this way the Definition of Done is the hidden \"Fourth Part\" of any user story, although it is focused on process for closing a story. The Definition of Done captures all the standard requirements for completing work defined in a User Story, such as: Standard approvals, Reviews by stakeholders Prototyping (if required) Documentation (for sustainability, reporting, etc.) Design constraints (for compliance, integration, etc.) In this way the Definition of Done helps to modularize the statements within an User Story; but it also provides a clear set of expectations around quality control and sustainability. These benefits are further elaborated in part in this course, and in greater depth throughout the Agile Project Management Certificate program. 3.2.4 Three-Part User Story References \u00b6 Credit for clearly developing an understanding of User Stories belongs to IBM's Agile Center of Excellence, and in particular Paul Gorans of IBM. His lessons derive from his work developing the Agile With Discipline (AWD) framework made famous by Scott Ambler as an open-source publication called Disciplined Agile Delivery (DAD). Additional Resources for great User Story Development Include: Write a Great User Story User Story Examples Guide to User Story Process","title":"Applied Scrum for Agile Project Management"},{"location":"courses/applied-scrum-for-agile/notes/#applied-scrum-for-agile-project-management","text":"","title":"Applied Scrum for Agile Project Management"},{"location":"courses/applied-scrum-for-agile/notes/#week-3-scrum-team-formation","text":"","title":"Week 3 Scrum Team Formation"},{"location":"courses/applied-scrum-for-agile/notes/#310-prep-for-scrum-team-formation","text":"founding document: a charter. Project Objectives Project Stakeholders all impacted people Project Constraints standard, interface, etc. Project Risks Definition of Done agree on how the work is closed. As you go through this lesson think: How many roles do you have in your project? How are you organized? As individual teams? Do you have support teams? Who defines work? What's the process? Who has the final say to \"accept\" work? Are they on the team? How might a Scrum team have advantages or disadvantages in achieving fast, innovative solutions? Scrum Team Members Product Owner (Variations) Business Rep Architect Owner Scrum Master (Variations) Project manager Junior PM Business Analyst Development Team Member (Variations) Developer Business Analyst Tech Writer Architect Support","title":"3.1.0 Prep for Scrum Team Formation"},{"location":"courses/applied-scrum-for-agile/notes/#312-scrum-team-formation-summary-points","text":"The key document that sets up the team is the Charter. This includes: Project Objectives - what the sponsors and/or customers expect from this project Stakeholders - who \"has a stake\" from sponsors to customers and why. Includes technical, security, business, and operational stakeholders Constraints - what must the project also do or do not in accomplishing the objective, such as standards, interfaces, and dependencies Risks - what are major risks (internal and external), this includes business, technical, political, social, environmental Definition of Done - the agreement among stakeholders of how work is closed by the Product Owner and supporting stakeholders Once the Team Charter is in-place, the team should assemble based on the skills needed to do the work: Product Owner - person responsible for managing the Product Backlog; can only be one person; Scrum Master - person responsible for facilitating and keeping the team on track; leads Sprint Planning and Retrospectives Development Team Member - person on the team who takes responsibility for the team's success in completing the Sprint objectives These are the general types of team members on a Scrum Team, however, we know that many times we need variations based on our organization. The following are typical variations Product Owners variations Types Business Representative - from the business line using the product, or subject matter expert (SME) for customer Technical Expert - from the systems engineering or technology group of the customer, has experience with building for business lines Sponsor - the executive, director, or product manager for a business line (internal or external); focuses on marketing and feature analysis Business Owner - the owner of the business selling the product; often a combination of other types of representatives Level of Availability Dedicated - always on the team and available to the team; focusing on the backlog refinement whenever not supporting other team members On-Call - available when needed at all times; however, may have limited time outside of team support for backlog refinement (e.g. other duties) Matrixed - works on multiple products or projects, balancing time based on their own direction or the direction of departmental goals Minimal - available only for Sprint ceremonies and minimal other times Absent - available with long lead times for set consultation hours; cannot predict if/when they will be avialable Scrum Master variations Types of facilitation Facilitator - facilitates planning meetings, daily scrums, coordination with stakeholders (requirements, etc.), and retrospectives Project Manager - works as facilitator, as well as managing human resources; and is responsible for reporting and project success Junior Project Manager - works as facilitator and is responsible for reporting and project success for the Scrum team Business Analyst - works as facilitator and also provides supports for Product Owner and Development Team with requirements elaboration Types of Availability * Dedicated - soles works on the Scrum team * Split - works across multiple Scrum teams (can be the same or different projects or product lines) * Rotating Team Member - can be a rotating member of the Development Team who acts as the Scrum Master for a Sprint * Matrixed - can be part of a department with responsibilities to the department, Program, Program Management Office, etc. * Minimal / Absent - can be only available for running ceremonies and on-call for other facilitation Development Team Member variations Types of representation Generalizing Specialist - a team member that can perform any role as needed, but is trained in a certain skill set (usually development) Developer (Hard/Software) - a technical team member who focuses primarily on building the product to specifications Business Analyst / Tester - an analyst who defines and checks the work being developed by the team meets the Product Owner's intent Technical Writer - an analyst who provides support for other team members in capturing notes, metadata, and communicating work Architect - an experienced team member in a technical or business domains that serves as subject matter expert (SME) Support Team - a team member that provides enabling technology, such as work tracking software, builds, deployments, machining, etc. Types of Availability Dedicated - soles works on the Scrum team Split - works across multiple Scrum teams (can be the same or different projects or product lines) Matrixed - can be part of a department with responsibilities to the department, other projects or product lines, or Centers of Excellence On-Call - is available as needed, when needed by the team to provide surge support or expert guidance Absent - available with long lead times for set consultation hours; cannot predict if/when they will be available There are many advantages to having fully dedicated team members. Most often the objections for fully dedicating team members arise when considering the Product Owner and the Scrum Master. It appears as if these team members have a lot of \"downtime\" during the actual Spring Execution phase. This is hardly the case: Product Owners need to use time between ceremonies to liaison with Stakeholders Product Owners need to research market changes (Political, Environmental, Social, Technological) to validate product features Product Owners need to work with team members and provide quick feedback to keep the Development Team running as fast as possible Scrum Masters should to facilitate meetings, especially with stakeholders - this can take a long time to research and prepare! Scrum Masters should to help guide requirements, development, and testing based on best practices Scrum Masters should to help escalate issues outside the team; and address real-world obstacles that get in the team's way Scrum Masters should always be analyzing the data of the team's performance to identify continuous improvement opportunities Scrum Master is often having to \"prep\" the stakeholders and retrain where necessary the Product Owner and Development Team How does this compare with your organization? Do you have \"professional facilitators\" who can ensure productive meetings? What about dedicated product owners to answer questions and make calls so development keeps on-track?","title":"3.1.2 Scrum Team Formation Summary Points"},{"location":"courses/applied-scrum-for-agile/notes/#314-scrum-team-formation-references","text":"Scrum Alliance - www.scrumalliance.org Agile Atlas Learn About Scrum Scrum.org What Is Scrum Scrum Guide Note that the real-world modifications and adjustments are based on the real-world experiences of the Instructor, Team, and their colleagues who all deserve credit for the production of this material.","title":"3.1.4 Scrum Team Formation References"},{"location":"courses/applied-scrum-for-agile/notes/#320-prep-for-three-part-user-story","text":"In this next lesson you'll be taken through the process of writing a good User Story, and it's three parts: Value Statement Assumptions Acceptance Criteria These elements are essential for any story because often what is captured is only one or two parts at most. The Value Statement guides development with a simple formula: As a ...[Who]... I want... [What Functionality]... in order to... [Why It's Important]. This framework is easy enough to read and learn, but hard to master. Using this framework enables inquiry-based elaboration by the team (asking good questions), and offers a means of empathizing with the user [Who] may not be able to fully communicate their needs. The Assumptions constrain the development based on additional considerations for the specific User Story, Theme, or Product. This is also the portion of the User Story that captures any non-functional requirements for implementation. By having a well specified set of assumptions the Development Team knows the bounds of the solution space. The Acceptance Criteria set the goal for the product component being built by stating what tests must be passed. These tests are logical and/or numerical in nature for clearly achievable goals of the user story. Often through elaborating Acceptance Criteria the Development Team can better assess the true needs and assumptions for implementing a given story. Critical thinking point - think about how your requirements are currently captured. Are they one line? A hiearchy of detailing constraints? How do you know the priority, reasoning, and means of validating your requirements are met?","title":"3.2.0 Prep for Three Part User Story"},{"location":"courses/applied-scrum-for-agile/notes/#322-three-part-user-story-summary-points","text":"There are three parts to a User Story: Value Statement Assumptions Acceptance Criteria These work together to for the complete \"User Story.\" It is NOT just the Value Statement. The proper means of creating a Value Statement is up for debate, but usually the statements are as follows: As a...[Who]...I want to...[What Functionality Desired]...in order to...[Why It's Important]. The \"Who\" is the user, either directly using or consuming outputs from the product the Development Team is building. It's important that if this user is not clearly defined already in the Project Charter then they are added by the Product Owner, with consensus by the Stakeholders. This ensures there's a clear understanding when say the term \"End User\" is put into the [Who] part of the Value Statement. Usually \"End User\" is too vague. It should be a descriptive title such as \"Research Librarian\" or \"Fighter Co-Pilot.\" The \"What\" should be the MOST important aspect of the component being built. Often because product features serve multiple users and needs or wants of users the User Stories can seem to compete with one another. When multiple types of value or outputs are created by a new product feature or component, make sure that the one that is put in the Value Statement is the priority. Priorities matter because a product that does everything for everyone in the end serves no one well. Most importantly the \"What\" or \"Functionality Desired\" should be stated in only business terms. Examples: I want to Login Using My User Name and Password --- WRONG! I want to access my account --- RIGHT! The \"Why It's Important\" is a critical failing of most User Stories. This is true for experienced and beginning Scrum teams. The easiest trap is to simply restate the \"What\" in another way. Such as \"I want to Login to access my account.\" As stated above, the proper \"What\" in this statement is \"access my account.\" So what's the \"Why?\" It could be: To check notifications To assign work To check the rankings of my fantasy football team The key is that it's important and the MOST important \"Why\" of the feature or function. Others that are less important belong in the Assumptions. Additional key points: Assumptions are a bucket for everything else Captures less important value created by the User Story Captures detailing of Why the user story is important Identifies constraints from preceding or proceeding tasks, work, components, etc. Identifies all the standards, influences, reference architectures, etc. Captures other reasons \"Why\" this story might be important Can limit the Acceptance Criteria and the Value Statement - not just the Value Statement The only information that is not needed to be listed here, are standard procedures captured in the Definition of Done. Acceptance Criteria are NOT restatements of the Value Statement Should clearly define the primary use cases for testing Must specify any performance or loading that the product increment should meet Must be comprehensive in detailing all tests that will be run to close the story Finally it's important to note that all User Stories should be modular because they capture business functionality . If written correctly these shouldn't ever be in conflict because of technical dependencies. User Stories are owned by the Product Owner when in the Product Backlog, and then once they are moved into the Sprint Backlog they are owned by the Development Team. The Development Team can update these stories with comments, but cannot change the Value Statement, Assumption, or Acceptance Criteria without the Product Owner and Development Team Members' consent. All User Stories are governed by the Project's Definition of Done. In this way the Definition of Done is the hidden \"Fourth Part\" of any user story, although it is focused on process for closing a story. The Definition of Done captures all the standard requirements for completing work defined in a User Story, such as: Standard approvals, Reviews by stakeholders Prototyping (if required) Documentation (for sustainability, reporting, etc.) Design constraints (for compliance, integration, etc.) In this way the Definition of Done helps to modularize the statements within an User Story; but it also provides a clear set of expectations around quality control and sustainability. These benefits are further elaborated in part in this course, and in greater depth throughout the Agile Project Management Certificate program.","title":"3.2.2 Three-Part User Story Summary Points"},{"location":"courses/applied-scrum-for-agile/notes/#324-three-part-user-story-references","text":"Credit for clearly developing an understanding of User Stories belongs to IBM's Agile Center of Excellence, and in particular Paul Gorans of IBM. His lessons derive from his work developing the Agile With Discipline (AWD) framework made famous by Scott Ambler as an open-source publication called Disciplined Agile Delivery (DAD). Additional Resources for great User Story Development Include: Write a Great User Story User Story Examples Guide to User Story Process","title":"3.2.4 Three-Part User Story References"},{"location":"courses/concurrent-prog-java/notes/","text":"Concurrent Programming in Java \u00b6 parallelStream() fork/join conditional variable Week 1 \u00b6 1.1 Threads \u00b6 Thread Example // main thread T0 t1 = new Thread ( s1 ); // s1 is the computation assign val x t1 . start (); // start thread t1 by main thread t0 s2 ; // execute computation task by t0 t1 . join (); // wait the val x computation from t1 for use in s3 s3 ; // computation read val x Deadlock // If s1 have t2.join() and s1` have t1.join(), deadlock will happen // main thread T0 t1 = new Thread ( s1 ); // s1 is the computation assign val x t2 = new Thread ( s1 ` ); // s1` is the computation assign val x t1 . start (); // start thread t1 by main thread t0 t2 . start () // start thread t2 by main thread t0 s2 ; // execute computation task by t0 t1 . join (); // wait the val x computation from t1 for use in s3 s3 ; // computation read val x 1.1 Lecture Summary In this lecture, we learned the concept of threads as lower-level building blocks for concurrent programs. A unique aspect of Java compared to prior mainstream programming languages is that Java included the notions of threads (as instances of the java.lang.Thread class in its language definition right from the start. When an instance of Thread is created (via a new operation), it does not start executing right away; instead, it can only start executing when its start() method is invoked. The statement or computation to be executed by the thread is specified as a parameter to the constructor. The Thread class also includes a wait operation in the form of a join() method. If thread t0 performs a t1.join() call, thread t0 will be forced to wait until thread t1 completes, after which point it can safely access any values computed by thread t1 . Since there is no restriction on which thread can perform a join on which other thread, it is possible for a programmer to erroneously create a deadlock cycle with join operations. (A deadlock occurs when two threads wait for each other indefinitely, so that neither can make any progress.) 1.1 Optional Reading Wikipedia article on Threads Tutorial on Java threads Documentation on Thread class in Java 8 1.2 Structured Locks \u00b6 Synchronized incr ( A ) { synchronized ( A ) { A . count = A . count + 1 ; } // release A } Unbounded Buffer // x = {BUF, IN, OUT} insert ( item ) { synchronized ( x ) { buf [ in ] = item ; in ++ ; } } remove () { synchronized ( x ) { item = buf [ out ] ; out ++ ; return item ; } } Bounded Buffer // x = {BUF, IN, OUT} with size K insert ( item ) { synchronized ( x ) { wait (); // wait if x is full buf [ in ] = item ; in ++ ; notify (); } } remove () { synchronized ( x ) { wait (); // wait if x is empty item = buf [ out ] ; out ++ ; notify (); return item ; } } 1.2 Lecture Summary In this lecture, we learned about structured locks, and how they can be implemented using synchronized statements and methods in Java. Structured locks can be used to enforce mutual exclusion and avoid data races, as illustrated by the incr() method in the A.count example, and the insert() and remove() methods in the the Buffer example. A major benefit of structured locks is that their acquire and release operations are implicit, since these operations are automatically performed by the Java runtime environment when entering and exiting the scope of a synchronized statement or method, even if an exception is thrown in the middle. We also learned about wait() and notify() operations that can be used to block and resume threads that need to wait for specific conditions. For example, a producer thread performing an insert() operation on a bounded buffer can call wait() when the buffer is full, so that it is only unblocked when a consumer thread performing a remove() operation calls notify() . Likewise, a consumer thread performing a remove() operation on a bounded buffer can call wait() when the buffer is empty, so that it is only unblocked when a producer thread performing an insert() operation calls notify() . Structured locks are also referred to as intrinsic locks or monitors. 1.2 Optional Reading Tutorial on Intrinsic Locks and Synchronization in Java Tutorial on Guarded Blocks in Java Wikipedia article on Monitors 1.3 Unstructured Locks \u00b6 Hand-over-hand lock // LinkedList: N1->N2->N3->N4 // Lock4Nodes: L1 L2 L3 L4 lock ( L1 ); lock ( L2 ); WORK with N1 , N2 unlock ( L1 ); lock ( L3 ); WORK with N2 , N3 unlock ( L2 ); lock ( L4 ); WORK with N3 , N4 unlock ( L3 ); Hand-over-hand lock using try_lock // LinkedList: N1->N2->N3->N4 // Lock4Nodes: L1 L2 L3 L4 try lock ( L1 ); if ( success ) { lock ( L2 ); WORK with N1 , N2 unlock ( L1 ); lock ( L3 ); WORK with N2 , N3 unlock ( L2 ); lock ( L4 ); WORK with N3 , N4 unlock ( L3 ); } R/W lock // data: Array[] // R/W lock: L search ( x ) { read_lock ( L ) A [ i ] == x ; // read only unlock ( L ) } update ( i , y ) { write_lock ( L ) A [ i ] = y ; // write unlock ( L ) } 1.3 Lecture Summary In this lecture, we introduced unstructured locks (which can be obtained in Java by creating instances of ReentrantLock() , and used three examples to demonstrate their generality relative to structured locks. The first example showed how explicit lock() and unlock() operations on unstructured locks can be used to support a hand-over-hand locking pattern that implements a non-nested pairing of lock/unlock operations which cannot be achieved with synchronized statements/methods. The second example showed how the tryLock() operations in unstructured locks can enable a thread to check the availability of a lock, and thereby acquire it if it is available or do something else if it is not. The third example illustrated the value of read-write locks (which can be obtained in Java by creating instances of ReentrantReadWriteLock() , whereby multiple threads are permitted to acquire a lock L in \"read mode\", L.readLock().lock() , but only one thread is permitted to acquire the lock in \u201cwrite mode\u201d, L.writeLock().lock() . However, it is also important to remember that the generality and power of unstructured locks is accompanied by an extra responsibility on the part of the programmer, e.g., ensuring that calls to unlock() are not forgotten, even in the presence of exceptions. 1.3 Optional Reading Tutorial on Lock Objects in Java Documentation on Java\u2019s Lock interfaces 1.4 Liveness \u00b6 Deadlock // T1 synchronized ( A ) { synchronized ( B ) { } } // T2 synchronized ( B ) { synchronized ( A ) { } } Livelock // T1 do { synchronized ( B ) { x . incr (); r = x . get (); } } while ( R < 2 ) // T2 do { synchronized ( B ) { x . decr (); r = x . get (); } } while ( R > - 2 ) Starvation // T1 do { int1 = s1 . readline (); print int1 ; } while (...) // ... // T100 do { int100 = s100 . readline (); print int100 ; } while (...) 1.4 Lecture Summary In this lecture, we studied three ways in which a parallel program may enter a state in which it stops making forward progress. For sequential programs, an \"infinite loop\" is a common way for a program to stop making forward progress, but there are other ways to obtain an absence of progress in a parallel program. The first is deadlock , in which all threads are blocked indefinitely, thereby preventing any forward progress. The second is livelock , in which all threads repeatedly perform an interaction that prevents forward progress, e.g., an infinite \u201cloop\u201d of repeating lock acquire/release patterns. The third is starvation , in which at least one thread is prevented from making any forward progress. The term \"liveness\" refers to a progress guarantee. The three progress guarantees that correspond to the absence of the conditions listed above are deadlock freedom, livelock freedom, and starvation freedom. 1.4 Optional Reading Deadlock example with synchronized methods in Java Starvation and Livelock examples in Java Wikipedia article on Deadlock and Livelock 1.5 Dining Philosophers \u00b6 think pick up chopsticks (left, right) eat put down chopsticks Structured locks // if all pick up the left first, deadlock while (...) { Think ; sychronized ( Left ) { sychronized ( Right ) { Eat ; } } } unstructured locks // if all pick up the left first, livelock, but no deadlock while (...) { Think ; s1 = tryLock ( Left ); if ( ! s1 ) continue ; s2 = tryLock ( Right ); if ( ! s2 ) { unlock ( Left ); continue ; } Eat ; } Modified version // have one of the philosophers pick right first and then left. // However, this can also cause the starvation problem. To solve // the problem completely, we need to use another synchronization // premitive called semaphore. 1.5 Lecture Summary In this lecture, we studied a classical concurrent programming example that is referred to as the Dining Philosophers Problem . In this problem, there are five threads, each of which models a \"philosopher\" that repeatedly performs a sequence of actions which include think, pick up chopsticks, eat, and put down chopsticks. First, we examined a solution to this problem using structured locks, and demonstrated how this solution could lead to a deadlock scenario (but not livelock). Second, we examined a solution using unstructured locks with tryLock() and unlock() operations that never block, and demonstrated how this solution could lead to a livelock scenario (but not deadlock). Finally, we observed how a simple modification to the first solution with structured locks, in which one philosopher picks up their right chopstick and their left, while the others pick up their left chopstick first and then their right, can guarantee an absence of deadlock. 1.5 Optional Reading Wikipedia article on the Dining Philosophers Problem 1.6 Mini Project \u00b6 The mini project used Java ReentrantLock and ReentrantReadWriteLock to ensure the list operations are free of bugs if run by multiple threads. One practical tip is that when using the basic lock primitive the lock statement can be put in the try and unlock in the finally statement, respectively. Week 2 \u00b6 2.1 Critical Sections (Isloated Construct) \u00b6 Balance Transfer // My balance: 5000 // Family balance: 1000 // Daughter balance: 0 T1 { myBal = myBal - 100 ; // R1 familyBal = familyBal + 100 ; // W1 } T2 { familyBal = familyBal - 100 ; // R2 DauBal = DarBal + 100 ; // W2 } Use isolated // My balance: 5000 // Family balance: 1000 // Daughter balance: 0 T1 { isolated { myBal = myBal - 100 ; // R1 familyBal = familyBal + 100 ; // W1 } } T2 { isolated { familyBal = familyBal - 100 ; // R2 DauBal = DarBal + 100 ; // W2 } } 2.1 Lecture Summary In this lecture, we learned how critical sections and the isolated construct can help concurrent threads manage their accesses to shared resources, at a higher level than just using locks. When programming with threads, it is well known that the following situation is defined to be a data race error \u2014 when two accesses on the same shared location can potentially execute in parallel, with least one access being a write. However, there are many cases in practice when two tasks may legitimately need to perform concurrent accesses to shared locations, as in the bank transfer example. With critical sections, two blocks of code that are marked as isolated, say A and B , are guaranteed to be executed in mutual exclusion with A executing before B or vice versa. With the use of isolated constructs, it is impossible for the bank transfer example to end up in an inconsistent state because all the reads and writes for one isolated section must complete before the start of another isolated construct. Thus, the parallel program will see the effect of one isolated section completed before another isolated section can start. 2.1 Optional Reading Wikipedia article on Critical Sections Wikipedia article on Atomicity 2.2 Object Based Isolation (Monitors) \u00b6 DoubleLinkedList Example // A <-> B <-> C <-> D <-> E delete ( cur ) { cur . prev . next = cur . next ; cur . next . prev = cur . prev ; } // T1: Delete(B) // T2: Delete(C) // T3: Delete(E) DoubleLinkedList using isolated // A <-> B <-> C <-> D <-> E delete ( cur ) { isolated ( cur , cur . prev , cur . next ) { cur . prev . next = cur . next ; cur . next . prev = cur . prev ; } } // T1: Delete(B) touch (A, B, C) // T2: Delete(C) touch (B, C, D) // T3: Delete(E) touch (D, E, F) // T1 and T3 can be done in the same time. Monitor intuition // P1, process 1 // P2, process 2 Isolated ( M1 ) M1 { P1 : Isolated ( M1 ); P2 : Isolated ( M1 ); } M2 { P1 : Isolated ( M2 ); P2 : Isolated ( M2 ); } Isolated ( M2 ) 2.2 Lecture Summary In this lecture, we studied object-based isolation, which generalizes the isolated construct and relates to the classical concept of monitors. The fundamental idea behind object-based isolation is that an isolated construct can be extended with a set of objects that indicate the scope of isolation, by using the following rules: if two isolated constructs have an empty intersection in their object sets they can execute in parallel, otherwise they must execute in mutual exclusion. We observed that implementing this capability can be very challenging with locks because a correct implementation must enforce the correct levels of mutual exclusion without entering into deadlock or livelock states. The linked-list example showed how the object set for a delete() method can be defined as consisting of three objects \u2014 the current, previous, and next objects in the list, and that this object set is sufficient to safely enable parallelism across multiple calls to delete() . The Java code sketch to achieve this object-based isolation using the PCDP library is as follows: isolated ( cur , cur . prev , cur . next , () -> { . . . // Body of object-based isolated construct }); The relationship between object-based isolation and monitors is that all methods in a monitor object, M1 , are executed as object-based isolated constructs with a singleton object set, M1 . Similarly, all methods in a monitor object, M2 , are executed as object-based isolated constructs with a singleton object set, M2 which has an empty intersection with M1 . 2.2 Optional Reading Wikipedia article on Monitors 2.3 Concurrent Spanning Tree Algorithm \u00b6 Spanning Tree Algorithm Compute ( v ) { for each neighbor c of v s <- MakeParent ( v , c ) if ( s ) Compute ( c ); } MakeParent ( v , c ) { if ( c . parent == Null ) { c . parent = v ; success = true ; } else { success = false ; return success ; } } Isolation Construct Compute ( v ) { for each neighbor c of v s <- MakeParent ( v , c ) if ( s ) Compute ( c ); } MakeParent ( v , c ) { isolated ( c ) { if ( c . parent == Null ) { // compare-and-set premitive c . parent = v ; success = true ; } else { success = false ; return success ; } } } 2.3 Lecture Summary In this lecture, we learned how to use object-based isolation to create a parallel algorithm to compute spanning trees for an undirected graph. Recall that a spanning tree specifies a subset of edges in the graph that form a tree (no cycles), and connect all vertices in the graph. A standard recursive method for creating a spanning tree is to perform a depth-first traversal of the graph (the Compute(v) function in our example), making the current vertex a parent of all its neighbors that don\u2019t already have a parent assigned in the tree (the MakeParent(v, c) function in the example). The approach described in this lecture to parallelize the spanning tree computation executes recursive Compute(c) method calls in parallel for all neighbors, c , of the current vertex, v . Object-based isolation helps avoid a data race in the MakeParent(v,c) method, when two parallel threads might attempt to call MakeParent(v1, c) and MakeParent(v2, c) on the same vertex c at the same time. In this example, the role of object-based isolation is to ensure that all calls to MakeParent(v,c) with the same c value must execute the object-based isolated statement in mutual exclusion, whereas calls with different values of c can proceed in parallel. 2.3 Optional Reading Wikipedia article on Spanning Trees 2.4 Atomic Variables \u00b6 Atomic Variables (Integer) // X[0, ... N-1] do { j = curr ; // curr.getAndAdd(1); atomic integer!!!. cur = cur + 1 ; if ( j >= N ) break ; process ( X [ j ] ) } while ( true ) Atomic Reference (compareAndSet) compareAndSet ( expected , new ) { isolated ( this ) { if ( this . val == expected ) { this . val = new ; return true ; } else { return false ; } } } 2.4 Lecture Summary In this lecture, we studied Atomic Variables , an important special case of object-based isolation which can be very efficiently implemented on modern computer systems. In the example given in the lecture, we have multiple threads processing an array, each using object-based isolation to safely increment a shared object, cur , to compute an index j which can then be used by the thread to access a thread-specific element of the array. However, instead of using object-based isolation, we can declare the index cur to be an Atomic Integer variable and use an atomic operation called getAndAdd() to atomically read the current value of cur and increment its value by 1. Thus, j = cur.getAndAdd(1) has the same semantics as isolated (cur) { j = cur; cur = cur+1;} but is implemented much more efficiently using hardware support on today\u2019s machines. Another example that we studied in the lecture concerns Atomic Reference variables, which are reference variables that can be atomically read and modified using methods such as compareAndSet() . If we have an atomic reference ref , then the call to ref.compareAndSet(expected, new) will compare the value of ref to expected , and if they are the same, set the value of ref to new and return true . This all occurs in one atomic operation that cannot be interrupted by any other methods invoked on the ref object. If ref and expected have different values, compareAndSet() will not modify anything and will simply return false. 2.4 Optional Reading Tutorial on Atomic Integers in Java Article in Java theory and practice series on Going atomic Wikipedia article on Atomic Wrapper Classes in Java 2.5 Read, Write Isolation \u00b6 Read/Write Isolation // T1: C.Get(k1), C.Get(k3), C.Get(k5) // T2: C.Get(k1), C.Get(k3), C.Get(k5) // T3: C.Put(k1) isolated ( read ( C )) { V = C . get ( k1 ); } isolated ( write ( C )) { C . put ( k2 , v2 ); } Read/Write Isolation in LinkedList example delete ( cur ) { isolated ( write ( cur . next , cur . prev ), read ( cur )) { cur . prev . next = cur . next ; cur . next . prev = cur . prev ; } } 2.5 Lecture Summary In this lecture we discussed Read-Write Isolation , which is a refinement of object-based isolation, and is a higher-level abstraction of the read-write locks studied earlier as part of Unstructured Locks. The main idea behind read-write isolation is to separate read accesses to shared objects from write accesses. This approach enables two threads that only read shared objects to freely execute in parallel since they are not modifying any shared objects. The need for mutual exclusion only arises when one or more threads attempt to enter an isolated section with write access to a shared object. This approach exposes more concurrency than object-based isolation since it allows read accesses to be executed in parallel. In the doubly-linked list example from our lecture, when deleting an object cur from the list by calling delete(cur) , we can replace object-based isolation on cur with read-only isolation, since deleting an object does not modify the object being deleted; only the previous and next objects in the list need to be modified. 2.5 Optional Reading Wikipedia article on Readers-writer lock 2.6 Mini project \u00b6 Week 3 \u00b6 Akka 3.1 Actors \u00b6 getAndAdd (){ j = cur ; cur = cur + delta ; return j ; } // method doesn't doing correct isolation foo () { cur = ... } Actor (reactive) Mailbox Method Local State 3.1 Lecture Summary In this lecture, we introduced the Actor Model as an even higher level of concurrency control than locks or isolated sections. One limitation of locks, and even isolated sections, is that, while many threads might correctly control the access to a shared object (e.g., by using object-based isolation) it only takes one thread that accesses the object directly to create subtle and hard-to-discover concurrency errors. The Actor model avoids this problem by forcing all accesses to an object to be isolated by default. The object is part of the local state of an actor, and cannot be accessed directly by any other actor. An Actor consists of a Mailbox, a set of Methods, and Local State. The Actor model is reactive, in that actors can only execute methods in response to messages; these methods can read/write local state and/or send messages to other actors. Thus, the only way to modify an object in a pure actor model is to send messages to the actor that owns that object as part of its local state. In general, messages sent to actors from different actors can be arbitrarily reordered in the system. However, in many actor models, messages sent between the same pair of actors preserve the order in which they are sent. 3.1 Optional Reading Wikipedia article on the Actor Model Documentation on the Akka Actor Library (though Akka is not used in this course, it is a useful library to be aware of if you are interested in using the actor model with Java and Scala applications) 3.2 Actor Examples \u00b6 // print actor process ( s ) { // like a callback if ( s == \"Exit\" ) exit (); else print s ; } 3.2 Lecture Summary In this lecture, we further studied the Actor Model through two simple examples of using actors to implement well-known concurrent programming patterns. The PrintActor in our first example processes simple String messages by printing them. If an EXIT message is sent, then the PrintActor completes its current computation and exits. As a reminder, we assume that messages sent between the same pair of actors preserve the order in which they are sent. In the second example, we created an actor pipeline, in which one actor checks the incoming messages and only forwards the ones that are in lower case. The second actor processes the lowercase messages and only forwards the ones that are of even length. This example illustrates the power of the actor model, as this concurrent system would be much more difficult to implement using threads, for example, since much care would have to be taken on how to implement a shared mailbox for correct and efficient processing by parallel threads. 3.2 Optional Reading Wikipedia article on Pipeline Parallelism . 3.3 Sieve of Eratosthenes Algorithm \u00b6 3.3 Lecture Summary In this lecture, we studied how to use actors to implement a pipelined variant of the Sieve of Eratosthenes algorithm for generating prime numbers. This example illustrates the power of the Actor Model, including dynamic creation of new actors during a computation. To implement the Sieve of Eratosthenes, we first create an actor, Non-Mul-2, that receives (positive) natural numbers as input (up to some limit), and then filters out the numbers that are multiples of 2. After receiving a number that is not a multiple of 2 (in our case, the first would be 3), the Non-Mul-2 actor creates the next actor in the pipeline, Non-Mul-3, with the goal of discarding all the numbers that are multiples of 3. The Non-Mul-2 actor then forwards all non-multiples of 2 to the Non-Mul-3 actor. Similarly, this new actor will create the next actor in the pipeline, Non-Mul-5, with the goal of discarding all the numbers that are multiples of 5. The power of the Actor Model is reflected in the dynamic nature of this problem, where pieces of the computation (new actors) are created dynamically as needed. A Java code sketch for the process() method for an actor responsible for filtering out multiples of the actor's \"local prime\" in the Sieve of Eratosthenes is as follows: public void process ( final Object msg ) { int candidate = ( Integer ) msg ; // Check if the candidate is a non-multiple of the \"local prime\". // For example, localPrime = 2 in the Non-Mul-2 actor boolean nonMul = (( candidate % localPrime ) != 0 ); // nothing needs to be done if nonMul = false if ( nonMul ) { if ( nextActor == null ) { . . . // create & start new actor with candidate as its local prime } else nextActor . send ( msg ); // forward message to next actor } } // process 3.3 Optional Reading Wikipedia article on the Sieve of Eratosthenes problem 3.4 Producer-Consumer Problem \u00b6 // consumer thread while ( buffer . empty ()) { } // process item removed from buffer 3.4 Lecture Summary In this lecture, we studied the producer-consumer pattern in concurrent programming which is used to solve the following classical problem: how can we safely coordinate accesses by multiple producer tasks, P_1 P_1 , P_2 P_2 , P_3 P_3 ... and multiple consumer tasks, C_1 C_1 , C_2 C_2 , C_3 C_3 \u200b, ... to a shared buffer of unbounded size without giving up any concurrency? Part of the reason that this problem can be challenging is that we cannot assume any a priori knowledge about the rate at which different tasks produce and consume items in the buffer. While it is possible to solve this problem by using locks with wait-notify operations or by using object-based isolation, both approaches will require low-level concurrent programming techniques to ensure correctness and maximum performance. Instead, a more elegant solution can be achieved by using actors as follows. The key idea behind any actor-based solution is to think of all objects involved in the concurrent program as actors, which in this case implies that producer tasks, consumer tasks, and the shared buffer should all be implemented as actors. The next step is to establish the communication protocols among the actors. A producer actor can simply send a message to the buffer actor whenever it has an item to produce. The protocol for consumer actors is a bit more complicated. Our solution requires a consumer actor to send a message to the buffer actor whenever it is ready to process an item. Thus, whenever the buffer actor receives a message from a producer, it knows which consumers are ready to process items and can forward the produced item to any one of them. Thus, with the actor model, all concurrent interactions involving the buffer can be encoded in messages, instead of using locks or isolated statements. 3.5 Bounded Buffer Problem \u00b6 3.5 Lecture Summary A major simplification made in the previous lecture was to assume that the shared buffer used by producer and consumer tasks can be unbounded in size. However, in practice, it is also important to consider a more realistic version of the the producer-consumer problem in which the buffer has a bounded size. In fact, the classical producer-consumer problem statement usually assumes a bounded buffer by default. In this lecture, we studied how the actor-based solution to the unbounded buffer case can be extended to support a bounded buffer. The main new challenge with bounding the size of the shared buffer is to ensure that producer tasks are not permitted to send items to the buffer when the buffer is full. Thus, the buffer actor needs to play a master role in the protocol by informing producer actors when they are permitted to send data. This is akin to the role played by the buffer/master actor with respect to consumer actors, even in the unbounded buffer case (in which the consumer actor informed the buffer actor when it is ready to consume an item). Now, the producer actor will only send data when requested to do so by the buffer actor. Though, this actor-based solution appears to be quite simple, it actually solves a classical problem that has been studied in advanced operating system classes for decades. 3.5 Optional Reading Wikipedia article on the Producer-Consumer problem Week 4 \u00b6 4.1 Optimistic concurrency \u00b6 class Integer { get (); set (); getAndAdd ( delta ) { curr = this . get (); next = cur + delta ; this . set ( next ); // RACE CONDITION return cur ; } } class AutomicInteger { get (); set (); compareAndSet (); getAndAdd ( delta ) { while ( true ) { curr = this . get (); next = cur + delta ; if ( this . compareAndSet ( curr , next )) { return cur ; } } } } 4.1 Lecture Summary In this lecture, we studied the optimistic concurrency pattern, which can be used to improve the performance of concurrent data structures. In practice, this pattern is most often used by experts who implement components of concurrent libraries, such as AtomicInteger and ConcurrentHashMap , but it is useful for all programmers to understand the underpinnings of this approach. As an example, we considered how the getAndAdd() method is typically implemented for a shared AtomicInteger object. The basic idea is to allow multiple threads to read the existing value of the shared object ( curVal ) without any synchronization, and also to compute its new value after the addition ( newVal ) without synchronization. These computations are performed optimistically under the assumption that no interference will occur with other threads during the period between reading curVal and computing newVal . However, it is necessary for each thread to confirm this assumption by using the compareAndSet() method as follows. ( compareAndSet() is used as an important building block for optimistic concurrency because it is implemented very efficiently on many hardware platforms.) The method call A.compareAndSet(curVal, newVal) invoked on AtomicInteger A checks that the value in A still equals curVal , and, if so, updates A \u2019s value to newVal before returning true; otherwise, the method simply returns false without updating A . Further, the compareAndSet() method is guaranteed to be performed atomically, as if it was in an object-based isolated statement with respect to object A . Thus, if two threads, T_1 T_1 and T_2 T_2 call compareAndSet() with the same curVal that matches A \u2019s current value, only one of them will succeed in updating A with their newVal . Furthermore, each thread will invoke an operation like compareAndSet() repeatedly in a loop until the operation succeeds. This approach is guaranteed to never result in a deadlock since there are no blocking operations. Also, since each call compareAndSet() is guaranteed to eventually succeed, there cannot be a livelock either. In general, so long as the contention on a single shared object like A is not high, the number of calls to compareAndSet() that return false will be very small, and the optimistic concurrency approach can perform much better in practice (but at the cost of more complex code logic) than using locks, isolation, or actors. 4.1 Optional Reading Wikipedia article on Optimistic concurrency control Documentation on Java\u2019s AtomicInteger class 4.2 Concurrent Queue \u00b6 Queue { enQ ( x ) { tail . next = x ; tail = x ; } deQ () { if ( Q . empty ) { // exception } r = head ; head = head . next ; return r ; } } Queue { enQ ( x ) { while ( true ) { if ( ! tail . next . compareAndSet ( null , x )) { continue ; } } } deQ () { if ( Q . empty ) { // exception } r = head ; head = head . next ; return r ; } } 4.2 Lecture Summary In this lecture, we studied concurrent queues, an extension of the popular queue data structure to support concurrent accesses. The most common operations on a queue are enq(x) , which enqueues object x at the end ( tail ) of the queue, and deq() which removes and returns the item at the start ( head ) of the queue. A correct implementation of a concurrent queue must ensure that calls to enq() and deq() maintain the correct semantics, even if the calls are invoked concurrently from different threads. While it is always possible to use locks, isolation, or actors to obtain correct but less efficient implementations of a concurrent queue, this lecture illustrated how an expert might implement a more efficient concurrent queue using the optimistic concurrency pattern. A common approach for such an implementation is to replace an object reference like tail by an AtomicReference . Since the compareAndSet() method can also be invoked on AtomicReference objects, we can use it to support (for example) concurrent calls to enq() by identifying which calls to compareAndSet() succeeded, and repeating the calls that failed. This provides the basic recipe for more efficient implementations of enq() and deq() , as are typically developed by concurrency experts. A popular implementation of concurrent queues available in Java is java.util.concurrent.ConcurrentLinkedQueue . 4.2 Optional Reading Documentation on Java\u2019s AtomicReference class Documentation on Java's ConcurrentLinkedQueue class 4.3 Linearizability \u00b6 4.3 Lecture Summary In this lecture, we studied an important correctness property of concurrent objects that is called Linearizability. A concurrent object is a data structure that is designed to support operations in parallel by multiple threads. The key question answered by linearizability is what return values are permissible when multiple threads perform these operations in parallel, taking into account what we know about the expected return values from those operations when they are performed sequentially. As an example, we considered two threads, T_1 T_1 and T_2 T_2 , performing enq(x) and enq(y) operations in parallel on a shared concurrent queue data structure, and considered what values can be returned by a deq() operation performed by T_2 T_2 after the call to enq(y) . From the viewpoint of linearizability, it is possible for the deq() operation to return item x or item y . One way to look at the definition of linearizability is as though you are a lawyer attempting to \"defend\" a friend who implemented a concurrent data structure, and that all you need to do to prove that your friend is \"not guilty\" (did not write a buggy implementation) is to show one scenario in which all the operations return values that would be consistent with a sequential execution by identifying logical moments of time at which the operations can be claimed to have taken effect. Thus, if deq() returned item x or item y you can claim that either scenario is plausible because we can reasonably assume that enq(x) took effect before enq(y) , or vice versa. However, there is absolutely no plausible scenario in which the call to deq() can correctly return a code/exception to indicate that the queue is empty since at least enq(y) must have taken effect before the call to deq() . Thus, a goal for any implementation of a concurrent data structure is to ensure that all its executions are linearizable by using whatever combination of constructs (e.g., locks, isolated, actors, optimistic concurrency) is deemed appropriate to ensure correctness while giving the maximum performance. 4.3 Optional Reading Wikipedia article on the Linearizability 4.4 ConcurrentHashMap \u00b6 java . util . concurrent . ConcurrentHashMap . get ( key ); java . util . concurrent . ConcurrentHashMap . put ( key , value ); java . util . concurrent . ConcurrentHashMap . putIfAbsent ( key , value ); java . util . concurrent . ConcurrentHashMap . clear ( key , value ); java . util . concurrent . ConcurrentHashMap . putAll ( key , value ); java . util . concurrent . ConcurrentLinkedQueue java . util . concurrent . ConcurrentSkipListSet 4.4 Lecture Summary In this lecture, we studied the ConcurrentHashMap data structure, which is available as part of the java.util.concurrent standard library in Java. A ConcurrentHashMap instance, chm , implements the Map interface, including the get(key) and put(key, value) operations. It also implements additional operations specified in the ConcurrentMap interface (which in turn extends the Map interface); one such operation is putIfAbsent(key, value) . The motivation for using putIfAbsent() is to ensure that only one instance of key is inserted in chm , even if multiple threads attempt to insert the same key in parallel. Thus, the semantics of calls to get() , put() , and putIfAbsent() can all be specified by the theory of linearizability studied earlier. However, it is worth noting that there are also some aggregate operations, such as clear() and putAll() , that cannot safely be performed in parallel with put() , get() and putIfAbsent() . Motivated by the large number of concurrent data structures available in the java.util.concurrent library, this lecture advocates that, when possible, you use libraries such as ConcurrentHashMap rather than try to implement your own version. 4.4 Optional Reading Documentation on Java\u2019s ConcurrentHashMap class Wikipedia article on Java\u2019s ConcurrentMap interface 4.5 Concurrent Minimum Spanning Tree Algorithm \u00b6 // Boruvka MST // L is the vertices set (ConcurrentLinkedQueue) while ( size () > 1 ) { n1 = remove ( L ); if ( ! tryLock ( n1 )) { continue ; } e = getMinEdge ( M ); n2 = getNeighbor ( n1 , e ); if ( ! tryLock ( n2 )) { // fixup continue ; } n3 = merge ( n1 , n2 ); remove ( n2 ); insert ( n3 ); } 4.5 Lecture Summary In this lecture, we discussed how to apply concepts learned in this course to design a concurrent algorithm that solves the problem of finding a minimum-cost spanning tree (MST) for an undirected graph. It is well known that undirected graphs can be used to represent all kinds of networks, including roadways, train routes, and air routes. A spanning tree is a data structure that contains a subset of edges from the graph which connect all nodes in the graph without including a cycle. The cost of a spanning tree is computed as the sum of the weights of all edges in the tree. The concurrent algorithm studied in this lecture builds on a well-known sequential algorithm that iteratively performs edge contraction operations, such that given a node N1 in the graph, GetMinEdge(N1) returns an edge adjacent to N1 with minimum cost for inclusion in the MST. If the minimum-cost edge is ( N1 , N2 ), the algorithm will attempt to combine nodes N1 and N2 in the graph and replace the pair by a single node, N3 . To perform edge contractions in parallel, we have to look out for the case when two threads may collide on the same vertex. For example, even if two threads started with vertices A and D , they may both end up with C as the neighbor with the minimum cost edge. We must avoid a situation in which the algorithm tries to combine both A and C and D and C . One possible approach is to use unstructured locks with calls to tryLock() to perform the combining safely, but without creating the possibility of deadlock or livelock situations. A key challenge with calling tryLock() is that some fix-up is required if the call returns false. Finally, it also helps to use a concurrent queue data structure to keep track of nodes that are available for processing. 4.5 Optional Reading Wikipedia article on Borvka\u2019s algorithm for finding a minimum cost spanning tree of an undirected graph","title":"Concurrent Programming in Java"},{"location":"courses/concurrent-prog-java/notes/#concurrent-programming-in-java","text":"parallelStream() fork/join conditional variable","title":"Concurrent Programming in Java"},{"location":"courses/concurrent-prog-java/notes/#week-1","text":"","title":"Week 1"},{"location":"courses/concurrent-prog-java/notes/#11-threads","text":"Thread Example // main thread T0 t1 = new Thread ( s1 ); // s1 is the computation assign val x t1 . start (); // start thread t1 by main thread t0 s2 ; // execute computation task by t0 t1 . join (); // wait the val x computation from t1 for use in s3 s3 ; // computation read val x Deadlock // If s1 have t2.join() and s1` have t1.join(), deadlock will happen // main thread T0 t1 = new Thread ( s1 ); // s1 is the computation assign val x t2 = new Thread ( s1 ` ); // s1` is the computation assign val x t1 . start (); // start thread t1 by main thread t0 t2 . start () // start thread t2 by main thread t0 s2 ; // execute computation task by t0 t1 . join (); // wait the val x computation from t1 for use in s3 s3 ; // computation read val x 1.1 Lecture Summary In this lecture, we learned the concept of threads as lower-level building blocks for concurrent programs. A unique aspect of Java compared to prior mainstream programming languages is that Java included the notions of threads (as instances of the java.lang.Thread class in its language definition right from the start. When an instance of Thread is created (via a new operation), it does not start executing right away; instead, it can only start executing when its start() method is invoked. The statement or computation to be executed by the thread is specified as a parameter to the constructor. The Thread class also includes a wait operation in the form of a join() method. If thread t0 performs a t1.join() call, thread t0 will be forced to wait until thread t1 completes, after which point it can safely access any values computed by thread t1 . Since there is no restriction on which thread can perform a join on which other thread, it is possible for a programmer to erroneously create a deadlock cycle with join operations. (A deadlock occurs when two threads wait for each other indefinitely, so that neither can make any progress.) 1.1 Optional Reading Wikipedia article on Threads Tutorial on Java threads Documentation on Thread class in Java 8","title":"1.1 Threads"},{"location":"courses/concurrent-prog-java/notes/#12-structured-locks","text":"Synchronized incr ( A ) { synchronized ( A ) { A . count = A . count + 1 ; } // release A } Unbounded Buffer // x = {BUF, IN, OUT} insert ( item ) { synchronized ( x ) { buf [ in ] = item ; in ++ ; } } remove () { synchronized ( x ) { item = buf [ out ] ; out ++ ; return item ; } } Bounded Buffer // x = {BUF, IN, OUT} with size K insert ( item ) { synchronized ( x ) { wait (); // wait if x is full buf [ in ] = item ; in ++ ; notify (); } } remove () { synchronized ( x ) { wait (); // wait if x is empty item = buf [ out ] ; out ++ ; notify (); return item ; } } 1.2 Lecture Summary In this lecture, we learned about structured locks, and how they can be implemented using synchronized statements and methods in Java. Structured locks can be used to enforce mutual exclusion and avoid data races, as illustrated by the incr() method in the A.count example, and the insert() and remove() methods in the the Buffer example. A major benefit of structured locks is that their acquire and release operations are implicit, since these operations are automatically performed by the Java runtime environment when entering and exiting the scope of a synchronized statement or method, even if an exception is thrown in the middle. We also learned about wait() and notify() operations that can be used to block and resume threads that need to wait for specific conditions. For example, a producer thread performing an insert() operation on a bounded buffer can call wait() when the buffer is full, so that it is only unblocked when a consumer thread performing a remove() operation calls notify() . Likewise, a consumer thread performing a remove() operation on a bounded buffer can call wait() when the buffer is empty, so that it is only unblocked when a producer thread performing an insert() operation calls notify() . Structured locks are also referred to as intrinsic locks or monitors. 1.2 Optional Reading Tutorial on Intrinsic Locks and Synchronization in Java Tutorial on Guarded Blocks in Java Wikipedia article on Monitors","title":"1.2 Structured Locks"},{"location":"courses/concurrent-prog-java/notes/#13-unstructured-locks","text":"Hand-over-hand lock // LinkedList: N1->N2->N3->N4 // Lock4Nodes: L1 L2 L3 L4 lock ( L1 ); lock ( L2 ); WORK with N1 , N2 unlock ( L1 ); lock ( L3 ); WORK with N2 , N3 unlock ( L2 ); lock ( L4 ); WORK with N3 , N4 unlock ( L3 ); Hand-over-hand lock using try_lock // LinkedList: N1->N2->N3->N4 // Lock4Nodes: L1 L2 L3 L4 try lock ( L1 ); if ( success ) { lock ( L2 ); WORK with N1 , N2 unlock ( L1 ); lock ( L3 ); WORK with N2 , N3 unlock ( L2 ); lock ( L4 ); WORK with N3 , N4 unlock ( L3 ); } R/W lock // data: Array[] // R/W lock: L search ( x ) { read_lock ( L ) A [ i ] == x ; // read only unlock ( L ) } update ( i , y ) { write_lock ( L ) A [ i ] = y ; // write unlock ( L ) } 1.3 Lecture Summary In this lecture, we introduced unstructured locks (which can be obtained in Java by creating instances of ReentrantLock() , and used three examples to demonstrate their generality relative to structured locks. The first example showed how explicit lock() and unlock() operations on unstructured locks can be used to support a hand-over-hand locking pattern that implements a non-nested pairing of lock/unlock operations which cannot be achieved with synchronized statements/methods. The second example showed how the tryLock() operations in unstructured locks can enable a thread to check the availability of a lock, and thereby acquire it if it is available or do something else if it is not. The third example illustrated the value of read-write locks (which can be obtained in Java by creating instances of ReentrantReadWriteLock() , whereby multiple threads are permitted to acquire a lock L in \"read mode\", L.readLock().lock() , but only one thread is permitted to acquire the lock in \u201cwrite mode\u201d, L.writeLock().lock() . However, it is also important to remember that the generality and power of unstructured locks is accompanied by an extra responsibility on the part of the programmer, e.g., ensuring that calls to unlock() are not forgotten, even in the presence of exceptions. 1.3 Optional Reading Tutorial on Lock Objects in Java Documentation on Java\u2019s Lock interfaces","title":"1.3 Unstructured Locks"},{"location":"courses/concurrent-prog-java/notes/#14-liveness","text":"Deadlock // T1 synchronized ( A ) { synchronized ( B ) { } } // T2 synchronized ( B ) { synchronized ( A ) { } } Livelock // T1 do { synchronized ( B ) { x . incr (); r = x . get (); } } while ( R < 2 ) // T2 do { synchronized ( B ) { x . decr (); r = x . get (); } } while ( R > - 2 ) Starvation // T1 do { int1 = s1 . readline (); print int1 ; } while (...) // ... // T100 do { int100 = s100 . readline (); print int100 ; } while (...) 1.4 Lecture Summary In this lecture, we studied three ways in which a parallel program may enter a state in which it stops making forward progress. For sequential programs, an \"infinite loop\" is a common way for a program to stop making forward progress, but there are other ways to obtain an absence of progress in a parallel program. The first is deadlock , in which all threads are blocked indefinitely, thereby preventing any forward progress. The second is livelock , in which all threads repeatedly perform an interaction that prevents forward progress, e.g., an infinite \u201cloop\u201d of repeating lock acquire/release patterns. The third is starvation , in which at least one thread is prevented from making any forward progress. The term \"liveness\" refers to a progress guarantee. The three progress guarantees that correspond to the absence of the conditions listed above are deadlock freedom, livelock freedom, and starvation freedom. 1.4 Optional Reading Deadlock example with synchronized methods in Java Starvation and Livelock examples in Java Wikipedia article on Deadlock and Livelock","title":"1.4 Liveness"},{"location":"courses/concurrent-prog-java/notes/#15-dining-philosophers","text":"think pick up chopsticks (left, right) eat put down chopsticks Structured locks // if all pick up the left first, deadlock while (...) { Think ; sychronized ( Left ) { sychronized ( Right ) { Eat ; } } } unstructured locks // if all pick up the left first, livelock, but no deadlock while (...) { Think ; s1 = tryLock ( Left ); if ( ! s1 ) continue ; s2 = tryLock ( Right ); if ( ! s2 ) { unlock ( Left ); continue ; } Eat ; } Modified version // have one of the philosophers pick right first and then left. // However, this can also cause the starvation problem. To solve // the problem completely, we need to use another synchronization // premitive called semaphore. 1.5 Lecture Summary In this lecture, we studied a classical concurrent programming example that is referred to as the Dining Philosophers Problem . In this problem, there are five threads, each of which models a \"philosopher\" that repeatedly performs a sequence of actions which include think, pick up chopsticks, eat, and put down chopsticks. First, we examined a solution to this problem using structured locks, and demonstrated how this solution could lead to a deadlock scenario (but not livelock). Second, we examined a solution using unstructured locks with tryLock() and unlock() operations that never block, and demonstrated how this solution could lead to a livelock scenario (but not deadlock). Finally, we observed how a simple modification to the first solution with structured locks, in which one philosopher picks up their right chopstick and their left, while the others pick up their left chopstick first and then their right, can guarantee an absence of deadlock. 1.5 Optional Reading Wikipedia article on the Dining Philosophers Problem","title":"1.5 Dining Philosophers"},{"location":"courses/concurrent-prog-java/notes/#16-mini-project","text":"The mini project used Java ReentrantLock and ReentrantReadWriteLock to ensure the list operations are free of bugs if run by multiple threads. One practical tip is that when using the basic lock primitive the lock statement can be put in the try and unlock in the finally statement, respectively.","title":"1.6 Mini Project"},{"location":"courses/concurrent-prog-java/notes/#week-2","text":"","title":"Week 2"},{"location":"courses/concurrent-prog-java/notes/#21-critical-sections-isloated-construct","text":"Balance Transfer // My balance: 5000 // Family balance: 1000 // Daughter balance: 0 T1 { myBal = myBal - 100 ; // R1 familyBal = familyBal + 100 ; // W1 } T2 { familyBal = familyBal - 100 ; // R2 DauBal = DarBal + 100 ; // W2 } Use isolated // My balance: 5000 // Family balance: 1000 // Daughter balance: 0 T1 { isolated { myBal = myBal - 100 ; // R1 familyBal = familyBal + 100 ; // W1 } } T2 { isolated { familyBal = familyBal - 100 ; // R2 DauBal = DarBal + 100 ; // W2 } } 2.1 Lecture Summary In this lecture, we learned how critical sections and the isolated construct can help concurrent threads manage their accesses to shared resources, at a higher level than just using locks. When programming with threads, it is well known that the following situation is defined to be a data race error \u2014 when two accesses on the same shared location can potentially execute in parallel, with least one access being a write. However, there are many cases in practice when two tasks may legitimately need to perform concurrent accesses to shared locations, as in the bank transfer example. With critical sections, two blocks of code that are marked as isolated, say A and B , are guaranteed to be executed in mutual exclusion with A executing before B or vice versa. With the use of isolated constructs, it is impossible for the bank transfer example to end up in an inconsistent state because all the reads and writes for one isolated section must complete before the start of another isolated construct. Thus, the parallel program will see the effect of one isolated section completed before another isolated section can start. 2.1 Optional Reading Wikipedia article on Critical Sections Wikipedia article on Atomicity","title":"2.1 Critical Sections (Isloated Construct)"},{"location":"courses/concurrent-prog-java/notes/#22-object-based-isolation-monitors","text":"DoubleLinkedList Example // A <-> B <-> C <-> D <-> E delete ( cur ) { cur . prev . next = cur . next ; cur . next . prev = cur . prev ; } // T1: Delete(B) // T2: Delete(C) // T3: Delete(E) DoubleLinkedList using isolated // A <-> B <-> C <-> D <-> E delete ( cur ) { isolated ( cur , cur . prev , cur . next ) { cur . prev . next = cur . next ; cur . next . prev = cur . prev ; } } // T1: Delete(B) touch (A, B, C) // T2: Delete(C) touch (B, C, D) // T3: Delete(E) touch (D, E, F) // T1 and T3 can be done in the same time. Monitor intuition // P1, process 1 // P2, process 2 Isolated ( M1 ) M1 { P1 : Isolated ( M1 ); P2 : Isolated ( M1 ); } M2 { P1 : Isolated ( M2 ); P2 : Isolated ( M2 ); } Isolated ( M2 ) 2.2 Lecture Summary In this lecture, we studied object-based isolation, which generalizes the isolated construct and relates to the classical concept of monitors. The fundamental idea behind object-based isolation is that an isolated construct can be extended with a set of objects that indicate the scope of isolation, by using the following rules: if two isolated constructs have an empty intersection in their object sets they can execute in parallel, otherwise they must execute in mutual exclusion. We observed that implementing this capability can be very challenging with locks because a correct implementation must enforce the correct levels of mutual exclusion without entering into deadlock or livelock states. The linked-list example showed how the object set for a delete() method can be defined as consisting of three objects \u2014 the current, previous, and next objects in the list, and that this object set is sufficient to safely enable parallelism across multiple calls to delete() . The Java code sketch to achieve this object-based isolation using the PCDP library is as follows: isolated ( cur , cur . prev , cur . next , () -> { . . . // Body of object-based isolated construct }); The relationship between object-based isolation and monitors is that all methods in a monitor object, M1 , are executed as object-based isolated constructs with a singleton object set, M1 . Similarly, all methods in a monitor object, M2 , are executed as object-based isolated constructs with a singleton object set, M2 which has an empty intersection with M1 . 2.2 Optional Reading Wikipedia article on Monitors","title":"2.2 Object Based Isolation (Monitors)"},{"location":"courses/concurrent-prog-java/notes/#23-concurrent-spanning-tree-algorithm","text":"Spanning Tree Algorithm Compute ( v ) { for each neighbor c of v s <- MakeParent ( v , c ) if ( s ) Compute ( c ); } MakeParent ( v , c ) { if ( c . parent == Null ) { c . parent = v ; success = true ; } else { success = false ; return success ; } } Isolation Construct Compute ( v ) { for each neighbor c of v s <- MakeParent ( v , c ) if ( s ) Compute ( c ); } MakeParent ( v , c ) { isolated ( c ) { if ( c . parent == Null ) { // compare-and-set premitive c . parent = v ; success = true ; } else { success = false ; return success ; } } } 2.3 Lecture Summary In this lecture, we learned how to use object-based isolation to create a parallel algorithm to compute spanning trees for an undirected graph. Recall that a spanning tree specifies a subset of edges in the graph that form a tree (no cycles), and connect all vertices in the graph. A standard recursive method for creating a spanning tree is to perform a depth-first traversal of the graph (the Compute(v) function in our example), making the current vertex a parent of all its neighbors that don\u2019t already have a parent assigned in the tree (the MakeParent(v, c) function in the example). The approach described in this lecture to parallelize the spanning tree computation executes recursive Compute(c) method calls in parallel for all neighbors, c , of the current vertex, v . Object-based isolation helps avoid a data race in the MakeParent(v,c) method, when two parallel threads might attempt to call MakeParent(v1, c) and MakeParent(v2, c) on the same vertex c at the same time. In this example, the role of object-based isolation is to ensure that all calls to MakeParent(v,c) with the same c value must execute the object-based isolated statement in mutual exclusion, whereas calls with different values of c can proceed in parallel. 2.3 Optional Reading Wikipedia article on Spanning Trees","title":"2.3 Concurrent Spanning Tree Algorithm"},{"location":"courses/concurrent-prog-java/notes/#24-atomic-variables","text":"Atomic Variables (Integer) // X[0, ... N-1] do { j = curr ; // curr.getAndAdd(1); atomic integer!!!. cur = cur + 1 ; if ( j >= N ) break ; process ( X [ j ] ) } while ( true ) Atomic Reference (compareAndSet) compareAndSet ( expected , new ) { isolated ( this ) { if ( this . val == expected ) { this . val = new ; return true ; } else { return false ; } } } 2.4 Lecture Summary In this lecture, we studied Atomic Variables , an important special case of object-based isolation which can be very efficiently implemented on modern computer systems. In the example given in the lecture, we have multiple threads processing an array, each using object-based isolation to safely increment a shared object, cur , to compute an index j which can then be used by the thread to access a thread-specific element of the array. However, instead of using object-based isolation, we can declare the index cur to be an Atomic Integer variable and use an atomic operation called getAndAdd() to atomically read the current value of cur and increment its value by 1. Thus, j = cur.getAndAdd(1) has the same semantics as isolated (cur) { j = cur; cur = cur+1;} but is implemented much more efficiently using hardware support on today\u2019s machines. Another example that we studied in the lecture concerns Atomic Reference variables, which are reference variables that can be atomically read and modified using methods such as compareAndSet() . If we have an atomic reference ref , then the call to ref.compareAndSet(expected, new) will compare the value of ref to expected , and if they are the same, set the value of ref to new and return true . This all occurs in one atomic operation that cannot be interrupted by any other methods invoked on the ref object. If ref and expected have different values, compareAndSet() will not modify anything and will simply return false. 2.4 Optional Reading Tutorial on Atomic Integers in Java Article in Java theory and practice series on Going atomic Wikipedia article on Atomic Wrapper Classes in Java","title":"2.4 Atomic Variables"},{"location":"courses/concurrent-prog-java/notes/#25-read-write-isolation","text":"Read/Write Isolation // T1: C.Get(k1), C.Get(k3), C.Get(k5) // T2: C.Get(k1), C.Get(k3), C.Get(k5) // T3: C.Put(k1) isolated ( read ( C )) { V = C . get ( k1 ); } isolated ( write ( C )) { C . put ( k2 , v2 ); } Read/Write Isolation in LinkedList example delete ( cur ) { isolated ( write ( cur . next , cur . prev ), read ( cur )) { cur . prev . next = cur . next ; cur . next . prev = cur . prev ; } } 2.5 Lecture Summary In this lecture we discussed Read-Write Isolation , which is a refinement of object-based isolation, and is a higher-level abstraction of the read-write locks studied earlier as part of Unstructured Locks. The main idea behind read-write isolation is to separate read accesses to shared objects from write accesses. This approach enables two threads that only read shared objects to freely execute in parallel since they are not modifying any shared objects. The need for mutual exclusion only arises when one or more threads attempt to enter an isolated section with write access to a shared object. This approach exposes more concurrency than object-based isolation since it allows read accesses to be executed in parallel. In the doubly-linked list example from our lecture, when deleting an object cur from the list by calling delete(cur) , we can replace object-based isolation on cur with read-only isolation, since deleting an object does not modify the object being deleted; only the previous and next objects in the list need to be modified. 2.5 Optional Reading Wikipedia article on Readers-writer lock","title":"2.5 Read, Write Isolation"},{"location":"courses/concurrent-prog-java/notes/#26-mini-project","text":"","title":"2.6 Mini project"},{"location":"courses/concurrent-prog-java/notes/#week-3","text":"Akka","title":"Week 3"},{"location":"courses/concurrent-prog-java/notes/#31-actors","text":"getAndAdd (){ j = cur ; cur = cur + delta ; return j ; } // method doesn't doing correct isolation foo () { cur = ... } Actor (reactive) Mailbox Method Local State 3.1 Lecture Summary In this lecture, we introduced the Actor Model as an even higher level of concurrency control than locks or isolated sections. One limitation of locks, and even isolated sections, is that, while many threads might correctly control the access to a shared object (e.g., by using object-based isolation) it only takes one thread that accesses the object directly to create subtle and hard-to-discover concurrency errors. The Actor model avoids this problem by forcing all accesses to an object to be isolated by default. The object is part of the local state of an actor, and cannot be accessed directly by any other actor. An Actor consists of a Mailbox, a set of Methods, and Local State. The Actor model is reactive, in that actors can only execute methods in response to messages; these methods can read/write local state and/or send messages to other actors. Thus, the only way to modify an object in a pure actor model is to send messages to the actor that owns that object as part of its local state. In general, messages sent to actors from different actors can be arbitrarily reordered in the system. However, in many actor models, messages sent between the same pair of actors preserve the order in which they are sent. 3.1 Optional Reading Wikipedia article on the Actor Model Documentation on the Akka Actor Library (though Akka is not used in this course, it is a useful library to be aware of if you are interested in using the actor model with Java and Scala applications)","title":"3.1 Actors"},{"location":"courses/concurrent-prog-java/notes/#32-actor-examples","text":"// print actor process ( s ) { // like a callback if ( s == \"Exit\" ) exit (); else print s ; } 3.2 Lecture Summary In this lecture, we further studied the Actor Model through two simple examples of using actors to implement well-known concurrent programming patterns. The PrintActor in our first example processes simple String messages by printing them. If an EXIT message is sent, then the PrintActor completes its current computation and exits. As a reminder, we assume that messages sent between the same pair of actors preserve the order in which they are sent. In the second example, we created an actor pipeline, in which one actor checks the incoming messages and only forwards the ones that are in lower case. The second actor processes the lowercase messages and only forwards the ones that are of even length. This example illustrates the power of the actor model, as this concurrent system would be much more difficult to implement using threads, for example, since much care would have to be taken on how to implement a shared mailbox for correct and efficient processing by parallel threads. 3.2 Optional Reading Wikipedia article on Pipeline Parallelism .","title":"3.2 Actor Examples"},{"location":"courses/concurrent-prog-java/notes/#33-sieve-of-eratosthenes-algorithm","text":"3.3 Lecture Summary In this lecture, we studied how to use actors to implement a pipelined variant of the Sieve of Eratosthenes algorithm for generating prime numbers. This example illustrates the power of the Actor Model, including dynamic creation of new actors during a computation. To implement the Sieve of Eratosthenes, we first create an actor, Non-Mul-2, that receives (positive) natural numbers as input (up to some limit), and then filters out the numbers that are multiples of 2. After receiving a number that is not a multiple of 2 (in our case, the first would be 3), the Non-Mul-2 actor creates the next actor in the pipeline, Non-Mul-3, with the goal of discarding all the numbers that are multiples of 3. The Non-Mul-2 actor then forwards all non-multiples of 2 to the Non-Mul-3 actor. Similarly, this new actor will create the next actor in the pipeline, Non-Mul-5, with the goal of discarding all the numbers that are multiples of 5. The power of the Actor Model is reflected in the dynamic nature of this problem, where pieces of the computation (new actors) are created dynamically as needed. A Java code sketch for the process() method for an actor responsible for filtering out multiples of the actor's \"local prime\" in the Sieve of Eratosthenes is as follows: public void process ( final Object msg ) { int candidate = ( Integer ) msg ; // Check if the candidate is a non-multiple of the \"local prime\". // For example, localPrime = 2 in the Non-Mul-2 actor boolean nonMul = (( candidate % localPrime ) != 0 ); // nothing needs to be done if nonMul = false if ( nonMul ) { if ( nextActor == null ) { . . . // create & start new actor with candidate as its local prime } else nextActor . send ( msg ); // forward message to next actor } } // process 3.3 Optional Reading Wikipedia article on the Sieve of Eratosthenes problem","title":"3.3 Sieve of Eratosthenes Algorithm"},{"location":"courses/concurrent-prog-java/notes/#34-producer-consumer-problem","text":"// consumer thread while ( buffer . empty ()) { } // process item removed from buffer 3.4 Lecture Summary In this lecture, we studied the producer-consumer pattern in concurrent programming which is used to solve the following classical problem: how can we safely coordinate accesses by multiple producer tasks, P_1 P_1 , P_2 P_2 , P_3 P_3 ... and multiple consumer tasks, C_1 C_1 , C_2 C_2 , C_3 C_3 \u200b, ... to a shared buffer of unbounded size without giving up any concurrency? Part of the reason that this problem can be challenging is that we cannot assume any a priori knowledge about the rate at which different tasks produce and consume items in the buffer. While it is possible to solve this problem by using locks with wait-notify operations or by using object-based isolation, both approaches will require low-level concurrent programming techniques to ensure correctness and maximum performance. Instead, a more elegant solution can be achieved by using actors as follows. The key idea behind any actor-based solution is to think of all objects involved in the concurrent program as actors, which in this case implies that producer tasks, consumer tasks, and the shared buffer should all be implemented as actors. The next step is to establish the communication protocols among the actors. A producer actor can simply send a message to the buffer actor whenever it has an item to produce. The protocol for consumer actors is a bit more complicated. Our solution requires a consumer actor to send a message to the buffer actor whenever it is ready to process an item. Thus, whenever the buffer actor receives a message from a producer, it knows which consumers are ready to process items and can forward the produced item to any one of them. Thus, with the actor model, all concurrent interactions involving the buffer can be encoded in messages, instead of using locks or isolated statements.","title":"3.4 Producer-Consumer Problem"},{"location":"courses/concurrent-prog-java/notes/#35-bounded-buffer-problem","text":"3.5 Lecture Summary A major simplification made in the previous lecture was to assume that the shared buffer used by producer and consumer tasks can be unbounded in size. However, in practice, it is also important to consider a more realistic version of the the producer-consumer problem in which the buffer has a bounded size. In fact, the classical producer-consumer problem statement usually assumes a bounded buffer by default. In this lecture, we studied how the actor-based solution to the unbounded buffer case can be extended to support a bounded buffer. The main new challenge with bounding the size of the shared buffer is to ensure that producer tasks are not permitted to send items to the buffer when the buffer is full. Thus, the buffer actor needs to play a master role in the protocol by informing producer actors when they are permitted to send data. This is akin to the role played by the buffer/master actor with respect to consumer actors, even in the unbounded buffer case (in which the consumer actor informed the buffer actor when it is ready to consume an item). Now, the producer actor will only send data when requested to do so by the buffer actor. Though, this actor-based solution appears to be quite simple, it actually solves a classical problem that has been studied in advanced operating system classes for decades. 3.5 Optional Reading Wikipedia article on the Producer-Consumer problem","title":"3.5 Bounded Buffer Problem"},{"location":"courses/concurrent-prog-java/notes/#week-4","text":"","title":"Week 4"},{"location":"courses/concurrent-prog-java/notes/#41-optimistic-concurrency","text":"class Integer { get (); set (); getAndAdd ( delta ) { curr = this . get (); next = cur + delta ; this . set ( next ); // RACE CONDITION return cur ; } } class AutomicInteger { get (); set (); compareAndSet (); getAndAdd ( delta ) { while ( true ) { curr = this . get (); next = cur + delta ; if ( this . compareAndSet ( curr , next )) { return cur ; } } } } 4.1 Lecture Summary In this lecture, we studied the optimistic concurrency pattern, which can be used to improve the performance of concurrent data structures. In practice, this pattern is most often used by experts who implement components of concurrent libraries, such as AtomicInteger and ConcurrentHashMap , but it is useful for all programmers to understand the underpinnings of this approach. As an example, we considered how the getAndAdd() method is typically implemented for a shared AtomicInteger object. The basic idea is to allow multiple threads to read the existing value of the shared object ( curVal ) without any synchronization, and also to compute its new value after the addition ( newVal ) without synchronization. These computations are performed optimistically under the assumption that no interference will occur with other threads during the period between reading curVal and computing newVal . However, it is necessary for each thread to confirm this assumption by using the compareAndSet() method as follows. ( compareAndSet() is used as an important building block for optimistic concurrency because it is implemented very efficiently on many hardware platforms.) The method call A.compareAndSet(curVal, newVal) invoked on AtomicInteger A checks that the value in A still equals curVal , and, if so, updates A \u2019s value to newVal before returning true; otherwise, the method simply returns false without updating A . Further, the compareAndSet() method is guaranteed to be performed atomically, as if it was in an object-based isolated statement with respect to object A . Thus, if two threads, T_1 T_1 and T_2 T_2 call compareAndSet() with the same curVal that matches A \u2019s current value, only one of them will succeed in updating A with their newVal . Furthermore, each thread will invoke an operation like compareAndSet() repeatedly in a loop until the operation succeeds. This approach is guaranteed to never result in a deadlock since there are no blocking operations. Also, since each call compareAndSet() is guaranteed to eventually succeed, there cannot be a livelock either. In general, so long as the contention on a single shared object like A is not high, the number of calls to compareAndSet() that return false will be very small, and the optimistic concurrency approach can perform much better in practice (but at the cost of more complex code logic) than using locks, isolation, or actors. 4.1 Optional Reading Wikipedia article on Optimistic concurrency control Documentation on Java\u2019s AtomicInteger class","title":"4.1 Optimistic concurrency"},{"location":"courses/concurrent-prog-java/notes/#42-concurrent-queue","text":"Queue { enQ ( x ) { tail . next = x ; tail = x ; } deQ () { if ( Q . empty ) { // exception } r = head ; head = head . next ; return r ; } } Queue { enQ ( x ) { while ( true ) { if ( ! tail . next . compareAndSet ( null , x )) { continue ; } } } deQ () { if ( Q . empty ) { // exception } r = head ; head = head . next ; return r ; } } 4.2 Lecture Summary In this lecture, we studied concurrent queues, an extension of the popular queue data structure to support concurrent accesses. The most common operations on a queue are enq(x) , which enqueues object x at the end ( tail ) of the queue, and deq() which removes and returns the item at the start ( head ) of the queue. A correct implementation of a concurrent queue must ensure that calls to enq() and deq() maintain the correct semantics, even if the calls are invoked concurrently from different threads. While it is always possible to use locks, isolation, or actors to obtain correct but less efficient implementations of a concurrent queue, this lecture illustrated how an expert might implement a more efficient concurrent queue using the optimistic concurrency pattern. A common approach for such an implementation is to replace an object reference like tail by an AtomicReference . Since the compareAndSet() method can also be invoked on AtomicReference objects, we can use it to support (for example) concurrent calls to enq() by identifying which calls to compareAndSet() succeeded, and repeating the calls that failed. This provides the basic recipe for more efficient implementations of enq() and deq() , as are typically developed by concurrency experts. A popular implementation of concurrent queues available in Java is java.util.concurrent.ConcurrentLinkedQueue . 4.2 Optional Reading Documentation on Java\u2019s AtomicReference class Documentation on Java's ConcurrentLinkedQueue class","title":"4.2 Concurrent Queue"},{"location":"courses/concurrent-prog-java/notes/#43-linearizability","text":"4.3 Lecture Summary In this lecture, we studied an important correctness property of concurrent objects that is called Linearizability. A concurrent object is a data structure that is designed to support operations in parallel by multiple threads. The key question answered by linearizability is what return values are permissible when multiple threads perform these operations in parallel, taking into account what we know about the expected return values from those operations when they are performed sequentially. As an example, we considered two threads, T_1 T_1 and T_2 T_2 , performing enq(x) and enq(y) operations in parallel on a shared concurrent queue data structure, and considered what values can be returned by a deq() operation performed by T_2 T_2 after the call to enq(y) . From the viewpoint of linearizability, it is possible for the deq() operation to return item x or item y . One way to look at the definition of linearizability is as though you are a lawyer attempting to \"defend\" a friend who implemented a concurrent data structure, and that all you need to do to prove that your friend is \"not guilty\" (did not write a buggy implementation) is to show one scenario in which all the operations return values that would be consistent with a sequential execution by identifying logical moments of time at which the operations can be claimed to have taken effect. Thus, if deq() returned item x or item y you can claim that either scenario is plausible because we can reasonably assume that enq(x) took effect before enq(y) , or vice versa. However, there is absolutely no plausible scenario in which the call to deq() can correctly return a code/exception to indicate that the queue is empty since at least enq(y) must have taken effect before the call to deq() . Thus, a goal for any implementation of a concurrent data structure is to ensure that all its executions are linearizable by using whatever combination of constructs (e.g., locks, isolated, actors, optimistic concurrency) is deemed appropriate to ensure correctness while giving the maximum performance. 4.3 Optional Reading Wikipedia article on the Linearizability","title":"4.3 Linearizability"},{"location":"courses/concurrent-prog-java/notes/#44-concurrenthashmap","text":"java . util . concurrent . ConcurrentHashMap . get ( key ); java . util . concurrent . ConcurrentHashMap . put ( key , value ); java . util . concurrent . ConcurrentHashMap . putIfAbsent ( key , value ); java . util . concurrent . ConcurrentHashMap . clear ( key , value ); java . util . concurrent . ConcurrentHashMap . putAll ( key , value ); java . util . concurrent . ConcurrentLinkedQueue java . util . concurrent . ConcurrentSkipListSet 4.4 Lecture Summary In this lecture, we studied the ConcurrentHashMap data structure, which is available as part of the java.util.concurrent standard library in Java. A ConcurrentHashMap instance, chm , implements the Map interface, including the get(key) and put(key, value) operations. It also implements additional operations specified in the ConcurrentMap interface (which in turn extends the Map interface); one such operation is putIfAbsent(key, value) . The motivation for using putIfAbsent() is to ensure that only one instance of key is inserted in chm , even if multiple threads attempt to insert the same key in parallel. Thus, the semantics of calls to get() , put() , and putIfAbsent() can all be specified by the theory of linearizability studied earlier. However, it is worth noting that there are also some aggregate operations, such as clear() and putAll() , that cannot safely be performed in parallel with put() , get() and putIfAbsent() . Motivated by the large number of concurrent data structures available in the java.util.concurrent library, this lecture advocates that, when possible, you use libraries such as ConcurrentHashMap rather than try to implement your own version. 4.4 Optional Reading Documentation on Java\u2019s ConcurrentHashMap class Wikipedia article on Java\u2019s ConcurrentMap interface","title":"4.4 ConcurrentHashMap"},{"location":"courses/concurrent-prog-java/notes/#45-concurrent-minimum-spanning-tree-algorithm","text":"// Boruvka MST // L is the vertices set (ConcurrentLinkedQueue) while ( size () > 1 ) { n1 = remove ( L ); if ( ! tryLock ( n1 )) { continue ; } e = getMinEdge ( M ); n2 = getNeighbor ( n1 , e ); if ( ! tryLock ( n2 )) { // fixup continue ; } n3 = merge ( n1 , n2 ); remove ( n2 ); insert ( n3 ); } 4.5 Lecture Summary In this lecture, we discussed how to apply concepts learned in this course to design a concurrent algorithm that solves the problem of finding a minimum-cost spanning tree (MST) for an undirected graph. It is well known that undirected graphs can be used to represent all kinds of networks, including roadways, train routes, and air routes. A spanning tree is a data structure that contains a subset of edges from the graph which connect all nodes in the graph without including a cycle. The cost of a spanning tree is computed as the sum of the weights of all edges in the tree. The concurrent algorithm studied in this lecture builds on a well-known sequential algorithm that iteratively performs edge contraction operations, such that given a node N1 in the graph, GetMinEdge(N1) returns an edge adjacent to N1 with minimum cost for inclusion in the MST. If the minimum-cost edge is ( N1 , N2 ), the algorithm will attempt to combine nodes N1 and N2 in the graph and replace the pair by a single node, N3 . To perform edge contractions in parallel, we have to look out for the case when two threads may collide on the same vertex. For example, even if two threads started with vertices A and D , they may both end up with C as the neighbor with the minimum cost edge. We must avoid a situation in which the algorithm tries to combine both A and C and D and C . One possible approach is to use unstructured locks with calls to tryLock() to perform the combining safely, but without creating the possibility of deadlock or livelock situations. A key challenge with calling tryLock() is that some fix-up is required if the call returns false. Finally, it also helps to use a concurrent queue data structure to keep track of nodes that are available for processing. 4.5 Optional Reading Wikipedia article on Borvka\u2019s algorithm for finding a minimum cost spanning tree of an undirected graph","title":"4.5 Concurrent Minimum Spanning Tree Algorithm"},{"location":"courses/coursera-dl4-cnn/notes/","text":"Convolutional Neural Networks \u00b6 no pad (valid): n, f -> n - f + 1. p is zero after pad (Same): n + p, f -> n + 2p - f + 1. p = \\frac{f - 1}{2} p = \\frac{f - 1}{2} is the layer of the pad The value f is always odd. stride s s . output dimention: floor of \\frac{n + 2p - f}{s} + 1 \\frac{n + 2p - f}{s} + 1 2D image num_channel must match input: 1 x 60 x 60, filter: 1 x 1 x 60,","title":"Convolutional Neural Networks"},{"location":"courses/coursera-dl4-cnn/notes/#convolutional-neural-networks","text":"no pad (valid): n, f -> n - f + 1. p is zero after pad (Same): n + p, f -> n + 2p - f + 1. p = \\frac{f - 1}{2} p = \\frac{f - 1}{2} is the layer of the pad The value f is always odd. stride s s . output dimention: floor of \\frac{n + 2p - f}{s} + 1 \\frac{n + 2p - f}{s} + 1 2D image num_channel must match input: 1 x 60 x 60, filter: 1 x 1 x 60,","title":"Convolutional Neural Networks"},{"location":"courses/cs224n/lec-notes/","text":"CS224N: Natural Language Processing with Deep Learning \u00b6 Lecture 1 Introduction to NLP and Deep Learning \u00b6 Representations of NLP levels: Semantics Traditional V.S. DL (rules v.s. sophisticated algorithm) Applications: Sentiment Analysis Question Answering system Dialogue agents / response generation Lecture 2 Word Vector Representations: word2vec \u00b6 \"one-hot\" representation, localist representation distributional similarity based representations \"You shall know a word by the company it keeps\u201d (J. R. Firth 1957:11)\" dense vector for each word type, chosen so that it is good at predicting other words appearing in its context (gets a bit recursive) Learning neural network word embeddings model p(\\text{context} | w_t) = ? p(\\text{context} | w_t) = ? loss function: J = 1 - p(w_{-t}|w_{t}) J = 1 - p(w_{-t}|w_{t}) , w_{-t} w_{-t} , context words that doesn't include word w_t w_t . word2vec Skip-grams (SG) - predict context words given target center words Continuous Bag of Words (CBOW) - predict target center word from bag-of-words context words 2 training methods hierarchical softmax negative sampling: tain binary logistic regression for a true pair versus a couple of noice pairs. Core ideas of SG prediction maximize the prediction of the model p(\\text{context} | w_t) = ? p(\\text{context} | w_t) = ? for all context words in the form of the cost function J(\\theta) J(\\theta) . cost function: $$ J'(\\theta) = \\prod_{t=1}^T\\prod_{\\substack{-m \\le j \\le m\\ j \\ne 0}} p(w_{t+j}|w_t; \\theta) $$ Negative log likelihood $$ J(\\theta) = -\\frac{1}{T} \\sum_{t=1}^T\\sum_{\\substack{-m \\le j \\le m\\ j \\ne 0}} \\log p(w_{t+j}|w_t) $$ softmax $$ p(o|c) = \\frac{\\exp(u_o^T v_c)}{\\sum_{w=1}^{v}\\exp(u_w^T v_c)} $$ What's really mean when you say train word2vec model optimize the parameter \\theta \\theta , which is a R^{2\\cdot d \\cdot V} R^{2\\cdot d \\cdot V} , d d is the word vector dimention, V V is the vacabular size, each word is represented by 2 vectors! Compute all vector gradients!!! Gradient calculation (lecture slides) Lecture 3 Advanced Word Vector Representations \u00b6 Compare count based and direct prediction count based: LSA, HAL (Lund & Burgess), COALS (Rohde et al), Hellinger-PCA (Lebret & Collobert) Fast training Efficient usage of statistics Primarily used to capture word similarity Disproportionate importance given to large counts direct prediction: NNLM, HLBL, RNN, Skip-gram/CBOW, (Bengio et al; Collobert & Weston; Huang et al; Mnih & Hinton; Mikolov et al;Mnih & Kavukcuoglu) Scales with corpus size Inefficient usage of statistics Can capture complex patterns beyond word similarity Generate improved performance on other tasks Combining the best of both worlds: GloVe Fast training Scalable to huge corpora Good performance even with small corpus, and small vectors How to evaluate word2vec? Intrinsic: Evaluation on a specific/intermediate subtask Fast to compute Helps to understand that system Not clear if really helpful unless correlation to real task is established Extrinsic: Evaluation on a real task Can take a long time to compute accuracy Unclear if the subsystem is the problem or its interaction or other subsystems If replacing exactly one subsystem with another improves accuracy --> Winning! Assignment 1 (Spring 2019) \u00b6 Singular Value Decomposition (SVD) is a kind of generalized PCA (Principal Components Analysis). Review materials \u00b6 Gradient Descent (SGD) Singular Value Decomposition (SVD) cross entropy loss max-margin loss Lecture 4 Word Window Classification and Neural Networks \u00b6 Window classification: Train softmax classifier by assigning a label to a center word and concatenating all word vectors surrounding it. max-margin loss; J(\\theta) = \\max(0, 1 - s + s_{corrupted}) J(\\theta) = \\max(0, 1 - s + s_{corrupted}) . s s is the good part, s_{corrupted} s_{corrupted} is the bad part, we would like the bad part is smaller than s - 1 s - 1 . backpropagation: insight: reuse the derivative computed previously Hadamard product ( \\circ, \\odot, \\otimes \\circ, \\odot, \\otimes ) Lecture 5 Backpropagation (Feb 24, 2019) \u00b6 Details of backpropagation \u00b6 The backprop algorithm is essentially compute the gradient (partial derivative) of the cost function with respect all the parameters, U, W, b, x U, W, b, x With the following setup: max-margin cost function: J = \\max(0, 1 - s + s_c) J = \\max(0, 1 - s + s_c) Scores: s = U^T f(Wx + b), s_c = U^T f(Wx_c + b) s = U^T f(Wx + b), s_c = U^T f(Wx_c + b) input: z = Wx + b z = Wx + b , hidden: a = f(z) a = f(z) , output: s = U^T a s = U^T a Derivatives: \\frac{\\partial s}{\\partial U} = \\frac{\\partial}{\\partial U} U^T a = a \\frac{\\partial s}{\\partial U} = \\frac{\\partial}{\\partial U} U^T a = a wrt one weight W_{ij} W_{ij} : \\frac{\\partial s}{\\partial W_{ij}} = \\delta_i x_j \\frac{\\partial s}{\\partial W_{ij}} = \\delta_i x_j , \\delta_i = U_i f'(z_i) x_j \\delta_i = U_i f'(z_i) x_j , where f'(z) = f(z)(1 - f(z)) f'(z) = f(z)(1 - f(z)) , f(x) f(x) is logistic function or sigmoid function. wrt all weights W W : \\frac{\\partial s}{\\partial W} = \\delta x^T \\frac{\\partial s}{\\partial W} = \\delta x^T wrt word vectors x x : \\frac{\\partial s}{\\partial x} = W^T\\delta \\frac{\\partial s}{\\partial x} = W^T\\delta Iterpretations of backpropagation using simple function. \u00b6 Lecture 6 Dependency Parsing (Feb 27, 2019) \u00b6 Lecture 8 \u00b6 Lecture 9 Machine Translation and Advanced Recurrent LSTMs and GRUs \u00b6 Reference \u00b6 Natural Language Processing with Python","title":"CS224N Lecture Notes"},{"location":"courses/cs224n/lec-notes/#cs224n-natural-language-processing-with-deep-learning","text":"","title":"CS224N: Natural Language Processing with Deep Learning"},{"location":"courses/cs224n/lec-notes/#lecture-1-introduction-to-nlp-and-deep-learning","text":"Representations of NLP levels: Semantics Traditional V.S. DL (rules v.s. sophisticated algorithm) Applications: Sentiment Analysis Question Answering system Dialogue agents / response generation","title":"Lecture 1 Introduction to NLP and Deep Learning"},{"location":"courses/cs224n/lec-notes/#lecture-2-word-vector-representations-word2vec","text":"\"one-hot\" representation, localist representation distributional similarity based representations \"You shall know a word by the company it keeps\u201d (J. R. Firth 1957:11)\" dense vector for each word type, chosen so that it is good at predicting other words appearing in its context (gets a bit recursive) Learning neural network word embeddings model p(\\text{context} | w_t) = ? p(\\text{context} | w_t) = ? loss function: J = 1 - p(w_{-t}|w_{t}) J = 1 - p(w_{-t}|w_{t}) , w_{-t} w_{-t} , context words that doesn't include word w_t w_t . word2vec Skip-grams (SG) - predict context words given target center words Continuous Bag of Words (CBOW) - predict target center word from bag-of-words context words 2 training methods hierarchical softmax negative sampling: tain binary logistic regression for a true pair versus a couple of noice pairs. Core ideas of SG prediction maximize the prediction of the model p(\\text{context} | w_t) = ? p(\\text{context} | w_t) = ? for all context words in the form of the cost function J(\\theta) J(\\theta) . cost function: $$ J'(\\theta) = \\prod_{t=1}^T\\prod_{\\substack{-m \\le j \\le m\\ j \\ne 0}} p(w_{t+j}|w_t; \\theta) $$ Negative log likelihood $$ J(\\theta) = -\\frac{1}{T} \\sum_{t=1}^T\\sum_{\\substack{-m \\le j \\le m\\ j \\ne 0}} \\log p(w_{t+j}|w_t) $$ softmax $$ p(o|c) = \\frac{\\exp(u_o^T v_c)}{\\sum_{w=1}^{v}\\exp(u_w^T v_c)} $$ What's really mean when you say train word2vec model optimize the parameter \\theta \\theta , which is a R^{2\\cdot d \\cdot V} R^{2\\cdot d \\cdot V} , d d is the word vector dimention, V V is the vacabular size, each word is represented by 2 vectors! Compute all vector gradients!!! Gradient calculation (lecture slides)","title":"Lecture 2 Word Vector Representations: word2vec"},{"location":"courses/cs224n/lec-notes/#lecture-3-advanced-word-vector-representations","text":"Compare count based and direct prediction count based: LSA, HAL (Lund & Burgess), COALS (Rohde et al), Hellinger-PCA (Lebret & Collobert) Fast training Efficient usage of statistics Primarily used to capture word similarity Disproportionate importance given to large counts direct prediction: NNLM, HLBL, RNN, Skip-gram/CBOW, (Bengio et al; Collobert & Weston; Huang et al; Mnih & Hinton; Mikolov et al;Mnih & Kavukcuoglu) Scales with corpus size Inefficient usage of statistics Can capture complex patterns beyond word similarity Generate improved performance on other tasks Combining the best of both worlds: GloVe Fast training Scalable to huge corpora Good performance even with small corpus, and small vectors How to evaluate word2vec? Intrinsic: Evaluation on a specific/intermediate subtask Fast to compute Helps to understand that system Not clear if really helpful unless correlation to real task is established Extrinsic: Evaluation on a real task Can take a long time to compute accuracy Unclear if the subsystem is the problem or its interaction or other subsystems If replacing exactly one subsystem with another improves accuracy --> Winning!","title":"Lecture 3 Advanced Word Vector Representations"},{"location":"courses/cs224n/lec-notes/#assignment-1-spring-2019","text":"Singular Value Decomposition (SVD) is a kind of generalized PCA (Principal Components Analysis).","title":"Assignment 1 (Spring 2019)"},{"location":"courses/cs224n/lec-notes/#review-materials","text":"Gradient Descent (SGD) Singular Value Decomposition (SVD) cross entropy loss max-margin loss","title":"Review materials"},{"location":"courses/cs224n/lec-notes/#lecture-4-word-window-classification-and-neural-networks","text":"Window classification: Train softmax classifier by assigning a label to a center word and concatenating all word vectors surrounding it. max-margin loss; J(\\theta) = \\max(0, 1 - s + s_{corrupted}) J(\\theta) = \\max(0, 1 - s + s_{corrupted}) . s s is the good part, s_{corrupted} s_{corrupted} is the bad part, we would like the bad part is smaller than s - 1 s - 1 . backpropagation: insight: reuse the derivative computed previously Hadamard product ( \\circ, \\odot, \\otimes \\circ, \\odot, \\otimes )","title":"Lecture 4 Word Window Classification and Neural Networks"},{"location":"courses/cs224n/lec-notes/#lecture-5-backpropagation-feb-24-2019","text":"","title":"Lecture 5 Backpropagation (Feb 24, 2019)"},{"location":"courses/cs224n/lec-notes/#details-of-backpropagation","text":"The backprop algorithm is essentially compute the gradient (partial derivative) of the cost function with respect all the parameters, U, W, b, x U, W, b, x With the following setup: max-margin cost function: J = \\max(0, 1 - s + s_c) J = \\max(0, 1 - s + s_c) Scores: s = U^T f(Wx + b), s_c = U^T f(Wx_c + b) s = U^T f(Wx + b), s_c = U^T f(Wx_c + b) input: z = Wx + b z = Wx + b , hidden: a = f(z) a = f(z) , output: s = U^T a s = U^T a Derivatives: \\frac{\\partial s}{\\partial U} = \\frac{\\partial}{\\partial U} U^T a = a \\frac{\\partial s}{\\partial U} = \\frac{\\partial}{\\partial U} U^T a = a wrt one weight W_{ij} W_{ij} : \\frac{\\partial s}{\\partial W_{ij}} = \\delta_i x_j \\frac{\\partial s}{\\partial W_{ij}} = \\delta_i x_j , \\delta_i = U_i f'(z_i) x_j \\delta_i = U_i f'(z_i) x_j , where f'(z) = f(z)(1 - f(z)) f'(z) = f(z)(1 - f(z)) , f(x) f(x) is logistic function or sigmoid function. wrt all weights W W : \\frac{\\partial s}{\\partial W} = \\delta x^T \\frac{\\partial s}{\\partial W} = \\delta x^T wrt word vectors x x : \\frac{\\partial s}{\\partial x} = W^T\\delta \\frac{\\partial s}{\\partial x} = W^T\\delta","title":"Details of backpropagation"},{"location":"courses/cs224n/lec-notes/#iterpretations-of-backpropagation-using-simple-function","text":"","title":"Iterpretations of backpropagation using simple function."},{"location":"courses/cs224n/lec-notes/#lecture-6-dependency-parsing-feb-27-2019","text":"","title":"Lecture 6 Dependency Parsing (Feb 27, 2019)"},{"location":"courses/cs224n/lec-notes/#lecture-8","text":"","title":"Lecture 8"},{"location":"courses/cs224n/lec-notes/#lecture-9-machine-translation-and-advanced-recurrent-lstms-and-grus","text":"","title":"Lecture 9 Machine Translation and Advanced Recurrent LSTMs and GRUs"},{"location":"courses/cs224n/lec-notes/#reference","text":"Natural Language Processing with Python","title":"Reference"},{"location":"courses/cs224n/write-up/","text":"CS224N NLP with Deep Learning \u00b6 Part 1 Word Embeddings Based on Distributional Semantic \u00b6 Date: April 20th 2019 If you have no experience in the Natrual Language Processing (NLP) field and asked to name a few practical NLP applications, what would you list? Google Translation or Amazon Echo voice assistant may first come into your mind. How did those products understand a sentence or human conversation? Engineers built computational models that understand our language as our brain does. To build such a computational model, the following components are necessary. A large corpus of text (input training data) A method to represent each word from the corpus (feature representation) A starting model that barely understands English at the beginning but can be improved by \"reading\" more words from the corpus (parametric function). An algorithm for the model to correct itself if it makes a mistake in understanding (learning algorithm/optimization method) A measurement that can qualify the mistake the model made (loss function) Introduction \u00b6 Starting with this post, I will write materials from my understanding of the Stanford CS224N. The plan is to journal down all the learning notes I learned about the above 5 components. The goal is to provide a systematic understand of the gist of the components for real applications. Part 1 is about language models and word embeddings. Part 2 discusses neural networks and backpropagation algorithm. Part 3 revisits the language model and introduces recurrent neural networks. Part 4 studies advanced RNN, Long short-term memory (LSTM), and gated recurrent networks (GRN). Word representations \u00b6 How do we represent word with meaning on a computer? Before 2013, wordNet and one-hot vector are most popular in word meaning representations. WordNet is a manually compiled thesaurus containing lists of synonym sets and hypernyms. Like most of the manual stuff, it is subjective, unscalable, inaccurate in computing word similarity, and impossible to maintain and keep up-to-data. One-hot vectors represent word meaning using discrete symbolic 1 1 s in a long stream of 0 0 of vector elements. It suffers from sparsity issues and many other drawbacks. We will not spend time on that outdated method. Instead, we will focus on the embedding method using the idea of word vector. The core idea of this embedding method is based on the remarkable insight on word meaning called distributional semantics . It conjectures that a word\u2019s meaning is given by the words that frequently appear close-by. It is proposed by J. R. Firth. Here is the famous quote: Quote \"You shall know a word by the company it keeps\" -- J. R. Firth 1957: 11 In this method, we will build a dense vector for each word, chosen so that it is similar to vectors of words that appear in similar contexts. word vector representation also called distributed representation , or word embeddings . A work vector might look likes this, \\mathrm{banking} = \\begin{pmatrix} 0.286\\\\ 0.792\\\\ \u22120.177\\\\ \u22120.107\\\\ 0.109\\\\ \u22120.542\\\\ 0.349\\\\ 0.271 \\end{pmatrix} \\mathrm{banking} = \\begin{pmatrix} 0.286\\\\ 0.792\\\\ \u22120.177\\\\ \u22120.107\\\\ 0.109\\\\ \u22120.542\\\\ 0.349\\\\ 0.271 \\end{pmatrix} word2vec \u00b6 Word2vec (Mikolov et al. 2013) is a framework for learning word vectors. Word2vec go through windows of words and calculate the probability of the context word given the center word (or vice versa) using the similarity of the word vectors. It keeps adjusting the word vectors to maximize this probability. Vividly, these two pictures show the general idea of how word2vec works. Formally, for a single prediction, the probability is P(o|c) P(o|c) , interpreted as the probability of the outer word o o given the center word c c . For the large corpus including T T words and each position t = 1, 2, \\cdots, T t = 1, 2, \\cdots, T , we predict context words within a window of fixed size m m , given center word w_t w_t . The model likelihood can be written as the following \\begin{equation*} \\mathrm{likelihood} = L(\\theta) = \\prod_{t=1}^T\\prod_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} p(w_{t+j}|w_t; \\theta) \\end{equation*} \\begin{equation*} \\mathrm{likelihood} = L(\\theta) = \\prod_{t=1}^T\\prod_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} p(w_{t+j}|w_t; \\theta) \\end{equation*} The objective function J(\\theta) J(\\theta) is the average negative log likelihood: \\begin{equation*} J(\\theta) = -\\frac{1}{T}\\log{L(\\theta)} = -\\frac{1}{T} \\sum_{t=1}^T\\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} \\log p(w_{t+j}|w_t; \\theta) \\end{equation*} \\begin{equation*} J(\\theta) = -\\frac{1}{T}\\log{L(\\theta)} = -\\frac{1}{T} \\sum_{t=1}^T\\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} \\log p(w_{t+j}|w_t; \\theta) \\end{equation*} We've defined the cost function by now. In order to minimize the loss, we need to know how p(w_{t+j}|w_t; \\theta) p(w_{t+j}|w_t; \\theta) can be calculated. One function we can use to calculate the probability value is the \\mathrm{softmax} \\mathrm{softmax} function. \\begin{equation*} \\mathrm{softmax}(x)_i = \\frac{\\exp(x_i)}{\\sum_{j=1}^{n}\\exp(x_j)} \\end{equation*} \\begin{equation*} \\mathrm{softmax}(x)_i = \\frac{\\exp(x_i)}{\\sum_{j=1}^{n}\\exp(x_j)} \\end{equation*} Particularly we will write the probability as the following format. \\begin{equation*} p(o|c) = \\frac{\\exp(u_o^T v_c)}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\end{equation*} \\begin{equation*} p(o|c) = \\frac{\\exp(u_o^T v_c)}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\end{equation*} There are several points need to be emphasized. Two vectors will be obtained for each individual word, one as center word v_w v_w , and the other context word u_w u_w . d d is the dimension of the word vector. V V is the vocabulary size. \\begin{equation*} \\theta = \\begin{bmatrix} v_{\\mathrm{aardvark}}\\\\ v_{\\mathrm{a}}\\\\ \\cdots \\\\ v_{\\mathrm{zebra}}\\\\ u_{\\mathrm{aardvark}}\\\\ u_{\\mathrm{a}}\\\\ \\cdots \\\\ u_{\\mathrm{zebra}}\\\\ \\end{bmatrix} \\in \\mathbb{R}^{2dV} \\end{equation*} \\begin{equation*} \\theta = \\begin{bmatrix} v_{\\mathrm{aardvark}}\\\\ v_{\\mathrm{a}}\\\\ \\cdots \\\\ v_{\\mathrm{zebra}}\\\\ u_{\\mathrm{aardvark}}\\\\ u_{\\mathrm{a}}\\\\ \\cdots \\\\ u_{\\mathrm{zebra}}\\\\ \\end{bmatrix} \\in \\mathbb{R}^{2dV} \\end{equation*} The dot product in the exponet compares similarity of o o and c c . Larger dot product indicates larger probability. The denorminator sum over entire vocabulary to give normalized probability distribution. The \\mathrm{softmax} \\mathrm{softmax} function maps aribitrary values x_i x_i to a probability distribution p_i p_i . \u201c \\mathrm{max} \\mathrm{max} \u201d because it amplifies the probability of the largest x_i x_i , \u201c \\mathrm{soft} \\mathrm{soft} \u201d because it still assigns some probabilities to smaller x_i x_i . It is very commonly used in deep learning. Gradient descent to optimize log likelihood loss function \u00b6 This section I will purely focus on how to derive the gradient of the log likelihood loss function with respect to center word using the chain rule. Once we have the computed gradients, we are ready to implement it in matrix form and train the word vectors. This model is called skip-gram. loss function in p(w_{t+j}|w_t) p(w_{t+j}|w_t) \\begin{equation*} J(\\theta) = -\\frac{1}{T} \\sum_{t=1}^T\\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} \\log p(w_{t+j}|w_t; \\theta) \\end{equation*} \\begin{equation*} J(\\theta) = -\\frac{1}{T} \\sum_{t=1}^T\\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} \\log p(w_{t+j}|w_t; \\theta) \\end{equation*} p(w_{t+j}|w_t) p(w_{t+j}|w_t) in softmax form \\begin{equation*} p(o|c) = \\frac{\\exp(u_o^T v_c)}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\end{equation*} \\begin{equation*} p(o|c) = \\frac{\\exp(u_o^T v_c)}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\end{equation*} We would like to find the following derivatives: \\begin{equation*} 1. \\frac{\\partial}{\\partial v_c} \\log p(o|c) \\\\ 2. \\frac{\\partial}{\\partial u_o} \\log p(o|c) \\end{equation*} \\begin{equation*} 1. \\frac{\\partial}{\\partial v_c} \\log p(o|c) \\\\ 2. \\frac{\\partial}{\\partial u_o} \\log p(o|c) \\end{equation*} Let's now start working with the first one, the derivative wrt. v_c v_c . \\begin{align*} \\frac{\\partial}{\\partial v_c} \\log p(o|c) & = \\frac{\\partial}{\\partial v_c} \\log{\\frac{\\exp(u_o^T v_c)}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)}} \\\\ &= \\frac{\\partial}{\\partial v_c} \\log{\\exp(u_o^T v_c)} - \\frac{\\partial}{\\partial v_c} \\log{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\;\\;\\;\\cdots (\\log\\frac{a}{b} = \\log a - \\log b) \\\\ & = \\frac{\\partial}{\\partial v_c} (u_o^T v_c) - \\frac{1}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\sum_{x=1}^{V} \\frac{\\partial}{\\partial v_c} \\exp(u_x^T v_c) \\;\\;\\;\\cdots \\mathrm{chain\\ rule}\\\\ & = u_o - \\frac{1}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\sum_{x=1}^{V} \\exp(u_x^{T} v_c) \\frac{\\partial}{\\partial v_c} (u_x^T v_c) \\;\\;\\;\\cdots \\mathrm{chain\\ rule}\\\\ & = u_o - \\frac{1}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\sum_{x=1}^{V} \\exp(u_x^{T} v_c) \\cdot u_x \\\\ & = u_o - \\sum_{x=1}^{V} \\frac{\\exp(u_x^{T} v_c)}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\cdot u_x \\\\ & = \\underbrace{u_o}_{\\mathrm{current}} - \\underbrace{\\sum_{x=1}^{V} p(x|c) \\cdot u_x}_{\\mathrm{expectation}} \\end{align*} \\begin{align*} \\frac{\\partial}{\\partial v_c} \\log p(o|c) & = \\frac{\\partial}{\\partial v_c} \\log{\\frac{\\exp(u_o^T v_c)}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)}} \\\\ &= \\frac{\\partial}{\\partial v_c} \\log{\\exp(u_o^T v_c)} - \\frac{\\partial}{\\partial v_c} \\log{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\;\\;\\;\\cdots (\\log\\frac{a}{b} = \\log a - \\log b) \\\\ & = \\frac{\\partial}{\\partial v_c} (u_o^T v_c) - \\frac{1}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\sum_{x=1}^{V} \\frac{\\partial}{\\partial v_c} \\exp(u_x^T v_c) \\;\\;\\;\\cdots \\mathrm{chain\\ rule}\\\\ & = u_o - \\frac{1}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\sum_{x=1}^{V} \\exp(u_x^{T} v_c) \\frac{\\partial}{\\partial v_c} (u_x^T v_c) \\;\\;\\;\\cdots \\mathrm{chain\\ rule}\\\\ & = u_o - \\frac{1}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\sum_{x=1}^{V} \\exp(u_x^{T} v_c) \\cdot u_x \\\\ & = u_o - \\sum_{x=1}^{V} \\frac{\\exp(u_x^{T} v_c)}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\cdot u_x \\\\ & = \\underbrace{u_o}_{\\mathrm{current}} - \\underbrace{\\sum_{x=1}^{V} p(x|c) \\cdot u_x}_{\\mathrm{expectation}} \\end{align*} The result is remarkable. It have great intuition in it. The gradient represent the slop in the multidimentional space that we should walk along to reach the optima. The result gradient we got tell us that the slop equals to the difference of current context vector \\boldsymbol{u}_o \\boldsymbol{u}_o and the expected context vector (the weighted average over all context vectors). It has nothing todo with the center word c c . To compute the gradient of J(\\theta) J(\\theta) with respect to the center word c c , you have to sum up all the gradients obtained from word windows when c c is the center word. The gradient with respect to the context word will be very similar, the chain rule is also handy in that case. Once all the gradient with respect to center words and context words are calculated. We can use gradient descent to update the model parameters, which in this case is all the word vectors. Because we have two vectors for each word, when update the parameters we will use the average of the two vectors to update. Gradient descent to optimize cross entropy loss function \u00b6 Alternatively, we could also use cross entropy loss function. CS224N 2017 assignment 1 requires to derive the gradient of cross entropy loss function. This section, we will go step by step to derive the gradient when using cross entropy loss function and \\mathrm{softmax} \\mathrm{softmax} activation function in the out put layer. the cross entropy function is defined as follows \\begin{equation*} \\mathrm{CE}(\\boldsymbol{y}, \\boldsymbol{\\hat y}) = - \\sum_i y_i \\log (\\hat y_i) \\end{equation*} \\begin{equation*} \\mathrm{CE}(\\boldsymbol{y}, \\boldsymbol{\\hat y}) = - \\sum_i y_i \\log (\\hat y_i) \\end{equation*} Notice the \\boldsymbol{y} \\boldsymbol{y} is the one-hot label vector, and \\boldsymbol{\\hat y} \\boldsymbol{\\hat y} is the predicted probability for all classes. The index i i is the index of the one-hot element in individual label vectors. Therefore the definition of cross entropy loss function is defined as followinng if we use \\mathrm{softmax} \\mathrm{softmax} to predict \\hat y_i \\hat y_i , \\begin{equation*} J_{\\mathrm{CE}}(\\theta) = - \\sum_i^N y_i \\log (\\hat y_i) = -\\sum_i^N y_i \\log (\\mathrm{softmax}(\\theta)_i) \\end{equation*} \\begin{equation*} J_{\\mathrm{CE}}(\\theta) = - \\sum_i^N y_i \\log (\\hat y_i) = -\\sum_i^N y_i \\log (\\mathrm{softmax}(\\theta)_i) \\end{equation*} The gradient of \\frac{\\partial}{\\partial \\theta}J_{\\mathrm{CE}}(\\theta) \\frac{\\partial}{\\partial \\theta}J_{\\mathrm{CE}}(\\theta) can be derived using chain rule. Because we will use the gradient of the \\mathrm{softmax} \\mathrm{softmax} for the derivation, Let's derive \\mathrm{softmax} \\mathrm{softmax} gradient first. \\begin{equation*} \\frac{\\partial \\hat y_i}{\\theta_j} = \\frac{\\partial }{\\theta_j}\\mathrm{softmax}(\\theta)_i = \\frac{\\partial }{\\theta_j} \\frac{e^{\\theta_i}}{\\sum_{k=1}^{n}e^{\\theta_k}} \\end{equation*} \\begin{equation*} \\frac{\\partial \\hat y_i}{\\theta_j} = \\frac{\\partial }{\\theta_j}\\mathrm{softmax}(\\theta)_i = \\frac{\\partial }{\\theta_j} \\frac{e^{\\theta_i}}{\\sum_{k=1}^{n}e^{\\theta_k}} \\end{equation*} Here we should use two tricks to derive this gradient. Quotient rule separate into 2 cases: when i = j i = j and i \\ne j i \\ne j . If i = j i = j , we have \\begin{align*} \\frac{\\partial }{\\theta_j} \\frac{e^{\\theta_i}}{\\sum_{k=1}^{n}e^{\\theta_k}} & = \\frac{e^{\\theta_i}\\sum_{k=1}^{n}e^{\\theta_k} - e^{\\theta_i}e^{\\theta_j} }{\\Big(\\sum_{k=1}^{n}e^{\\theta_k}\\Big)^2} \\\\ & = \\frac{e^{\\theta_i}\\Big(\\sum_{k=1}^{n}e^{\\theta_k} - e^{\\theta_j}\\Big)}{\\Big(\\sum_{k=1}^{n}e^{\\theta_k}\\Big)^2} \\\\ & = \\frac{e^{\\theta_i}}{\\sum_{k=1}^{n}e^{\\theta_k}} \\frac{\\sum_{k=1}^{n}e^{\\theta_k} - e^{\\theta_j}}{\\sum_{k=1}^{n}e^{\\theta_k}} \\\\ & = \\hat y_i(1 - \\hat y_j) \\end{align*} \\begin{align*} \\frac{\\partial }{\\theta_j} \\frac{e^{\\theta_i}}{\\sum_{k=1}^{n}e^{\\theta_k}} & = \\frac{e^{\\theta_i}\\sum_{k=1}^{n}e^{\\theta_k} - e^{\\theta_i}e^{\\theta_j} }{\\Big(\\sum_{k=1}^{n}e^{\\theta_k}\\Big)^2} \\\\ & = \\frac{e^{\\theta_i}\\Big(\\sum_{k=1}^{n}e^{\\theta_k} - e^{\\theta_j}\\Big)}{\\Big(\\sum_{k=1}^{n}e^{\\theta_k}\\Big)^2} \\\\ & = \\frac{e^{\\theta_i}}{\\sum_{k=1}^{n}e^{\\theta_k}} \\frac{\\sum_{k=1}^{n}e^{\\theta_k} - e^{\\theta_j}}{\\sum_{k=1}^{n}e^{\\theta_k}} \\\\ & = \\hat y_i(1 - \\hat y_j) \\end{align*} if i \\ne j i \\ne j , we have \\begin{align*} \\frac{\\partial }{\\theta_j} \\frac{e^{\\theta_i}}{\\sum_{k=1}^{n}e^{\\theta_k}} & = \\frac{0 - e^{\\theta_i}e^{\\theta_j} }{\\Big(\\sum_{k=1}^{n}e^{\\theta_k}\\Big)^2} \\\\ & = \\frac{e^{\\theta_i}}{\\sum_{k=1}^{n}e^{\\theta_k}} \\frac{ - e^{\\theta_j}}{\\sum_{k=1}^{n}e^{\\theta_k}} \\\\ & = -\\hat y_i \\hat y_j \\end{align*} \\begin{align*} \\frac{\\partial }{\\theta_j} \\frac{e^{\\theta_i}}{\\sum_{k=1}^{n}e^{\\theta_k}} & = \\frac{0 - e^{\\theta_i}e^{\\theta_j} }{\\Big(\\sum_{k=1}^{n}e^{\\theta_k}\\Big)^2} \\\\ & = \\frac{e^{\\theta_i}}{\\sum_{k=1}^{n}e^{\\theta_k}} \\frac{ - e^{\\theta_j}}{\\sum_{k=1}^{n}e^{\\theta_k}} \\\\ & = -\\hat y_i \\hat y_j \\end{align*} Now let calculate the gradient of the cross-entropy loss function. Notice the gradient is concerning the i i th parameter. This is because we can use the gradient of the \\mathrm{softmax} \\mathrm{softmax} (the \\frac{\\partial \\hat y_k}{\\partial \\theta_i} \\frac{\\partial \\hat y_k}{\\partial \\theta_i} term) conveniently. \\begin{align*} \\frac{\\partial J_{\\mathrm{CE}}(\\theta)}{\\partial \\theta_i} & = - \\sum_k^N y_k \\frac{\\partial \\log (\\hat y_k)}{\\partial \\theta_i} \\\\ & = - \\sum_k^N y_k \\frac{\\partial \\log (\\hat y_k)}{\\partial \\hat y_k} \\frac{\\partial \\hat y_k}{\\partial \\theta_i} \\\\ & = - \\sum_k^N y_k \\frac{1}{\\hat y_k} \\frac{\\partial \\hat y_k}{\\partial \\theta_i} \\\\ & = - \\Big(\\underbrace{y_k \\frac{1}{\\hat y_k} \\hat y_k(1 - \\hat y_i)}_{k = i} + \\underbrace{\\sum_{k\\ne i}^N y_k \\frac{1}{\\hat y_k} (-\\hat y_k \\hat y_i) \\Big)}_{k \\ne i} \\\\ & = - \\Big(y_i (1 - \\hat y_i) - \\sum_{k\\ne i}^N y_k \\hat y_i \\Big) \\\\ & = - y_i + y_i \\hat y_i + \\sum_{k\\ne i}^N y_k \\hat y_i \\\\ & = \\hat y_i\\Big(y_i + \\sum_{k\\ne i}^N y_k\\Big) - \\hat y_i \\\\ & = \\hat y_i \\cdot 1 - y_i = \\hat y_i - y_i \\end{align*} \\begin{align*} \\frac{\\partial J_{\\mathrm{CE}}(\\theta)}{\\partial \\theta_i} & = - \\sum_k^N y_k \\frac{\\partial \\log (\\hat y_k)}{\\partial \\theta_i} \\\\ & = - \\sum_k^N y_k \\frac{\\partial \\log (\\hat y_k)}{\\partial \\hat y_k} \\frac{\\partial \\hat y_k}{\\partial \\theta_i} \\\\ & = - \\sum_k^N y_k \\frac{1}{\\hat y_k} \\frac{\\partial \\hat y_k}{\\partial \\theta_i} \\\\ & = - \\Big(\\underbrace{y_k \\frac{1}{\\hat y_k} \\hat y_k(1 - \\hat y_i)}_{k = i} + \\underbrace{\\sum_{k\\ne i}^N y_k \\frac{1}{\\hat y_k} (-\\hat y_k \\hat y_i) \\Big)}_{k \\ne i} \\\\ & = - \\Big(y_i (1 - \\hat y_i) - \\sum_{k\\ne i}^N y_k \\hat y_i \\Big) \\\\ & = - y_i + y_i \\hat y_i + \\sum_{k\\ne i}^N y_k \\hat y_i \\\\ & = \\hat y_i\\Big(y_i + \\sum_{k\\ne i}^N y_k\\Big) - \\hat y_i \\\\ & = \\hat y_i \\cdot 1 - y_i = \\hat y_i - y_i \\end{align*} Write in vector form, we will have \\frac{\\partial J_{\\mathrm{CE}}(\\boldsymbol{y}, \\boldsymbol{\\hat y})}{\\partial \\boldsymbol{\\theta}} = \\boldsymbol{\\hat y} - \\boldsymbol{y}. \\frac{\\partial J_{\\mathrm{CE}}(\\boldsymbol{y}, \\boldsymbol{\\hat y})}{\\partial \\boldsymbol{\\theta}} = \\boldsymbol{\\hat y} - \\boldsymbol{y}. With this gradient, we can update the model parameters, namely the word vectors. In the next post, I will discuss neural networks, which have more layers than the word2vec. In neural networks, there are more parameters need to be trained and the gradients with respect all the parameters need to be derived. Skip-gram model \u00b6 Skip-gram model uses the center word to predict the surrounding. We have derived the gradient for the skip-gram model using log-likelihood loss function and \\mathrm{softmax} \\mathrm{softmax} . This section will derive the gradient for cross-entropy loss function. The probability output function will keep using the \\mathrm{softmax} \\mathrm{softmax} . Cross entropy loss function \u00b6 Since we have derived above that \\frac{\\partial J_{\\mathrm{CE}}(\\boldsymbol{y}, \\boldsymbol{\\hat y})}{\\partial \\boldsymbol{\\theta}} = \\boldsymbol{\\hat y} - \\boldsymbol{y} \\frac{\\partial J_{\\mathrm{CE}}(\\boldsymbol{y}, \\boldsymbol{\\hat y})}{\\partial \\boldsymbol{\\theta}} = \\boldsymbol{\\hat y} - \\boldsymbol{y} . In this case, the vector \\boldsymbol{\\theta} = [\\boldsymbol{u_1^{\\mathsf{T}}}\\boldsymbol{v_c}, \\boldsymbol{u_2^{\\mathsf{T}}}\\boldsymbol{v_c}, \\cdots, \\boldsymbol{u_W^{\\mathsf{T}}}\\boldsymbol{v_c}] \\boldsymbol{\\theta} = [\\boldsymbol{u_1^{\\mathsf{T}}}\\boldsymbol{v_c}, \\boldsymbol{u_2^{\\mathsf{T}}}\\boldsymbol{v_c}, \\cdots, \\boldsymbol{u_W^{\\mathsf{T}}}\\boldsymbol{v_c}] . Gradient for center word \u00b6 Borrow the above steps to derive the gradient with respect to \\boldsymbol{\\theta} \\boldsymbol{\\theta} , we have \\begin{align*} \\frac{\\partial }{\\partial \\boldsymbol{v_c}}J_{\\mathrm{softmax-CE}}(\\boldsymbol{o}, \\boldsymbol{v_c}, \\boldsymbol{U}) & = \\frac{\\partial J_{\\mathrm{CE}}(\\theta)}{\\partial \\theta_i} \\frac{\\partial \\theta_i}{\\boldsymbol{v_c}} \\\\ & = (\\hat y_i - y_i) \\boldsymbol{u_i^{\\mathsf{T}}} \\end{align*} \\begin{align*} \\frac{\\partial }{\\partial \\boldsymbol{v_c}}J_{\\mathrm{softmax-CE}}(\\boldsymbol{o}, \\boldsymbol{v_c}, \\boldsymbol{U}) & = \\frac{\\partial J_{\\mathrm{CE}}(\\theta)}{\\partial \\theta_i} \\frac{\\partial \\theta_i}{\\boldsymbol{v_c}} \\\\ & = (\\hat y_i - y_i) \\boldsymbol{u_i^{\\mathsf{T}}} \\end{align*} Notice here the \\boldsymbol{u_i} \\boldsymbol{u_i} is a column vector. And the derivative of \\boldsymbol{y} = \\boldsymbol{a}^{\\mathsf{T}}\\boldsymbol{x} \\boldsymbol{y} = \\boldsymbol{a}^{\\mathsf{T}}\\boldsymbol{x} with respect to \\boldsymbol{x} \\boldsymbol{x} is \\boldsymbol{a}^{\\mathsf{T}} \\boldsymbol{a}^{\\mathsf{T}} . Written in vector form, we will get \\begin{equation*} \\frac{\\partial J}{\\partial \\boldsymbol{v_c}} = \\boldsymbol{U}(\\boldsymbol{\\hat y} - \\boldsymbol{y}) \\end{equation*} \\begin{equation*} \\frac{\\partial J}{\\partial \\boldsymbol{v_c}} = \\boldsymbol{U}(\\boldsymbol{\\hat y} - \\boldsymbol{y}) \\end{equation*} In the above gradient, U = [\\boldsymbol{u_1}, \\boldsymbol{u_1}, \\cdots, \\boldsymbol{u_W}] U = [\\boldsymbol{u_1}, \\boldsymbol{u_1}, \\cdots, \\boldsymbol{u_W}] is the matrix of all the output vectors. \\boldsymbol{u_1} \\boldsymbol{u_1} is a column word vector. The component \\boldsymbol{\\hat y} - \\boldsymbol{y} \\boldsymbol{\\hat y} - \\boldsymbol{y} is a also a column vector with length of W W . The above gradient can be viewed as scaling each output vector \\boldsymbol{u_i} \\boldsymbol{u_i} by the scaler \\hat y_i - y. \\hat y_i - y. Alternatively, the gradient can also be wrote as distributive form \\begin{equation*} \\frac{\\partial J}{\\partial \\boldsymbol{v_c}} = \\boldsymbol{U}\\boldsymbol{\\hat y} - \\boldsymbol{U}\\boldsymbol{y} = -\\boldsymbol{u_i} + \\sum_{w=1}^{W}\\hat y_w \\boldsymbol{u_w} \\end{equation*} \\begin{equation*} \\frac{\\partial J}{\\partial \\boldsymbol{v_c}} = \\boldsymbol{U}\\boldsymbol{\\hat y} - \\boldsymbol{U}\\boldsymbol{y} = -\\boldsymbol{u_i} + \\sum_{w=1}^{W}\\hat y_w \\boldsymbol{u_w} \\end{equation*} The index i i in the above equation is corresponding to the index of the none zero element in the one-hot vector \\boldsymbol{y} \\boldsymbol{y} . Here we can see \\boldsymbol{y} \\boldsymbol{y} as the true label of the output word. Gradient for output word \u00b6 We can also calculate the gradient with respect to the output word. \\begin{align*} \\frac{\\partial }{\\partial \\boldsymbol{u_w}}J_{\\mathrm{softmax-CE}}(\\boldsymbol{o}, \\boldsymbol{v_c}, \\boldsymbol{U}) & = \\frac{\\partial J_{\\mathrm{CE}}(\\theta)}{\\partial \\theta_i} \\frac{\\partial \\theta_i}{\\boldsymbol{u_w}} \\\\ & = (\\hat y_i - y_i) \\boldsymbol{v_c} \\end{align*} \\begin{align*} \\frac{\\partial }{\\partial \\boldsymbol{u_w}}J_{\\mathrm{softmax-CE}}(\\boldsymbol{o}, \\boldsymbol{v_c}, \\boldsymbol{U}) & = \\frac{\\partial J_{\\mathrm{CE}}(\\theta)}{\\partial \\theta_i} \\frac{\\partial \\theta_i}{\\boldsymbol{u_w}} \\\\ & = (\\hat y_i - y_i) \\boldsymbol{v_c} \\end{align*} Notice here we apply \\frac{\\partial \\boldsymbol{a}^{\\mathsf{T}} \\boldsymbol{x}}{\\partial \\boldsymbol{a}} = x \\frac{\\partial \\boldsymbol{a}^{\\mathsf{T}} \\boldsymbol{x}}{\\partial \\boldsymbol{a}} = x . Writen the gradient in matrix format, we have \\begin{equation*} \\frac{\\partial J}{\\partial \\boldsymbol{U}} = \\boldsymbol{v_c}(\\boldsymbol{\\hat y} - \\boldsymbol{y})^{\\mathsf{T}} \\end{equation*} \\begin{equation*} \\frac{\\partial J}{\\partial \\boldsymbol{U}} = \\boldsymbol{v_c}(\\boldsymbol{\\hat y} - \\boldsymbol{y})^{\\mathsf{T}} \\end{equation*} Notice the shape of the gradient. It determines how the notation looks like. In the above notation notice the shape of the output word gradient \\frac{\\partial J}{\\partial \\boldsymbol{U}} \\frac{\\partial J}{\\partial \\boldsymbol{U}} is d \\times W d \\times W . As we will discussion in the next post that it is a convention to make the shape of gradient as the shape of the input vectors. in this case the shape of U U and \\frac{\\partial J}{\\partial \\boldsymbol{U}} \\frac{\\partial J}{\\partial \\boldsymbol{U}} are the same. Negative sampling \u00b6 The cost function for a single word prediction using nagative sampling is the following \\begin{align*} J_{\\mathrm{neg-sample}}(\\boldsymbol{o},\\boldsymbol{v_c},\\boldsymbol{U}) & = -\\log(\\sigma(\\boldsymbol{u_o}^{\\mathsf{T}} \\boldsymbol{v_c})) - \\sum_{k=1}^{K} \\log(\\sigma(-\\boldsymbol{u_k}^{\\mathsf{T}} \\boldsymbol{v_c})) \\end{align*} \\begin{align*} J_{\\mathrm{neg-sample}}(\\boldsymbol{o},\\boldsymbol{v_c},\\boldsymbol{U}) & = -\\log(\\sigma(\\boldsymbol{u_o}^{\\mathsf{T}} \\boldsymbol{v_c})) - \\sum_{k=1}^{K} \\log(\\sigma(-\\boldsymbol{u_k}^{\\mathsf{T}} \\boldsymbol{v_c})) \\end{align*} It comes from the original paper by Mikolov et al. \\sigma \\sigma is the sigmoid function \\sigma(x) = \\frac{1}{1+e^{-x}} \\sigma(x) = \\frac{1}{1+e^{-x}} . The ideas is to reduce the optimization computation by only sampling a small part of the vocabulary that have a lower probability of being context words of one another. The first lecture notes from CS224N discussed briefly the origin and intuition of the negative sampling loss function. Here we will focus on deriving the gradient and implementation ideas. With the fact that \\frac{\\mathrm{d}\\sigma(x)}{\\mathrm{d} x} = \\sigma(x)(1-\\sigma(x)) \\frac{\\mathrm{d}\\sigma(x)}{\\mathrm{d} x} = \\sigma(x)(1-\\sigma(x)) and the chain rule, it is not hard to derive the gradients result as following \\begin{align*} \\frac{\\partial J}{\\partial \\boldsymbol{v_c}} & = (\\sigma(\\boldsymbol{u_o}^{\\mathsf{T}}\\boldsymbol{v_c}) - 1)\\boldsymbol{u_o} - \\sum_{k=1}^{K}(\\sigma(-\\boldsymbol{u_k}^{\\mathsf{T}}\\boldsymbol{v_c}) - 1)\\boldsymbol{u_k} \\\\ \\frac{\\partial J}{\\partial \\boldsymbol{u_o}} & = (\\sigma(\\boldsymbol{u_o}^{\\mathsf{T}}\\boldsymbol{v_c}) - 1)\\boldsymbol{v_c} \\\\ \\frac{\\partial J}{\\partial \\boldsymbol{u_k}} & = -\\sigma(-\\boldsymbol{u_k}^{\\mathsf{T}}\\boldsymbol{v_c}) - 1)\\boldsymbol{v_c}, \\mathrm{for\\ all\\ } k = 1, 2, \\cdots, K \\end{align*} \\begin{align*} \\frac{\\partial J}{\\partial \\boldsymbol{v_c}} & = (\\sigma(\\boldsymbol{u_o}^{\\mathsf{T}}\\boldsymbol{v_c}) - 1)\\boldsymbol{u_o} - \\sum_{k=1}^{K}(\\sigma(-\\boldsymbol{u_k}^{\\mathsf{T}}\\boldsymbol{v_c}) - 1)\\boldsymbol{u_k} \\\\ \\frac{\\partial J}{\\partial \\boldsymbol{u_o}} & = (\\sigma(\\boldsymbol{u_o}^{\\mathsf{T}}\\boldsymbol{v_c}) - 1)\\boldsymbol{v_c} \\\\ \\frac{\\partial J}{\\partial \\boldsymbol{u_k}} & = -\\sigma(-\\boldsymbol{u_k}^{\\mathsf{T}}\\boldsymbol{v_c}) - 1)\\boldsymbol{v_c}, \\mathrm{for\\ all\\ } k = 1, 2, \\cdots, K \\end{align*} How to sample the \\boldsymbol{u_k} \\boldsymbol{u_k} in practice? The best known sampling method is based on the Unigram Model raise to the power of 3/4. Unigram Model is the counts of each word in the particular corpus (not the vocabulary). Gradient for all of word vectors \u00b6 Since skip-gram model is using one center word to predict all its context words. Given a word context size of m m , we obtain a set of context words [\\mathrm{word}_{c-m}, \\cdots, \\mathrm{word}_{c-1}, \\mathrm{word}_{c}, \\mathrm{word}_{c+1}, \\cdots, \\mathrm{word}_{c+m}] [\\mathrm{word}_{c-m}, \\cdots, \\mathrm{word}_{c-1}, \\mathrm{word}_{c}, \\mathrm{word}_{c+1}, \\cdots, \\mathrm{word}_{c+m}] . For each window, We need to predict 2m 2m context word given the center word. Denote the \"input\" and \"output\" word vectors for \\mathrm{word}_k \\mathrm{word}_k as \\boldsymbol{v}_k \\boldsymbol{v}_k and \\boldsymbol{u}_k \\boldsymbol{u}_k respectively. The cost for the entire context window with size m m centered around \\mathrm{word}_c \\mathrm{word}_c would be \\begin{equation*} J_{\\mathrm{skip-gram}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m}) = \\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} F(\\boldsymbol{u}_{c+j}, \\boldsymbol{v}_c). \\end{equation*} \\begin{equation*} J_{\\mathrm{skip-gram}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m}) = \\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} F(\\boldsymbol{u}_{c+j}, \\boldsymbol{v}_c). \\end{equation*} F F is a placeholder notation to represent the cost function given the center word for different model. Therefore for skip-gram, the gradients for the cost of one context window are \\begin{align*} \\frac{J_{\\mathrm{skip-gram}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{U}} & = \\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} \\frac{\\partial F(\\boldsymbol{u}_{c+j}, \\boldsymbol{v}_c)}{\\partial \\boldsymbol{v}_c},\\\\ \\frac{J_{\\mathrm{skip-gram}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{v}_c} & = \\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} \\frac{\\partial F(\\boldsymbol{u}_{c+j}, \\boldsymbol{v}_c)}{\\partial \\boldsymbol{v}_c}, \\\\ \\frac{J_{\\mathrm{skip-gram}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{v}_j} & = 0, \\forall j \\ne c \\end{align*} \\begin{align*} \\frac{J_{\\mathrm{skip-gram}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{U}} & = \\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} \\frac{\\partial F(\\boldsymbol{u}_{c+j}, \\boldsymbol{v}_c)}{\\partial \\boldsymbol{v}_c},\\\\ \\frac{J_{\\mathrm{skip-gram}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{v}_c} & = \\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} \\frac{\\partial F(\\boldsymbol{u}_{c+j}, \\boldsymbol{v}_c)}{\\partial \\boldsymbol{v}_c}, \\\\ \\frac{J_{\\mathrm{skip-gram}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{v}_j} & = 0, \\forall j \\ne c \\end{align*} CBOW model \u00b6 Continuous bag-of-words (CBOW) is using the context words to predect the center words. Different from skip-gram model, in CBOW, we will use 2m 2m context word vectors as we predict probability of a word is the center word. For a simple variant of CBOW, we could sum up all the 2m 2m word vectors in one context vector \\hat{\\boldsymbol{v}} \\hat{\\boldsymbol{v}} and use the similar cost function with \\mathrm{softmax} \\mathrm{softmax} as we did in skip-gram model. \\begin{equation*} \\hat{\\boldsymbol{v}} = \\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} \\boldsymbol{v}_{c+j}. \\end{equation*} \\begin{equation*} \\hat{\\boldsymbol{v}} = \\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} \\boldsymbol{v}_{c+j}. \\end{equation*} Similar to skip-gram, we have \\begin{align*} \\frac{J_{\\mathrm{CBOW}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{U}} & = \\frac{\\partial F(\\boldsymbol{u}_{c}, \\hat{\\boldsymbol{v}})}{\\partial \\boldsymbol{U}},\\\\ \\frac{J_{\\mathrm{CBOW}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{v}_j} & = \\frac{\\partial F(\\boldsymbol{u}_{c}, \\hat{\\boldsymbol{v}})}{\\partial \\boldsymbol{v}_j}, \\forall j \\in \\{c-m, \\cdots, c-1, c+1, \\cdots, c+m\\}\\\\ \\frac{J_{\\mathrm{CBOW}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{v}_j} & = 0, \\forall j \\notin \\{c-m, \\cdots, c-1, c+1, \\cdots, c+m\\}. \\end{align*} \\begin{align*} \\frac{J_{\\mathrm{CBOW}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{U}} & = \\frac{\\partial F(\\boldsymbol{u}_{c}, \\hat{\\boldsymbol{v}})}{\\partial \\boldsymbol{U}},\\\\ \\frac{J_{\\mathrm{CBOW}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{v}_j} & = \\frac{\\partial F(\\boldsymbol{u}_{c}, \\hat{\\boldsymbol{v}})}{\\partial \\boldsymbol{v}_j}, \\forall j \\in \\{c-m, \\cdots, c-1, c+1, \\cdots, c+m\\}\\\\ \\frac{J_{\\mathrm{CBOW}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{v}_j} & = 0, \\forall j \\notin \\{c-m, \\cdots, c-1, c+1, \\cdots, c+m\\}. \\end{align*} GloVe model \u00b6 TODO, the paper and lecture. Word vector evaluation \u00b6 Summary \u00b6 This post focused on gradient derivation for various word embedding models. The bootstrap model is based on the distributional semantic, by which to predict the probability of a word given some other words from the fixed corpus. We use the \\mathrm{softmax} \\mathrm{softmax} function to compute the probability. To update the word vectors, we introduce the likelihood function and derived its gradient. In that derivation, the parameter \\boldsymbol{\\theta} \\boldsymbol{\\theta} is the hyper parameters related to word vector that can be used to compute the probability. We continue to derive the gradient of the cross entropy loss function for a single word prediction. The result gradient of the cross entropy loss function is the key for all our later gradient derivation. After that we introduce the word2vec family of word embedding, skip-gram and CBOW models. In word2vec we use the dot product of the center word and its context words to compute the probability. We derive the gradient with respect to both center word and context word when using cross entropy loss function. Negative sampling is also discussed as it improves the computation cost by many factors. With the discussion of skip-gram model, CBOW model is easier to present. One thing you need to distinguish is that whether the gradient is for a particular word prediction, for the whole window of the words, or the over all objective for the corpus. For skip-gram, we first compute the gradient for each word prediction in the given context window, then sum up all the gradient to update the cost function of that window. We move the window over the corpus until finish all the word updates. This whole process is only one update. While using negative sampling, the process becomes more efficient, we don't have to go through all the window, only sampling K K windows with Unigram Model raise to the 3/4 3/4 power. In the simple CBOW model discussed, we add up all the context vectors first, then only update a single gradient for that window corresponding to the center word. We repeat this process for different windows to complete one update. Part 2 Neural Networks and Backpropagation \u00b6 Date: April 15th 2019 We discussed the softmax classifier in Part 1 and its major drawback that the classifier only gives linear decision boundaries. In Part 2, Neural Networks will be introduced to demonstrate that it can learn much more complex functions and nonlinear decision boundaries. Intro to Neural Networks \u00b6 Biological Neuron Mathematical Model Simplified Neuron The neuron can be modeled as a binary logistic regression unit as in the last row of the table above. It can be further simplified as following functions, \\begin{equation*} h_{w, b} (\\boldsymbol{x})=f(\\boldsymbol{w}^\\mathsf{T} \\boldsymbol{x} + b) \\\\ f(z) = \\frac{1}{1 + e^{-z}}. \\end{equation*} \\begin{equation*} h_{w, b} (\\boldsymbol{x})=f(\\boldsymbol{w}^\\mathsf{T} \\boldsymbol{x} + b) \\\\ f(z) = \\frac{1}{1 + e^{-z}}. \\end{equation*} \\boldsymbol{x} \\boldsymbol{x} is the inputs \\boldsymbol{w} \\boldsymbol{w} is the weights b b is a bias term h h is a hidden layer function f f is a nonlinear activation function (sigmoid, tanh, etc.) If we feed a vector of inputs through a bunch of logistic regression (sigmoid) functions, then we get a vector of outputs. The CS224N lecture note 3 section 1.2 have a complete derivation on multiple sigmoid units. But we don\u2019t have to decide ahead of time what variables these logistic regressions are trying to predict! It is the loss function that will direct what the intermediate hidden variables should be, so as to do a good job at predicting the targets for the next layer, etc. Forward Propagation in Matrix Notation \u00b6 In a multilayer neural network, not only did we have multiple sigmoid units, but we also have more than one layer. Let's explicitly write down the signal transformation (aka forward propagation) from one layer to another referring to this network from Andrew's ML course. We will use the following notation convention a^{(j)}_i a^{(j)}_i to represent the \"activation\" of unit i i in layer j j . W^{(j)} W^{(j)} to represent the matrix of weights controlling function mapping from layer j j to layer j + 1 j + 1 . The value at each node can be calculated as \\begin{equation*} a^{(2)}_1 = f(w^{(1)}_{10} x_0 + w^{(1)}_{11} x_1 + w^{(1)}_{12} x_2 + w^{(1)}_{13} x_3) \\\\ a^{(2)}_2 = f(w^{(1)}_{20} x_0 + w^{(1)}_{21} x_1 + w^{(1)}_{22} x_2 + w^{(1)}_{23} x_3) \\\\ a^{(2)}_3 = f(w^{(1)}_{30} x_0 + w^{(1)}_{31} x_1 + w^{(1)}_{32} x_2 + w^{(1)}_{33} x_3) \\\\ \\text{and} \\\\ h_{w} (\\boldsymbol{x}) = a^{(3)}_1 = f(w^{(2)}_{10} a^{(2)}_0 + w^{(2)}_{11} a^{(2)}_1 + w^{(2)}_{12} a^{(2)}_2 + w^{(2)}_{13} a^{(2)}_3) \\end{equation*} \\begin{equation*} a^{(2)}_1 = f(w^{(1)}_{10} x_0 + w^{(1)}_{11} x_1 + w^{(1)}_{12} x_2 + w^{(1)}_{13} x_3) \\\\ a^{(2)}_2 = f(w^{(1)}_{20} x_0 + w^{(1)}_{21} x_1 + w^{(1)}_{22} x_2 + w^{(1)}_{23} x_3) \\\\ a^{(2)}_3 = f(w^{(1)}_{30} x_0 + w^{(1)}_{31} x_1 + w^{(1)}_{32} x_2 + w^{(1)}_{33} x_3) \\\\ \\text{and} \\\\ h_{w} (\\boldsymbol{x}) = a^{(3)}_1 = f(w^{(2)}_{10} a^{(2)}_0 + w^{(2)}_{11} a^{(2)}_1 + w^{(2)}_{12} a^{(2)}_2 + w^{(2)}_{13} a^{(2)}_3) \\end{equation*} write the matrix W^{(j)} W^{(j)} explicity, \\begin{equation*} W^{(1)} = \\begin{bmatrix} w^{(1)}_{10} & w^{(1)}_{11} & w^{(1)}_{12} & w^{(1)}_{13} \\\\ w^{(1)}_{20} & w^{(1)}_{21} & w^{(1)}_{22} & w^{(1)}_{23} \\\\ w^{(1)}_{30} & w^{(1)}_{31} & w^{(1)}_{32} & w^{(1)}_{33} \\end{bmatrix} \\end{equation*} \\begin{equation*} W^{(1)} = \\begin{bmatrix} w^{(1)}_{10} & w^{(1)}_{11} & w^{(1)}_{12} & w^{(1)}_{13} \\\\ w^{(1)}_{20} & w^{(1)}_{21} & w^{(1)}_{22} & w^{(1)}_{23} \\\\ w^{(1)}_{30} & w^{(1)}_{31} & w^{(1)}_{32} & w^{(1)}_{33} \\end{bmatrix} \\end{equation*} \\begin{equation*} W^{(2)} = \\begin{bmatrix} w^{(2)}_{10} & w^{(2)}_{11} & w^{(2)}_{12} & w^{(2)}_{13} \\end{bmatrix} \\end{equation*} \\begin{equation*} W^{(2)} = \\begin{bmatrix} w^{(2)}_{10} & w^{(2)}_{11} & w^{(2)}_{12} & w^{(2)}_{13} \\end{bmatrix} \\end{equation*} With the above form, we can use matrix notation as \\begin{align*} & \\boldsymbol{a}^{(2)} = f(\\boldsymbol{W}^{(1)} \\boldsymbol{x}) \\\\ h_{w} (\\boldsymbol{x}) = & \\boldsymbol{a}^{(3)} = f(\\boldsymbol{W}^{(2)} \\boldsymbol{a}^{(2)}) \\end{align*} \\begin{align*} & \\boldsymbol{a}^{(2)} = f(\\boldsymbol{W}^{(1)} \\boldsymbol{x}) \\\\ h_{w} (\\boldsymbol{x}) = & \\boldsymbol{a}^{(3)} = f(\\boldsymbol{W}^{(2)} \\boldsymbol{a}^{(2)}) \\end{align*} We can see from above notations that if the network has s_j s_j units in layer j j and s_{j+1} s_{j+1} in layer j + 1 j + 1 , the matrix W^{(j)} W^{(j)} will be of dimention s_{j+1} \\times s_{j}+1 s_{j+1} \\times s_{j}+1 . It could be interpreted as \"the dimention of W^{(j)} W^{(j)} is the number of nodes in the next layer (layer j + 1 j + 1 ) \\times \\times the number of nodes in the current layer + + 1. Note that in cs224n the matrix notation is slightly different \\begin{align*} & \\boldsymbol{a}^{(2)} = f(\\boldsymbol{W}^{(1)} \\boldsymbol{x} + \\boldsymbol{b}^{(1)}) \\\\ h_{w} (\\boldsymbol{x}) = & \\boldsymbol{a}^{(3)} = f(\\boldsymbol{W}^{(2)} \\boldsymbol{a}^{(2)} + \\boldsymbol{b}^{(2)}) \\end{align*} \\begin{align*} & \\boldsymbol{a}^{(2)} = f(\\boldsymbol{W}^{(1)} \\boldsymbol{x} + \\boldsymbol{b}^{(1)}) \\\\ h_{w} (\\boldsymbol{x}) = & \\boldsymbol{a}^{(3)} = f(\\boldsymbol{W}^{(2)} \\boldsymbol{a}^{(2)} + \\boldsymbol{b}^{(2)}) \\end{align*} the different is from how we denote the bias. The two are enssentially the same, but be cautions that the matrix dimentions are different. Word Window Classification Using Neural Networks \u00b6 From now on, let's switch the notation to cs224n so that we can derive the backpropagation algorithm for word window classification and get more intuition about the backprop. The drawing of the neural nets in cs224n for word window classification is less dynamic and slitly different from the drawing from Andrew's ML class. The figure from cs224n may be slightly confusing at first, but it is good to understand it from this word window classification application. Forward propagation \u00b6 Firstly, the goal of this classification task is to classify whether the center word is a location. Similar to word2vec, we will go over all positions in a corpus. But this time, it will be supervised and only some positions should get a high score. The figure above illustrate the feed-forward process. We use the method by Collobert & Weston (2008, 2011). An unnormalized score will be calculated from the activation \\boldsymbol{a} = [a_1, a_2, a_3, \\cdots]^\\mathsf{T} \\boldsymbol{a} = [a_1, a_2, a_3, \\cdots]^\\mathsf{T} . \\begin{equation*} \\text{scroe}(\\boldsymbol{x}) = U^\\mathsf{T} \\boldsymbol{a} \\in \\mathbb{R} \\end{equation*} \\begin{equation*} \\text{scroe}(\\boldsymbol{x}) = U^\\mathsf{T} \\boldsymbol{a} \\in \\mathbb{R} \\end{equation*} We will use max-margin loss as our loss function. The training is essentially to find the optimal weights W W by minimize the max-margin loss \\begin{equation*} J = \\max(0, 1 - s + s_c) \\end{equation*} \\begin{equation*} J = \\max(0, 1 - s + s_c) \\end{equation*} s s is the score of a window that have a location in the center. s_c s_c is the score of a window that doesn't have a location in the center. For full objective function: Sample several corrupt windows per true one. Sum over all training windows. It is similar to negative sampling in word2vec. We will use gradient descent to update the parameter so as to minimize the loss function. The key is how to calculate the gradient with respect to the model parameters, namely \\nabla_{\\theta} J(\\theta) \\nabla_{\\theta} J(\\theta) . Here we use \\theta \\theta to represent the hyperthetic parameters, it can include the W W and other parameters of the model. \\begin{equation*} \\theta^{\\text{new}} = \\theta^{\\text{old}} - \\alpha\\nabla_{\\theta} J(\\theta) \\end{equation*} \\begin{equation*} \\theta^{\\text{new}} = \\theta^{\\text{old}} - \\alpha\\nabla_{\\theta} J(\\theta) \\end{equation*} Gradients and Jacobians Matrix \u00b6 At first, let us layout the input and all the equations in this simple neural network. Input Layer Hidden Layer Output Layer \\boldsymbol{x} \\boldsymbol{x} \\boldsymbol{z} = \\boldsymbol{W} \\boldsymbol{x} + \\boldsymbol{b}, \\\\ \\boldsymbol{h} = f(\\boldsymbol{z}) \\boldsymbol{z} = \\boldsymbol{W} \\boldsymbol{x} + \\boldsymbol{b}, \\\\ \\boldsymbol{h} = f(\\boldsymbol{z}) s = \\boldsymbol{u}^{\\mathsf{T}}\\boldsymbol{h} s = \\boldsymbol{u}^{\\mathsf{T}}\\boldsymbol{h} To update the parameters in this model, namely \\boldsymbol{x}, \\boldsymbol{W}, \\boldsymbol{b} \\boldsymbol{x}, \\boldsymbol{W}, \\boldsymbol{b} , we would like to compute the derivitavies of s s with respect to all these parameters. We have to use the chain rule to compute it. What chain rule says is that we can compute the partial derivatives of each individual functions and then multiply them together to get the derivative with respect the specific variable. For example, \\frac{\\partial s}{\\partial \\boldsymbol{b}} \\frac{\\partial s}{\\partial \\boldsymbol{b}} is computed as \\frac{\\partial s}{\\partial \\boldsymbol{b}} = \\frac{\\partial s}{\\partial \\boldsymbol{h}} \\cdot \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\cdot \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} \\frac{\\partial s}{\\partial \\boldsymbol{b}} = \\frac{\\partial s}{\\partial \\boldsymbol{h}} \\cdot \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\cdot \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} . This seems very easy to understand, but when it comes to implemenation in vectorized format, it become confusing for those who doesn't work on matrix calculus for quite a while like me. I want to get the points straight here. What exactly is \\frac{\\partial s}{\\partial \\boldsymbol{b}} \\frac{\\partial s}{\\partial \\boldsymbol{b}} and \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} . Note both \\boldsymbol{h} \\boldsymbol{h} and \\boldsymbol{z} \\boldsymbol{z} are vectors. To calculate these two gradient, simply remember the following two rules: Given a function with 1 output and n n inputs f(\\boldsymbol{x}) = f(x_1, x_2, \\cdots, x_n) f(\\boldsymbol{x}) = f(x_1, x_2, \\cdots, x_n) , it's gradient is a vector of partial derivatives with respect to each input (take the gradient element wise). $$ \\frac{\\partial f}{\\partial \\boldsymbol{x}} = \\Bigg [ \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\cdots, \\frac{\\partial f}{\\partial x_n}, \\Bigg ] $$ Given a function with m m output and n n inputs $$ \\boldsymbol{f}(\\boldsymbol{x}) = \\big [f_1(x_1, x_2, \\cdots, x_n), \\cdots, f_m(x_1, x_2, \\cdots, x_n) \\big ], $$ it's gradient is an m \\times n m \\times n matrix of partial derivatives (\\frac{\\partial \\boldsymbol{f}}{\\partial \\boldsymbol{x}})_{ij} = \\dfrac{\\partial f_i}{\\partial x_j} (\\frac{\\partial \\boldsymbol{f}}{\\partial \\boldsymbol{x}})_{ij} = \\dfrac{\\partial f_i}{\\partial x_j} . This matrix is also called Jacobian matrix. \\begin{equation*} \\frac{\\partial \\boldsymbol{f}}{\\partial \\boldsymbol{x}} = \\begin{bmatrix} \\dfrac{\\partial \\boldsymbol{f}}{\\partial x_1} & \\cdots & \\dfrac{\\partial \\boldsymbol{f}}{\\partial x_n} \\end{bmatrix} = \\begin{bmatrix} \\dfrac{\\partial f_1}{\\partial x_1} & \\cdots & \\dfrac{\\partial f_1}{\\partial x_n}\\\\ \\vdots & \\ddots & \\vdots\\\\ \\dfrac{\\partial f_m}{\\partial x_1} & \\cdots & \\dfrac{\\partial f_m}{\\partial x_n} \\end{bmatrix} \\end{equation*} \\begin{equation*} \\frac{\\partial \\boldsymbol{f}}{\\partial \\boldsymbol{x}} = \\begin{bmatrix} \\dfrac{\\partial \\boldsymbol{f}}{\\partial x_1} & \\cdots & \\dfrac{\\partial \\boldsymbol{f}}{\\partial x_n} \\end{bmatrix} = \\begin{bmatrix} \\dfrac{\\partial f_1}{\\partial x_1} & \\cdots & \\dfrac{\\partial f_1}{\\partial x_n}\\\\ \\vdots & \\ddots & \\vdots\\\\ \\dfrac{\\partial f_m}{\\partial x_1} & \\cdots & \\dfrac{\\partial f_m}{\\partial x_n} \\end{bmatrix} \\end{equation*} Computing gradients with the chain rule \u00b6 With these two rules we can calculate the partials. We will use \\frac{\\partial s}{\\partial \\boldsymbol{b}} \\frac{\\partial s}{\\partial \\boldsymbol{b}} as an example. \\begin{equation*} \\frac{\\partial s}{\\partial \\boldsymbol{b}} = \\frac{\\partial s}{\\partial \\boldsymbol{h}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} \\end{equation*} \\begin{equation*} \\frac{\\partial s}{\\partial \\boldsymbol{b}} = \\frac{\\partial s}{\\partial \\boldsymbol{h}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} \\end{equation*} \\frac{\\partial s}{\\partial \\boldsymbol{h}} = \\frac{\\partial}{\\partial \\boldsymbol{h}}(\\boldsymbol{u}^{\\mathsf{T}}\\boldsymbol{h}) = \\boldsymbol{u}^{\\mathsf{T}} \\frac{\\partial s}{\\partial \\boldsymbol{h}} = \\frac{\\partial}{\\partial \\boldsymbol{h}}(\\boldsymbol{u}^{\\mathsf{T}}\\boldsymbol{h}) = \\boldsymbol{u}^{\\mathsf{T}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} = \\frac{\\partial}{\\partial \\boldsymbol{z}}(f(\\boldsymbol{z})) = \\begin{pmatrix} f'(z_1) & & 0 \\\\ & \\ddots & \\\\ 0 & & f'(z_n) \\end{pmatrix} = \\mathrm{diag} (f' (\\boldsymbol{z})) \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} = \\frac{\\partial}{\\partial \\boldsymbol{z}}(f(\\boldsymbol{z})) = \\begin{pmatrix} f'(z_1) & & 0 \\\\ & \\ddots & \\\\ 0 & & f'(z_n) \\end{pmatrix} = \\mathrm{diag} (f' (\\boldsymbol{z})) \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} = \\frac{\\partial}{\\partial \\boldsymbol{b}}(\\boldsymbol{W}\\boldsymbol{x} + \\boldsymbol{b}) = \\boldsymbol{I} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} = \\frac{\\partial}{\\partial \\boldsymbol{b}}(\\boldsymbol{W}\\boldsymbol{x} + \\boldsymbol{b}) = \\boldsymbol{I} Therefore, \\begin{equation*} \\frac{\\partial s}{\\partial \\boldsymbol{b}} = \\boldsymbol{u}^{\\mathsf{T}} \\mathrm{diag}(f'(\\boldsymbol{z})) \\boldsymbol{I} = \\boldsymbol{u}^{\\mathsf{T}} \\circ f'(\\boldsymbol{z}) \\end{equation*} \\begin{equation*} \\frac{\\partial s}{\\partial \\boldsymbol{b}} = \\boldsymbol{u}^{\\mathsf{T}} \\mathrm{diag}(f'(\\boldsymbol{z})) \\boldsymbol{I} = \\boldsymbol{u}^{\\mathsf{T}} \\circ f'(\\boldsymbol{z}) \\end{equation*} Similarly we can calculate the partial with respect to \\boldsymbol{W} \\boldsymbol{W} and \\boldsymbol{x} \\boldsymbol{x} . Since the top layer partials are already calculated, we can reuese the results. We denote those reusable partials as \\delta \\delta meaning local error signal. \\begin{align*} \\frac{\\partial s}{\\partial \\boldsymbol{W}} & = \\frac{\\partial s}{\\partial \\boldsymbol{h}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{W}} \\\\ \\\\ \\frac{\\partial s}{\\partial \\boldsymbol{x}} & = \\frac{\\partial s}{\\partial \\boldsymbol{h}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{x}} \\\\ \\\\ \\frac{\\partial s}{\\partial \\boldsymbol{b}} & = \\underbrace{\\frac{\\partial s}{\\partial \\boldsymbol{h}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}}}_{\\mathrm{reuse}} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} = \\boldsymbol{\\delta} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} = \\boldsymbol{\\delta} = \\boldsymbol{u}^{\\mathsf{T}} \\circ f'(\\boldsymbol{z}) \\end{align*} \\begin{align*} \\frac{\\partial s}{\\partial \\boldsymbol{W}} & = \\frac{\\partial s}{\\partial \\boldsymbol{h}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{W}} \\\\ \\\\ \\frac{\\partial s}{\\partial \\boldsymbol{x}} & = \\frac{\\partial s}{\\partial \\boldsymbol{h}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{x}} \\\\ \\\\ \\frac{\\partial s}{\\partial \\boldsymbol{b}} & = \\underbrace{\\frac{\\partial s}{\\partial \\boldsymbol{h}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}}}_{\\mathrm{reuse}} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} = \\boldsymbol{\\delta} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} = \\boldsymbol{\\delta} = \\boldsymbol{u}^{\\mathsf{T}} \\circ f'(\\boldsymbol{z}) \\end{align*} Shape convention \u00b6 What does the shape of the derivatives looks like in practice? How can we make the chain rule computation efficient? According to the aforementioned gradient calculation rules, \\frac{\\partial s}{\\partial \\boldsymbol{W}} \\frac{\\partial s}{\\partial \\boldsymbol{W}} is a row vector. The chain rule gave \\begin{equation*} \\frac{\\partial s}{\\partial \\boldsymbol{W}} = \\boldsymbol{\\delta} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{W}}. \\end{equation*} \\begin{equation*} \\frac{\\partial s}{\\partial \\boldsymbol{W}} = \\boldsymbol{\\delta} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{W}}. \\end{equation*} We know from Jacobians that \\frac{\\partial\\boldsymbol{z}}{\\partial \\boldsymbol{W}} = \\frac{\\partial}{\\partial \\boldsymbol{W}}(\\boldsymbol{W}\\boldsymbol{x} + \\boldsymbol{b}) = \\boldsymbol{x}^{\\mathsf{T}} \\frac{\\partial\\boldsymbol{z}}{\\partial \\boldsymbol{W}} = \\frac{\\partial}{\\partial \\boldsymbol{W}}(\\boldsymbol{W}\\boldsymbol{x} + \\boldsymbol{b}) = \\boldsymbol{x}^{\\mathsf{T}} . We may arrived the result that \\frac{\\partial s}{\\partial \\boldsymbol{W}} = \\boldsymbol{\\delta} \\ \\boldsymbol{x}^{\\mathsf{T}} \\frac{\\partial s}{\\partial \\boldsymbol{W}} = \\boldsymbol{\\delta} \\ \\boldsymbol{x}^{\\mathsf{T}} . This is actually not quite wirte. The correct form should be \\begin{align*} \\underbrace{\\frac{\\partial s}{\\partial \\boldsymbol{W}}}_{n \\times m} = \\underbrace{\\boldsymbol{\\delta}^{\\mathsf{T}}}_{n \\times 1} \\ \\underbrace{\\boldsymbol{x}^{\\mathsf{T}}}_{1 \\times m}. \\end{align*} \\begin{align*} \\underbrace{\\frac{\\partial s}{\\partial \\boldsymbol{W}}}_{n \\times m} = \\underbrace{\\boldsymbol{\\delta}^{\\mathsf{T}}}_{n \\times 1} \\ \\underbrace{\\boldsymbol{x}^{\\mathsf{T}}}_{1 \\times m}. \\end{align*} Note You may wonder why is this form instead of the one we derived directly. The explanation from the CS224N is that we would like to follow the shape convention so as to make the chain rule implementation more efficient (matrix multiplication instead of loops). Different from the Jacobian form, shape convention states that the shape of the gradient is the same shape of parameters. The resolution here is to use Jacobian form as much as possible and reshape to follow the convention at the end. Because Jacobian form makes the chain rule easy and the shape convention makes the implementation of gradient descent easy. Computation graphs and backpropagation \u00b6 We have shown how to compute the partial derivatives using the chain rule. This is almost the backpropagation algorithm. If we want to add on to that and make the algorithm complete, the only thing we need is how to reuse gradient computed for higher layers. We leverage the computation graph to explain this. From the computation graph, you'll get the intuition of reusing the partial derivatives computed for higher layers in computing derivatives for lower layers so as to minimize computation. We represent our neural net equations as a computation graph as following: source nodes represent inputs interior nodes represent operations edges pass along result of the operation When following computation order to carry out the computations from inputs, it is forward propagation. Backpropagation is to pass along gradients backwards. This can be illustrated in the following computation graph. The partial derivative with respect to a parameter reflect how changing the parameter would effect the output value. The output value in the backprop is usually a loss function (or error). Intuitively, you can see backprop is push the error back to the lower layer through a bunch of operation nodes, the arrived error is a measure of the error at that particular layer, training is try to reduce this backprop-ed error by adjusting the local parameters, the effect of reducing the local error will be forward propagated to the output, the error at the output should also reduced. We use gradient descent to make this process to converge as soon as possible. To understand it better, let's look at a single node. We define \"local gradient\" for each node as the gradient of it's output with respect to it's input. By chain rule, the downstream gradient is equal to the multiplication of the upstream gradient and the local gradient. When having multipe local gradients, they are pushed back to each input using chain rule. CS224N lecture 4 slides have step by step example of backprop. From the example, we can got some intuitions about some nodes' effects. For example, when push back gradients along outward branches from a single node, the gradients should be sumed up; \"+\" node distributes the upstream gradient to each summand; \"max\" node simply routes the upstream gradients. When update gradient with backprop, you should compute all the gradients at once. With this computation graph notion, following routine captures the gist of backprop in a very decent manner. Fprop: visit nodes in topological sorted order Compute value of node given predecessors Bprop: initialize output gradient = 1 visit nodes in reverse order: Compute gradient wrt each node using gradient wrt successors \\{y_1, y_2, \\cdots, y_2\\} = \\mathrm{successors\\ of\\ } x \\{y_1, y_2, \\cdots, y_2\\} = \\mathrm{successors\\ of\\ } x \\begin{equation*}\\frac{\\partial z}{\\partial \\boldsymbol{x}} = \\sum_{i=i}^n \\frac{\\partial z}{\\partial y_i} \\frac{\\partial y_i}{\\partial x} \\end{equation*} \\begin{equation*}\\frac{\\partial z}{\\partial \\boldsymbol{x}} = \\sum_{i=i}^n \\frac{\\partial z}{\\partial y_i} \\frac{\\partial y_i}{\\partial x} \\end{equation*} If done correctly, the big O O complexity of forward propagation and backpropagation is the same. Automatic differentiation \u00b6 The gradient computation can be automatically inferred from the symbolic expression of the fprop. but this is not commonly used in practice. Each node type needs to know how to compute its output and how to compute the gradient wrt its inputs given the gradient wrt its output. Modern DL frameworks (Tensorflow, PyTorch, etc.) do backpropagation for you but mainly leave layer/node writer to hand-calculate the local derivative. Following is a simple demo of how to implement forward propagation and backpropagation. class MultiplyGate ( object ): def forward ( x , y ): z = x * y self . x = x # must keep these around! self . y = y return z def backword ( dz ): dx = self . y * dz # [dz/dx * dL/dz] dy = self . x * dz # [dz/dy * dL/dz] return [ dx , dy ] nonlinear activation functions: sigmoid, tanh, hard tanh, ReLU Learning rates: start 0.001. power of 10. halve the learning rate every k epochs. formula: lr = lr_0 e^{-k t} lr = lr_0 e^{-k t} for each epoch t t Regularization \u00b6 LFD starting point: L2 regularization Part 3 Language Model and RNN \u00b6 n-gram \u00b6 assumption: \\boldsymbol{x}^{(t+1)} \\boldsymbol{x}^{(t+1)} depends only on previous n-1 n-1 words. Sparsity problem, --> backoff (n-1)-gram practically, n has to be less than 5. Neural language model \u00b6 fixed-window neural language model Recurrent Neural Network","title":"CS224N Write-up"},{"location":"courses/cs224n/write-up/#cs224n-nlp-with-deep-learning","text":"","title":"CS224N NLP with Deep Learning"},{"location":"courses/cs224n/write-up/#part-1-word-embeddings-based-on-distributional-semantic","text":"Date: April 20th 2019 If you have no experience in the Natrual Language Processing (NLP) field and asked to name a few practical NLP applications, what would you list? Google Translation or Amazon Echo voice assistant may first come into your mind. How did those products understand a sentence or human conversation? Engineers built computational models that understand our language as our brain does. To build such a computational model, the following components are necessary. A large corpus of text (input training data) A method to represent each word from the corpus (feature representation) A starting model that barely understands English at the beginning but can be improved by \"reading\" more words from the corpus (parametric function). An algorithm for the model to correct itself if it makes a mistake in understanding (learning algorithm/optimization method) A measurement that can qualify the mistake the model made (loss function)","title":"Part 1 Word Embeddings Based on Distributional Semantic"},{"location":"courses/cs224n/write-up/#introduction","text":"Starting with this post, I will write materials from my understanding of the Stanford CS224N. The plan is to journal down all the learning notes I learned about the above 5 components. The goal is to provide a systematic understand of the gist of the components for real applications. Part 1 is about language models and word embeddings. Part 2 discusses neural networks and backpropagation algorithm. Part 3 revisits the language model and introduces recurrent neural networks. Part 4 studies advanced RNN, Long short-term memory (LSTM), and gated recurrent networks (GRN).","title":"Introduction"},{"location":"courses/cs224n/write-up/#word-representations","text":"How do we represent word with meaning on a computer? Before 2013, wordNet and one-hot vector are most popular in word meaning representations. WordNet is a manually compiled thesaurus containing lists of synonym sets and hypernyms. Like most of the manual stuff, it is subjective, unscalable, inaccurate in computing word similarity, and impossible to maintain and keep up-to-data. One-hot vectors represent word meaning using discrete symbolic 1 1 s in a long stream of 0 0 of vector elements. It suffers from sparsity issues and many other drawbacks. We will not spend time on that outdated method. Instead, we will focus on the embedding method using the idea of word vector. The core idea of this embedding method is based on the remarkable insight on word meaning called distributional semantics . It conjectures that a word\u2019s meaning is given by the words that frequently appear close-by. It is proposed by J. R. Firth. Here is the famous quote: Quote \"You shall know a word by the company it keeps\" -- J. R. Firth 1957: 11 In this method, we will build a dense vector for each word, chosen so that it is similar to vectors of words that appear in similar contexts. word vector representation also called distributed representation , or word embeddings . A work vector might look likes this, \\mathrm{banking} = \\begin{pmatrix} 0.286\\\\ 0.792\\\\ \u22120.177\\\\ \u22120.107\\\\ 0.109\\\\ \u22120.542\\\\ 0.349\\\\ 0.271 \\end{pmatrix} \\mathrm{banking} = \\begin{pmatrix} 0.286\\\\ 0.792\\\\ \u22120.177\\\\ \u22120.107\\\\ 0.109\\\\ \u22120.542\\\\ 0.349\\\\ 0.271 \\end{pmatrix}","title":"Word representations"},{"location":"courses/cs224n/write-up/#word2vec","text":"Word2vec (Mikolov et al. 2013) is a framework for learning word vectors. Word2vec go through windows of words and calculate the probability of the context word given the center word (or vice versa) using the similarity of the word vectors. It keeps adjusting the word vectors to maximize this probability. Vividly, these two pictures show the general idea of how word2vec works. Formally, for a single prediction, the probability is P(o|c) P(o|c) , interpreted as the probability of the outer word o o given the center word c c . For the large corpus including T T words and each position t = 1, 2, \\cdots, T t = 1, 2, \\cdots, T , we predict context words within a window of fixed size m m , given center word w_t w_t . The model likelihood can be written as the following \\begin{equation*} \\mathrm{likelihood} = L(\\theta) = \\prod_{t=1}^T\\prod_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} p(w_{t+j}|w_t; \\theta) \\end{equation*} \\begin{equation*} \\mathrm{likelihood} = L(\\theta) = \\prod_{t=1}^T\\prod_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} p(w_{t+j}|w_t; \\theta) \\end{equation*} The objective function J(\\theta) J(\\theta) is the average negative log likelihood: \\begin{equation*} J(\\theta) = -\\frac{1}{T}\\log{L(\\theta)} = -\\frac{1}{T} \\sum_{t=1}^T\\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} \\log p(w_{t+j}|w_t; \\theta) \\end{equation*} \\begin{equation*} J(\\theta) = -\\frac{1}{T}\\log{L(\\theta)} = -\\frac{1}{T} \\sum_{t=1}^T\\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} \\log p(w_{t+j}|w_t; \\theta) \\end{equation*} We've defined the cost function by now. In order to minimize the loss, we need to know how p(w_{t+j}|w_t; \\theta) p(w_{t+j}|w_t; \\theta) can be calculated. One function we can use to calculate the probability value is the \\mathrm{softmax} \\mathrm{softmax} function. \\begin{equation*} \\mathrm{softmax}(x)_i = \\frac{\\exp(x_i)}{\\sum_{j=1}^{n}\\exp(x_j)} \\end{equation*} \\begin{equation*} \\mathrm{softmax}(x)_i = \\frac{\\exp(x_i)}{\\sum_{j=1}^{n}\\exp(x_j)} \\end{equation*} Particularly we will write the probability as the following format. \\begin{equation*} p(o|c) = \\frac{\\exp(u_o^T v_c)}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\end{equation*} \\begin{equation*} p(o|c) = \\frac{\\exp(u_o^T v_c)}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\end{equation*} There are several points need to be emphasized. Two vectors will be obtained for each individual word, one as center word v_w v_w , and the other context word u_w u_w . d d is the dimension of the word vector. V V is the vocabulary size. \\begin{equation*} \\theta = \\begin{bmatrix} v_{\\mathrm{aardvark}}\\\\ v_{\\mathrm{a}}\\\\ \\cdots \\\\ v_{\\mathrm{zebra}}\\\\ u_{\\mathrm{aardvark}}\\\\ u_{\\mathrm{a}}\\\\ \\cdots \\\\ u_{\\mathrm{zebra}}\\\\ \\end{bmatrix} \\in \\mathbb{R}^{2dV} \\end{equation*} \\begin{equation*} \\theta = \\begin{bmatrix} v_{\\mathrm{aardvark}}\\\\ v_{\\mathrm{a}}\\\\ \\cdots \\\\ v_{\\mathrm{zebra}}\\\\ u_{\\mathrm{aardvark}}\\\\ u_{\\mathrm{a}}\\\\ \\cdots \\\\ u_{\\mathrm{zebra}}\\\\ \\end{bmatrix} \\in \\mathbb{R}^{2dV} \\end{equation*} The dot product in the exponet compares similarity of o o and c c . Larger dot product indicates larger probability. The denorminator sum over entire vocabulary to give normalized probability distribution. The \\mathrm{softmax} \\mathrm{softmax} function maps aribitrary values x_i x_i to a probability distribution p_i p_i . \u201c \\mathrm{max} \\mathrm{max} \u201d because it amplifies the probability of the largest x_i x_i , \u201c \\mathrm{soft} \\mathrm{soft} \u201d because it still assigns some probabilities to smaller x_i x_i . It is very commonly used in deep learning.","title":"word2vec"},{"location":"courses/cs224n/write-up/#gradient-descent-to-optimize-log-likelihood-loss-function","text":"This section I will purely focus on how to derive the gradient of the log likelihood loss function with respect to center word using the chain rule. Once we have the computed gradients, we are ready to implement it in matrix form and train the word vectors. This model is called skip-gram. loss function in p(w_{t+j}|w_t) p(w_{t+j}|w_t) \\begin{equation*} J(\\theta) = -\\frac{1}{T} \\sum_{t=1}^T\\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} \\log p(w_{t+j}|w_t; \\theta) \\end{equation*} \\begin{equation*} J(\\theta) = -\\frac{1}{T} \\sum_{t=1}^T\\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} \\log p(w_{t+j}|w_t; \\theta) \\end{equation*} p(w_{t+j}|w_t) p(w_{t+j}|w_t) in softmax form \\begin{equation*} p(o|c) = \\frac{\\exp(u_o^T v_c)}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\end{equation*} \\begin{equation*} p(o|c) = \\frac{\\exp(u_o^T v_c)}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\end{equation*} We would like to find the following derivatives: \\begin{equation*} 1. \\frac{\\partial}{\\partial v_c} \\log p(o|c) \\\\ 2. \\frac{\\partial}{\\partial u_o} \\log p(o|c) \\end{equation*} \\begin{equation*} 1. \\frac{\\partial}{\\partial v_c} \\log p(o|c) \\\\ 2. \\frac{\\partial}{\\partial u_o} \\log p(o|c) \\end{equation*} Let's now start working with the first one, the derivative wrt. v_c v_c . \\begin{align*} \\frac{\\partial}{\\partial v_c} \\log p(o|c) & = \\frac{\\partial}{\\partial v_c} \\log{\\frac{\\exp(u_o^T v_c)}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)}} \\\\ &= \\frac{\\partial}{\\partial v_c} \\log{\\exp(u_o^T v_c)} - \\frac{\\partial}{\\partial v_c} \\log{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\;\\;\\;\\cdots (\\log\\frac{a}{b} = \\log a - \\log b) \\\\ & = \\frac{\\partial}{\\partial v_c} (u_o^T v_c) - \\frac{1}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\sum_{x=1}^{V} \\frac{\\partial}{\\partial v_c} \\exp(u_x^T v_c) \\;\\;\\;\\cdots \\mathrm{chain\\ rule}\\\\ & = u_o - \\frac{1}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\sum_{x=1}^{V} \\exp(u_x^{T} v_c) \\frac{\\partial}{\\partial v_c} (u_x^T v_c) \\;\\;\\;\\cdots \\mathrm{chain\\ rule}\\\\ & = u_o - \\frac{1}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\sum_{x=1}^{V} \\exp(u_x^{T} v_c) \\cdot u_x \\\\ & = u_o - \\sum_{x=1}^{V} \\frac{\\exp(u_x^{T} v_c)}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\cdot u_x \\\\ & = \\underbrace{u_o}_{\\mathrm{current}} - \\underbrace{\\sum_{x=1}^{V} p(x|c) \\cdot u_x}_{\\mathrm{expectation}} \\end{align*} \\begin{align*} \\frac{\\partial}{\\partial v_c} \\log p(o|c) & = \\frac{\\partial}{\\partial v_c} \\log{\\frac{\\exp(u_o^T v_c)}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)}} \\\\ &= \\frac{\\partial}{\\partial v_c} \\log{\\exp(u_o^T v_c)} - \\frac{\\partial}{\\partial v_c} \\log{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\;\\;\\;\\cdots (\\log\\frac{a}{b} = \\log a - \\log b) \\\\ & = \\frac{\\partial}{\\partial v_c} (u_o^T v_c) - \\frac{1}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\sum_{x=1}^{V} \\frac{\\partial}{\\partial v_c} \\exp(u_x^T v_c) \\;\\;\\;\\cdots \\mathrm{chain\\ rule}\\\\ & = u_o - \\frac{1}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\sum_{x=1}^{V} \\exp(u_x^{T} v_c) \\frac{\\partial}{\\partial v_c} (u_x^T v_c) \\;\\;\\;\\cdots \\mathrm{chain\\ rule}\\\\ & = u_o - \\frac{1}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\sum_{x=1}^{V} \\exp(u_x^{T} v_c) \\cdot u_x \\\\ & = u_o - \\sum_{x=1}^{V} \\frac{\\exp(u_x^{T} v_c)}{\\sum_{w=1}^{V}\\exp(u_w^T v_c)} \\cdot u_x \\\\ & = \\underbrace{u_o}_{\\mathrm{current}} - \\underbrace{\\sum_{x=1}^{V} p(x|c) \\cdot u_x}_{\\mathrm{expectation}} \\end{align*} The result is remarkable. It have great intuition in it. The gradient represent the slop in the multidimentional space that we should walk along to reach the optima. The result gradient we got tell us that the slop equals to the difference of current context vector \\boldsymbol{u}_o \\boldsymbol{u}_o and the expected context vector (the weighted average over all context vectors). It has nothing todo with the center word c c . To compute the gradient of J(\\theta) J(\\theta) with respect to the center word c c , you have to sum up all the gradients obtained from word windows when c c is the center word. The gradient with respect to the context word will be very similar, the chain rule is also handy in that case. Once all the gradient with respect to center words and context words are calculated. We can use gradient descent to update the model parameters, which in this case is all the word vectors. Because we have two vectors for each word, when update the parameters we will use the average of the two vectors to update.","title":"Gradient descent to optimize log likelihood loss function"},{"location":"courses/cs224n/write-up/#gradient-descent-to-optimize-cross-entropy-loss-function","text":"Alternatively, we could also use cross entropy loss function. CS224N 2017 assignment 1 requires to derive the gradient of cross entropy loss function. This section, we will go step by step to derive the gradient when using cross entropy loss function and \\mathrm{softmax} \\mathrm{softmax} activation function in the out put layer. the cross entropy function is defined as follows \\begin{equation*} \\mathrm{CE}(\\boldsymbol{y}, \\boldsymbol{\\hat y}) = - \\sum_i y_i \\log (\\hat y_i) \\end{equation*} \\begin{equation*} \\mathrm{CE}(\\boldsymbol{y}, \\boldsymbol{\\hat y}) = - \\sum_i y_i \\log (\\hat y_i) \\end{equation*} Notice the \\boldsymbol{y} \\boldsymbol{y} is the one-hot label vector, and \\boldsymbol{\\hat y} \\boldsymbol{\\hat y} is the predicted probability for all classes. The index i i is the index of the one-hot element in individual label vectors. Therefore the definition of cross entropy loss function is defined as followinng if we use \\mathrm{softmax} \\mathrm{softmax} to predict \\hat y_i \\hat y_i , \\begin{equation*} J_{\\mathrm{CE}}(\\theta) = - \\sum_i^N y_i \\log (\\hat y_i) = -\\sum_i^N y_i \\log (\\mathrm{softmax}(\\theta)_i) \\end{equation*} \\begin{equation*} J_{\\mathrm{CE}}(\\theta) = - \\sum_i^N y_i \\log (\\hat y_i) = -\\sum_i^N y_i \\log (\\mathrm{softmax}(\\theta)_i) \\end{equation*} The gradient of \\frac{\\partial}{\\partial \\theta}J_{\\mathrm{CE}}(\\theta) \\frac{\\partial}{\\partial \\theta}J_{\\mathrm{CE}}(\\theta) can be derived using chain rule. Because we will use the gradient of the \\mathrm{softmax} \\mathrm{softmax} for the derivation, Let's derive \\mathrm{softmax} \\mathrm{softmax} gradient first. \\begin{equation*} \\frac{\\partial \\hat y_i}{\\theta_j} = \\frac{\\partial }{\\theta_j}\\mathrm{softmax}(\\theta)_i = \\frac{\\partial }{\\theta_j} \\frac{e^{\\theta_i}}{\\sum_{k=1}^{n}e^{\\theta_k}} \\end{equation*} \\begin{equation*} \\frac{\\partial \\hat y_i}{\\theta_j} = \\frac{\\partial }{\\theta_j}\\mathrm{softmax}(\\theta)_i = \\frac{\\partial }{\\theta_j} \\frac{e^{\\theta_i}}{\\sum_{k=1}^{n}e^{\\theta_k}} \\end{equation*} Here we should use two tricks to derive this gradient. Quotient rule separate into 2 cases: when i = j i = j and i \\ne j i \\ne j . If i = j i = j , we have \\begin{align*} \\frac{\\partial }{\\theta_j} \\frac{e^{\\theta_i}}{\\sum_{k=1}^{n}e^{\\theta_k}} & = \\frac{e^{\\theta_i}\\sum_{k=1}^{n}e^{\\theta_k} - e^{\\theta_i}e^{\\theta_j} }{\\Big(\\sum_{k=1}^{n}e^{\\theta_k}\\Big)^2} \\\\ & = \\frac{e^{\\theta_i}\\Big(\\sum_{k=1}^{n}e^{\\theta_k} - e^{\\theta_j}\\Big)}{\\Big(\\sum_{k=1}^{n}e^{\\theta_k}\\Big)^2} \\\\ & = \\frac{e^{\\theta_i}}{\\sum_{k=1}^{n}e^{\\theta_k}} \\frac{\\sum_{k=1}^{n}e^{\\theta_k} - e^{\\theta_j}}{\\sum_{k=1}^{n}e^{\\theta_k}} \\\\ & = \\hat y_i(1 - \\hat y_j) \\end{align*} \\begin{align*} \\frac{\\partial }{\\theta_j} \\frac{e^{\\theta_i}}{\\sum_{k=1}^{n}e^{\\theta_k}} & = \\frac{e^{\\theta_i}\\sum_{k=1}^{n}e^{\\theta_k} - e^{\\theta_i}e^{\\theta_j} }{\\Big(\\sum_{k=1}^{n}e^{\\theta_k}\\Big)^2} \\\\ & = \\frac{e^{\\theta_i}\\Big(\\sum_{k=1}^{n}e^{\\theta_k} - e^{\\theta_j}\\Big)}{\\Big(\\sum_{k=1}^{n}e^{\\theta_k}\\Big)^2} \\\\ & = \\frac{e^{\\theta_i}}{\\sum_{k=1}^{n}e^{\\theta_k}} \\frac{\\sum_{k=1}^{n}e^{\\theta_k} - e^{\\theta_j}}{\\sum_{k=1}^{n}e^{\\theta_k}} \\\\ & = \\hat y_i(1 - \\hat y_j) \\end{align*} if i \\ne j i \\ne j , we have \\begin{align*} \\frac{\\partial }{\\theta_j} \\frac{e^{\\theta_i}}{\\sum_{k=1}^{n}e^{\\theta_k}} & = \\frac{0 - e^{\\theta_i}e^{\\theta_j} }{\\Big(\\sum_{k=1}^{n}e^{\\theta_k}\\Big)^2} \\\\ & = \\frac{e^{\\theta_i}}{\\sum_{k=1}^{n}e^{\\theta_k}} \\frac{ - e^{\\theta_j}}{\\sum_{k=1}^{n}e^{\\theta_k}} \\\\ & = -\\hat y_i \\hat y_j \\end{align*} \\begin{align*} \\frac{\\partial }{\\theta_j} \\frac{e^{\\theta_i}}{\\sum_{k=1}^{n}e^{\\theta_k}} & = \\frac{0 - e^{\\theta_i}e^{\\theta_j} }{\\Big(\\sum_{k=1}^{n}e^{\\theta_k}\\Big)^2} \\\\ & = \\frac{e^{\\theta_i}}{\\sum_{k=1}^{n}e^{\\theta_k}} \\frac{ - e^{\\theta_j}}{\\sum_{k=1}^{n}e^{\\theta_k}} \\\\ & = -\\hat y_i \\hat y_j \\end{align*} Now let calculate the gradient of the cross-entropy loss function. Notice the gradient is concerning the i i th parameter. This is because we can use the gradient of the \\mathrm{softmax} \\mathrm{softmax} (the \\frac{\\partial \\hat y_k}{\\partial \\theta_i} \\frac{\\partial \\hat y_k}{\\partial \\theta_i} term) conveniently. \\begin{align*} \\frac{\\partial J_{\\mathrm{CE}}(\\theta)}{\\partial \\theta_i} & = - \\sum_k^N y_k \\frac{\\partial \\log (\\hat y_k)}{\\partial \\theta_i} \\\\ & = - \\sum_k^N y_k \\frac{\\partial \\log (\\hat y_k)}{\\partial \\hat y_k} \\frac{\\partial \\hat y_k}{\\partial \\theta_i} \\\\ & = - \\sum_k^N y_k \\frac{1}{\\hat y_k} \\frac{\\partial \\hat y_k}{\\partial \\theta_i} \\\\ & = - \\Big(\\underbrace{y_k \\frac{1}{\\hat y_k} \\hat y_k(1 - \\hat y_i)}_{k = i} + \\underbrace{\\sum_{k\\ne i}^N y_k \\frac{1}{\\hat y_k} (-\\hat y_k \\hat y_i) \\Big)}_{k \\ne i} \\\\ & = - \\Big(y_i (1 - \\hat y_i) - \\sum_{k\\ne i}^N y_k \\hat y_i \\Big) \\\\ & = - y_i + y_i \\hat y_i + \\sum_{k\\ne i}^N y_k \\hat y_i \\\\ & = \\hat y_i\\Big(y_i + \\sum_{k\\ne i}^N y_k\\Big) - \\hat y_i \\\\ & = \\hat y_i \\cdot 1 - y_i = \\hat y_i - y_i \\end{align*} \\begin{align*} \\frac{\\partial J_{\\mathrm{CE}}(\\theta)}{\\partial \\theta_i} & = - \\sum_k^N y_k \\frac{\\partial \\log (\\hat y_k)}{\\partial \\theta_i} \\\\ & = - \\sum_k^N y_k \\frac{\\partial \\log (\\hat y_k)}{\\partial \\hat y_k} \\frac{\\partial \\hat y_k}{\\partial \\theta_i} \\\\ & = - \\sum_k^N y_k \\frac{1}{\\hat y_k} \\frac{\\partial \\hat y_k}{\\partial \\theta_i} \\\\ & = - \\Big(\\underbrace{y_k \\frac{1}{\\hat y_k} \\hat y_k(1 - \\hat y_i)}_{k = i} + \\underbrace{\\sum_{k\\ne i}^N y_k \\frac{1}{\\hat y_k} (-\\hat y_k \\hat y_i) \\Big)}_{k \\ne i} \\\\ & = - \\Big(y_i (1 - \\hat y_i) - \\sum_{k\\ne i}^N y_k \\hat y_i \\Big) \\\\ & = - y_i + y_i \\hat y_i + \\sum_{k\\ne i}^N y_k \\hat y_i \\\\ & = \\hat y_i\\Big(y_i + \\sum_{k\\ne i}^N y_k\\Big) - \\hat y_i \\\\ & = \\hat y_i \\cdot 1 - y_i = \\hat y_i - y_i \\end{align*} Write in vector form, we will have \\frac{\\partial J_{\\mathrm{CE}}(\\boldsymbol{y}, \\boldsymbol{\\hat y})}{\\partial \\boldsymbol{\\theta}} = \\boldsymbol{\\hat y} - \\boldsymbol{y}. \\frac{\\partial J_{\\mathrm{CE}}(\\boldsymbol{y}, \\boldsymbol{\\hat y})}{\\partial \\boldsymbol{\\theta}} = \\boldsymbol{\\hat y} - \\boldsymbol{y}. With this gradient, we can update the model parameters, namely the word vectors. In the next post, I will discuss neural networks, which have more layers than the word2vec. In neural networks, there are more parameters need to be trained and the gradients with respect all the parameters need to be derived.","title":"Gradient descent to optimize cross entropy loss function"},{"location":"courses/cs224n/write-up/#skip-gram-model","text":"Skip-gram model uses the center word to predict the surrounding. We have derived the gradient for the skip-gram model using log-likelihood loss function and \\mathrm{softmax} \\mathrm{softmax} . This section will derive the gradient for cross-entropy loss function. The probability output function will keep using the \\mathrm{softmax} \\mathrm{softmax} .","title":"Skip-gram model"},{"location":"courses/cs224n/write-up/#cross-entropy-loss-function","text":"Since we have derived above that \\frac{\\partial J_{\\mathrm{CE}}(\\boldsymbol{y}, \\boldsymbol{\\hat y})}{\\partial \\boldsymbol{\\theta}} = \\boldsymbol{\\hat y} - \\boldsymbol{y} \\frac{\\partial J_{\\mathrm{CE}}(\\boldsymbol{y}, \\boldsymbol{\\hat y})}{\\partial \\boldsymbol{\\theta}} = \\boldsymbol{\\hat y} - \\boldsymbol{y} . In this case, the vector \\boldsymbol{\\theta} = [\\boldsymbol{u_1^{\\mathsf{T}}}\\boldsymbol{v_c}, \\boldsymbol{u_2^{\\mathsf{T}}}\\boldsymbol{v_c}, \\cdots, \\boldsymbol{u_W^{\\mathsf{T}}}\\boldsymbol{v_c}] \\boldsymbol{\\theta} = [\\boldsymbol{u_1^{\\mathsf{T}}}\\boldsymbol{v_c}, \\boldsymbol{u_2^{\\mathsf{T}}}\\boldsymbol{v_c}, \\cdots, \\boldsymbol{u_W^{\\mathsf{T}}}\\boldsymbol{v_c}] .","title":"Cross entropy loss function"},{"location":"courses/cs224n/write-up/#gradient-for-center-word","text":"Borrow the above steps to derive the gradient with respect to \\boldsymbol{\\theta} \\boldsymbol{\\theta} , we have \\begin{align*} \\frac{\\partial }{\\partial \\boldsymbol{v_c}}J_{\\mathrm{softmax-CE}}(\\boldsymbol{o}, \\boldsymbol{v_c}, \\boldsymbol{U}) & = \\frac{\\partial J_{\\mathrm{CE}}(\\theta)}{\\partial \\theta_i} \\frac{\\partial \\theta_i}{\\boldsymbol{v_c}} \\\\ & = (\\hat y_i - y_i) \\boldsymbol{u_i^{\\mathsf{T}}} \\end{align*} \\begin{align*} \\frac{\\partial }{\\partial \\boldsymbol{v_c}}J_{\\mathrm{softmax-CE}}(\\boldsymbol{o}, \\boldsymbol{v_c}, \\boldsymbol{U}) & = \\frac{\\partial J_{\\mathrm{CE}}(\\theta)}{\\partial \\theta_i} \\frac{\\partial \\theta_i}{\\boldsymbol{v_c}} \\\\ & = (\\hat y_i - y_i) \\boldsymbol{u_i^{\\mathsf{T}}} \\end{align*} Notice here the \\boldsymbol{u_i} \\boldsymbol{u_i} is a column vector. And the derivative of \\boldsymbol{y} = \\boldsymbol{a}^{\\mathsf{T}}\\boldsymbol{x} \\boldsymbol{y} = \\boldsymbol{a}^{\\mathsf{T}}\\boldsymbol{x} with respect to \\boldsymbol{x} \\boldsymbol{x} is \\boldsymbol{a}^{\\mathsf{T}} \\boldsymbol{a}^{\\mathsf{T}} . Written in vector form, we will get \\begin{equation*} \\frac{\\partial J}{\\partial \\boldsymbol{v_c}} = \\boldsymbol{U}(\\boldsymbol{\\hat y} - \\boldsymbol{y}) \\end{equation*} \\begin{equation*} \\frac{\\partial J}{\\partial \\boldsymbol{v_c}} = \\boldsymbol{U}(\\boldsymbol{\\hat y} - \\boldsymbol{y}) \\end{equation*} In the above gradient, U = [\\boldsymbol{u_1}, \\boldsymbol{u_1}, \\cdots, \\boldsymbol{u_W}] U = [\\boldsymbol{u_1}, \\boldsymbol{u_1}, \\cdots, \\boldsymbol{u_W}] is the matrix of all the output vectors. \\boldsymbol{u_1} \\boldsymbol{u_1} is a column word vector. The component \\boldsymbol{\\hat y} - \\boldsymbol{y} \\boldsymbol{\\hat y} - \\boldsymbol{y} is a also a column vector with length of W W . The above gradient can be viewed as scaling each output vector \\boldsymbol{u_i} \\boldsymbol{u_i} by the scaler \\hat y_i - y. \\hat y_i - y. Alternatively, the gradient can also be wrote as distributive form \\begin{equation*} \\frac{\\partial J}{\\partial \\boldsymbol{v_c}} = \\boldsymbol{U}\\boldsymbol{\\hat y} - \\boldsymbol{U}\\boldsymbol{y} = -\\boldsymbol{u_i} + \\sum_{w=1}^{W}\\hat y_w \\boldsymbol{u_w} \\end{equation*} \\begin{equation*} \\frac{\\partial J}{\\partial \\boldsymbol{v_c}} = \\boldsymbol{U}\\boldsymbol{\\hat y} - \\boldsymbol{U}\\boldsymbol{y} = -\\boldsymbol{u_i} + \\sum_{w=1}^{W}\\hat y_w \\boldsymbol{u_w} \\end{equation*} The index i i in the above equation is corresponding to the index of the none zero element in the one-hot vector \\boldsymbol{y} \\boldsymbol{y} . Here we can see \\boldsymbol{y} \\boldsymbol{y} as the true label of the output word.","title":"Gradient for center word"},{"location":"courses/cs224n/write-up/#gradient-for-output-word","text":"We can also calculate the gradient with respect to the output word. \\begin{align*} \\frac{\\partial }{\\partial \\boldsymbol{u_w}}J_{\\mathrm{softmax-CE}}(\\boldsymbol{o}, \\boldsymbol{v_c}, \\boldsymbol{U}) & = \\frac{\\partial J_{\\mathrm{CE}}(\\theta)}{\\partial \\theta_i} \\frac{\\partial \\theta_i}{\\boldsymbol{u_w}} \\\\ & = (\\hat y_i - y_i) \\boldsymbol{v_c} \\end{align*} \\begin{align*} \\frac{\\partial }{\\partial \\boldsymbol{u_w}}J_{\\mathrm{softmax-CE}}(\\boldsymbol{o}, \\boldsymbol{v_c}, \\boldsymbol{U}) & = \\frac{\\partial J_{\\mathrm{CE}}(\\theta)}{\\partial \\theta_i} \\frac{\\partial \\theta_i}{\\boldsymbol{u_w}} \\\\ & = (\\hat y_i - y_i) \\boldsymbol{v_c} \\end{align*} Notice here we apply \\frac{\\partial \\boldsymbol{a}^{\\mathsf{T}} \\boldsymbol{x}}{\\partial \\boldsymbol{a}} = x \\frac{\\partial \\boldsymbol{a}^{\\mathsf{T}} \\boldsymbol{x}}{\\partial \\boldsymbol{a}} = x . Writen the gradient in matrix format, we have \\begin{equation*} \\frac{\\partial J}{\\partial \\boldsymbol{U}} = \\boldsymbol{v_c}(\\boldsymbol{\\hat y} - \\boldsymbol{y})^{\\mathsf{T}} \\end{equation*} \\begin{equation*} \\frac{\\partial J}{\\partial \\boldsymbol{U}} = \\boldsymbol{v_c}(\\boldsymbol{\\hat y} - \\boldsymbol{y})^{\\mathsf{T}} \\end{equation*} Notice the shape of the gradient. It determines how the notation looks like. In the above notation notice the shape of the output word gradient \\frac{\\partial J}{\\partial \\boldsymbol{U}} \\frac{\\partial J}{\\partial \\boldsymbol{U}} is d \\times W d \\times W . As we will discussion in the next post that it is a convention to make the shape of gradient as the shape of the input vectors. in this case the shape of U U and \\frac{\\partial J}{\\partial \\boldsymbol{U}} \\frac{\\partial J}{\\partial \\boldsymbol{U}} are the same.","title":"Gradient for output word"},{"location":"courses/cs224n/write-up/#negative-sampling","text":"The cost function for a single word prediction using nagative sampling is the following \\begin{align*} J_{\\mathrm{neg-sample}}(\\boldsymbol{o},\\boldsymbol{v_c},\\boldsymbol{U}) & = -\\log(\\sigma(\\boldsymbol{u_o}^{\\mathsf{T}} \\boldsymbol{v_c})) - \\sum_{k=1}^{K} \\log(\\sigma(-\\boldsymbol{u_k}^{\\mathsf{T}} \\boldsymbol{v_c})) \\end{align*} \\begin{align*} J_{\\mathrm{neg-sample}}(\\boldsymbol{o},\\boldsymbol{v_c},\\boldsymbol{U}) & = -\\log(\\sigma(\\boldsymbol{u_o}^{\\mathsf{T}} \\boldsymbol{v_c})) - \\sum_{k=1}^{K} \\log(\\sigma(-\\boldsymbol{u_k}^{\\mathsf{T}} \\boldsymbol{v_c})) \\end{align*} It comes from the original paper by Mikolov et al. \\sigma \\sigma is the sigmoid function \\sigma(x) = \\frac{1}{1+e^{-x}} \\sigma(x) = \\frac{1}{1+e^{-x}} . The ideas is to reduce the optimization computation by only sampling a small part of the vocabulary that have a lower probability of being context words of one another. The first lecture notes from CS224N discussed briefly the origin and intuition of the negative sampling loss function. Here we will focus on deriving the gradient and implementation ideas. With the fact that \\frac{\\mathrm{d}\\sigma(x)}{\\mathrm{d} x} = \\sigma(x)(1-\\sigma(x)) \\frac{\\mathrm{d}\\sigma(x)}{\\mathrm{d} x} = \\sigma(x)(1-\\sigma(x)) and the chain rule, it is not hard to derive the gradients result as following \\begin{align*} \\frac{\\partial J}{\\partial \\boldsymbol{v_c}} & = (\\sigma(\\boldsymbol{u_o}^{\\mathsf{T}}\\boldsymbol{v_c}) - 1)\\boldsymbol{u_o} - \\sum_{k=1}^{K}(\\sigma(-\\boldsymbol{u_k}^{\\mathsf{T}}\\boldsymbol{v_c}) - 1)\\boldsymbol{u_k} \\\\ \\frac{\\partial J}{\\partial \\boldsymbol{u_o}} & = (\\sigma(\\boldsymbol{u_o}^{\\mathsf{T}}\\boldsymbol{v_c}) - 1)\\boldsymbol{v_c} \\\\ \\frac{\\partial J}{\\partial \\boldsymbol{u_k}} & = -\\sigma(-\\boldsymbol{u_k}^{\\mathsf{T}}\\boldsymbol{v_c}) - 1)\\boldsymbol{v_c}, \\mathrm{for\\ all\\ } k = 1, 2, \\cdots, K \\end{align*} \\begin{align*} \\frac{\\partial J}{\\partial \\boldsymbol{v_c}} & = (\\sigma(\\boldsymbol{u_o}^{\\mathsf{T}}\\boldsymbol{v_c}) - 1)\\boldsymbol{u_o} - \\sum_{k=1}^{K}(\\sigma(-\\boldsymbol{u_k}^{\\mathsf{T}}\\boldsymbol{v_c}) - 1)\\boldsymbol{u_k} \\\\ \\frac{\\partial J}{\\partial \\boldsymbol{u_o}} & = (\\sigma(\\boldsymbol{u_o}^{\\mathsf{T}}\\boldsymbol{v_c}) - 1)\\boldsymbol{v_c} \\\\ \\frac{\\partial J}{\\partial \\boldsymbol{u_k}} & = -\\sigma(-\\boldsymbol{u_k}^{\\mathsf{T}}\\boldsymbol{v_c}) - 1)\\boldsymbol{v_c}, \\mathrm{for\\ all\\ } k = 1, 2, \\cdots, K \\end{align*} How to sample the \\boldsymbol{u_k} \\boldsymbol{u_k} in practice? The best known sampling method is based on the Unigram Model raise to the power of 3/4. Unigram Model is the counts of each word in the particular corpus (not the vocabulary).","title":"Negative sampling"},{"location":"courses/cs224n/write-up/#gradient-for-all-of-word-vectors","text":"Since skip-gram model is using one center word to predict all its context words. Given a word context size of m m , we obtain a set of context words [\\mathrm{word}_{c-m}, \\cdots, \\mathrm{word}_{c-1}, \\mathrm{word}_{c}, \\mathrm{word}_{c+1}, \\cdots, \\mathrm{word}_{c+m}] [\\mathrm{word}_{c-m}, \\cdots, \\mathrm{word}_{c-1}, \\mathrm{word}_{c}, \\mathrm{word}_{c+1}, \\cdots, \\mathrm{word}_{c+m}] . For each window, We need to predict 2m 2m context word given the center word. Denote the \"input\" and \"output\" word vectors for \\mathrm{word}_k \\mathrm{word}_k as \\boldsymbol{v}_k \\boldsymbol{v}_k and \\boldsymbol{u}_k \\boldsymbol{u}_k respectively. The cost for the entire context window with size m m centered around \\mathrm{word}_c \\mathrm{word}_c would be \\begin{equation*} J_{\\mathrm{skip-gram}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m}) = \\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} F(\\boldsymbol{u}_{c+j}, \\boldsymbol{v}_c). \\end{equation*} \\begin{equation*} J_{\\mathrm{skip-gram}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m}) = \\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} F(\\boldsymbol{u}_{c+j}, \\boldsymbol{v}_c). \\end{equation*} F F is a placeholder notation to represent the cost function given the center word for different model. Therefore for skip-gram, the gradients for the cost of one context window are \\begin{align*} \\frac{J_{\\mathrm{skip-gram}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{U}} & = \\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} \\frac{\\partial F(\\boldsymbol{u}_{c+j}, \\boldsymbol{v}_c)}{\\partial \\boldsymbol{v}_c},\\\\ \\frac{J_{\\mathrm{skip-gram}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{v}_c} & = \\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} \\frac{\\partial F(\\boldsymbol{u}_{c+j}, \\boldsymbol{v}_c)}{\\partial \\boldsymbol{v}_c}, \\\\ \\frac{J_{\\mathrm{skip-gram}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{v}_j} & = 0, \\forall j \\ne c \\end{align*} \\begin{align*} \\frac{J_{\\mathrm{skip-gram}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{U}} & = \\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} \\frac{\\partial F(\\boldsymbol{u}_{c+j}, \\boldsymbol{v}_c)}{\\partial \\boldsymbol{v}_c},\\\\ \\frac{J_{\\mathrm{skip-gram}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{v}_c} & = \\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} \\frac{\\partial F(\\boldsymbol{u}_{c+j}, \\boldsymbol{v}_c)}{\\partial \\boldsymbol{v}_c}, \\\\ \\frac{J_{\\mathrm{skip-gram}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{v}_j} & = 0, \\forall j \\ne c \\end{align*}","title":"Gradient for all of word vectors"},{"location":"courses/cs224n/write-up/#cbow-model","text":"Continuous bag-of-words (CBOW) is using the context words to predect the center words. Different from skip-gram model, in CBOW, we will use 2m 2m context word vectors as we predict probability of a word is the center word. For a simple variant of CBOW, we could sum up all the 2m 2m word vectors in one context vector \\hat{\\boldsymbol{v}} \\hat{\\boldsymbol{v}} and use the similar cost function with \\mathrm{softmax} \\mathrm{softmax} as we did in skip-gram model. \\begin{equation*} \\hat{\\boldsymbol{v}} = \\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} \\boldsymbol{v}_{c+j}. \\end{equation*} \\begin{equation*} \\hat{\\boldsymbol{v}} = \\sum_{\\substack{-m \\le j \\le m\\\\ j \\ne 0}} \\boldsymbol{v}_{c+j}. \\end{equation*} Similar to skip-gram, we have \\begin{align*} \\frac{J_{\\mathrm{CBOW}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{U}} & = \\frac{\\partial F(\\boldsymbol{u}_{c}, \\hat{\\boldsymbol{v}})}{\\partial \\boldsymbol{U}},\\\\ \\frac{J_{\\mathrm{CBOW}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{v}_j} & = \\frac{\\partial F(\\boldsymbol{u}_{c}, \\hat{\\boldsymbol{v}})}{\\partial \\boldsymbol{v}_j}, \\forall j \\in \\{c-m, \\cdots, c-1, c+1, \\cdots, c+m\\}\\\\ \\frac{J_{\\mathrm{CBOW}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{v}_j} & = 0, \\forall j \\notin \\{c-m, \\cdots, c-1, c+1, \\cdots, c+m\\}. \\end{align*} \\begin{align*} \\frac{J_{\\mathrm{CBOW}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{U}} & = \\frac{\\partial F(\\boldsymbol{u}_{c}, \\hat{\\boldsymbol{v}})}{\\partial \\boldsymbol{U}},\\\\ \\frac{J_{\\mathrm{CBOW}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{v}_j} & = \\frac{\\partial F(\\boldsymbol{u}_{c}, \\hat{\\boldsymbol{v}})}{\\partial \\boldsymbol{v}_j}, \\forall j \\in \\{c-m, \\cdots, c-1, c+1, \\cdots, c+m\\}\\\\ \\frac{J_{\\mathrm{CBOW}}(\\mathrm{word}_{c-m \\;\\cdots\\; c+m})}{\\partial \\boldsymbol{v}_j} & = 0, \\forall j \\notin \\{c-m, \\cdots, c-1, c+1, \\cdots, c+m\\}. \\end{align*}","title":"CBOW model"},{"location":"courses/cs224n/write-up/#glove-model","text":"TODO, the paper and lecture.","title":"GloVe model"},{"location":"courses/cs224n/write-up/#word-vector-evaluation","text":"","title":"Word vector evaluation"},{"location":"courses/cs224n/write-up/#summary","text":"This post focused on gradient derivation for various word embedding models. The bootstrap model is based on the distributional semantic, by which to predict the probability of a word given some other words from the fixed corpus. We use the \\mathrm{softmax} \\mathrm{softmax} function to compute the probability. To update the word vectors, we introduce the likelihood function and derived its gradient. In that derivation, the parameter \\boldsymbol{\\theta} \\boldsymbol{\\theta} is the hyper parameters related to word vector that can be used to compute the probability. We continue to derive the gradient of the cross entropy loss function for a single word prediction. The result gradient of the cross entropy loss function is the key for all our later gradient derivation. After that we introduce the word2vec family of word embedding, skip-gram and CBOW models. In word2vec we use the dot product of the center word and its context words to compute the probability. We derive the gradient with respect to both center word and context word when using cross entropy loss function. Negative sampling is also discussed as it improves the computation cost by many factors. With the discussion of skip-gram model, CBOW model is easier to present. One thing you need to distinguish is that whether the gradient is for a particular word prediction, for the whole window of the words, or the over all objective for the corpus. For skip-gram, we first compute the gradient for each word prediction in the given context window, then sum up all the gradient to update the cost function of that window. We move the window over the corpus until finish all the word updates. This whole process is only one update. While using negative sampling, the process becomes more efficient, we don't have to go through all the window, only sampling K K windows with Unigram Model raise to the 3/4 3/4 power. In the simple CBOW model discussed, we add up all the context vectors first, then only update a single gradient for that window corresponding to the center word. We repeat this process for different windows to complete one update.","title":"Summary"},{"location":"courses/cs224n/write-up/#part-2-neural-networks-and-backpropagation","text":"Date: April 15th 2019 We discussed the softmax classifier in Part 1 and its major drawback that the classifier only gives linear decision boundaries. In Part 2, Neural Networks will be introduced to demonstrate that it can learn much more complex functions and nonlinear decision boundaries.","title":"Part 2 Neural Networks and Backpropagation"},{"location":"courses/cs224n/write-up/#intro-to-neural-networks","text":"Biological Neuron Mathematical Model Simplified Neuron The neuron can be modeled as a binary logistic regression unit as in the last row of the table above. It can be further simplified as following functions, \\begin{equation*} h_{w, b} (\\boldsymbol{x})=f(\\boldsymbol{w}^\\mathsf{T} \\boldsymbol{x} + b) \\\\ f(z) = \\frac{1}{1 + e^{-z}}. \\end{equation*} \\begin{equation*} h_{w, b} (\\boldsymbol{x})=f(\\boldsymbol{w}^\\mathsf{T} \\boldsymbol{x} + b) \\\\ f(z) = \\frac{1}{1 + e^{-z}}. \\end{equation*} \\boldsymbol{x} \\boldsymbol{x} is the inputs \\boldsymbol{w} \\boldsymbol{w} is the weights b b is a bias term h h is a hidden layer function f f is a nonlinear activation function (sigmoid, tanh, etc.) If we feed a vector of inputs through a bunch of logistic regression (sigmoid) functions, then we get a vector of outputs. The CS224N lecture note 3 section 1.2 have a complete derivation on multiple sigmoid units. But we don\u2019t have to decide ahead of time what variables these logistic regressions are trying to predict! It is the loss function that will direct what the intermediate hidden variables should be, so as to do a good job at predicting the targets for the next layer, etc.","title":"Intro to Neural Networks"},{"location":"courses/cs224n/write-up/#forward-propagation-in-matrix-notation","text":"In a multilayer neural network, not only did we have multiple sigmoid units, but we also have more than one layer. Let's explicitly write down the signal transformation (aka forward propagation) from one layer to another referring to this network from Andrew's ML course. We will use the following notation convention a^{(j)}_i a^{(j)}_i to represent the \"activation\" of unit i i in layer j j . W^{(j)} W^{(j)} to represent the matrix of weights controlling function mapping from layer j j to layer j + 1 j + 1 . The value at each node can be calculated as \\begin{equation*} a^{(2)}_1 = f(w^{(1)}_{10} x_0 + w^{(1)}_{11} x_1 + w^{(1)}_{12} x_2 + w^{(1)}_{13} x_3) \\\\ a^{(2)}_2 = f(w^{(1)}_{20} x_0 + w^{(1)}_{21} x_1 + w^{(1)}_{22} x_2 + w^{(1)}_{23} x_3) \\\\ a^{(2)}_3 = f(w^{(1)}_{30} x_0 + w^{(1)}_{31} x_1 + w^{(1)}_{32} x_2 + w^{(1)}_{33} x_3) \\\\ \\text{and} \\\\ h_{w} (\\boldsymbol{x}) = a^{(3)}_1 = f(w^{(2)}_{10} a^{(2)}_0 + w^{(2)}_{11} a^{(2)}_1 + w^{(2)}_{12} a^{(2)}_2 + w^{(2)}_{13} a^{(2)}_3) \\end{equation*} \\begin{equation*} a^{(2)}_1 = f(w^{(1)}_{10} x_0 + w^{(1)}_{11} x_1 + w^{(1)}_{12} x_2 + w^{(1)}_{13} x_3) \\\\ a^{(2)}_2 = f(w^{(1)}_{20} x_0 + w^{(1)}_{21} x_1 + w^{(1)}_{22} x_2 + w^{(1)}_{23} x_3) \\\\ a^{(2)}_3 = f(w^{(1)}_{30} x_0 + w^{(1)}_{31} x_1 + w^{(1)}_{32} x_2 + w^{(1)}_{33} x_3) \\\\ \\text{and} \\\\ h_{w} (\\boldsymbol{x}) = a^{(3)}_1 = f(w^{(2)}_{10} a^{(2)}_0 + w^{(2)}_{11} a^{(2)}_1 + w^{(2)}_{12} a^{(2)}_2 + w^{(2)}_{13} a^{(2)}_3) \\end{equation*} write the matrix W^{(j)} W^{(j)} explicity, \\begin{equation*} W^{(1)} = \\begin{bmatrix} w^{(1)}_{10} & w^{(1)}_{11} & w^{(1)}_{12} & w^{(1)}_{13} \\\\ w^{(1)}_{20} & w^{(1)}_{21} & w^{(1)}_{22} & w^{(1)}_{23} \\\\ w^{(1)}_{30} & w^{(1)}_{31} & w^{(1)}_{32} & w^{(1)}_{33} \\end{bmatrix} \\end{equation*} \\begin{equation*} W^{(1)} = \\begin{bmatrix} w^{(1)}_{10} & w^{(1)}_{11} & w^{(1)}_{12} & w^{(1)}_{13} \\\\ w^{(1)}_{20} & w^{(1)}_{21} & w^{(1)}_{22} & w^{(1)}_{23} \\\\ w^{(1)}_{30} & w^{(1)}_{31} & w^{(1)}_{32} & w^{(1)}_{33} \\end{bmatrix} \\end{equation*} \\begin{equation*} W^{(2)} = \\begin{bmatrix} w^{(2)}_{10} & w^{(2)}_{11} & w^{(2)}_{12} & w^{(2)}_{13} \\end{bmatrix} \\end{equation*} \\begin{equation*} W^{(2)} = \\begin{bmatrix} w^{(2)}_{10} & w^{(2)}_{11} & w^{(2)}_{12} & w^{(2)}_{13} \\end{bmatrix} \\end{equation*} With the above form, we can use matrix notation as \\begin{align*} & \\boldsymbol{a}^{(2)} = f(\\boldsymbol{W}^{(1)} \\boldsymbol{x}) \\\\ h_{w} (\\boldsymbol{x}) = & \\boldsymbol{a}^{(3)} = f(\\boldsymbol{W}^{(2)} \\boldsymbol{a}^{(2)}) \\end{align*} \\begin{align*} & \\boldsymbol{a}^{(2)} = f(\\boldsymbol{W}^{(1)} \\boldsymbol{x}) \\\\ h_{w} (\\boldsymbol{x}) = & \\boldsymbol{a}^{(3)} = f(\\boldsymbol{W}^{(2)} \\boldsymbol{a}^{(2)}) \\end{align*} We can see from above notations that if the network has s_j s_j units in layer j j and s_{j+1} s_{j+1} in layer j + 1 j + 1 , the matrix W^{(j)} W^{(j)} will be of dimention s_{j+1} \\times s_{j}+1 s_{j+1} \\times s_{j}+1 . It could be interpreted as \"the dimention of W^{(j)} W^{(j)} is the number of nodes in the next layer (layer j + 1 j + 1 ) \\times \\times the number of nodes in the current layer + + 1. Note that in cs224n the matrix notation is slightly different \\begin{align*} & \\boldsymbol{a}^{(2)} = f(\\boldsymbol{W}^{(1)} \\boldsymbol{x} + \\boldsymbol{b}^{(1)}) \\\\ h_{w} (\\boldsymbol{x}) = & \\boldsymbol{a}^{(3)} = f(\\boldsymbol{W}^{(2)} \\boldsymbol{a}^{(2)} + \\boldsymbol{b}^{(2)}) \\end{align*} \\begin{align*} & \\boldsymbol{a}^{(2)} = f(\\boldsymbol{W}^{(1)} \\boldsymbol{x} + \\boldsymbol{b}^{(1)}) \\\\ h_{w} (\\boldsymbol{x}) = & \\boldsymbol{a}^{(3)} = f(\\boldsymbol{W}^{(2)} \\boldsymbol{a}^{(2)} + \\boldsymbol{b}^{(2)}) \\end{align*} the different is from how we denote the bias. The two are enssentially the same, but be cautions that the matrix dimentions are different.","title":"Forward Propagation in Matrix Notation"},{"location":"courses/cs224n/write-up/#word-window-classification-using-neural-networks","text":"From now on, let's switch the notation to cs224n so that we can derive the backpropagation algorithm for word window classification and get more intuition about the backprop. The drawing of the neural nets in cs224n for word window classification is less dynamic and slitly different from the drawing from Andrew's ML class. The figure from cs224n may be slightly confusing at first, but it is good to understand it from this word window classification application.","title":"Word Window Classification Using Neural Networks"},{"location":"courses/cs224n/write-up/#forward-propagation","text":"Firstly, the goal of this classification task is to classify whether the center word is a location. Similar to word2vec, we will go over all positions in a corpus. But this time, it will be supervised and only some positions should get a high score. The figure above illustrate the feed-forward process. We use the method by Collobert & Weston (2008, 2011). An unnormalized score will be calculated from the activation \\boldsymbol{a} = [a_1, a_2, a_3, \\cdots]^\\mathsf{T} \\boldsymbol{a} = [a_1, a_2, a_3, \\cdots]^\\mathsf{T} . \\begin{equation*} \\text{scroe}(\\boldsymbol{x}) = U^\\mathsf{T} \\boldsymbol{a} \\in \\mathbb{R} \\end{equation*} \\begin{equation*} \\text{scroe}(\\boldsymbol{x}) = U^\\mathsf{T} \\boldsymbol{a} \\in \\mathbb{R} \\end{equation*} We will use max-margin loss as our loss function. The training is essentially to find the optimal weights W W by minimize the max-margin loss \\begin{equation*} J = \\max(0, 1 - s + s_c) \\end{equation*} \\begin{equation*} J = \\max(0, 1 - s + s_c) \\end{equation*} s s is the score of a window that have a location in the center. s_c s_c is the score of a window that doesn't have a location in the center. For full objective function: Sample several corrupt windows per true one. Sum over all training windows. It is similar to negative sampling in word2vec. We will use gradient descent to update the parameter so as to minimize the loss function. The key is how to calculate the gradient with respect to the model parameters, namely \\nabla_{\\theta} J(\\theta) \\nabla_{\\theta} J(\\theta) . Here we use \\theta \\theta to represent the hyperthetic parameters, it can include the W W and other parameters of the model. \\begin{equation*} \\theta^{\\text{new}} = \\theta^{\\text{old}} - \\alpha\\nabla_{\\theta} J(\\theta) \\end{equation*} \\begin{equation*} \\theta^{\\text{new}} = \\theta^{\\text{old}} - \\alpha\\nabla_{\\theta} J(\\theta) \\end{equation*}","title":"Forward propagation"},{"location":"courses/cs224n/write-up/#gradients-and-jacobians-matrix","text":"At first, let us layout the input and all the equations in this simple neural network. Input Layer Hidden Layer Output Layer \\boldsymbol{x} \\boldsymbol{x} \\boldsymbol{z} = \\boldsymbol{W} \\boldsymbol{x} + \\boldsymbol{b}, \\\\ \\boldsymbol{h} = f(\\boldsymbol{z}) \\boldsymbol{z} = \\boldsymbol{W} \\boldsymbol{x} + \\boldsymbol{b}, \\\\ \\boldsymbol{h} = f(\\boldsymbol{z}) s = \\boldsymbol{u}^{\\mathsf{T}}\\boldsymbol{h} s = \\boldsymbol{u}^{\\mathsf{T}}\\boldsymbol{h} To update the parameters in this model, namely \\boldsymbol{x}, \\boldsymbol{W}, \\boldsymbol{b} \\boldsymbol{x}, \\boldsymbol{W}, \\boldsymbol{b} , we would like to compute the derivitavies of s s with respect to all these parameters. We have to use the chain rule to compute it. What chain rule says is that we can compute the partial derivatives of each individual functions and then multiply them together to get the derivative with respect the specific variable. For example, \\frac{\\partial s}{\\partial \\boldsymbol{b}} \\frac{\\partial s}{\\partial \\boldsymbol{b}} is computed as \\frac{\\partial s}{\\partial \\boldsymbol{b}} = \\frac{\\partial s}{\\partial \\boldsymbol{h}} \\cdot \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\cdot \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} \\frac{\\partial s}{\\partial \\boldsymbol{b}} = \\frac{\\partial s}{\\partial \\boldsymbol{h}} \\cdot \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\cdot \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} . This seems very easy to understand, but when it comes to implemenation in vectorized format, it become confusing for those who doesn't work on matrix calculus for quite a while like me. I want to get the points straight here. What exactly is \\frac{\\partial s}{\\partial \\boldsymbol{b}} \\frac{\\partial s}{\\partial \\boldsymbol{b}} and \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} . Note both \\boldsymbol{h} \\boldsymbol{h} and \\boldsymbol{z} \\boldsymbol{z} are vectors. To calculate these two gradient, simply remember the following two rules: Given a function with 1 output and n n inputs f(\\boldsymbol{x}) = f(x_1, x_2, \\cdots, x_n) f(\\boldsymbol{x}) = f(x_1, x_2, \\cdots, x_n) , it's gradient is a vector of partial derivatives with respect to each input (take the gradient element wise). $$ \\frac{\\partial f}{\\partial \\boldsymbol{x}} = \\Bigg [ \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\cdots, \\frac{\\partial f}{\\partial x_n}, \\Bigg ] $$ Given a function with m m output and n n inputs $$ \\boldsymbol{f}(\\boldsymbol{x}) = \\big [f_1(x_1, x_2, \\cdots, x_n), \\cdots, f_m(x_1, x_2, \\cdots, x_n) \\big ], $$ it's gradient is an m \\times n m \\times n matrix of partial derivatives (\\frac{\\partial \\boldsymbol{f}}{\\partial \\boldsymbol{x}})_{ij} = \\dfrac{\\partial f_i}{\\partial x_j} (\\frac{\\partial \\boldsymbol{f}}{\\partial \\boldsymbol{x}})_{ij} = \\dfrac{\\partial f_i}{\\partial x_j} . This matrix is also called Jacobian matrix. \\begin{equation*} \\frac{\\partial \\boldsymbol{f}}{\\partial \\boldsymbol{x}} = \\begin{bmatrix} \\dfrac{\\partial \\boldsymbol{f}}{\\partial x_1} & \\cdots & \\dfrac{\\partial \\boldsymbol{f}}{\\partial x_n} \\end{bmatrix} = \\begin{bmatrix} \\dfrac{\\partial f_1}{\\partial x_1} & \\cdots & \\dfrac{\\partial f_1}{\\partial x_n}\\\\ \\vdots & \\ddots & \\vdots\\\\ \\dfrac{\\partial f_m}{\\partial x_1} & \\cdots & \\dfrac{\\partial f_m}{\\partial x_n} \\end{bmatrix} \\end{equation*} \\begin{equation*} \\frac{\\partial \\boldsymbol{f}}{\\partial \\boldsymbol{x}} = \\begin{bmatrix} \\dfrac{\\partial \\boldsymbol{f}}{\\partial x_1} & \\cdots & \\dfrac{\\partial \\boldsymbol{f}}{\\partial x_n} \\end{bmatrix} = \\begin{bmatrix} \\dfrac{\\partial f_1}{\\partial x_1} & \\cdots & \\dfrac{\\partial f_1}{\\partial x_n}\\\\ \\vdots & \\ddots & \\vdots\\\\ \\dfrac{\\partial f_m}{\\partial x_1} & \\cdots & \\dfrac{\\partial f_m}{\\partial x_n} \\end{bmatrix} \\end{equation*}","title":"Gradients and Jacobians Matrix"},{"location":"courses/cs224n/write-up/#computing-gradients-with-the-chain-rule","text":"With these two rules we can calculate the partials. We will use \\frac{\\partial s}{\\partial \\boldsymbol{b}} \\frac{\\partial s}{\\partial \\boldsymbol{b}} as an example. \\begin{equation*} \\frac{\\partial s}{\\partial \\boldsymbol{b}} = \\frac{\\partial s}{\\partial \\boldsymbol{h}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} \\end{equation*} \\begin{equation*} \\frac{\\partial s}{\\partial \\boldsymbol{b}} = \\frac{\\partial s}{\\partial \\boldsymbol{h}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} \\end{equation*} \\frac{\\partial s}{\\partial \\boldsymbol{h}} = \\frac{\\partial}{\\partial \\boldsymbol{h}}(\\boldsymbol{u}^{\\mathsf{T}}\\boldsymbol{h}) = \\boldsymbol{u}^{\\mathsf{T}} \\frac{\\partial s}{\\partial \\boldsymbol{h}} = \\frac{\\partial}{\\partial \\boldsymbol{h}}(\\boldsymbol{u}^{\\mathsf{T}}\\boldsymbol{h}) = \\boldsymbol{u}^{\\mathsf{T}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} = \\frac{\\partial}{\\partial \\boldsymbol{z}}(f(\\boldsymbol{z})) = \\begin{pmatrix} f'(z_1) & & 0 \\\\ & \\ddots & \\\\ 0 & & f'(z_n) \\end{pmatrix} = \\mathrm{diag} (f' (\\boldsymbol{z})) \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} = \\frac{\\partial}{\\partial \\boldsymbol{z}}(f(\\boldsymbol{z})) = \\begin{pmatrix} f'(z_1) & & 0 \\\\ & \\ddots & \\\\ 0 & & f'(z_n) \\end{pmatrix} = \\mathrm{diag} (f' (\\boldsymbol{z})) \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} = \\frac{\\partial}{\\partial \\boldsymbol{b}}(\\boldsymbol{W}\\boldsymbol{x} + \\boldsymbol{b}) = \\boldsymbol{I} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} = \\frac{\\partial}{\\partial \\boldsymbol{b}}(\\boldsymbol{W}\\boldsymbol{x} + \\boldsymbol{b}) = \\boldsymbol{I} Therefore, \\begin{equation*} \\frac{\\partial s}{\\partial \\boldsymbol{b}} = \\boldsymbol{u}^{\\mathsf{T}} \\mathrm{diag}(f'(\\boldsymbol{z})) \\boldsymbol{I} = \\boldsymbol{u}^{\\mathsf{T}} \\circ f'(\\boldsymbol{z}) \\end{equation*} \\begin{equation*} \\frac{\\partial s}{\\partial \\boldsymbol{b}} = \\boldsymbol{u}^{\\mathsf{T}} \\mathrm{diag}(f'(\\boldsymbol{z})) \\boldsymbol{I} = \\boldsymbol{u}^{\\mathsf{T}} \\circ f'(\\boldsymbol{z}) \\end{equation*} Similarly we can calculate the partial with respect to \\boldsymbol{W} \\boldsymbol{W} and \\boldsymbol{x} \\boldsymbol{x} . Since the top layer partials are already calculated, we can reuese the results. We denote those reusable partials as \\delta \\delta meaning local error signal. \\begin{align*} \\frac{\\partial s}{\\partial \\boldsymbol{W}} & = \\frac{\\partial s}{\\partial \\boldsymbol{h}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{W}} \\\\ \\\\ \\frac{\\partial s}{\\partial \\boldsymbol{x}} & = \\frac{\\partial s}{\\partial \\boldsymbol{h}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{x}} \\\\ \\\\ \\frac{\\partial s}{\\partial \\boldsymbol{b}} & = \\underbrace{\\frac{\\partial s}{\\partial \\boldsymbol{h}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}}}_{\\mathrm{reuse}} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} = \\boldsymbol{\\delta} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} = \\boldsymbol{\\delta} = \\boldsymbol{u}^{\\mathsf{T}} \\circ f'(\\boldsymbol{z}) \\end{align*} \\begin{align*} \\frac{\\partial s}{\\partial \\boldsymbol{W}} & = \\frac{\\partial s}{\\partial \\boldsymbol{h}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{W}} \\\\ \\\\ \\frac{\\partial s}{\\partial \\boldsymbol{x}} & = \\frac{\\partial s}{\\partial \\boldsymbol{h}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{x}} \\\\ \\\\ \\frac{\\partial s}{\\partial \\boldsymbol{b}} & = \\underbrace{\\frac{\\partial s}{\\partial \\boldsymbol{h}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}}}_{\\mathrm{reuse}} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} = \\boldsymbol{\\delta} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{b}} = \\boldsymbol{\\delta} = \\boldsymbol{u}^{\\mathsf{T}} \\circ f'(\\boldsymbol{z}) \\end{align*}","title":"Computing gradients with the chain rule"},{"location":"courses/cs224n/write-up/#shape-convention","text":"What does the shape of the derivatives looks like in practice? How can we make the chain rule computation efficient? According to the aforementioned gradient calculation rules, \\frac{\\partial s}{\\partial \\boldsymbol{W}} \\frac{\\partial s}{\\partial \\boldsymbol{W}} is a row vector. The chain rule gave \\begin{equation*} \\frac{\\partial s}{\\partial \\boldsymbol{W}} = \\boldsymbol{\\delta} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{W}}. \\end{equation*} \\begin{equation*} \\frac{\\partial s}{\\partial \\boldsymbol{W}} = \\boldsymbol{\\delta} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{W}}. \\end{equation*} We know from Jacobians that \\frac{\\partial\\boldsymbol{z}}{\\partial \\boldsymbol{W}} = \\frac{\\partial}{\\partial \\boldsymbol{W}}(\\boldsymbol{W}\\boldsymbol{x} + \\boldsymbol{b}) = \\boldsymbol{x}^{\\mathsf{T}} \\frac{\\partial\\boldsymbol{z}}{\\partial \\boldsymbol{W}} = \\frac{\\partial}{\\partial \\boldsymbol{W}}(\\boldsymbol{W}\\boldsymbol{x} + \\boldsymbol{b}) = \\boldsymbol{x}^{\\mathsf{T}} . We may arrived the result that \\frac{\\partial s}{\\partial \\boldsymbol{W}} = \\boldsymbol{\\delta} \\ \\boldsymbol{x}^{\\mathsf{T}} \\frac{\\partial s}{\\partial \\boldsymbol{W}} = \\boldsymbol{\\delta} \\ \\boldsymbol{x}^{\\mathsf{T}} . This is actually not quite wirte. The correct form should be \\begin{align*} \\underbrace{\\frac{\\partial s}{\\partial \\boldsymbol{W}}}_{n \\times m} = \\underbrace{\\boldsymbol{\\delta}^{\\mathsf{T}}}_{n \\times 1} \\ \\underbrace{\\boldsymbol{x}^{\\mathsf{T}}}_{1 \\times m}. \\end{align*} \\begin{align*} \\underbrace{\\frac{\\partial s}{\\partial \\boldsymbol{W}}}_{n \\times m} = \\underbrace{\\boldsymbol{\\delta}^{\\mathsf{T}}}_{n \\times 1} \\ \\underbrace{\\boldsymbol{x}^{\\mathsf{T}}}_{1 \\times m}. \\end{align*} Note You may wonder why is this form instead of the one we derived directly. The explanation from the CS224N is that we would like to follow the shape convention so as to make the chain rule implementation more efficient (matrix multiplication instead of loops). Different from the Jacobian form, shape convention states that the shape of the gradient is the same shape of parameters. The resolution here is to use Jacobian form as much as possible and reshape to follow the convention at the end. Because Jacobian form makes the chain rule easy and the shape convention makes the implementation of gradient descent easy.","title":"Shape convention"},{"location":"courses/cs224n/write-up/#computation-graphs-and-backpropagation","text":"We have shown how to compute the partial derivatives using the chain rule. This is almost the backpropagation algorithm. If we want to add on to that and make the algorithm complete, the only thing we need is how to reuse gradient computed for higher layers. We leverage the computation graph to explain this. From the computation graph, you'll get the intuition of reusing the partial derivatives computed for higher layers in computing derivatives for lower layers so as to minimize computation. We represent our neural net equations as a computation graph as following: source nodes represent inputs interior nodes represent operations edges pass along result of the operation When following computation order to carry out the computations from inputs, it is forward propagation. Backpropagation is to pass along gradients backwards. This can be illustrated in the following computation graph. The partial derivative with respect to a parameter reflect how changing the parameter would effect the output value. The output value in the backprop is usually a loss function (or error). Intuitively, you can see backprop is push the error back to the lower layer through a bunch of operation nodes, the arrived error is a measure of the error at that particular layer, training is try to reduce this backprop-ed error by adjusting the local parameters, the effect of reducing the local error will be forward propagated to the output, the error at the output should also reduced. We use gradient descent to make this process to converge as soon as possible. To understand it better, let's look at a single node. We define \"local gradient\" for each node as the gradient of it's output with respect to it's input. By chain rule, the downstream gradient is equal to the multiplication of the upstream gradient and the local gradient. When having multipe local gradients, they are pushed back to each input using chain rule. CS224N lecture 4 slides have step by step example of backprop. From the example, we can got some intuitions about some nodes' effects. For example, when push back gradients along outward branches from a single node, the gradients should be sumed up; \"+\" node distributes the upstream gradient to each summand; \"max\" node simply routes the upstream gradients. When update gradient with backprop, you should compute all the gradients at once. With this computation graph notion, following routine captures the gist of backprop in a very decent manner. Fprop: visit nodes in topological sorted order Compute value of node given predecessors Bprop: initialize output gradient = 1 visit nodes in reverse order: Compute gradient wrt each node using gradient wrt successors \\{y_1, y_2, \\cdots, y_2\\} = \\mathrm{successors\\ of\\ } x \\{y_1, y_2, \\cdots, y_2\\} = \\mathrm{successors\\ of\\ } x \\begin{equation*}\\frac{\\partial z}{\\partial \\boldsymbol{x}} = \\sum_{i=i}^n \\frac{\\partial z}{\\partial y_i} \\frac{\\partial y_i}{\\partial x} \\end{equation*} \\begin{equation*}\\frac{\\partial z}{\\partial \\boldsymbol{x}} = \\sum_{i=i}^n \\frac{\\partial z}{\\partial y_i} \\frac{\\partial y_i}{\\partial x} \\end{equation*} If done correctly, the big O O complexity of forward propagation and backpropagation is the same.","title":"Computation graphs and backpropagation"},{"location":"courses/cs224n/write-up/#automatic-differentiation","text":"The gradient computation can be automatically inferred from the symbolic expression of the fprop. but this is not commonly used in practice. Each node type needs to know how to compute its output and how to compute the gradient wrt its inputs given the gradient wrt its output. Modern DL frameworks (Tensorflow, PyTorch, etc.) do backpropagation for you but mainly leave layer/node writer to hand-calculate the local derivative. Following is a simple demo of how to implement forward propagation and backpropagation. class MultiplyGate ( object ): def forward ( x , y ): z = x * y self . x = x # must keep these around! self . y = y return z def backword ( dz ): dx = self . y * dz # [dz/dx * dL/dz] dy = self . x * dz # [dz/dy * dL/dz] return [ dx , dy ] nonlinear activation functions: sigmoid, tanh, hard tanh, ReLU Learning rates: start 0.001. power of 10. halve the learning rate every k epochs. formula: lr = lr_0 e^{-k t} lr = lr_0 e^{-k t} for each epoch t t","title":"Automatic differentiation"},{"location":"courses/cs224n/write-up/#regularization","text":"LFD starting point: L2 regularization","title":"Regularization"},{"location":"courses/cs224n/write-up/#part-3-language-model-and-rnn","text":"","title":"Part 3 Language Model and RNN"},{"location":"courses/cs224n/write-up/#n-gram","text":"assumption: \\boldsymbol{x}^{(t+1)} \\boldsymbol{x}^{(t+1)} depends only on previous n-1 n-1 words. Sparsity problem, --> backoff (n-1)-gram practically, n has to be less than 5.","title":"n-gram"},{"location":"courses/cs224n/write-up/#neural-language-model","text":"fixed-window neural language model Recurrent Neural Network","title":"Neural language model"},{"location":"courses/func-prog-in-scala/notes/","text":"Functional Programming Principles in Scala \u00b6 Week 1 \u00b6 Theory wise, pure imperative programming languages is limited by \"Von Neumann\" bottleneck Theory: one or more data types operations on these types laws that describe the relationship between values and operations Normally, a theory does not describe mutations (change something but keep the identity) Theories without Mutation Pylonomial String consequence in programming avoid mutations have a power way to abstract operations and functions. The substitution model lambda calculus only to expression don't have a side effect termination def loop : Int = loop unreduced arguments Call-by-name and call-by-value def test ( x : Int , y : Int ) = x * x test ( 2 , 3 ) test ( 3 + 4 , 8 ) test ( 7 , 2 * 4 ) test ( 3 + 4 , 2 * 4 ) CBV, CBN, and termination CBV termination --> CBN termination Not vice versa example def first ( x : Int , y : Int ) = x first ( 7 , loop ) enforce call by name => Conditional expressions if-else not a statement, just a expression. Value and definitions scala > def loop : Boolean = loop && loop loop : Boolean scala > def x = loop x : Boolean scala > val x = loop java . lang . StackOverflowError at . loop ( < console >: 12 ) Square root with Newton's Method def abs ( x : Double ) = if ( x < 0 ) - x else x def sqrtIter ( guess : Double , x : Double ): Double = if ( isGoodEnough ( guess , x )) guess else sqrtIter ( improve ( guess , x ), x ) def isGoodEnough ( guess : Double , x : Double ) = abs ( guess * guess - x ) < 0.001 def improve ( guess : Double , x : Double ) = ( guess + x / guess ) / 2 def sqrt ( x : Double ) = sqrtIter ( 1.0 , x ) Tail recursion: Implementation consideration. If a function calls itself as its last action, the function's stack frame can be reused. This is called tail recursion. Tail recursive function are iterative processes. (anotation: @tailrec) gcd is tail recursion def gcd ( a : Int , b : Int ): Int = { if ( b == 0 ) a else gcd ( b , a % b ) } factorial is not def factorial ( n : Int ) = if ( n == 0 ) 1 else n * factorial ( n - 1 ) but we can rewrite factorial in tail recursion form. def factorial ( n : Int ): Int = { def loop ( acc : Int , n : Int ): Int = if ( n == 0 ) acc else loop ( acc * n , n - 1 ) loop ( 1 , n ) }","title":"Functional Programming Principles in Scala"},{"location":"courses/func-prog-in-scala/notes/#functional-programming-principles-in-scala","text":"","title":"Functional Programming Principles in Scala"},{"location":"courses/func-prog-in-scala/notes/#week-1","text":"Theory wise, pure imperative programming languages is limited by \"Von Neumann\" bottleneck Theory: one or more data types operations on these types laws that describe the relationship between values and operations Normally, a theory does not describe mutations (change something but keep the identity) Theories without Mutation Pylonomial String consequence in programming avoid mutations have a power way to abstract operations and functions. The substitution model lambda calculus only to expression don't have a side effect termination def loop : Int = loop unreduced arguments Call-by-name and call-by-value def test ( x : Int , y : Int ) = x * x test ( 2 , 3 ) test ( 3 + 4 , 8 ) test ( 7 , 2 * 4 ) test ( 3 + 4 , 2 * 4 ) CBV, CBN, and termination CBV termination --> CBN termination Not vice versa example def first ( x : Int , y : Int ) = x first ( 7 , loop ) enforce call by name => Conditional expressions if-else not a statement, just a expression. Value and definitions scala > def loop : Boolean = loop && loop loop : Boolean scala > def x = loop x : Boolean scala > val x = loop java . lang . StackOverflowError at . loop ( < console >: 12 ) Square root with Newton's Method def abs ( x : Double ) = if ( x < 0 ) - x else x def sqrtIter ( guess : Double , x : Double ): Double = if ( isGoodEnough ( guess , x )) guess else sqrtIter ( improve ( guess , x ), x ) def isGoodEnough ( guess : Double , x : Double ) = abs ( guess * guess - x ) < 0.001 def improve ( guess : Double , x : Double ) = ( guess + x / guess ) / 2 def sqrt ( x : Double ) = sqrtIter ( 1.0 , x ) Tail recursion: Implementation consideration. If a function calls itself as its last action, the function's stack frame can be reused. This is called tail recursion. Tail recursive function are iterative processes. (anotation: @tailrec) gcd is tail recursion def gcd ( a : Int , b : Int ): Int = { if ( b == 0 ) a else gcd ( b , a % b ) } factorial is not def factorial ( n : Int ) = if ( n == 0 ) 1 else n * factorial ( n - 1 ) but we can rewrite factorial in tail recursion form. def factorial ( n : Int ): Int = { def loop ( acc : Int , n : Int ): Int = if ( n == 0 ) acc else loop ( acc * n , n - 1 ) loop ( 1 , n ) }","title":"Week 1"},{"location":"courses/java-collections/notes/","text":"Java Collections \u00b6 NB: Course is started with educative.io version and will be expanded the list to include more collection Collection v.s. Collections \u00b6 Collection is an interface, collections is a class. A Collection interface provides the standard functionality of a data structure to List, Set, and Queue. However, the Collections class provides the utility methods that can be used to search, sort, and synchronize collection elements. ArrayList \u00b6 Default size is 10. Java 8 lazy allocation, 0 on creation, resize to 10 on adding. The capacity is checked when added, it reallocate strategy is create n + n / 2 + 1 n + n / 2 + 1 . Basic Methods \u00b6 List.add(E e) List.add(int index, E e) List.addAll(Collection c) List.addAll(int index, Collection c) List.get(int index) List.size() List.remove(int index) , e.g. list.remove(2); List.remove(Object o) , e.g. list.remove(new Integer(2)); List.removeRange(int fromIndex, int toIndex) List.removeAll(Collection<?> c) List.clear() List.replaceAll(UnaryOperator<E> operator) , e.g. list.replaceAll((element) -> element.toUpperCase()); List.set(int index, E e) List.contains(Object o) List.indexOf(E e) List.lastIndexOf(E e) import java.util.ArrayList ; import java.util.List ; public class ArrayListDemo { List list = new ArrayList (); list . add ( 1 ); list . add ( 2 ); list . add ( 3 ); System . out . println ( list ); list . add ( 4 ); System . out . println ( list ); list . add ( 1 , 50 ); System . out . println ( list ); List newList = new ArrayList (); newList . add ( 500 ); newList . add ( 600 ); System . out . println ( newList ); list . addAll ( newList ); System . out . println ( list ); list . addAll ( 1 , newList ); System . out . println ( list ); } // output: // [1, 2, 3] // [1, 2, 3, 4] // [1, 50, 2, 3, 4] // [1, 50, 2, 3, 4, 150, 160] // [1, 150, 160, 50, 2, 3, 4, 150, 160] Using an Iterator \u00b6 Interator<Integer> iter = List.iterator(); iter.hasNext() iter.next() iter.remove() iter.forEachRemaining(Consumer<? super E> action) Once a interator is created, we can not modify the List while interating elements. Iterator < Integer > iter = list . iterator (); // remove a element WRONG WRONG while ( iter . hasNext ()) { int next = iter . next (); if ( next == 30 ) { list . remove ( new Integer ( 30 )); } } // CORRECT while ( iter . hasNext ()) { int next = iter . next (); if ( next == 30 ) { iter . remove (); } } // add an element WRONG WRONG Iterator < Integer > iter = list . iterator (); list . add ( 10 ); while ( iter . hasNext ()) { System . out . println ( iter . next ()); } Using an ListIterator \u00b6 ListIterator<Integer> listIterator = list.ListIterator(); List.hasNext(); List.next(); List.hasPrevious(); List.previous(); List.nextIndex(); List.previousIndex(); List.remove(); , can only be made for each call to next() or previous() . List.set(E e); , replace the element returned by next() or previous() . List.add(E e); , add before the element returned by next() . Sort ArrayList \u00b6 Collections.sort(List<T> list) , type T must implement the Comparable interface. List<Integer> sortedList = list.stream().sorted().collect(Collectors.toList()); , Java 8. Collections.sort(list, Collections.reverseOrder()); List<Integer> sortedList = list.stream().sorted(Comparator.reverseOrder()).collect(Collectors.toList()); , Java 8. Comparable Interface \u00b6 public class Vehicle implements Comparable < Vehicle > { String brand ; Integer makeYear ; public Vehicle ( String brand , Integer makeYear ) { super (); this . brand = brand ; this . makeYear = makeYear ; } @Override public int compareTo ( Vehicle o ) { return this . makeYear - o . makeYear ; // We can also use the compareTo() method of the Integer class. //return this.makeYear.compareTo(o.makeYear); } } Comparator Interface \u00b6 Collections.sort(List<T> list, Comparator<? super T> c) . Comparator interface has a method, compare(T o1, T o2) . // BrandComparator.java import java.util.Comparator ; public class BrandComparator implements Comparator < Vehicle > { @Override public int compare ( Vehicle o1 , Vehicle o2 ) { return o1 . brand . compareTo ( o2 . brand ); } } // use the comparator in code Collections . sort ( list , new BrandComparator ()); // alternatively use anonymous comparator class Collections . sort ( list , new Comparator < Vehicle > () { @Override public int compare ( Vehicle o1 , Vehicle o2 ) { return o1 . brand . compareTo ( o2 . brand ); } }); // Use Java 8 lambda comparator (functional style) Collections . sort ( list , ( o1 , o2 ) -> o1 . brand . compareTo ( o2 . brand )); LinkedList \u00b6 LinkedList class implement both the List and Deque interfaces. LinkedList definition private static class Node < E > { E item ; Node < E > next ; Node < E > prev ; Node ( Node < E > prev , E element , Node < E > next ) { this . item = element ; this . prev = prev ; this . next = next ; } } // construct a LinkedList List < Integer > list = new LinkedList < Integer > (); // construct a LinkedList with existing list List < Integer > list = new LinkedList < Integer > ( oldList ); LinkedList operation methods // add elements LinkedList . add ( E e ) LinkedList . addFirst ( E e ) LinkedList . addLast ( E e ) LinkedList . add ( int index , E e ) LinkedList . addAll ( Collection c ) LinkedList . addAll ( int index , Collection c ) // fetch elements LinkedList . getFirst () LinkedList . getLast () LinkedList . get ( int index ) // remove elements LinkedList . removeFirst () LinkedList . removeLast () LinkedList . remove ( int index ) LinkedList . remove ( Object 0 ) // sorting Collections . sort ( linkedList ); CopyOnWriteArrayList \u00b6 creating a CopyOnWriteArrayList private transient volatile Object [] array ; public CopyOnWriteArrayList () { setArray ( new Object [ 0 ] ); } final void setArray ( Object [] a ) { array = a ; } List list = new CopyOnWriteArrayList (); // using an existing array public CopyOnWriteArrayList ( E [] toCopyIn ) { setArray ( Array . copyOf ( toCopyin , toCopyIn . length , Object [] . class )); } inserting elements into a CopyOnWriteArrayList CopyOnWriteArrayList . add ( E e ) CopyOnWriteArrayList . add ( int index , E element ) CopyOnWriteArrayList . addAll ( Collection c ) CopyOnWriteArrayList . addIfAbsent ( E e ) CopyOnWriteArrayList . addAllAbsent ( Collection c ) CopyOnWriteArrayList internal \u00b6 Using a reentrant lock final transient ReentrantLock lock = new ReentrantLock(); Steps: writing thread aquire the by lock.lock() ; make a copy of the data with size of length + 1 ; add the element at the end of the copied data; point to the new data; release the lock. Interation \u00b6 Using forEach(Consumer<? super E> action) . Using iterator() . No synchronization is needed while traversing the iterator because the iteration is being done on a snapshot. import java.util.Iterator ; import java.util.List ; import java.util.concurrent.CopyOnWriteArrayList ; public class CopyOnWriteArrayListDemo { public static void main ( String args [] ) { List < String > list = new CopyOnWriteArrayList <> (); list . add ( \"Apple\" ); list . add ( \"Banana\" ); list . add ( \"Orange\" ); //Created an iterator Iterator < String > itr = list . iterator (); //Adding elements after creating iterator. ConcurrentModificationException will not be thrown. list . add ( \"Papaya\" ); //Iterating the list through the iterator that was created earlier. Papaya will not be present. while ( itr . hasNext ()) { System . out . println ( itr . next ()); } System . out . println ( \"Again getting the iterator\" ); //Again creating the iterator. This time papaya will be present. itr = list . iterator (); while ( itr . hasNext ()) { System . out . println ( itr . next ()); } } } iterator() of CopyOnWriteArrayList class doesn't support remove() method. We can directly remove a element while iterating throught the list. import java.util.Iterator ; import java.util.List ; import java.util.concurrent.CopyOnWriteArrayList ; public class CopyOnWriteArrayListDemo { public static void main ( String args [] ) { List < String > list = new CopyOnWriteArrayList <> (); list . add ( \"Apple\" ); list . add ( \"Banana\" ); list . add ( \"Orange\" ); //Created an iterator Iterator < String > itr = list . iterator (); while ( itr . hasNext ()) { System . out . println ( itr . next ()); list . remove ( \"Orange\" ); } System . out . println ( \"Again creating the iterator\" ); //Created an iterator itr = list . iterator (); while ( itr . hasNext ()) { System . out . println ( itr . next ()); } } } Sets \u00b6","title":"Java Collections"},{"location":"courses/java-collections/notes/#java-collections","text":"NB: Course is started with educative.io version and will be expanded the list to include more collection","title":"Java Collections"},{"location":"courses/java-collections/notes/#collection-vs-collections","text":"Collection is an interface, collections is a class. A Collection interface provides the standard functionality of a data structure to List, Set, and Queue. However, the Collections class provides the utility methods that can be used to search, sort, and synchronize collection elements.","title":"Collection v.s. Collections"},{"location":"courses/java-collections/notes/#arraylist","text":"Default size is 10. Java 8 lazy allocation, 0 on creation, resize to 10 on adding. The capacity is checked when added, it reallocate strategy is create n + n / 2 + 1 n + n / 2 + 1 .","title":"ArrayList"},{"location":"courses/java-collections/notes/#basic-methods","text":"List.add(E e) List.add(int index, E e) List.addAll(Collection c) List.addAll(int index, Collection c) List.get(int index) List.size() List.remove(int index) , e.g. list.remove(2); List.remove(Object o) , e.g. list.remove(new Integer(2)); List.removeRange(int fromIndex, int toIndex) List.removeAll(Collection<?> c) List.clear() List.replaceAll(UnaryOperator<E> operator) , e.g. list.replaceAll((element) -> element.toUpperCase()); List.set(int index, E e) List.contains(Object o) List.indexOf(E e) List.lastIndexOf(E e) import java.util.ArrayList ; import java.util.List ; public class ArrayListDemo { List list = new ArrayList (); list . add ( 1 ); list . add ( 2 ); list . add ( 3 ); System . out . println ( list ); list . add ( 4 ); System . out . println ( list ); list . add ( 1 , 50 ); System . out . println ( list ); List newList = new ArrayList (); newList . add ( 500 ); newList . add ( 600 ); System . out . println ( newList ); list . addAll ( newList ); System . out . println ( list ); list . addAll ( 1 , newList ); System . out . println ( list ); } // output: // [1, 2, 3] // [1, 2, 3, 4] // [1, 50, 2, 3, 4] // [1, 50, 2, 3, 4, 150, 160] // [1, 150, 160, 50, 2, 3, 4, 150, 160]","title":"Basic Methods"},{"location":"courses/java-collections/notes/#using-an-iterator","text":"Interator<Integer> iter = List.iterator(); iter.hasNext() iter.next() iter.remove() iter.forEachRemaining(Consumer<? super E> action) Once a interator is created, we can not modify the List while interating elements. Iterator < Integer > iter = list . iterator (); // remove a element WRONG WRONG while ( iter . hasNext ()) { int next = iter . next (); if ( next == 30 ) { list . remove ( new Integer ( 30 )); } } // CORRECT while ( iter . hasNext ()) { int next = iter . next (); if ( next == 30 ) { iter . remove (); } } // add an element WRONG WRONG Iterator < Integer > iter = list . iterator (); list . add ( 10 ); while ( iter . hasNext ()) { System . out . println ( iter . next ()); }","title":"Using an Iterator"},{"location":"courses/java-collections/notes/#using-an-listiterator","text":"ListIterator<Integer> listIterator = list.ListIterator(); List.hasNext(); List.next(); List.hasPrevious(); List.previous(); List.nextIndex(); List.previousIndex(); List.remove(); , can only be made for each call to next() or previous() . List.set(E e); , replace the element returned by next() or previous() . List.add(E e); , add before the element returned by next() .","title":"Using an ListIterator"},{"location":"courses/java-collections/notes/#sort-arraylist","text":"Collections.sort(List<T> list) , type T must implement the Comparable interface. List<Integer> sortedList = list.stream().sorted().collect(Collectors.toList()); , Java 8. Collections.sort(list, Collections.reverseOrder()); List<Integer> sortedList = list.stream().sorted(Comparator.reverseOrder()).collect(Collectors.toList()); , Java 8.","title":"Sort ArrayList"},{"location":"courses/java-collections/notes/#comparable-interface","text":"public class Vehicle implements Comparable < Vehicle > { String brand ; Integer makeYear ; public Vehicle ( String brand , Integer makeYear ) { super (); this . brand = brand ; this . makeYear = makeYear ; } @Override public int compareTo ( Vehicle o ) { return this . makeYear - o . makeYear ; // We can also use the compareTo() method of the Integer class. //return this.makeYear.compareTo(o.makeYear); } }","title":"Comparable Interface"},{"location":"courses/java-collections/notes/#comparator-interface","text":"Collections.sort(List<T> list, Comparator<? super T> c) . Comparator interface has a method, compare(T o1, T o2) . // BrandComparator.java import java.util.Comparator ; public class BrandComparator implements Comparator < Vehicle > { @Override public int compare ( Vehicle o1 , Vehicle o2 ) { return o1 . brand . compareTo ( o2 . brand ); } } // use the comparator in code Collections . sort ( list , new BrandComparator ()); // alternatively use anonymous comparator class Collections . sort ( list , new Comparator < Vehicle > () { @Override public int compare ( Vehicle o1 , Vehicle o2 ) { return o1 . brand . compareTo ( o2 . brand ); } }); // Use Java 8 lambda comparator (functional style) Collections . sort ( list , ( o1 , o2 ) -> o1 . brand . compareTo ( o2 . brand ));","title":"Comparator Interface"},{"location":"courses/java-collections/notes/#linkedlist","text":"LinkedList class implement both the List and Deque interfaces. LinkedList definition private static class Node < E > { E item ; Node < E > next ; Node < E > prev ; Node ( Node < E > prev , E element , Node < E > next ) { this . item = element ; this . prev = prev ; this . next = next ; } } // construct a LinkedList List < Integer > list = new LinkedList < Integer > (); // construct a LinkedList with existing list List < Integer > list = new LinkedList < Integer > ( oldList ); LinkedList operation methods // add elements LinkedList . add ( E e ) LinkedList . addFirst ( E e ) LinkedList . addLast ( E e ) LinkedList . add ( int index , E e ) LinkedList . addAll ( Collection c ) LinkedList . addAll ( int index , Collection c ) // fetch elements LinkedList . getFirst () LinkedList . getLast () LinkedList . get ( int index ) // remove elements LinkedList . removeFirst () LinkedList . removeLast () LinkedList . remove ( int index ) LinkedList . remove ( Object 0 ) // sorting Collections . sort ( linkedList );","title":"LinkedList"},{"location":"courses/java-collections/notes/#copyonwritearraylist","text":"creating a CopyOnWriteArrayList private transient volatile Object [] array ; public CopyOnWriteArrayList () { setArray ( new Object [ 0 ] ); } final void setArray ( Object [] a ) { array = a ; } List list = new CopyOnWriteArrayList (); // using an existing array public CopyOnWriteArrayList ( E [] toCopyIn ) { setArray ( Array . copyOf ( toCopyin , toCopyIn . length , Object [] . class )); } inserting elements into a CopyOnWriteArrayList CopyOnWriteArrayList . add ( E e ) CopyOnWriteArrayList . add ( int index , E element ) CopyOnWriteArrayList . addAll ( Collection c ) CopyOnWriteArrayList . addIfAbsent ( E e ) CopyOnWriteArrayList . addAllAbsent ( Collection c )","title":"CopyOnWriteArrayList"},{"location":"courses/java-collections/notes/#copyonwritearraylist-internal","text":"Using a reentrant lock final transient ReentrantLock lock = new ReentrantLock(); Steps: writing thread aquire the by lock.lock() ; make a copy of the data with size of length + 1 ; add the element at the end of the copied data; point to the new data; release the lock.","title":"CopyOnWriteArrayList internal"},{"location":"courses/java-collections/notes/#interation","text":"Using forEach(Consumer<? super E> action) . Using iterator() . No synchronization is needed while traversing the iterator because the iteration is being done on a snapshot. import java.util.Iterator ; import java.util.List ; import java.util.concurrent.CopyOnWriteArrayList ; public class CopyOnWriteArrayListDemo { public static void main ( String args [] ) { List < String > list = new CopyOnWriteArrayList <> (); list . add ( \"Apple\" ); list . add ( \"Banana\" ); list . add ( \"Orange\" ); //Created an iterator Iterator < String > itr = list . iterator (); //Adding elements after creating iterator. ConcurrentModificationException will not be thrown. list . add ( \"Papaya\" ); //Iterating the list through the iterator that was created earlier. Papaya will not be present. while ( itr . hasNext ()) { System . out . println ( itr . next ()); } System . out . println ( \"Again getting the iterator\" ); //Again creating the iterator. This time papaya will be present. itr = list . iterator (); while ( itr . hasNext ()) { System . out . println ( itr . next ()); } } } iterator() of CopyOnWriteArrayList class doesn't support remove() method. We can directly remove a element while iterating throught the list. import java.util.Iterator ; import java.util.List ; import java.util.concurrent.CopyOnWriteArrayList ; public class CopyOnWriteArrayListDemo { public static void main ( String args [] ) { List < String > list = new CopyOnWriteArrayList <> (); list . add ( \"Apple\" ); list . add ( \"Banana\" ); list . add ( \"Orange\" ); //Created an iterator Iterator < String > itr = list . iterator (); while ( itr . hasNext ()) { System . out . println ( itr . next ()); list . remove ( \"Orange\" ); } System . out . println ( \"Again creating the iterator\" ); //Created an iterator itr = list . iterator (); while ( itr . hasNext ()) { System . out . println ( itr . next ()); } } }","title":"Interation"},{"location":"courses/java-collections/notes/#sets","text":"","title":"Sets"},{"location":"courses/leadership-courses/business-writing-colorado/","text":"Business Writing \u00b6 Week 1 \u00b6 Be the windowpane, clarity above all Waste No Time Don't sound smart, be smart Own your ideas (be the authority) Avoid use wishy-washy words Should, could, maybe, ought, might, think Bad examples \u00b6 We think these changes might create the growth we need. We think these changes might create the efficiency we could use to further improve our supply chain. Good examples \u00b6 These changes will create the efficiency we need to improve our supply chain. Design your writings \u00b6 short paragraph use bullet points use bold faces Week 2 \u00b6 Orgnize or die Great writers are great revisers Week 3 \u00b6 Week 4 \u00b6","title":"Business Writing Colorado"},{"location":"courses/leadership-courses/business-writing-colorado/#business-writing","text":"","title":"Business Writing"},{"location":"courses/leadership-courses/business-writing-colorado/#week-1","text":"Be the windowpane, clarity above all Waste No Time Don't sound smart, be smart Own your ideas (be the authority) Avoid use wishy-washy words Should, could, maybe, ought, might, think","title":"Week 1"},{"location":"courses/leadership-courses/business-writing-colorado/#bad-examples","text":"We think these changes might create the growth we need. We think these changes might create the efficiency we could use to further improve our supply chain.","title":"Bad examples"},{"location":"courses/leadership-courses/business-writing-colorado/#good-examples","text":"These changes will create the efficiency we need to improve our supply chain.","title":"Good examples"},{"location":"courses/leadership-courses/business-writing-colorado/#design-your-writings","text":"short paragraph use bullet points use bold faces","title":"Design your writings"},{"location":"courses/leadership-courses/business-writing-colorado/#week-2","text":"Orgnize or die Great writers are great revisers","title":"Week 2"},{"location":"courses/leadership-courses/business-writing-colorado/#week-3","text":"","title":"Week 3"},{"location":"courses/leadership-courses/business-writing-colorado/#week-4","text":"","title":"Week 4"},{"location":"courses/leadership-courses/interpersonal-communication-rice/","text":"Interpersonal Communication for Engineering Leaders \u00b6 Week 1 \u00b6 Layer of culture inherited learned learned & inherited long pole : the critical component of the project Hofstede's cultural dimention \u00b6 Power distance individualism versus collectivism masculinity versus femininity Uncertainty avoidance index long term orientation versus short term normative orientation indulgence versus restraint Videos and Articles \u00b6 Make my trip - Aisle HSBC 'Eels' Ad The Power of Talk: Who Gets Heard and Why The Power of Talk: Who Gets Heard and Why \u00b6 TODO: add note about the reading. How Gender Influences Communication Styles, Habits, and Behaviors? \u00b6 Man and women brain functions are overlap. Personal Communication Improvement Plan \u00b6 Area of competence to improve Actions to take good attitudes hide all your negative emotion on people and things, be pleasant and friendly nervous relax and show your passion on the topic tone and intonation adding more tone and intonation variations into your speaking affirmation speaking with affirmed tone Week 2 Leadership Presence \u00b6 Do you feel like a leader? Do you look like a leader? Do you act like a leader? Do you talk like a leader? Personal Brand \u00b6 \"A brand is what people say about you when you're not in the room\" \"With more than 25 years experience as a communication professional, I offer excellent writing, editing and research skills to clients in the information technology industry.\" \"I help engineering students learn leadership skills so that they can leverage their creativity and technical skills to create a better world for all of us. My passion is teaching, my joy is seeing other successfully use what I help them learn.\" \"High energy motivational speaker and communication coach who loves the color red. I help business professionals learn how to come out of their shells so they can present themselves with confidence at business and social events and network their way to success.\" Personal Presence \u00b6 charisma (old) Power, influence, big personality, forcefull presence (new) calm, confidence leadership presence Look confident communication clearly 3 dimentions How you act How you communicate How you look Verbal Skill \u00b6 Use active rather than passive Hedge phrases Reduce filler word Use inclusive language leaders should talk to their teams. use strong, active verbs The team analyzed the material. Use specific words Make your tone professional and enthusiastic Voice quality volume talk slowly use phrasing and inflection Use pauses vary your voice Effective communication is about engagement Videos and Articles \u00b6 Hey. Did You Catch That?: Why They\u2019re Talking as Fast as They Can The Brand Called You Hold Great Meetings \u00b6 Your reputation for getting things done in meetings, of not wasting time, of gathering information, of facilitating strong group decisions, will carry over into your other activities and encounters with people. One of the first thing you can do when you start a meeting is to set a positive and productive tone. Your own demeanor and attitude are likely to be reflected by the people attending. Make sure you're pleasant and friendly even if today's agenda includes some difficult topics. Elon Musk has said that he leads by example, he brings his A-game to meetings, setting the standard for everyone to follow. You should already know what the goals of the meeting are. And you should've already sent the agenda and any reading material to the participants, expect them to come prepared. And be ready to hold them accountable in a firm but friendly way for the information you have distributed. In continuing to keep the tone of the meeting productive, make sure that everyone understands how each topic, each discussion and each decision fits into the big picture. For example, a decision may seem small, it may apply to a local team process but it's your job as a leader to connect that small decision to the overall goals of the organization and the needs of the business. Yes, and... \u00b6 Let's look at an example. Speaker one says, I think we can streamline our process if we combine our first two reviews into one and put it at the midpoint. Speaker two says no, that won't work. But I think we need that early review to help work out the bugs. (Yes, and..) Speaker 1 says, I think we can streamline our process if we combine our first two reviews into one and put it at the midpoint. Speaker 2 says, yes, and we can put the extra time and effort into better planning up front so we have fewer bugs. Make Sure Everyone Gets Heard \u00b6 Be aware of your role as a leader. Your demeanor and your attitude can contribute to a positive atmosphere. Next, create some ground rules for how to handle hot spots, such as disagreements, controversial topics, aggressive interruptions and dominating speakers. Make Sure Everyone Gets Heard Ask Insightful Questions \u00b6 Today's leaders Ask insightfull questions assimilate information collaboration make decisions support the big pictures Week 3 Difficult conversation \u00b6 what you style of handling difficult conversation? ask, what emotions make me think this way? Where do they come from? What is real, and what are my assumptions? How to make a person feel safe? avoid asking why question. avoid use the work but, but using and. Practicel tools \u00b6 not responding to effort --> keep the dialogue open stubbornness --> recognize their position and go back to goals Develop informal interpersonal relationships be consistent in your behavior explain your motivations be succint communicate and over communicate Listening \u00b6 Receive Understand Evaluate Remember Responding Levels of listening \u00b6 Leve 3 Fake attention tuning out the speaker More interested in talking Level 2 Hearing words, but missing intent not looking at non-verbals Level 1 Listening with understanding and respect Videos and Articles \u00b6 Holly Weeks, Taking Stress Out of Stressful Conversations Amy Gallo, How to Mentally Prepare for a Difficult Conversations Accessing Your Listening Skills Crucial Confrontations Cricket Buchler Week 4 Crisis Communication \u00b6 What is a crisis? How do you plan for crisis? How to communicate effectively in a crisis? Crisis Communication plan Identify worse-case scenario Identify your audiences and plan how to reach each audience employees are your spokespeople The key rule: communicate Early and Often \u00b6 What if you don't have all the facts? prepare a holding statement establish regular media updates communicate company values avoid wating and \"no comments\" tell then what you'll do Holding statemenet: this is what we know this is what we care about we are investigating we will update you in an hour living our value everyday: We care about the pepole involved We are committed to the full disclosure And I think it's critical to point out today that if I don't understand what you are saying, I don't trust you. And if what you are saying is so technical and beyond my ability to understand, I'm not going to invest the time to figure it out. I just don't trust you. I don't like you. I don't believe you. So companies that take the time to help people understand how to make communication inclusive so that people can instantly understand what you are saying, I think is the secret here.","title":"Interpersonal Communication Rice"},{"location":"courses/leadership-courses/interpersonal-communication-rice/#interpersonal-communication-for-engineering-leaders","text":"","title":"Interpersonal Communication for Engineering Leaders"},{"location":"courses/leadership-courses/interpersonal-communication-rice/#week-1","text":"Layer of culture inherited learned learned & inherited long pole : the critical component of the project","title":"Week 1"},{"location":"courses/leadership-courses/interpersonal-communication-rice/#hofstedes-cultural-dimention","text":"Power distance individualism versus collectivism masculinity versus femininity Uncertainty avoidance index long term orientation versus short term normative orientation indulgence versus restraint","title":"Hofstede's cultural dimention"},{"location":"courses/leadership-courses/interpersonal-communication-rice/#videos-and-articles","text":"Make my trip - Aisle HSBC 'Eels' Ad The Power of Talk: Who Gets Heard and Why","title":"Videos and Articles"},{"location":"courses/leadership-courses/interpersonal-communication-rice/#the-power-of-talk-who-gets-heard-and-why","text":"TODO: add note about the reading.","title":"The Power of Talk: Who Gets Heard and Why"},{"location":"courses/leadership-courses/interpersonal-communication-rice/#how-gender-influences-communication-styles-habits-and-behaviors","text":"Man and women brain functions are overlap.","title":"How Gender Influences Communication Styles, Habits, and Behaviors?"},{"location":"courses/leadership-courses/interpersonal-communication-rice/#personal-communication-improvement-plan","text":"Area of competence to improve Actions to take good attitudes hide all your negative emotion on people and things, be pleasant and friendly nervous relax and show your passion on the topic tone and intonation adding more tone and intonation variations into your speaking affirmation speaking with affirmed tone","title":"Personal Communication Improvement Plan"},{"location":"courses/leadership-courses/interpersonal-communication-rice/#week-2-leadership-presence","text":"Do you feel like a leader? Do you look like a leader? Do you act like a leader? Do you talk like a leader?","title":"Week 2 Leadership Presence"},{"location":"courses/leadership-courses/interpersonal-communication-rice/#personal-brand","text":"\"A brand is what people say about you when you're not in the room\" \"With more than 25 years experience as a communication professional, I offer excellent writing, editing and research skills to clients in the information technology industry.\" \"I help engineering students learn leadership skills so that they can leverage their creativity and technical skills to create a better world for all of us. My passion is teaching, my joy is seeing other successfully use what I help them learn.\" \"High energy motivational speaker and communication coach who loves the color red. I help business professionals learn how to come out of their shells so they can present themselves with confidence at business and social events and network their way to success.\"","title":"Personal Brand"},{"location":"courses/leadership-courses/interpersonal-communication-rice/#personal-presence","text":"charisma (old) Power, influence, big personality, forcefull presence (new) calm, confidence leadership presence Look confident communication clearly 3 dimentions How you act How you communicate How you look","title":"Personal Presence"},{"location":"courses/leadership-courses/interpersonal-communication-rice/#verbal-skill","text":"Use active rather than passive Hedge phrases Reduce filler word Use inclusive language leaders should talk to their teams. use strong, active verbs The team analyzed the material. Use specific words Make your tone professional and enthusiastic Voice quality volume talk slowly use phrasing and inflection Use pauses vary your voice Effective communication is about engagement","title":"Verbal Skill"},{"location":"courses/leadership-courses/interpersonal-communication-rice/#videos-and-articles_1","text":"Hey. Did You Catch That?: Why They\u2019re Talking as Fast as They Can The Brand Called You","title":"Videos and Articles"},{"location":"courses/leadership-courses/interpersonal-communication-rice/#hold-great-meetings","text":"Your reputation for getting things done in meetings, of not wasting time, of gathering information, of facilitating strong group decisions, will carry over into your other activities and encounters with people. One of the first thing you can do when you start a meeting is to set a positive and productive tone. Your own demeanor and attitude are likely to be reflected by the people attending. Make sure you're pleasant and friendly even if today's agenda includes some difficult topics. Elon Musk has said that he leads by example, he brings his A-game to meetings, setting the standard for everyone to follow. You should already know what the goals of the meeting are. And you should've already sent the agenda and any reading material to the participants, expect them to come prepared. And be ready to hold them accountable in a firm but friendly way for the information you have distributed. In continuing to keep the tone of the meeting productive, make sure that everyone understands how each topic, each discussion and each decision fits into the big picture. For example, a decision may seem small, it may apply to a local team process but it's your job as a leader to connect that small decision to the overall goals of the organization and the needs of the business.","title":"Hold Great Meetings"},{"location":"courses/leadership-courses/interpersonal-communication-rice/#yes-and","text":"Let's look at an example. Speaker one says, I think we can streamline our process if we combine our first two reviews into one and put it at the midpoint. Speaker two says no, that won't work. But I think we need that early review to help work out the bugs. (Yes, and..) Speaker 1 says, I think we can streamline our process if we combine our first two reviews into one and put it at the midpoint. Speaker 2 says, yes, and we can put the extra time and effort into better planning up front so we have fewer bugs.","title":"Yes, and..."},{"location":"courses/leadership-courses/interpersonal-communication-rice/#make-sure-everyone-gets-heard","text":"Be aware of your role as a leader. Your demeanor and your attitude can contribute to a positive atmosphere. Next, create some ground rules for how to handle hot spots, such as disagreements, controversial topics, aggressive interruptions and dominating speakers. Make Sure Everyone Gets Heard","title":"Make Sure Everyone Gets Heard"},{"location":"courses/leadership-courses/interpersonal-communication-rice/#ask-insightful-questions","text":"Today's leaders Ask insightfull questions assimilate information collaboration make decisions support the big pictures","title":"Ask Insightful Questions"},{"location":"courses/leadership-courses/interpersonal-communication-rice/#week-3-difficult-conversation","text":"what you style of handling difficult conversation? ask, what emotions make me think this way? Where do they come from? What is real, and what are my assumptions? How to make a person feel safe? avoid asking why question. avoid use the work but, but using and.","title":"Week 3 Difficult conversation"},{"location":"courses/leadership-courses/interpersonal-communication-rice/#practicel-tools","text":"not responding to effort --> keep the dialogue open stubbornness --> recognize their position and go back to goals Develop informal interpersonal relationships be consistent in your behavior explain your motivations be succint communicate and over communicate","title":"Practicel tools"},{"location":"courses/leadership-courses/interpersonal-communication-rice/#listening","text":"Receive Understand Evaluate Remember Responding","title":"Listening"},{"location":"courses/leadership-courses/interpersonal-communication-rice/#levels-of-listening","text":"Leve 3 Fake attention tuning out the speaker More interested in talking Level 2 Hearing words, but missing intent not looking at non-verbals Level 1 Listening with understanding and respect","title":"Levels of listening"},{"location":"courses/leadership-courses/interpersonal-communication-rice/#videos-and-articles_2","text":"Holly Weeks, Taking Stress Out of Stressful Conversations Amy Gallo, How to Mentally Prepare for a Difficult Conversations Accessing Your Listening Skills Crucial Confrontations Cricket Buchler","title":"Videos and Articles"},{"location":"courses/leadership-courses/interpersonal-communication-rice/#week-4-crisis-communication","text":"What is a crisis? How do you plan for crisis? How to communicate effectively in a crisis? Crisis Communication plan Identify worse-case scenario Identify your audiences and plan how to reach each audience employees are your spokespeople","title":"Week 4 Crisis Communication"},{"location":"courses/leadership-courses/interpersonal-communication-rice/#the-key-rule-communicate-early-and-often","text":"What if you don't have all the facts? prepare a holding statement establish regular media updates communicate company values avoid wating and \"no comments\" tell then what you'll do Holding statemenet: this is what we know this is what we care about we are investigating we will update you in an hour living our value everyday: We care about the pepole involved We are committed to the full disclosure And I think it's critical to point out today that if I don't understand what you are saying, I don't trust you. And if what you are saying is so technical and beyond my ability to understand, I'm not going to invest the time to figure it out. I just don't trust you. I don't like you. I don't believe you. So companies that take the time to help people understand how to make communication inclusive so that people can instantly understand what you are saying, I think is the secret here.","title":"The key rule: communicate Early and Often"},{"location":"courses/machine-learning-coursera/notes/","text":"Machine Learning (Coursera) \u00b6 Week 1 Introduction \u00b6 Quote Machine Learning is a field of study that gives computers the ability to learn without being explicitly programmed. -- Arthur Samuel (1959) Quote A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E. -- Tom Mitchell (1998) Well-posed Learning Problem Experience E Task T Performance P Supervised learning: give the \"right answer\" of existing Unsupervised learning Reinforcement learning Recommendation system Regression V.S. classification Regression, predict a continuous value across range a function of independent variable Classification, predict a discrete value Cost Function \u00b6 Cost function (squared error function) J(\\theta_0, \\theta_1) = \\frac{1}{2m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})^2 J(\\theta_0, \\theta_1) = \\frac{1}{2m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})^2 Goal \\underset{\\theta_0, \\theta_1}{\\text{minimize}} J(\\theta_0, \\theta_1) \\underset{\\theta_0, \\theta_1}{\\text{minimize}} J(\\theta_0, \\theta_1) Some Intuitions when the sample is {1, 1}, {2, 2}, and {3, 3} . h_\\theta(x) = x, (\\theta_0 = 0, \\theta_1 = 1) h_\\theta(x) = x, (\\theta_0 = 0, \\theta_1 = 1) , we can calculate J(\\theta_0, \\theta_1) = 0. J(\\theta_0, \\theta_1) = 0. when the sample is {1, 1}, {2, 2}, and {3, 3} . h_\\theta(x) = 0.5 x, (\\theta_0 = 0, \\theta_1 = 0.5) h_\\theta(x) = 0.5 x, (\\theta_0 = 0, \\theta_1 = 0.5) , we can calculate J(\\theta_1) = \\frac{3.5}{6}. J(\\theta_1) = \\frac{3.5}{6}. we can plot J(\\theta_1) J(\\theta_1) vs. \\theta_1 \\theta_1 , the function plotted would be a quadratic curve. Cost Function Intuition I \u00b6 Andrew plot the hypothesis \\textstyle h_\\theta(x) \\textstyle h_\\theta(x) and the cost function \\textstyle {J(\\theta_1)} \\textstyle {J(\\theta_1)} \uff08setting \\theta_0 = 0 \\theta_0 = 0 \uff09, The function \\textstyle J(\\theta_1) \\textstyle J(\\theta_1) is a curve have a minimum value. Cost Function Intuition II \u00b6 The plot of h(x) h(x) and J(\\theta_0, \\theta_1) J(\\theta_0, \\theta_1) show that J(\\theta_0, \\theta_1) J(\\theta_0, \\theta_1) has to be contour plot. Our goal is to \\min_{\\theta_0, \\theta_1}J(\\theta_0, \\theta_1) \\min_{\\theta_0, \\theta_1}J(\\theta_0, \\theta_1) To summarize, in this lecture, we formulate the hypothesis function and defined the cost function. We also plotted them to get some insight into the two functions. The machine learning problem we learn from this lecture is a minimization problem. Given a hypothesis, we looking for the \\theta_0 \\theta_0 and \\theta_1 \\theta_1 which minimize the cost function. Then we solve the problem. We use the Gradient Descent algorithm to search this minimum value. Gradient Descent \u00b6 It could solve general minimization problems Given function J(\\theta_0, \\theta_1) J(\\theta_0, \\theta_1) Objective: \\min_{\\theta_0, \\theta_1}J(\\theta_0, \\theta_1) \\min_{\\theta_0, \\theta_1}J(\\theta_0, \\theta_1) Gradient Descent Algorithm Start with some \\theta_0, \\theta_1 \\theta_0, \\theta_1 Keep changing \\theta_0, \\theta_1 \\theta_0, \\theta_1 to reduce J(\\theta_0, \\theta_1) J(\\theta_0, \\theta_1) Simultianeously update: \\theta_j := \\theta_j - \\alpha \\frac{\\partial }{\\partial \\theta_j}J(\\theta_0, \\theta_1) \\theta_j := \\theta_j - \\alpha \\frac{\\partial }{\\partial \\theta_j}J(\\theta_0, \\theta_1) , for ( j = 0, j = 1) ( j = 0, j = 1) Untill we hopefully end up at the minimum.(convergence) Question: How to implement the algorithm. Gradient Descent Intuition \u00b6 learning rate don't have to adjust the learning rate derivative would reduce automatically Gradient Descent and Linear Regression \u00b6 Take the linear regression cost function and apply gradient descent algorithm. Model: h_\\theta(x) = \\theta_0 + \\theta_1x h_\\theta(x) = \\theta_0 + \\theta_1x Cost function: J(\\theta_0, \\theta_1) = \\frac{1}{2m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})^2 J(\\theta_0, \\theta_1) = \\frac{1}{2m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})^2 Gradient Descent in Linear Regression. repeat \\begin{Bmatrix} \\theta_0 := \\theta_0 - \\alpha \\frac{1}{m} \\sum_{i = 1}^{m}(h_\\theta(x^{(i)}) - y^{(i)}) \\\\ \\theta_1 := \\theta_1 - \\alpha \\frac{1}{m} \\sum_{i = 1}^{m}(h_\\theta(x^{(i)}) - y^{(i)}) x^{(i)} \\end{Bmatrix} \\begin{Bmatrix} \\theta_0 := \\theta_0 - \\alpha \\frac{1}{m} \\sum_{i = 1}^{m}(h_\\theta(x^{(i)}) - y^{(i)}) \\\\ \\theta_1 := \\theta_1 - \\alpha \\frac{1}{m} \\sum_{i = 1}^{m}(h_\\theta(x^{(i)}) - y^{(i)}) x^{(i)} \\end{Bmatrix} simultaneously. The linear regression cost function is always a convex function - always has a single minimum (bowl Shaped). Linear Algebra Review \u00b6 Not commutative: A x B != B x A except that matrix B is identity matrix. Associativity: (A x B) x C = A x (B x C) Identity Matrix: Any matrix A which can be multiplied by an identity matrix gives you matrix A back I[m, m] x A[m, n] = A[m, n] A[m, n] x I[n, n] = A[m,n] Inverse Multiplicative inverse(reciprocal): a number multiply it by to get the identify element. i.e. if you have x , x * 1/x = 1 . Additive inverse: -3 is 3's additive inverse. Matrix inverse A \\cdot A^{-1} = I A \\cdot A^{-1} = I Only square matrix, but not all square matrix have an inverse. singularity If A is all zeros then there is no inverse matrix Some others don't, intuition should be matrices that don't have an inverse are a singular matrix or a degenerate matrix (i.e. when it's too close to 0) So if all the values of a matrix reach zero, this can be described as reaching singularity Week 2 Multiple features(variables) linear regression \u00b6 From feature x^{(i)} x^{(i)} to features x_j^{(i)} x_j^{(i)} , means the j j th feature element of the i i th feature vector. Hypothesis becomes: \\textstyle h_\\theta(x) = \\theta^{T}x = \\theta_0x_0 + \\theta_1x_1 + ... + \\theta_nx_n \\textstyle h_\\theta(x) = \\theta^{T}x = \\theta_0x_0 + \\theta_1x_1 + ... + \\theta_nx_n . Cost Function: J(\\theta_0, \\theta_1, ..., \\theta_n) = \\frac{1}{2m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})^2 J(\\theta_0, \\theta_1, ..., \\theta_n) = \\frac{1}{2m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})^2 Gradient descent: \\theta_j := \\theta_j - \\alpha \\frac{\\partial }{\\partial \\theta_j}J(\\theta_0, \\theta_1, ..., \\theta_n) \\theta_j := \\theta_j - \\alpha \\frac{\\partial }{\\partial \\theta_j}J(\\theta_0, \\theta_1, ..., \\theta_n) , simultaneously update for every j = 0, 1, 2, ..., n j = 0, 1, 2, ..., n . Compare of Gradient descent for multiple variables: Feature scaling \u00b6 One of the idea to improve the result in gradient descent is to make different features in the same scale, i.e. -1 \\leq x_i \\leq 1 -1 \\leq x_i \\leq 1 range . For example, in the housing price prediction problem, we have to make the size of the house (0-2000 square feet), and the number of rooms in the house (1-5 rooms) in the same scale. We do this by the method of mean normalization. Mean normalization Replace x_i x_i with x_i - \\mu_i x_i - \\mu_i to make features have approximately zero mean (Do not apply to x_0 = 1 x_0 = 1 ) E.g. x_1 = \\frac{size - 1000}{2000} x_1 = \\frac{size - 1000}{2000} , x_2 = \\frac{\\#rooms - 2}{5(range)} x_2 = \\frac{\\#rooms - 2}{5(range)} to make -0.5 \\le x_1 \\le 0.5, -0.5 \\le x_2 \\le 0.5, -0.5 \\le x_1 \\le 0.5, -0.5 \\le x_2 \\le 0.5, Learning rate \u00b6 Another idea to improve the gradient descent algorithm is to select the learning rate \\alpha \\alpha to make the algorithm works correctly. Convergence test: we can declare convergence if J(\\theta) J(\\theta) decreases by less than 10^{-3} 10^{-3} in one iteration. For sufficient small \\alpha \\alpha , J(\\theta) J(\\theta) should decrease on every iteration, but if \\alpha \\alpha is too small, gradient descent can be slow to converge. If \\alpha \\alpha is too large: J(\\theta) J(\\theta) may not decrease on every iteration, may not converge. Rule of Thumb: to choose \\alpha \\alpha , try these numbers ..., 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, ... Polynomial Regression \u00b6 In the house price prediction proble. if two features selected are frontage and depth. we can also take the polynomial problem, take the area as a single feature, thusly reduce the problem to a linear regression. We could even take the higher polynomials of the size feature, like second order, third order, and so on. for example, h_\\theta = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\theta_3x_3 = \\theta_0 + \\theta_1(size) + \\theta_2(size)^2 + \\theta_3(size)^3 h_\\theta = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\theta_3x_3 = \\theta_0 + \\theta_1(size) + \\theta_2(size)^2 + \\theta_3(size)^3 In this case, the polynomial regression problem will fit in a curve instead of a line. Normal Equation \u00b6 Normal Equation: Method to solve for \u03b8 analytically. The intuition comes that we can mathematically sovle \\frac{\\partial}{\\partial\\theta_j}J(\\Theta) = 0 \\frac{\\partial}{\\partial\\theta_j}J(\\Theta) = 0 for \\theta_0, \\theta_1, \\dots, \\theta_n \\theta_0, \\theta_1, \\dots, \\theta_n , given the J(\\Theta) J(\\Theta) . The equation to get the value of \\Theta \\Theta is \\Theta = (X^TX)^{-1}X^Ty \\Theta = (X^TX)^{-1}X^Ty , in Octave, you can calculate by the command pinv(X'*X)*X'*y) , pay attention to the dimention of the the matrix X and y . Normal equation and non-invertibility \u00b6 TBD Week 3 Logistic regression model \u00b6 Classification problem \u00b6 Email: spam / Not spam? Tumor: Malignant / Benign? Example: y is either 0 or 1 0: Negative class 1: Positive class. With linear regression and a threshold value, we can use linear regression to solve classification problem. However linear regression could result in the case when h_\\theta(x) h_\\theta(x) is > 1 or < 0. Need a different method which will make 0 \\le h_\\theta(x) \\le 1 0 \\le h_\\theta(x) \\le 1 . This is why logistic regression comes in. Hypothesis representation \u00b6 Because we want to have 0 \\le h_\\theta(x) \\le 1 0 \\le h_\\theta(x) \\le 1 , the domain of sigmoid function is in the range. From linear regression h_\\theta(x) = \\theta^Tx h_\\theta(x) = \\theta^Tx , we can have h_\\theta(x) h_\\theta(x) for logistic regression: h_{\\theta}(x) = g_\\theta(\\theta^Tx) = \\frac{1}{1 + e^{-\\theta^Tx}}, 0 \\le h_\\theta(x) \\le 1 h_{\\theta}(x) = g_\\theta(\\theta^Tx) = \\frac{1}{1 + e^{-\\theta^Tx}}, 0 \\le h_\\theta(x) \\le 1 g_\\theta(z) = \\frac{1}{1 + e^{-z}} g_\\theta(z) = \\frac{1}{1 + e^{-z}} is Sigmoid function , also called logistic function. now in logistic regress model, we can write h_\\theta(x) = \\frac{1}{1 + e^{-\\theta^Tx}}. h_\\theta(x) = \\frac{1}{1 + e^{-\\theta^Tx}}. Interpretation \u00b6 Logistic regression model h_\\theta(x) = \\frac{1}{1 + e^{-\\theta^Tx}}, h_\\theta(x) = \\frac{1}{1 + e^{-\\theta^Tx}}, is the estimated probability that y = 1 on input x. i.e. h_\\theta(x) = 0.7 h_\\theta(x) = 0.7 tell patient that 70% chance of tumore being malignant. h_\\theta(x) = P(y = 1|x;\\theta) h_\\theta(x) = P(y = 1|x;\\theta) , P(y=0|x; \\theta) + P(y=1|x;\\theta) = 1 P(y=0|x; \\theta) + P(y=1|x;\\theta) = 1 Logistic regression decision boundary \u00b6 Gives a better sense of what the hypothesis function is computing Better understanding of what the hypothesis function looks like One way of using the sigmoid function is: When the probability of y being 1 is greater than 0.5 then we can predict y = 1 Else we predict y = 0 When is it exactly that h_\\theta(x) h_\\theta(x) is greater than 0.5? Look at sigmoid function g(z) is greater than or equal to 0.5 when z is greater than or equal to 0. So if z is positive, g(z) is greater than 0.5 z = (\\theta^T x) z = (\\theta^T x) So when \\theta^T x >= 0 \\theta^T x >= 0 Then h_\\theta(x) >= 0.5 h_\\theta(x) >= 0.5 So what we've shown is that the hypothesis predicts y = 1 when \\theta^T x >= 0 \\theta^T x >= 0 The corollary of that when \\theta^T x <= 0 \\theta^T x <= 0 then the hypothesis predicts y = 0 Let's use this to better understand how the hypothesis makes its predictions Linear Decision boundary and non-linear decision boundary Logistic regression cost function \u00b6 Problem description \u00b6 Logistic Regression Cost Function \u00b6 Out goal to solve a logistic regression problem is to reduce the cost incurred when predict wrong result. In linear regression, we minimize the cost function with respect to vector \\theta \\theta . Generally, we can think the cost function as a penalty of the incorrect classification. It is a qualitative measure of such penalty. For example, we use the squared error cost function in linear regression: J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})^2. J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})^2. In logistic regression, the h_\\theta(x) h_\\theta(x) is a much complex function, so the cost function J(\\theta) J(\\theta) used in linear regression will be a non-convex function in logisitic regress. This will produce a hard problem to solve logistic problems numerically. So we define a convex logistic regression cost function Cost(h_\\theta(x), y) =\\begin{cases} -log(h_\\theta(x)) & \\text{if}\\ y = 1\\\\ -log(1-h_\\theta(x)) & \\text{if}\\ y = 0 \\end{cases} Cost(h_\\theta(x), y) =\\begin{cases} -log(h_\\theta(x)) & \\text{if}\\ y = 1\\\\ -log(1-h_\\theta(x)) & \\text{if}\\ y = 0 \\end{cases} It is the logistic regression cost function, it can be interpreted as the penalty the algorithm pays. We can plot the function as following: Some intuition/properties about the simplified logistic regression cost function: X axis is what we predict Y axis is the cost associated with that prediction. If y = 1, and h_\\theta(x) = 1 h_\\theta(x) = 1 , hypothesis predicts exactly 1 (exactly correct) then cost corresponds to 0 (no cost for correct predict) If y = 1, and h_\\theta(x) = 0 h_\\theta(x) = 0 , predict P(y = 1|x; \\theta) = 0 P(y = 1|x; \\theta) = 0 , this is wrong, and it penalized with a massive cost (the cost approach positive infinity). Similar reasoning holds for y = 0. Gradient descent and cost function \u00b6 We can neatly write the logistic regression function in the following format: cost(h_\\theta(x), y) = -y\\log(h_\\theta(x)) - (1 - y)\\log(1-h_\\theta(x)) cost(h_\\theta(x), y) = -y\\log(h_\\theta(x)) - (1 - y)\\log(1-h_\\theta(x)) We can take this cost function and obtained the logistic regression cost function J(\\theta) J(\\theta) : \\begin{align*} J(\\theta) & = \\frac{1}{m}\\sum_{i=1}^{m}Cost(h_\\theta(x^{(i)}),y^{(i)}) \\\\ & = -\\frac{1}{m}\\Big[\\sum_{i=1}^{m}y^{(i)}\\log(h_\\theta(x^{(i)})) + (1 - y^{(i)})\\log(1-h_\\theta(x^{(i)}))\\Big] \\end{align*} \\begin{align*} J(\\theta) & = \\frac{1}{m}\\sum_{i=1}^{m}Cost(h_\\theta(x^{(i)}),y^{(i)}) \\\\ & = -\\frac{1}{m}\\Big[\\sum_{i=1}^{m}y^{(i)}\\log(h_\\theta(x^{(i)})) + (1 - y^{(i)})\\log(1-h_\\theta(x^{(i)}))\\Big] \\end{align*} Why do we chose this function when other cost functions exist? This cost function can be derived from statistics using the principle of maximum likelihood estimation . Note this does mean there's an underlying Gaussian assumption relating to the distribution of features. Also has the nice property that it's convex function. Gradient Descent for logistic regression \u00b6 The algorithm looks identical to linear regression except the hypothesis function is Sigmoid function and not linear any more. Advanced Optimization \u00b6 Beside gradient descent algorithm, there are many other optimization algorithm such as conjugate gradient, BFGS, and L-BFGS, can minimization the cost function for solving logistic regression problem. Most of those advanced algorithms are more efficient to compute, you don't have to select the convergent rate \\alpha \\alpha . Here is one function in Octave library, possibly also in Matlab, can be used for finding the \\theta \\theta Function fminunc() \u00b6 Octave have a function fminunc() . ^1 To use it, we should first call optimset() . There is three steps we need to take care to solve optimization problem using these functions calculate the cost function J(\\theta) J(\\theta) calculate the gradient functions \\frac{\\partial}{\\partial\\theta_j}J(\\theta) \\frac{\\partial}{\\partial\\theta_j}J(\\theta) give initial \\theta \\theta value call the function optimset() and fminunc() as following function [jVal, gradient] = costFunction ( theta ) jVal = ( theta ( 1 ) - 5 ) ^ 2 + ( theta ( 2 ) - 5 ) ^ 2 ; gradient = zeros ( 2 , 1 ); gradient ( 1 ) = 2 * ( theta ( 1 ) - 5 ); gradient ( 2 ) = 2 * ( theta ( 2 ) - 5 ); options = optimset ( \u2018 GradObj \u2019 , \u2018 on \u2019 , \u2018 MaxIter \u2019 , \u2018 100 \u2019 ); initialTheta = zeros ( 2 , 1 ); [ optTheta , functionVal , exitFlag ] = fminunc (@ costFunction , initialTheta , options ); Note we have n+1 parameters, we implement the code in the following way. Multiclass classification \u00b6 Train a logistic regression classifier h_{\\theta}^{(i)}(x) h_{\\theta}^{(i)}(x) for each class i i to predict the probability that y = i y = i . On a new input x x , to make a prediction, pick the class i i that maximize the h h , \\operatorname*{argmax}_i h_{\\theta}^{(i)}(x) \\operatorname*{argmax}_i h_{\\theta}^{(i)}(x) Regularization \u00b6 Overfitting If we have too many features, the learned hypothesis may fit the training set very well ( J(\\theta) \\approx 0 J(\\theta) \\approx 0 ), but fail to generalize to new examples (predict prices on new examples). Address overfitting problems \u00b6 Reduce the number of features Manually select what features to keep feature selection algorithm Regularization Keep all the features, reduce the value of the parameter \\theta_j \\theta_j . work well when we have a lot of data when each of the features contribute to the algorithm a bit to predict y y . Regularization Intuition \u00b6 To penalize the higher order of polynomial by adding extra terms of high coefficient for \\theta^n \\theta^n terms. i.e. from the cost function minimization problem, J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})^2 J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})^2 , we can instead use the regularized form J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})^2 + \\lambda\\sum_{j = 1}^{n}\\theta_j^2 J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})^2 + \\lambda\\sum_{j = 1}^{n}\\theta_j^2 . Small values for parameters \\theta_j \\theta_j will make \"Simpler\" hypothesis and Less prone to overfitting. Please note that the \\theta_0 \\theta_0 is excluded from the regularization term \\lambda\\sum_{n = 1}^{n}\\theta_j^2 \\lambda\\sum_{n = 1}^{n}\\theta_j^2 . Regularized linear regression \u00b6 Gradient Descent: We don't regularize \\theta_0 \\theta_0 , so we explicitly update it in the formula and it is the same with the non-regularized linear regression gradient descent. All other \\theta_j \\theta_j , j = 1, ..., n j = 1, ..., n will update differently. Regularized logistic regression \u00b6 Doesn't regularize the \\theta_0 \\theta_0 The cost function is different from the linear regression Advanced optimization \u00b6 Add the regularized term in the cost function and gradient calculation Week 4 Neural networks model \u00b6 Neural Networks is originated when people try to mimic the functionality of brain by an algorithm. Model representation \u00b6 In an artificial neural network, a neuron is a logistic unit that Feed input via input wires Logistic unit does the computation Sends output down output wires The logistic computation is just like our previous logistic regression hypothesis calculation. Neural Network \u00b6 Use the following notation convention: a_i^{(j)} a_i^{(j)} to represent the \\\"activation\\\" of unit i i in layer j j \\Theta^{(j)} \\Theta^{(j)} to represent the matrix of weights controlling function mapping from layer j j to layer j+1 j+1 The value at each node can be calculated as a_1^{(2)} = g(\\theta_{10}^{(1)}x_0 + \\theta_{11}^{(1)}x_1 + \\theta_{12}^{(1)}x_2 + \\theta_{13}^{(1)}x_3) \\\\ a_2^{(2)} = g(\\theta_{20}^{(1)}x_0 + \\theta_{21}^{(1)}x_1 + \\theta_{22}^{(1)}x_2 + \\theta_{23}^{(1)}x_3) \\\\ a_3^{(2)} = g(\\theta_{30}^{(1)}x_0 + \\theta_{31}^{(1)}x_1 + \\theta_{32}^{(1)}x_2 + \\theta_{33}^{(1)}x_3) \\\\ h_{\\Theta}(x) = a_1^{(3)} = g(\\theta_{10}^{(2)}a_0^{(2)} + \\theta_{11}^{(2)}a_1^{(2)} + \\theta_{12}^{(2)}a_2^{(2)} + \\theta_{13}^{(2)}a_3^{(2)}) a_1^{(2)} = g(\\theta_{10}^{(1)}x_0 + \\theta_{11}^{(1)}x_1 + \\theta_{12}^{(1)}x_2 + \\theta_{13}^{(1)}x_3) \\\\ a_2^{(2)} = g(\\theta_{20}^{(1)}x_0 + \\theta_{21}^{(1)}x_1 + \\theta_{22}^{(1)}x_2 + \\theta_{23}^{(1)}x_3) \\\\ a_3^{(2)} = g(\\theta_{30}^{(1)}x_0 + \\theta_{31}^{(1)}x_1 + \\theta_{32}^{(1)}x_2 + \\theta_{33}^{(1)}x_3) \\\\ h_{\\Theta}(x) = a_1^{(3)} = g(\\theta_{10}^{(2)}a_0^{(2)} + \\theta_{11}^{(2)}a_1^{(2)} + \\theta_{12}^{(2)}a_2^{(2)} + \\theta_{13}^{(2)}a_3^{(2)}) If the network has s_j s_j units in layer j , s_{j+1} s_{j+1} units in layer j+1 , then \\Theta^{(j)} \\Theta^{(j)} will be of dimension s_{j+1} \\times (s_j + 1) s_{j+1} \\times (s_j + 1) . It could be interpreted as \"the dimension of \\Theta^{(j)} \\Theta^{(j)} is number of nodes in the next layer \\times \\times the current layer's node + 1\". Forward propagation implementation \u00b6 By defining z_1^{(2)} z_1^{(2)} , z_1^{(2)} z_1^{(2)} , and z_1^{(2)} z_1^{(2)} , we could obtain a_1^{(2)} = g(z_1^{(2)}) a_1^{(2)} = g(z_1^{(2)}) , where z_1^{(2)} = \\theta_{10}^{(1)}x_0 + \\theta_{11}^{(1)}x_1 + \\theta_{12}^{(1)}x_2 + \\theta_{13}^{(1)}x_3 z_1^{(2)} = \\theta_{10}^{(1)}x_0 + \\theta_{11}^{(1)}x_1 + \\theta_{12}^{(1)}x_2 + \\theta_{13}^{(1)}x_3 . To make it more compact, we define x = \\left[ \\begin{array}{c} x_0 \\\\ x_1 \\\\ x_2 \\\\ x_3 \\end{array} \\right] x = \\left[ \\begin{array}{c} x_0 \\\\ x_1 \\\\ x_2 \\\\ x_3 \\end{array} \\right] and z^{(2)} = \\left[ \\begin{array}{c}z_1^{(2)} \\\\ z_2^{(2)} \\\\ z_3^{(2)} \\end{array} \\right] z^{(2)} = \\left[ \\begin{array}{c}z_1^{(2)} \\\\ z_2^{(2)} \\\\ z_3^{(2)} \\end{array} \\right] , so we have reached the following vectorized implementation of neural network. It is also called forward propagation algorithm . Input: x x Forward Propagation Algorithm: z^{(2)} = \\Theta^{(1)}x z^{(2)} = \\Theta^{(1)}x a^{(2)} = g(z^{(2)}) a^{(2)} = g(z^{(2)}) and a_0^{(2)} =1 a_0^{(2)} =1 . z^{(3)} = \\Theta^{(3)}a^{(2)} z^{(3)} = \\Theta^{(3)}a^{(2)} h_{\\Theta}(x) = a^{(3)} = g(z^{(3)}) h_{\\Theta}(x) = a^{(3)} = g(z^{(3)}) Learning its own features \u00b6 What neural network is doing is it just like logistic regression, rather than using hte original feature, x_1 x_1 , x_2 x_2 , x_3 x_3 , it use new feature a_1^{(2)} a_1^{(2)} , a_1^{(2)} a_1^{(2)} , a_1^{(2)} a_1^{(2)} . Those new feature are learned from the original input x_1 x_1 , x_2 x_2 , x_3 x_3 . XNOR example \u00b6 We calculate a Non-linear classification example: XOR/XNOR, where x_1 x_1 and x_2 x_2 are binary 0 or 1. y = x_1 \\text{ XNOR } x_2 = \\text{ NOT } (x_1 \\text{ XOR } x_2) y = x_1 \\text{ XNOR } x_2 = \\text{ NOT } (x_1 \\text{ XOR } x_2) . But those are all build on the basic non-linear operation AND , OR , and NOT . Don't worry about how we get the \\theta \\theta . This example just to give some intuitions of how neural network problem can be solved. Multiclass classification \u00b6 Suppose our algorithm is to recognize pedestrian, car, motorbike or truck, we need to build a neural network with four output units. We could use a vector to do this When image is a pedestrian get [1,0,0,0] and so on. 1 is 0/1 pedestrain 2 is 0/1 car 3 is 0/1 motorcycle 4 is 0/1 truck Just like one vs. all classifier described earlier. here we have four logistic regression classifiers Contrast to the previous notation we wrote y y as an integer {1, 2, 3, 4} {1, 2, 3, 4} to represent four classes. Now we use the following notation to represent y^{(i)} y^{(i)} as one of \\left[ \\begin{array}{c} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{array} \\right] \\left[ \\begin{array}{c} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{array} \\right] , \\left[ \\begin{array}{c} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{array} \\right] \\left[ \\begin{array}{c} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{array} \\right] , \\left[ \\begin{array}{c} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{array} \\right] \\left[ \\begin{array}{c} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{array} \\right] , \\left[ \\begin{array}{c} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{array} \\right] \\left[ \\begin{array}{c} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{array} \\right] . Week 5 Neural networks learning \u00b6 Neural network classification \u00b6 The cost function of neural network would be a generalized form of the regularized logistic regression cost function as shown below. Cost Function \u00b6 Logistic regression \u00b6 J(\\theta) = -\\frac{1}{m}\\Bigg[\\sum_{i=1}^{n}y^{(i)}\\log(h_\\theta(x^{(i)})) + (1 - y^{(i)})\\log(1-h_\\theta(x^{(i)}))\\Bigg] + \\frac{\\lambda}{2m}\\sum_{j=1}^{n}\\theta_j^2 J(\\theta) = -\\frac{1}{m}\\Bigg[\\sum_{i=1}^{n}y^{(i)}\\log(h_\\theta(x^{(i)})) + (1 - y^{(i)})\\log(1-h_\\theta(x^{(i)}))\\Bigg] + \\frac{\\lambda}{2m}\\sum_{j=1}^{n}\\theta_j^2 Neural Network \u00b6 We have the hypothesis in the K dimensional Euclidean space, denoted as h_{\\Theta}(x) \\in \\mathbb{R}^K h_{\\Theta}(x) \\in \\mathbb{R}^K , K is the number of output units. (h_{\\Theta}(x))_i = i^{th} (h_{\\Theta}(x))_i = i^{th} output. J(\\Theta) = -\\frac{1}{m}\\Bigg[\\sum_{i=1}^{m}\\sum_{k=1}^Ky_k^{(i)}\\log(h_\\Theta(x^{(i)}))_k + (1 - y_k^{(i)})\\log(1-(h_\\Theta(x^{(i)}))_k)\\Bigg] + \\frac{\\lambda}{2m}\\sum_{l=1}^{L-1}\\sum_{i=1}^{s_l}\\sum_{j=1}^{s_{l+1}}(\\Theta_{ji}^{(l)})^2 J(\\Theta) = -\\frac{1}{m}\\Bigg[\\sum_{i=1}^{m}\\sum_{k=1}^Ky_k^{(i)}\\log(h_\\Theta(x^{(i)}))_k + (1 - y_k^{(i)})\\log(1-(h_\\Theta(x^{(i)}))_k)\\Bigg] + \\frac{\\lambda}{2m}\\sum_{l=1}^{L-1}\\sum_{i=1}^{s_l}\\sum_{j=1}^{s_{l+1}}(\\Theta_{ji}^{(l)})^2 Noticed that all the summation is start from 1 not 0 . For example, the index k and j . Because we don't regularize the bias unit which is with subscript 0 . Gradient computation \u00b6 We have the cost function for the neural network. Next step we need to calculate the gradient of the cost function. As we can see that the cost function is too complicated to derive its gradient. Here we will use back propagation method to calculate the partial derivative. Before dive into the details of the back propagation, we need to summarize some of the ideas we had for the neural network so far. Forward propagation \u00b6 Forward propagation takes initial input into that neural network and pushes the input through the neural network It leads to the generation of an output hypothesis, which may be a single real number, but can be a vector. Back propagation \u00b6 The appearance of the back propagation algorithm isn't very natural. Instead, it is a very smart way to solve the optimization problem in neural network. Because it is difficult to follow the method we used in linear regression or logistic regression to analytically get the derivative of the cost function. Back propagation come to help solving the optimization problem numerically. To compute the partial derivative \\frac{\\partial}{\\partial\\Theta_{i,j}^{(l)}}J(\\Theta) \\frac{\\partial}{\\partial\\Theta_{i,j}^{(l)}}J(\\Theta) (the gradient of cost function in neural network), thus allow us to find the parameter \\Theta \\Theta that minimize the cost J(\\Theta) J(\\Theta) (using gradient descent or advanced optimization algorithm), which will result in a hypothesis with parameters learn from the existing data. How to derive the back-prop \u00b6 We define \\delta_{j}^{(l)} = \\delta_{j}^{(l)} = \\\"error\\\" of node j j in layer l l We calculate \\delta^{(L)} = a^{(L)} - y \\delta^{(L)} = a^{(L)} - y , which is the vector of errors defined above at last layer L L . Note \\delta^{(L)} \\delta^{(L)} is vectorized representation of \\delta_{j}^{(l)} \\delta_{j}^{(l)} . So our \"error values\" for the last layer are simply the differences of our actual results a^{(L)} a^{(L)} in the last layer and the correct outputs in y y . To get the delta values of the layers before the last layer, we can use an equation that steps us back from right to left, thereby back propagate , by \\delta^{(l)} = ((\\Theta^{(l)})^T\\delta^{(l+1)}).*g'(z^{(l)}) \\delta^{(l)} = ((\\Theta^{(l)})^T\\delta^{(l+1)}).*g'(z^{(l)}) . It can be interpreted as the error value of layer l l are calculated by multiplying the error values in the next layer with theta matrix of layer l l . We then element wise multiply that with the derivative of the activation function a^{(l)} = g(z^{(l)}) a^{(l)} = g(z^{(l)}) \uff0c namely g'(z^{(l)}) = a^{(l)}.*(1-a^{(l)}). g'(z^{(l)}) = a^{(l)}.*(1-a^{(l)}). The derivation of this deviative can be found at the course wiki. Partial derivative. With all the calculated \\delta^{(l)} \\delta^{(l)} values, We can compute our partial derivative terms by multiplying our activation values and our error values for each training example t t , \\dfrac{\\partial}{\\partial \\Theta_{i,j}^{(l)}}J(\\Theta) = \\frac{1}{m}\\sum_{t=1}^m a_j^{(t)(l)} {\\delta}_i^{(t)(l+1)} \\dfrac{\\partial}{\\partial \\Theta_{i,j}^{(l)}}J(\\Theta) = \\frac{1}{m}\\sum_{t=1}^m a_j^{(t)(l)} {\\delta}_i^{(t)(l+1)} . This is ignoring regularization terms. Notations: t t is the index of the training examples, we have total of m m samples. l l is the layer, we have total layer of L L . i i is the error affected node in the target layer, layer l+1 l+1 . j j is the node in the current layer l l . \\delta^{(l+1)} \\delta^{(l+1)} and \\ a^{(l+1)} \\ a^{(l+1)} are vectors with s_{l+1} s_{l+1} elements. Similarly, \\ a^{(l)} \\ a^{(l)} is a vector with s_{l} s_{l} elements. Multiplying them produces a matrix that is s_{l+1} s_{l+1} by s_{l} s_{l} which is the same dimension as \\Theta^{(l)} \\Theta^{(l)} . That is, the process produces a gradient term for every element in \\Theta^{(l)} \\Theta^{(l)} . | Backpropagation Algorithm \u00b6 Given training set \\lbrace (x^{(1)}, y^{(1)}) \\cdots (x^{(m)}, y^{(m)})\\rbrace \\lbrace (x^{(1)}, y^{(1)}) \\cdots (x^{(m)}, y^{(m)})\\rbrace Set \\Delta^{(l)}_{i,j} := 0 \\Delta^{(l)}_{i,j} := 0 for all (l,i,j) (l,i,j) For training example t = 1 t = 1 to m m : Set a^{(1)} := x^{(t)} a^{(1)} := x^{(t)} Perform forward propagation to compute a^{(l)} a^{(l)} for l = 2,3,\\dots ,L l = 2,3,\\dots ,L Using y^{(t)} y^{(t)} , compute \\delta^{(L)} = a^{(L)} - y^{(t)} \\delta^{(L)} = a^{(L)} - y^{(t)} Compute \\delta^{(L-1)}, \\delta^{(L-2)},\\dots,\\delta^{(2)} \\delta^{(L-1)}, \\delta^{(L-2)},\\dots,\\delta^{(2)} using \\delta^{(l)} = ((\\Theta^{(l)})^T \\delta^{(l+1)})\\ .*\\ a^{(l)}\\ .*\\ (1 - a^{(l)}) \\delta^{(l)} = ((\\Theta^{(l)})^T \\delta^{(l+1)})\\ .*\\ a^{(l)}\\ .*\\ (1 - a^{(l)}) \\Delta^{(l)}_{i,j} := \\Delta^{(l)}_{i,j} + a_j^{(l)} \\delta_i^{(l+1)} \\Delta^{(l)}_{i,j} := \\Delta^{(l)}_{i,j} + a_j^{(l)} \\delta_i^{(l+1)} or with vectorization, \\Delta^{(l)} := \\Delta^{(l)} + \\delta^{(l+1)}(a^{(l)})^T \\Delta^{(l)} := \\Delta^{(l)} + \\delta^{(l+1)}(a^{(l)})^T (programmatic calculation of \\frac{1}{m}\\sum_{t=1}^m a_j^{(t)(l)} {\\delta}_i^{(t)(l+1)} \\frac{1}{m}\\sum_{t=1}^m a_j^{(t)(l)} {\\delta}_i^{(t)(l+1)} ) D^{(l)}_{i,j} := \\dfrac{1}{m}\\left(\\Delta^{(l)}_{i,j} + \\lambda\\Theta^{(l)}_{i,j}\\right) D^{(l)}_{i,j} := \\dfrac{1}{m}\\left(\\Delta^{(l)}_{i,j} + \\lambda\\Theta^{(l)}_{i,j}\\right) If j \\neq 0 j \\neq 0 (NOTE: Typo in lecture slide omits outside parentheses. This version is correct.) D^{(l)}_{i,j} := \\dfrac{1}{m}\\Delta^{(l)}_{i,j} D^{(l)}_{i,j} := \\dfrac{1}{m}\\Delta^{(l)}_{i,j} If j = 0 j = 0 The capital-delta matrix is used as an \\\"accumulator\\\" to add up our values as we go along and eventually compute our partial derivative. The actual proof is quite involved, but, the D^{(l)}_{i,j} D^{(l)}_{i,j} terms are the partial derivatives and the results we are looking for: D_{i,j}^{(l)} = \\dfrac{\\partial J(\\Theta)}{\\partial \\Theta_{i,j}^{(l)}}. D_{i,j}^{(l)} = \\dfrac{\\partial J(\\Theta)}{\\partial \\Theta_{i,j}^{(l)}}. Math representation \u00b6 There is a \\Theta \\Theta matrix for each layer in the network This has each node in layer l l as one dimension and each node in l+1 l+1 as the other dimension Similarly, there is going to be a \\Delta \\Delta matrix for each layer This has each node as one dimension and each training data example as the other A high level description \u00b6 Back propagation basically takes the output you got from your network, compares it to the real value (y) and calculates how wrong the network was (i.e. how wrong the parameters were) It then, using the error you\\'ve just calculated, back-calculates the error associated with each unit from right to left. This goes on until you reach the input layer (where obviously there is no error, as the activation is the input) These \\\"error\\\" measurements for each unit can be used to calculate the partial derivatives Partial derivatives are the bomb, because gradient descent needs them to minimize the cost function We use the partial derivatives with gradient descent to try minimize the cost function and update all the \\Theta \\Theta values This repeats until gradient descent reports convergence What we need to watch out? \u00b6 We don\\'t have \\delta^{(1)} \\delta^{(1)} because we have all x x , it is zero. We don\\'t calculate the bias node. Backpropagation Intuition \u00b6 Backpropagation very similar to the forward propagation, we can use a example to calculate the \\lambda_i^{(\\ell)} \\lambda_i^{(\\ell)} or the z^{(\\ell)} z^{(\\ell)} as bellow. To further understand the algorithm, refer to the code in assignment 4. As it is shown in the slide, z_1^{(2)} z_1^{(2)} is calculate by multiply the corresponding \\Theta^{(2)}_{ij} \\Theta^{(2)}_{ij} value. the index i i is the same as in z_i^{(2)} z_i^{(2)} . The index j j means the j j th node in the previous layer. Unrolling parameters \u00b6 With neural networks, we are working with sets of matrices: \\Theta^{(1)} \\Theta^{(1)} , \\Theta^{(2)} \\Theta^{(2)} , \\Theta^{(3)}, \\dots \\Theta^{(3)}, \\dots . D^{(1)}, D^{(1)}, D^{(3)}, \\dots D^{(1)}, D^{(1)}, D^{(3)}, \\dots In order to use optimizing functions such as \"fminunc()\", we will want to \"unroll\" all the elements and put them into one long vector # unroll the matrices thetaVector = [ Theta1 (:); Theta2 (:); Theta3 (:); ] deltaVector = [ D1 (:); D2 (:); D3 (:) ] If the dimensions of Theta1 is 10x11, Theta2 is 10x11 and Theta3 is 1x11, then we can get back our original matrices from the \"unrolled\" versions as follows # using reshape function to build matrix from vectors Theta1 = reshape ( thetaVector ( 1 : 110 ), 10 , 11 ) Theta2 = reshape ( thetaVector ( 111 : 220 ), 10 , 11 ) Theta3 = reshape ( thetaVector ( 221 : 231 ), 1 , 11 ) In Octave, if we want to implement the back propagation algorithm, we have to do the unrolling and reshaping back and forth in order to calculate the gradientVec . Notice that the last two lines are shifted in the picture, it should read \"Use forward propagation and back propagation to compute D^{(1)}, D^{(1)}, D^{(3)} D^{(1)}, D^{(1)}, D^{(3)} and J(\\Theta) J(\\Theta) . Unroll D^{(1)}, D^{(1)}, D^{(3)} D^{(1)}, D^{(1)}, D^{(3)} to get gradientVec . Gradient Checking \u00b6 To ensure a bug free back propagation implementation, one method we can use is gradient checking. It is a way of numerically estimate the gradient value in each computation point. It works as a reference value to check the back propagation is correct. In Octave, we can implement this gradient checking as follows. We then want to check that gradApprox \\approx \\approx deltaVector. Remove the gradient checking when you checking is done, otherwise the code is going to be very slow. epsilon = 1e-4 ; for i = 1 : n , thetaPlus = theta ; thetaPlus ( i ) += epsilon ; thetaMinus = theta ; thetaMinus ( i ) -= epsilon ; gradApprox ( i ) = ( J ( thetaPlus ) - J ( thetaMinus )) / ( 2 * epsilon ) end ; Random Initialization \u00b6 Initializing all theta weights to zero does not work with neural networks. When we back propagate, all nodes will update to the same value repeatedly. Instead we can randomly initialize our weights: initialize each \\Theta_{ij}^{(l)} \\Theta_{ij}^{(l)} to a random value between [-\\epsilon, \\epsilon] [-\\epsilon, \\epsilon] . In Octave, we can use the following code to generate the initial theta values. #If the dimensions of Theta1 is 10x11, Theta2 is 10x11 and Theta3 is 1x11. Theta1 = rand ( 10 , 11 ) * ( 2 * INIT_EPSILON ) - INIT_EPSILON ; Theta2 = rand ( 10 , 11 ) * ( 2 * INIT_EPSILON ) - INIT_EPSILON ; Theta3 = rand ( 1 , 11 ) * ( 2 * INIT_EPSILON ) - INIT_EPSILON ; rand(x,y) will initialize a matrix of random real numbers between 0 and 1. (Note: this \\epsilon \\epsilon is unrelated to the \\epsilon \\epsilon from Gradient Checking) Putting it Together \u00b6 First, pick a network architecture; choose the layout of your neural network, including how many hidden units in each layer and how many layers total. Number of input units = dimension of features x^{(i)} x^{(i)} Number of output units = number of classes Number of hidden units per layer = usually more the better (must balance with the cost of computation as it increases with more hidden units) Defaults: 1 hidden layer. If more than 1 hidden layer, then the same number of units in every hidden layer. Week 6 Advice for applying machine learning \u00b6 Deciding what to try next \u00b6 Get more training examples Try smaller sets\\'of features Try getting additional features Try adding polynomial features Try decreasing \\lambda \\lambda Try increasing \\lambda \\lambda Machine learning diagnostic \u00b6 Diagnostic : A test that you can run to gain insight what is/isn\\'t working with a learning algorithm, and gain guidance as to how best to improve its performance. It can take time to implement, but doing so can be a very good use of your time. Evaluating a hypothesis \u00b6 Diagnostic over-fitting by splitting the training set and test set Training set (70%) Test set (30%) If the cost function J(\\theta) J(\\theta) calculated for training set is low, it is high for test set, over-fitting happens. This is apply to both logistic regression and linear regression. Model selection and Train/Validation/Test sets \u00b6 Normally we choose a model from a series of different degree of polynomial hypothesis, We choose the model that have the least cost for testing data. More specifically, we learned the parameter \\theta^{(d)} \\theta^{(d)} (d is the degree of polynomial) from the training set and calculate the cost for the test dataset with the learned \\theta \\theta . This is not the best practice for the reason that the hypothesis we selected has been fit to the test set. It is very likely the model will not fit new data very well. We can improve the model selection by introduce the cross validation set. We divide our available data as Training set (60%) Cross validation set (20%) test set (20%) We use the cross validation data to help us to select the model, and using the test data, which the model never see before to evaluate generalization error. Warning Avoid to use the test data to select the model (select the least cost model calculated on the test data set) and then report the generalization error using the test data. Diagnosing bias vs. variance \u00b6 The important point to understand bias and variance is to understand how the cost for training data, cross validation data (or test data) changes. Regularization and bias/variance \u00b6 The regularization parameter \\lambda \\lambda could play important role in selecting the best model to fit the data (both cross validation data and test data). This section will study how \\lambda \\lambda affect the cost function both for the training data and cross validation data. Learning curve \u00b6 This section mainly discuss how the sample size affect the cost function. There are two kinds of problem we face, high bias and high variance High bias \u00b6 High variance \u00b6 Deciding what to do next revisited \u00b6 bias/variance underfitting/overfitting regularization to improve High bias underfitting bigger lambda lambda more sample will not help much High variance overfitting (high polynomial) smaller lambda lambda more sample will likely to help Prioritizing what to work on \u00b6 Building a spam classifier \u00b6 Error analysis \u00b6 Start simple algorithm, implement it and test it on cross-validation data. Plot learning curves to decide if more data, more features, etc. are like to help. Error analysis: Manually examine the examples (in cross validation set) that you algorithm make errors on. See if you spot any systematic trend in what type of examples it it making error on. Error metrics for skewed classes \u00b6 Problem arise in error analysis when only very small(less than 0.5%) negative samples possibly exist. Take the cancer classification as a example. Precision and Recall \u00b6 Trading off precision and recall \u00b6 F score {#f_score} \u00b6 Data for machine learning \u00b6 Week 7 Support Vector Machine \u00b6 Optimization objective \u00b6 SVM will be derived from logistic regression. There are not that much of logic, just following the derivation in the lecture. Let's first review the hypothesis of the logistic regress and its cost function: Notice in the second figure above, we define approximate functions cost_0(z) cost_0(z) and cost_1(z) cost_1(z) . These functions will be used in the SVM cost function we will see next. SVM cost function \u00b6 Compare to logistic regression, SVM use a different parameters C C . As in logistic regression, we use \\lambda \\lambda to control the trade off optimizing the first term and the second term in the objective function. What's more, the SVM hypothesis doesn\\'t give us a probability prediction, it give use either 1 or 0 . Such as h_\\theta = \\begin{cases} 1 \\qquad \\text{if}\\ \\Theta^Tx \\geq 0\\\\ 0 \\qquad \\text{otherwise} \\end{cases} h_\\theta = \\begin{cases} 1 \\qquad \\text{if}\\ \\Theta^Tx \\geq 0\\\\ 0 \\qquad \\text{otherwise} \\end{cases} Large margin intuition \u00b6 Suppose we make the parameter C C very large, we want to make the first term in the summation approximate 0 . We can derive the problem as a optimization problem subject to some condition. This optimization problem will lead us to discuss the decision boundary. when C C is very large, the decision boundary will have a large margin, thus large margin classifier. Later we will see that using different kernel and parameters can control this margin. Question How the large margin classifier related to the optimization problem when C is very large? When C C is very large, we have the magneto decision boundary, if C C is not very large, we will have the black decision boundary, which is having the large margin properties. Mathematics behind large margin classification \u00b6 Using the vector inner product to transform the constrain from \\theta^T x^{(i)} \\geq 1 \\theta^T x^{(i)} \\geq 1 to p^{(i)}\\Vert\\theta \\Vert \\geq 1 p^{(i)}\\Vert\\theta \\Vert \\geq 1 for y^{(i)} = 1 y^{(i)} = 1 The point of this section is to discuss the mathematical insight why SVM choose the large margin decision boundary. (because, the large margin decision boundary will produce a larger p^{(i)} p^{(i)} , thus a smaller \\Vert\\theta \\Vert \\Vert\\theta \\Vert in the constrain of the optimization problem, so as to minimize the objective function \\min_\\theta \\frac{1}{2} \\sum_{j=1}^{n} \\theta_j^2 \\min_\\theta \\frac{1}{2} \\sum_{j=1}^{n} \\theta_j^2 ) Kernel \u00b6 Non-linear decision boundary \u00b6 Is there a different/better choice of the features f_1, f_2, f_3, \\dots, f_1, f_2, f_3, \\dots, Landmarks \u00b6 To respond the above question, we use the following way to generate the new features f_1, f_2, f_3, \\dots, f_1, f_2, f_3, \\dots, . Compare the similarity between the examples and the landmarks using a similarity function. Gaussian kernel \u00b6 We use the Guassian kernel k(x, \\ell^{(i)}) = \\exp(-\\frac{\\Vert x - \\ell^{(i)}\\Vert^2}{2\\delta^2}) k(x, \\ell^{(i)}) = \\exp(-\\frac{\\Vert x - \\ell^{(i)}\\Vert^2}{2\\delta^2}) as the similarity function to generate the new feature. Choosing the landmarks \u00b6 Naturally, we are going to choose the example itself as the landmark, thus we are able to transform the feature x^{(i)} x^{(i)} to feature vector f^{(i)} f^{(i)} for training example (x^{(i)}, y^{(i)}) (x^{(i)}, y^{(i)}) . Using SVM \u00b6 Choice of parameter C C . Choice of kernel (similarity function): No kernel (\"linear Kernel\") Gaussian kernel (need to choose \\delta^2 \\delta^2 ) Because the similarity function is apply to each feature, we\\'d better to perform feature scaling before using the Gaussian kernel. (take the house price features as an example, the areas and the number of rooms are not in the same scale.) Note Not all similarity functions similarity(x, \\ell) similarity(x, \\ell) make valid kernels. Need to satisfy technical condition called \"Mercer's Theorem\" to make sure SVM packagesl' optimizations run correctly, and do not diverge). Many off-the-shelf kernels available: Polynomial kernel: k(X, \\ell) = (X^T\\ell + c)^{p} k(X, \\ell) = (X^T\\ell + c)^{p} , such as k(x, \\ell) = (X^T\\ell + 1)^2. k(x, \\ell) = (X^T\\ell + 1)^2. More esoteric: String kernel, chi-square kernel, histogram intersection kernel, ... Note Many SVM packages already have built-in multi-class classification functionality. Otherwise, we can use one-vs.-all method. Specifically, train K K SVMs, one to distinguish y = i y = i from the rest, for i = 1, 2, \\dots, K i = 1, 2, \\dots, K , get \\theta^{(1)}, \\theta^{(2)}, \\dots, \\theta^{(K)} \\theta^{(1)}, \\theta^{(2)}, \\dots, \\theta^{(K)} , pick class i i with largest (\\theta^{(i)})^Tx (\\theta^{(i)})^Tx . Logistic regression vs. SMVs \u00b6 This section mainly discuss when to use logistic regression vs. SVM from the high level. I summarize the content of the slides in the table. n m logistic regression vs. SVMs large (relative to mm) i.e. 10,000 10 - 1000 use logistic regression, or SVM without a kerenl (\"linear kernel\") small (i.e. 1-1000) intermediate (10-10,000) use SVM with Gaussian kernel small (i.e. 1-1000) large (50,000+) create/add more features, then use logistic regression or SVM without a kernel Week 8 Unsupervised Algorithms \u00b6 K-Means Algorithm \u00b6 In the k-means clustering algorithm. We have input K K (number of clusters) and traning set \\{x^{(1)}, x^{(1)}, \\dots, x^{(m)}\\} \\{x^{(1)}, x^{(1)}, \\dots, x^{(m)}\\} , for each of the training example x^{(i)}\\in \\R^n x^{(i)}\\in \\R^n . Note here we drop the x_0 = 1 x_0 = 1 convention. To visualize the algorithm, we have the following three picture. Concretely, the k-means algorithm is: Input: training set \\! \\{x^{(i)}\\}_{i=1}^m, \\! \\{x^{(i)}\\}_{i=1}^m, and number of clusters K K . Algorith: Randomly initialize K K cluster centroids \\mu_1, \\mu_1, \\dots, \\mu_K\\in \\R^n \\mu_1, \\mu_1, \\dots, \\mu_K\\in \\R^n . Repeat: for i = 1 i = 1 to m m : // cluster assignment step c^{(i)} c^{(i)} := index (from 1 to K K ) of cluster centroid closest to x^{(i)} x^{(i)} for k = 1 k = 1 to K K : // move centroid step \\mu_k \\mu_k := average (mean) of points assigned to cluster k k Until convergence Optimization objective \u00b6 Random initialization \u00b6 We can random initialize the centroid by randomly pick K K training examples and set \\mu_1, \\dots, \\mu_K \\mu_1, \\dots, \\mu_K equal to these K K examples. Obviously, we should have K < m K < m . However, some of the random initialization would lead the algorithm to achieve local optima, which is shown below in the pictures. To solve this problem, we can random initialize multiple times then to pick clustering that gave lowest cost J(c^{(1)}, \\dots, c^{(m)}, \\mu_1, \\dots \\mu_K) J(c^{(1)}, \\dots, c^{(m)}, \\mu_1, \\dots \\mu_K) . Choosing the K \u00b6 It depends on the problem we are trying to solve. One straightforward problem is using a method called the Elbow method as the following picture. Secondly, the K value should be picked to maximize some practical utility function, for example, we cluster the size of different T-shirts, whether we want to have 3 sizes or 5 sizes is depends on the profitability. Dimensionality reduction \u00b6 The two motivation of Dimensionality Reduction are: Data compression, for example, reduce data from 2D to 1D. Visualization, we can reduce the high dimensionality data to 2 or 3 dimensions, and visualize them. For example, to visualize some complex economical data for different countries. Principal Component Analysis (PCA) \u00b6 To reduce from 2-dimension to 1-dimension: Find a direction (a vector u^{(1)}\\in \\R^n u^{(1)}\\in \\R^n ) onto which to project the data so as to minimize the projection error. To reduce from n-dimension to k-dimension: Find k direction vectors u^{(1)}, u^{(2)}, \\dots, u^{(k)} u^{(1)}, u^{(2)}, \\dots, u^{(k)} onto which to project the data so as to minimize the projection error. Note Note the difference between PCA and linear regression. PCA algorithm \u00b6 Before carry out the PCA, we\\'d better to do some feature scaling/mean normalization. As in the slides below, the S_i S_i could be the maximum/minimum value or other values such as variance \\sigma \\sigma . once the pre-proccessing steps are done we can run the following algorithm in Octave to implement PCA. Reconstruction from PCA compressed data \u00b6 Choosing the number of principal components \u00b6 Two concepts we should define here: Average squared projection error: \\frac{1}{m}\\sum_{i=1}^{m}\\lVert x^{(i)} - x_{approx}^{(i)}\\rVert^2 \\frac{1}{m}\\sum_{i=1}^{m}\\lVert x^{(i)} - x_{approx}^{(i)}\\rVert^2 Total variation in the data: \\frac{1}{m}\\sum_{i=1}^{m}\\lVert x^{(i)} \\rVert^2 \\frac{1}{m}\\sum_{i=1}^{m}\\lVert x^{(i)} \\rVert^2 Typically, we choose k k to be smallest value so that ration of the two quality should be less than 1%. Advice for applying PCA \u00b6 One important point should keep in mind when using PCA: don't try to use PCA to prevent overfitting, use regularization instead. Week 9 Anomaly Detection \u00b6 Anomaly Detection \u00b6 Given a model p(x) p(x) , test on the example x^{(i)} x^{(i)} to check whether we have p(x) < \\epsilon p(x) < \\epsilon . Generally, the anomaly detection system apply to the scenario that we don't have much anomaly example, such as a dataset about aircraft engines. In this particular lecture, the p(x) p(x) is multivariate Gaussian distribution . The algorithm is to find the parameter \\mu \\mu and \\sigma^2 \\sigma^2 . Once we fit the data with a multivariate Gaussian distribution, we are able to obtain a probability value, which can be use to detect the anomaly by select a probability threshold \\epsilon \\epsilon . Here is the anomaly detection algorithm: Developing and evaluating \u00b6 Anomaly detection vs. supervised learning \u00b6 Multivariate Gaussian distribution \u00b6 There are plots of different multivariate Gaussian distributions with different mean and covariance matrix. It intuitively show how changes in the mean and covariance matrix can change the shape of the plot of the distribution. It also compared the original model (multiple single variate Gaussian distribution) to multivariate Gaussian distribution. Generally, if we have multiple features, we tend to use multivariate Gaussian distribution to fit the data, even thought the original model is more computationally cheaper. Predicting movie ratings \u00b6 Problem formulation \u00b6 Content based recommendations \u00b6 Suppose we have a feature vector for each of the movie, combining with the rating values, we can solve the minimization problem to get \\theta^{(j)} \\theta^{(j)} , which is the parameter vector of user j j . With this parameter vector, we can predicting the rating of the movie by (\\theta^{(j)})^T x^{(i)} (\\theta^{(j)})^T x^{(i)} . Collaborate filtering \u00b6 In collaborate filtering, we don\\'t have the feature, we only have the parameter vector \\theta^{(j)} \\theta^{(j)} for user j j . We can solve a optimization problem in regarding to the feature vector x^{(i)} x^{(i)} through the rating values. What interesting about this if we don\\'t have the initial \\theta^{(j)} \\theta^{(j)} , we can generate a small random value of \\theta^{(j)} \\theta^{(j)} , and repetitively to get the feature vector x^{(i)} x^{(i)} . As later we can say, we can solve a optimization problem with regarding to both \\theta^{(j)} \\theta^{(j)} and x^{(i)} x^{(i)} all at once. See the slides for details. Collaborative filtering algorithm \u00b6 As discussed above, in practice, we solve the optimization problem respect to both \\theta^{(j)} \\theta^{(j)} and x^{(i)} x^{(i)} . See the slides for the optimization problem we are trying to solve, and the gradient descent algorithm to solve it.","title":"Machine Learning (Coursera)"},{"location":"courses/machine-learning-coursera/notes/#machine-learning-coursera","text":"","title":"Machine Learning (Coursera)"},{"location":"courses/machine-learning-coursera/notes/#week-1-introduction","text":"Quote Machine Learning is a field of study that gives computers the ability to learn without being explicitly programmed. -- Arthur Samuel (1959) Quote A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E. -- Tom Mitchell (1998) Well-posed Learning Problem Experience E Task T Performance P Supervised learning: give the \"right answer\" of existing Unsupervised learning Reinforcement learning Recommendation system Regression V.S. classification Regression, predict a continuous value across range a function of independent variable Classification, predict a discrete value","title":"Week 1 Introduction"},{"location":"courses/machine-learning-coursera/notes/#cost-function","text":"Cost function (squared error function) J(\\theta_0, \\theta_1) = \\frac{1}{2m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})^2 J(\\theta_0, \\theta_1) = \\frac{1}{2m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})^2 Goal \\underset{\\theta_0, \\theta_1}{\\text{minimize}} J(\\theta_0, \\theta_1) \\underset{\\theta_0, \\theta_1}{\\text{minimize}} J(\\theta_0, \\theta_1) Some Intuitions when the sample is {1, 1}, {2, 2}, and {3, 3} . h_\\theta(x) = x, (\\theta_0 = 0, \\theta_1 = 1) h_\\theta(x) = x, (\\theta_0 = 0, \\theta_1 = 1) , we can calculate J(\\theta_0, \\theta_1) = 0. J(\\theta_0, \\theta_1) = 0. when the sample is {1, 1}, {2, 2}, and {3, 3} . h_\\theta(x) = 0.5 x, (\\theta_0 = 0, \\theta_1 = 0.5) h_\\theta(x) = 0.5 x, (\\theta_0 = 0, \\theta_1 = 0.5) , we can calculate J(\\theta_1) = \\frac{3.5}{6}. J(\\theta_1) = \\frac{3.5}{6}. we can plot J(\\theta_1) J(\\theta_1) vs. \\theta_1 \\theta_1 , the function plotted would be a quadratic curve.","title":"Cost Function"},{"location":"courses/machine-learning-coursera/notes/#cost-function-intuition-i","text":"Andrew plot the hypothesis \\textstyle h_\\theta(x) \\textstyle h_\\theta(x) and the cost function \\textstyle {J(\\theta_1)} \\textstyle {J(\\theta_1)} \uff08setting \\theta_0 = 0 \\theta_0 = 0 \uff09, The function \\textstyle J(\\theta_1) \\textstyle J(\\theta_1) is a curve have a minimum value.","title":"Cost Function Intuition I"},{"location":"courses/machine-learning-coursera/notes/#cost-function-intuition-ii","text":"The plot of h(x) h(x) and J(\\theta_0, \\theta_1) J(\\theta_0, \\theta_1) show that J(\\theta_0, \\theta_1) J(\\theta_0, \\theta_1) has to be contour plot. Our goal is to \\min_{\\theta_0, \\theta_1}J(\\theta_0, \\theta_1) \\min_{\\theta_0, \\theta_1}J(\\theta_0, \\theta_1) To summarize, in this lecture, we formulate the hypothesis function and defined the cost function. We also plotted them to get some insight into the two functions. The machine learning problem we learn from this lecture is a minimization problem. Given a hypothesis, we looking for the \\theta_0 \\theta_0 and \\theta_1 \\theta_1 which minimize the cost function. Then we solve the problem. We use the Gradient Descent algorithm to search this minimum value.","title":"Cost Function Intuition II"},{"location":"courses/machine-learning-coursera/notes/#gradient-descent","text":"It could solve general minimization problems Given function J(\\theta_0, \\theta_1) J(\\theta_0, \\theta_1) Objective: \\min_{\\theta_0, \\theta_1}J(\\theta_0, \\theta_1) \\min_{\\theta_0, \\theta_1}J(\\theta_0, \\theta_1) Gradient Descent Algorithm Start with some \\theta_0, \\theta_1 \\theta_0, \\theta_1 Keep changing \\theta_0, \\theta_1 \\theta_0, \\theta_1 to reduce J(\\theta_0, \\theta_1) J(\\theta_0, \\theta_1) Simultianeously update: \\theta_j := \\theta_j - \\alpha \\frac{\\partial }{\\partial \\theta_j}J(\\theta_0, \\theta_1) \\theta_j := \\theta_j - \\alpha \\frac{\\partial }{\\partial \\theta_j}J(\\theta_0, \\theta_1) , for ( j = 0, j = 1) ( j = 0, j = 1) Untill we hopefully end up at the minimum.(convergence) Question: How to implement the algorithm.","title":"Gradient Descent"},{"location":"courses/machine-learning-coursera/notes/#gradient-descent-intuition","text":"learning rate don't have to adjust the learning rate derivative would reduce automatically","title":"Gradient Descent Intuition"},{"location":"courses/machine-learning-coursera/notes/#gradient-descent-and-linear-regression","text":"Take the linear regression cost function and apply gradient descent algorithm. Model: h_\\theta(x) = \\theta_0 + \\theta_1x h_\\theta(x) = \\theta_0 + \\theta_1x Cost function: J(\\theta_0, \\theta_1) = \\frac{1}{2m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})^2 J(\\theta_0, \\theta_1) = \\frac{1}{2m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})^2 Gradient Descent in Linear Regression. repeat \\begin{Bmatrix} \\theta_0 := \\theta_0 - \\alpha \\frac{1}{m} \\sum_{i = 1}^{m}(h_\\theta(x^{(i)}) - y^{(i)}) \\\\ \\theta_1 := \\theta_1 - \\alpha \\frac{1}{m} \\sum_{i = 1}^{m}(h_\\theta(x^{(i)}) - y^{(i)}) x^{(i)} \\end{Bmatrix} \\begin{Bmatrix} \\theta_0 := \\theta_0 - \\alpha \\frac{1}{m} \\sum_{i = 1}^{m}(h_\\theta(x^{(i)}) - y^{(i)}) \\\\ \\theta_1 := \\theta_1 - \\alpha \\frac{1}{m} \\sum_{i = 1}^{m}(h_\\theta(x^{(i)}) - y^{(i)}) x^{(i)} \\end{Bmatrix} simultaneously. The linear regression cost function is always a convex function - always has a single minimum (bowl Shaped).","title":"Gradient Descent and Linear Regression"},{"location":"courses/machine-learning-coursera/notes/#linear-algebra-review","text":"Not commutative: A x B != B x A except that matrix B is identity matrix. Associativity: (A x B) x C = A x (B x C) Identity Matrix: Any matrix A which can be multiplied by an identity matrix gives you matrix A back I[m, m] x A[m, n] = A[m, n] A[m, n] x I[n, n] = A[m,n] Inverse Multiplicative inverse(reciprocal): a number multiply it by to get the identify element. i.e. if you have x , x * 1/x = 1 . Additive inverse: -3 is 3's additive inverse. Matrix inverse A \\cdot A^{-1} = I A \\cdot A^{-1} = I Only square matrix, but not all square matrix have an inverse. singularity If A is all zeros then there is no inverse matrix Some others don't, intuition should be matrices that don't have an inverse are a singular matrix or a degenerate matrix (i.e. when it's too close to 0) So if all the values of a matrix reach zero, this can be described as reaching singularity","title":"Linear Algebra Review"},{"location":"courses/machine-learning-coursera/notes/#week-2-multiple-featuresvariables-linear-regression","text":"From feature x^{(i)} x^{(i)} to features x_j^{(i)} x_j^{(i)} , means the j j th feature element of the i i th feature vector. Hypothesis becomes: \\textstyle h_\\theta(x) = \\theta^{T}x = \\theta_0x_0 + \\theta_1x_1 + ... + \\theta_nx_n \\textstyle h_\\theta(x) = \\theta^{T}x = \\theta_0x_0 + \\theta_1x_1 + ... + \\theta_nx_n . Cost Function: J(\\theta_0, \\theta_1, ..., \\theta_n) = \\frac{1}{2m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})^2 J(\\theta_0, \\theta_1, ..., \\theta_n) = \\frac{1}{2m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})^2 Gradient descent: \\theta_j := \\theta_j - \\alpha \\frac{\\partial }{\\partial \\theta_j}J(\\theta_0, \\theta_1, ..., \\theta_n) \\theta_j := \\theta_j - \\alpha \\frac{\\partial }{\\partial \\theta_j}J(\\theta_0, \\theta_1, ..., \\theta_n) , simultaneously update for every j = 0, 1, 2, ..., n j = 0, 1, 2, ..., n . Compare of Gradient descent for multiple variables:","title":"Week 2 Multiple features(variables) linear regression"},{"location":"courses/machine-learning-coursera/notes/#feature-scaling","text":"One of the idea to improve the result in gradient descent is to make different features in the same scale, i.e. -1 \\leq x_i \\leq 1 -1 \\leq x_i \\leq 1 range . For example, in the housing price prediction problem, we have to make the size of the house (0-2000 square feet), and the number of rooms in the house (1-5 rooms) in the same scale. We do this by the method of mean normalization. Mean normalization Replace x_i x_i with x_i - \\mu_i x_i - \\mu_i to make features have approximately zero mean (Do not apply to x_0 = 1 x_0 = 1 ) E.g. x_1 = \\frac{size - 1000}{2000} x_1 = \\frac{size - 1000}{2000} , x_2 = \\frac{\\#rooms - 2}{5(range)} x_2 = \\frac{\\#rooms - 2}{5(range)} to make -0.5 \\le x_1 \\le 0.5, -0.5 \\le x_2 \\le 0.5, -0.5 \\le x_1 \\le 0.5, -0.5 \\le x_2 \\le 0.5,","title":"Feature scaling"},{"location":"courses/machine-learning-coursera/notes/#learning-rate","text":"Another idea to improve the gradient descent algorithm is to select the learning rate \\alpha \\alpha to make the algorithm works correctly. Convergence test: we can declare convergence if J(\\theta) J(\\theta) decreases by less than 10^{-3} 10^{-3} in one iteration. For sufficient small \\alpha \\alpha , J(\\theta) J(\\theta) should decrease on every iteration, but if \\alpha \\alpha is too small, gradient descent can be slow to converge. If \\alpha \\alpha is too large: J(\\theta) J(\\theta) may not decrease on every iteration, may not converge. Rule of Thumb: to choose \\alpha \\alpha , try these numbers ..., 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, ...","title":"Learning rate"},{"location":"courses/machine-learning-coursera/notes/#polynomial-regression","text":"In the house price prediction proble. if two features selected are frontage and depth. we can also take the polynomial problem, take the area as a single feature, thusly reduce the problem to a linear regression. We could even take the higher polynomials of the size feature, like second order, third order, and so on. for example, h_\\theta = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\theta_3x_3 = \\theta_0 + \\theta_1(size) + \\theta_2(size)^2 + \\theta_3(size)^3 h_\\theta = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\theta_3x_3 = \\theta_0 + \\theta_1(size) + \\theta_2(size)^2 + \\theta_3(size)^3 In this case, the polynomial regression problem will fit in a curve instead of a line.","title":"Polynomial Regression"},{"location":"courses/machine-learning-coursera/notes/#normal-equation","text":"Normal Equation: Method to solve for \u03b8 analytically. The intuition comes that we can mathematically sovle \\frac{\\partial}{\\partial\\theta_j}J(\\Theta) = 0 \\frac{\\partial}{\\partial\\theta_j}J(\\Theta) = 0 for \\theta_0, \\theta_1, \\dots, \\theta_n \\theta_0, \\theta_1, \\dots, \\theta_n , given the J(\\Theta) J(\\Theta) . The equation to get the value of \\Theta \\Theta is \\Theta = (X^TX)^{-1}X^Ty \\Theta = (X^TX)^{-1}X^Ty , in Octave, you can calculate by the command pinv(X'*X)*X'*y) , pay attention to the dimention of the the matrix X and y .","title":"Normal Equation"},{"location":"courses/machine-learning-coursera/notes/#normal-equation-and-non-invertibility","text":"TBD","title":"Normal equation and non-invertibility"},{"location":"courses/machine-learning-coursera/notes/#week-3-logistic-regression-model","text":"","title":"Week 3 Logistic regression model"},{"location":"courses/machine-learning-coursera/notes/#classification-problem","text":"Email: spam / Not spam? Tumor: Malignant / Benign? Example: y is either 0 or 1 0: Negative class 1: Positive class. With linear regression and a threshold value, we can use linear regression to solve classification problem. However linear regression could result in the case when h_\\theta(x) h_\\theta(x) is > 1 or < 0. Need a different method which will make 0 \\le h_\\theta(x) \\le 1 0 \\le h_\\theta(x) \\le 1 . This is why logistic regression comes in.","title":"Classification problem"},{"location":"courses/machine-learning-coursera/notes/#hypothesis-representation","text":"Because we want to have 0 \\le h_\\theta(x) \\le 1 0 \\le h_\\theta(x) \\le 1 , the domain of sigmoid function is in the range. From linear regression h_\\theta(x) = \\theta^Tx h_\\theta(x) = \\theta^Tx , we can have h_\\theta(x) h_\\theta(x) for logistic regression: h_{\\theta}(x) = g_\\theta(\\theta^Tx) = \\frac{1}{1 + e^{-\\theta^Tx}}, 0 \\le h_\\theta(x) \\le 1 h_{\\theta}(x) = g_\\theta(\\theta^Tx) = \\frac{1}{1 + e^{-\\theta^Tx}}, 0 \\le h_\\theta(x) \\le 1 g_\\theta(z) = \\frac{1}{1 + e^{-z}} g_\\theta(z) = \\frac{1}{1 + e^{-z}} is Sigmoid function , also called logistic function. now in logistic regress model, we can write h_\\theta(x) = \\frac{1}{1 + e^{-\\theta^Tx}}. h_\\theta(x) = \\frac{1}{1 + e^{-\\theta^Tx}}.","title":"Hypothesis representation"},{"location":"courses/machine-learning-coursera/notes/#interpretation","text":"Logistic regression model h_\\theta(x) = \\frac{1}{1 + e^{-\\theta^Tx}}, h_\\theta(x) = \\frac{1}{1 + e^{-\\theta^Tx}}, is the estimated probability that y = 1 on input x. i.e. h_\\theta(x) = 0.7 h_\\theta(x) = 0.7 tell patient that 70% chance of tumore being malignant. h_\\theta(x) = P(y = 1|x;\\theta) h_\\theta(x) = P(y = 1|x;\\theta) , P(y=0|x; \\theta) + P(y=1|x;\\theta) = 1 P(y=0|x; \\theta) + P(y=1|x;\\theta) = 1","title":"Interpretation"},{"location":"courses/machine-learning-coursera/notes/#logistic-regression-decision-boundary","text":"Gives a better sense of what the hypothesis function is computing Better understanding of what the hypothesis function looks like One way of using the sigmoid function is: When the probability of y being 1 is greater than 0.5 then we can predict y = 1 Else we predict y = 0 When is it exactly that h_\\theta(x) h_\\theta(x) is greater than 0.5? Look at sigmoid function g(z) is greater than or equal to 0.5 when z is greater than or equal to 0. So if z is positive, g(z) is greater than 0.5 z = (\\theta^T x) z = (\\theta^T x) So when \\theta^T x >= 0 \\theta^T x >= 0 Then h_\\theta(x) >= 0.5 h_\\theta(x) >= 0.5 So what we've shown is that the hypothesis predicts y = 1 when \\theta^T x >= 0 \\theta^T x >= 0 The corollary of that when \\theta^T x <= 0 \\theta^T x <= 0 then the hypothesis predicts y = 0 Let's use this to better understand how the hypothesis makes its predictions Linear Decision boundary and non-linear decision boundary","title":"Logistic regression decision boundary"},{"location":"courses/machine-learning-coursera/notes/#logistic-regression-cost-function","text":"","title":"Logistic regression cost function"},{"location":"courses/machine-learning-coursera/notes/#problem-description","text":"","title":"Problem description"},{"location":"courses/machine-learning-coursera/notes/#logistic-regression-cost-function_1","text":"Out goal to solve a logistic regression problem is to reduce the cost incurred when predict wrong result. In linear regression, we minimize the cost function with respect to vector \\theta \\theta . Generally, we can think the cost function as a penalty of the incorrect classification. It is a qualitative measure of such penalty. For example, we use the squared error cost function in linear regression: J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})^2. J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})^2. In logistic regression, the h_\\theta(x) h_\\theta(x) is a much complex function, so the cost function J(\\theta) J(\\theta) used in linear regression will be a non-convex function in logisitic regress. This will produce a hard problem to solve logistic problems numerically. So we define a convex logistic regression cost function Cost(h_\\theta(x), y) =\\begin{cases} -log(h_\\theta(x)) & \\text{if}\\ y = 1\\\\ -log(1-h_\\theta(x)) & \\text{if}\\ y = 0 \\end{cases} Cost(h_\\theta(x), y) =\\begin{cases} -log(h_\\theta(x)) & \\text{if}\\ y = 1\\\\ -log(1-h_\\theta(x)) & \\text{if}\\ y = 0 \\end{cases} It is the logistic regression cost function, it can be interpreted as the penalty the algorithm pays. We can plot the function as following: Some intuition/properties about the simplified logistic regression cost function: X axis is what we predict Y axis is the cost associated with that prediction. If y = 1, and h_\\theta(x) = 1 h_\\theta(x) = 1 , hypothesis predicts exactly 1 (exactly correct) then cost corresponds to 0 (no cost for correct predict) If y = 1, and h_\\theta(x) = 0 h_\\theta(x) = 0 , predict P(y = 1|x; \\theta) = 0 P(y = 1|x; \\theta) = 0 , this is wrong, and it penalized with a massive cost (the cost approach positive infinity). Similar reasoning holds for y = 0.","title":"Logistic Regression Cost Function"},{"location":"courses/machine-learning-coursera/notes/#gradient-descent-and-cost-function","text":"We can neatly write the logistic regression function in the following format: cost(h_\\theta(x), y) = -y\\log(h_\\theta(x)) - (1 - y)\\log(1-h_\\theta(x)) cost(h_\\theta(x), y) = -y\\log(h_\\theta(x)) - (1 - y)\\log(1-h_\\theta(x)) We can take this cost function and obtained the logistic regression cost function J(\\theta) J(\\theta) : \\begin{align*} J(\\theta) & = \\frac{1}{m}\\sum_{i=1}^{m}Cost(h_\\theta(x^{(i)}),y^{(i)}) \\\\ & = -\\frac{1}{m}\\Big[\\sum_{i=1}^{m}y^{(i)}\\log(h_\\theta(x^{(i)})) + (1 - y^{(i)})\\log(1-h_\\theta(x^{(i)}))\\Big] \\end{align*} \\begin{align*} J(\\theta) & = \\frac{1}{m}\\sum_{i=1}^{m}Cost(h_\\theta(x^{(i)}),y^{(i)}) \\\\ & = -\\frac{1}{m}\\Big[\\sum_{i=1}^{m}y^{(i)}\\log(h_\\theta(x^{(i)})) + (1 - y^{(i)})\\log(1-h_\\theta(x^{(i)}))\\Big] \\end{align*} Why do we chose this function when other cost functions exist? This cost function can be derived from statistics using the principle of maximum likelihood estimation . Note this does mean there's an underlying Gaussian assumption relating to the distribution of features. Also has the nice property that it's convex function.","title":"Gradient descent and cost function"},{"location":"courses/machine-learning-coursera/notes/#gradient-descent-for-logistic-regression","text":"The algorithm looks identical to linear regression except the hypothesis function is Sigmoid function and not linear any more.","title":"Gradient Descent for logistic regression"},{"location":"courses/machine-learning-coursera/notes/#advanced-optimization","text":"Beside gradient descent algorithm, there are many other optimization algorithm such as conjugate gradient, BFGS, and L-BFGS, can minimization the cost function for solving logistic regression problem. Most of those advanced algorithms are more efficient to compute, you don't have to select the convergent rate \\alpha \\alpha . Here is one function in Octave library, possibly also in Matlab, can be used for finding the \\theta \\theta","title":"Advanced Optimization"},{"location":"courses/machine-learning-coursera/notes/#function-fminunc","text":"Octave have a function fminunc() . ^1 To use it, we should first call optimset() . There is three steps we need to take care to solve optimization problem using these functions calculate the cost function J(\\theta) J(\\theta) calculate the gradient functions \\frac{\\partial}{\\partial\\theta_j}J(\\theta) \\frac{\\partial}{\\partial\\theta_j}J(\\theta) give initial \\theta \\theta value call the function optimset() and fminunc() as following function [jVal, gradient] = costFunction ( theta ) jVal = ( theta ( 1 ) - 5 ) ^ 2 + ( theta ( 2 ) - 5 ) ^ 2 ; gradient = zeros ( 2 , 1 ); gradient ( 1 ) = 2 * ( theta ( 1 ) - 5 ); gradient ( 2 ) = 2 * ( theta ( 2 ) - 5 ); options = optimset ( \u2018 GradObj \u2019 , \u2018 on \u2019 , \u2018 MaxIter \u2019 , \u2018 100 \u2019 ); initialTheta = zeros ( 2 , 1 ); [ optTheta , functionVal , exitFlag ] = fminunc (@ costFunction , initialTheta , options ); Note we have n+1 parameters, we implement the code in the following way.","title":"Function fminunc()"},{"location":"courses/machine-learning-coursera/notes/#multiclass-classification","text":"Train a logistic regression classifier h_{\\theta}^{(i)}(x) h_{\\theta}^{(i)}(x) for each class i i to predict the probability that y = i y = i . On a new input x x , to make a prediction, pick the class i i that maximize the h h , \\operatorname*{argmax}_i h_{\\theta}^{(i)}(x) \\operatorname*{argmax}_i h_{\\theta}^{(i)}(x)","title":"Multiclass classification"},{"location":"courses/machine-learning-coursera/notes/#regularization","text":"Overfitting If we have too many features, the learned hypothesis may fit the training set very well ( J(\\theta) \\approx 0 J(\\theta) \\approx 0 ), but fail to generalize to new examples (predict prices on new examples).","title":"Regularization"},{"location":"courses/machine-learning-coursera/notes/#address-overfitting-problems","text":"Reduce the number of features Manually select what features to keep feature selection algorithm Regularization Keep all the features, reduce the value of the parameter \\theta_j \\theta_j . work well when we have a lot of data when each of the features contribute to the algorithm a bit to predict y y .","title":"Address overfitting problems"},{"location":"courses/machine-learning-coursera/notes/#regularization-intuition","text":"To penalize the higher order of polynomial by adding extra terms of high coefficient for \\theta^n \\theta^n terms. i.e. from the cost function minimization problem, J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})^2 J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})^2 , we can instead use the regularized form J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})^2 + \\lambda\\sum_{j = 1}^{n}\\theta_j^2 J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})^2 + \\lambda\\sum_{j = 1}^{n}\\theta_j^2 . Small values for parameters \\theta_j \\theta_j will make \"Simpler\" hypothesis and Less prone to overfitting. Please note that the \\theta_0 \\theta_0 is excluded from the regularization term \\lambda\\sum_{n = 1}^{n}\\theta_j^2 \\lambda\\sum_{n = 1}^{n}\\theta_j^2 .","title":"Regularization Intuition"},{"location":"courses/machine-learning-coursera/notes/#regularized-linear-regression","text":"Gradient Descent: We don't regularize \\theta_0 \\theta_0 , so we explicitly update it in the formula and it is the same with the non-regularized linear regression gradient descent. All other \\theta_j \\theta_j , j = 1, ..., n j = 1, ..., n will update differently.","title":"Regularized linear regression"},{"location":"courses/machine-learning-coursera/notes/#regularized-logistic-regression","text":"Doesn't regularize the \\theta_0 \\theta_0 The cost function is different from the linear regression","title":"Regularized logistic regression"},{"location":"courses/machine-learning-coursera/notes/#advanced-optimization_1","text":"Add the regularized term in the cost function and gradient calculation","title":"Advanced optimization"},{"location":"courses/machine-learning-coursera/notes/#week-4-neural-networks-model","text":"Neural Networks is originated when people try to mimic the functionality of brain by an algorithm.","title":"Week 4 Neural networks model"},{"location":"courses/machine-learning-coursera/notes/#model-representation","text":"In an artificial neural network, a neuron is a logistic unit that Feed input via input wires Logistic unit does the computation Sends output down output wires The logistic computation is just like our previous logistic regression hypothesis calculation.","title":"Model representation"},{"location":"courses/machine-learning-coursera/notes/#neural-network","text":"Use the following notation convention: a_i^{(j)} a_i^{(j)} to represent the \\\"activation\\\" of unit i i in layer j j \\Theta^{(j)} \\Theta^{(j)} to represent the matrix of weights controlling function mapping from layer j j to layer j+1 j+1 The value at each node can be calculated as a_1^{(2)} = g(\\theta_{10}^{(1)}x_0 + \\theta_{11}^{(1)}x_1 + \\theta_{12}^{(1)}x_2 + \\theta_{13}^{(1)}x_3) \\\\ a_2^{(2)} = g(\\theta_{20}^{(1)}x_0 + \\theta_{21}^{(1)}x_1 + \\theta_{22}^{(1)}x_2 + \\theta_{23}^{(1)}x_3) \\\\ a_3^{(2)} = g(\\theta_{30}^{(1)}x_0 + \\theta_{31}^{(1)}x_1 + \\theta_{32}^{(1)}x_2 + \\theta_{33}^{(1)}x_3) \\\\ h_{\\Theta}(x) = a_1^{(3)} = g(\\theta_{10}^{(2)}a_0^{(2)} + \\theta_{11}^{(2)}a_1^{(2)} + \\theta_{12}^{(2)}a_2^{(2)} + \\theta_{13}^{(2)}a_3^{(2)}) a_1^{(2)} = g(\\theta_{10}^{(1)}x_0 + \\theta_{11}^{(1)}x_1 + \\theta_{12}^{(1)}x_2 + \\theta_{13}^{(1)}x_3) \\\\ a_2^{(2)} = g(\\theta_{20}^{(1)}x_0 + \\theta_{21}^{(1)}x_1 + \\theta_{22}^{(1)}x_2 + \\theta_{23}^{(1)}x_3) \\\\ a_3^{(2)} = g(\\theta_{30}^{(1)}x_0 + \\theta_{31}^{(1)}x_1 + \\theta_{32}^{(1)}x_2 + \\theta_{33}^{(1)}x_3) \\\\ h_{\\Theta}(x) = a_1^{(3)} = g(\\theta_{10}^{(2)}a_0^{(2)} + \\theta_{11}^{(2)}a_1^{(2)} + \\theta_{12}^{(2)}a_2^{(2)} + \\theta_{13}^{(2)}a_3^{(2)}) If the network has s_j s_j units in layer j , s_{j+1} s_{j+1} units in layer j+1 , then \\Theta^{(j)} \\Theta^{(j)} will be of dimension s_{j+1} \\times (s_j + 1) s_{j+1} \\times (s_j + 1) . It could be interpreted as \"the dimension of \\Theta^{(j)} \\Theta^{(j)} is number of nodes in the next layer \\times \\times the current layer's node + 1\".","title":"Neural Network"},{"location":"courses/machine-learning-coursera/notes/#forward-propagation-implementation","text":"By defining z_1^{(2)} z_1^{(2)} , z_1^{(2)} z_1^{(2)} , and z_1^{(2)} z_1^{(2)} , we could obtain a_1^{(2)} = g(z_1^{(2)}) a_1^{(2)} = g(z_1^{(2)}) , where z_1^{(2)} = \\theta_{10}^{(1)}x_0 + \\theta_{11}^{(1)}x_1 + \\theta_{12}^{(1)}x_2 + \\theta_{13}^{(1)}x_3 z_1^{(2)} = \\theta_{10}^{(1)}x_0 + \\theta_{11}^{(1)}x_1 + \\theta_{12}^{(1)}x_2 + \\theta_{13}^{(1)}x_3 . To make it more compact, we define x = \\left[ \\begin{array}{c} x_0 \\\\ x_1 \\\\ x_2 \\\\ x_3 \\end{array} \\right] x = \\left[ \\begin{array}{c} x_0 \\\\ x_1 \\\\ x_2 \\\\ x_3 \\end{array} \\right] and z^{(2)} = \\left[ \\begin{array}{c}z_1^{(2)} \\\\ z_2^{(2)} \\\\ z_3^{(2)} \\end{array} \\right] z^{(2)} = \\left[ \\begin{array}{c}z_1^{(2)} \\\\ z_2^{(2)} \\\\ z_3^{(2)} \\end{array} \\right] , so we have reached the following vectorized implementation of neural network. It is also called forward propagation algorithm . Input: x x Forward Propagation Algorithm: z^{(2)} = \\Theta^{(1)}x z^{(2)} = \\Theta^{(1)}x a^{(2)} = g(z^{(2)}) a^{(2)} = g(z^{(2)}) and a_0^{(2)} =1 a_0^{(2)} =1 . z^{(3)} = \\Theta^{(3)}a^{(2)} z^{(3)} = \\Theta^{(3)}a^{(2)} h_{\\Theta}(x) = a^{(3)} = g(z^{(3)}) h_{\\Theta}(x) = a^{(3)} = g(z^{(3)})","title":"Forward propagation implementation"},{"location":"courses/machine-learning-coursera/notes/#learning-its-own-features","text":"What neural network is doing is it just like logistic regression, rather than using hte original feature, x_1 x_1 , x_2 x_2 , x_3 x_3 , it use new feature a_1^{(2)} a_1^{(2)} , a_1^{(2)} a_1^{(2)} , a_1^{(2)} a_1^{(2)} . Those new feature are learned from the original input x_1 x_1 , x_2 x_2 , x_3 x_3 .","title":"Learning its own features"},{"location":"courses/machine-learning-coursera/notes/#xnor-example","text":"We calculate a Non-linear classification example: XOR/XNOR, where x_1 x_1 and x_2 x_2 are binary 0 or 1. y = x_1 \\text{ XNOR } x_2 = \\text{ NOT } (x_1 \\text{ XOR } x_2) y = x_1 \\text{ XNOR } x_2 = \\text{ NOT } (x_1 \\text{ XOR } x_2) . But those are all build on the basic non-linear operation AND , OR , and NOT . Don't worry about how we get the \\theta \\theta . This example just to give some intuitions of how neural network problem can be solved.","title":"XNOR example"},{"location":"courses/machine-learning-coursera/notes/#multiclass-classification_1","text":"Suppose our algorithm is to recognize pedestrian, car, motorbike or truck, we need to build a neural network with four output units. We could use a vector to do this When image is a pedestrian get [1,0,0,0] and so on. 1 is 0/1 pedestrain 2 is 0/1 car 3 is 0/1 motorcycle 4 is 0/1 truck Just like one vs. all classifier described earlier. here we have four logistic regression classifiers Contrast to the previous notation we wrote y y as an integer {1, 2, 3, 4} {1, 2, 3, 4} to represent four classes. Now we use the following notation to represent y^{(i)} y^{(i)} as one of \\left[ \\begin{array}{c} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{array} \\right] \\left[ \\begin{array}{c} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{array} \\right] , \\left[ \\begin{array}{c} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{array} \\right] \\left[ \\begin{array}{c} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{array} \\right] , \\left[ \\begin{array}{c} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{array} \\right] \\left[ \\begin{array}{c} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{array} \\right] , \\left[ \\begin{array}{c} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{array} \\right] \\left[ \\begin{array}{c} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{array} \\right] .","title":"Multiclass classification"},{"location":"courses/machine-learning-coursera/notes/#week-5-neural-networks-learning","text":"","title":"Week 5 Neural networks learning"},{"location":"courses/machine-learning-coursera/notes/#neural-network-classification","text":"The cost function of neural network would be a generalized form of the regularized logistic regression cost function as shown below.","title":"Neural network classification"},{"location":"courses/machine-learning-coursera/notes/#cost-function_1","text":"","title":"Cost Function"},{"location":"courses/machine-learning-coursera/notes/#logistic-regression","text":"J(\\theta) = -\\frac{1}{m}\\Bigg[\\sum_{i=1}^{n}y^{(i)}\\log(h_\\theta(x^{(i)})) + (1 - y^{(i)})\\log(1-h_\\theta(x^{(i)}))\\Bigg] + \\frac{\\lambda}{2m}\\sum_{j=1}^{n}\\theta_j^2 J(\\theta) = -\\frac{1}{m}\\Bigg[\\sum_{i=1}^{n}y^{(i)}\\log(h_\\theta(x^{(i)})) + (1 - y^{(i)})\\log(1-h_\\theta(x^{(i)}))\\Bigg] + \\frac{\\lambda}{2m}\\sum_{j=1}^{n}\\theta_j^2","title":"Logistic regression"},{"location":"courses/machine-learning-coursera/notes/#neural-network_1","text":"We have the hypothesis in the K dimensional Euclidean space, denoted as h_{\\Theta}(x) \\in \\mathbb{R}^K h_{\\Theta}(x) \\in \\mathbb{R}^K , K is the number of output units. (h_{\\Theta}(x))_i = i^{th} (h_{\\Theta}(x))_i = i^{th} output. J(\\Theta) = -\\frac{1}{m}\\Bigg[\\sum_{i=1}^{m}\\sum_{k=1}^Ky_k^{(i)}\\log(h_\\Theta(x^{(i)}))_k + (1 - y_k^{(i)})\\log(1-(h_\\Theta(x^{(i)}))_k)\\Bigg] + \\frac{\\lambda}{2m}\\sum_{l=1}^{L-1}\\sum_{i=1}^{s_l}\\sum_{j=1}^{s_{l+1}}(\\Theta_{ji}^{(l)})^2 J(\\Theta) = -\\frac{1}{m}\\Bigg[\\sum_{i=1}^{m}\\sum_{k=1}^Ky_k^{(i)}\\log(h_\\Theta(x^{(i)}))_k + (1 - y_k^{(i)})\\log(1-(h_\\Theta(x^{(i)}))_k)\\Bigg] + \\frac{\\lambda}{2m}\\sum_{l=1}^{L-1}\\sum_{i=1}^{s_l}\\sum_{j=1}^{s_{l+1}}(\\Theta_{ji}^{(l)})^2 Noticed that all the summation is start from 1 not 0 . For example, the index k and j . Because we don't regularize the bias unit which is with subscript 0 .","title":"Neural Network"},{"location":"courses/machine-learning-coursera/notes/#gradient-computation","text":"We have the cost function for the neural network. Next step we need to calculate the gradient of the cost function. As we can see that the cost function is too complicated to derive its gradient. Here we will use back propagation method to calculate the partial derivative. Before dive into the details of the back propagation, we need to summarize some of the ideas we had for the neural network so far.","title":"Gradient computation"},{"location":"courses/machine-learning-coursera/notes/#forward-propagation","text":"Forward propagation takes initial input into that neural network and pushes the input through the neural network It leads to the generation of an output hypothesis, which may be a single real number, but can be a vector.","title":"Forward propagation"},{"location":"courses/machine-learning-coursera/notes/#back-propagation","text":"The appearance of the back propagation algorithm isn't very natural. Instead, it is a very smart way to solve the optimization problem in neural network. Because it is difficult to follow the method we used in linear regression or logistic regression to analytically get the derivative of the cost function. Back propagation come to help solving the optimization problem numerically. To compute the partial derivative \\frac{\\partial}{\\partial\\Theta_{i,j}^{(l)}}J(\\Theta) \\frac{\\partial}{\\partial\\Theta_{i,j}^{(l)}}J(\\Theta) (the gradient of cost function in neural network), thus allow us to find the parameter \\Theta \\Theta that minimize the cost J(\\Theta) J(\\Theta) (using gradient descent or advanced optimization algorithm), which will result in a hypothesis with parameters learn from the existing data.","title":"Back propagation"},{"location":"courses/machine-learning-coursera/notes/#how-to-derive-the-back-prop","text":"We define \\delta_{j}^{(l)} = \\delta_{j}^{(l)} = \\\"error\\\" of node j j in layer l l We calculate \\delta^{(L)} = a^{(L)} - y \\delta^{(L)} = a^{(L)} - y , which is the vector of errors defined above at last layer L L . Note \\delta^{(L)} \\delta^{(L)} is vectorized representation of \\delta_{j}^{(l)} \\delta_{j}^{(l)} . So our \"error values\" for the last layer are simply the differences of our actual results a^{(L)} a^{(L)} in the last layer and the correct outputs in y y . To get the delta values of the layers before the last layer, we can use an equation that steps us back from right to left, thereby back propagate , by \\delta^{(l)} = ((\\Theta^{(l)})^T\\delta^{(l+1)}).*g'(z^{(l)}) \\delta^{(l)} = ((\\Theta^{(l)})^T\\delta^{(l+1)}).*g'(z^{(l)}) . It can be interpreted as the error value of layer l l are calculated by multiplying the error values in the next layer with theta matrix of layer l l . We then element wise multiply that with the derivative of the activation function a^{(l)} = g(z^{(l)}) a^{(l)} = g(z^{(l)}) \uff0c namely g'(z^{(l)}) = a^{(l)}.*(1-a^{(l)}). g'(z^{(l)}) = a^{(l)}.*(1-a^{(l)}). The derivation of this deviative can be found at the course wiki. Partial derivative. With all the calculated \\delta^{(l)} \\delta^{(l)} values, We can compute our partial derivative terms by multiplying our activation values and our error values for each training example t t , \\dfrac{\\partial}{\\partial \\Theta_{i,j}^{(l)}}J(\\Theta) = \\frac{1}{m}\\sum_{t=1}^m a_j^{(t)(l)} {\\delta}_i^{(t)(l+1)} \\dfrac{\\partial}{\\partial \\Theta_{i,j}^{(l)}}J(\\Theta) = \\frac{1}{m}\\sum_{t=1}^m a_j^{(t)(l)} {\\delta}_i^{(t)(l+1)} . This is ignoring regularization terms. Notations: t t is the index of the training examples, we have total of m m samples. l l is the layer, we have total layer of L L . i i is the error affected node in the target layer, layer l+1 l+1 . j j is the node in the current layer l l . \\delta^{(l+1)} \\delta^{(l+1)} and \\ a^{(l+1)} \\ a^{(l+1)} are vectors with s_{l+1} s_{l+1} elements. Similarly, \\ a^{(l)} \\ a^{(l)} is a vector with s_{l} s_{l} elements. Multiplying them produces a matrix that is s_{l+1} s_{l+1} by s_{l} s_{l} which is the same dimension as \\Theta^{(l)} \\Theta^{(l)} . That is, the process produces a gradient term for every element in \\Theta^{(l)} \\Theta^{(l)} . |","title":"How to derive the back-prop"},{"location":"courses/machine-learning-coursera/notes/#backpropagation-algorithm","text":"Given training set \\lbrace (x^{(1)}, y^{(1)}) \\cdots (x^{(m)}, y^{(m)})\\rbrace \\lbrace (x^{(1)}, y^{(1)}) \\cdots (x^{(m)}, y^{(m)})\\rbrace Set \\Delta^{(l)}_{i,j} := 0 \\Delta^{(l)}_{i,j} := 0 for all (l,i,j) (l,i,j) For training example t = 1 t = 1 to m m : Set a^{(1)} := x^{(t)} a^{(1)} := x^{(t)} Perform forward propagation to compute a^{(l)} a^{(l)} for l = 2,3,\\dots ,L l = 2,3,\\dots ,L Using y^{(t)} y^{(t)} , compute \\delta^{(L)} = a^{(L)} - y^{(t)} \\delta^{(L)} = a^{(L)} - y^{(t)} Compute \\delta^{(L-1)}, \\delta^{(L-2)},\\dots,\\delta^{(2)} \\delta^{(L-1)}, \\delta^{(L-2)},\\dots,\\delta^{(2)} using \\delta^{(l)} = ((\\Theta^{(l)})^T \\delta^{(l+1)})\\ .*\\ a^{(l)}\\ .*\\ (1 - a^{(l)}) \\delta^{(l)} = ((\\Theta^{(l)})^T \\delta^{(l+1)})\\ .*\\ a^{(l)}\\ .*\\ (1 - a^{(l)}) \\Delta^{(l)}_{i,j} := \\Delta^{(l)}_{i,j} + a_j^{(l)} \\delta_i^{(l+1)} \\Delta^{(l)}_{i,j} := \\Delta^{(l)}_{i,j} + a_j^{(l)} \\delta_i^{(l+1)} or with vectorization, \\Delta^{(l)} := \\Delta^{(l)} + \\delta^{(l+1)}(a^{(l)})^T \\Delta^{(l)} := \\Delta^{(l)} + \\delta^{(l+1)}(a^{(l)})^T (programmatic calculation of \\frac{1}{m}\\sum_{t=1}^m a_j^{(t)(l)} {\\delta}_i^{(t)(l+1)} \\frac{1}{m}\\sum_{t=1}^m a_j^{(t)(l)} {\\delta}_i^{(t)(l+1)} ) D^{(l)}_{i,j} := \\dfrac{1}{m}\\left(\\Delta^{(l)}_{i,j} + \\lambda\\Theta^{(l)}_{i,j}\\right) D^{(l)}_{i,j} := \\dfrac{1}{m}\\left(\\Delta^{(l)}_{i,j} + \\lambda\\Theta^{(l)}_{i,j}\\right) If j \\neq 0 j \\neq 0 (NOTE: Typo in lecture slide omits outside parentheses. This version is correct.) D^{(l)}_{i,j} := \\dfrac{1}{m}\\Delta^{(l)}_{i,j} D^{(l)}_{i,j} := \\dfrac{1}{m}\\Delta^{(l)}_{i,j} If j = 0 j = 0 The capital-delta matrix is used as an \\\"accumulator\\\" to add up our values as we go along and eventually compute our partial derivative. The actual proof is quite involved, but, the D^{(l)}_{i,j} D^{(l)}_{i,j} terms are the partial derivatives and the results we are looking for: D_{i,j}^{(l)} = \\dfrac{\\partial J(\\Theta)}{\\partial \\Theta_{i,j}^{(l)}}. D_{i,j}^{(l)} = \\dfrac{\\partial J(\\Theta)}{\\partial \\Theta_{i,j}^{(l)}}.","title":"Backpropagation Algorithm"},{"location":"courses/machine-learning-coursera/notes/#math-representation","text":"There is a \\Theta \\Theta matrix for each layer in the network This has each node in layer l l as one dimension and each node in l+1 l+1 as the other dimension Similarly, there is going to be a \\Delta \\Delta matrix for each layer This has each node as one dimension and each training data example as the other","title":"Math representation"},{"location":"courses/machine-learning-coursera/notes/#a-high-level-description","text":"Back propagation basically takes the output you got from your network, compares it to the real value (y) and calculates how wrong the network was (i.e. how wrong the parameters were) It then, using the error you\\'ve just calculated, back-calculates the error associated with each unit from right to left. This goes on until you reach the input layer (where obviously there is no error, as the activation is the input) These \\\"error\\\" measurements for each unit can be used to calculate the partial derivatives Partial derivatives are the bomb, because gradient descent needs them to minimize the cost function We use the partial derivatives with gradient descent to try minimize the cost function and update all the \\Theta \\Theta values This repeats until gradient descent reports convergence","title":"A high level description"},{"location":"courses/machine-learning-coursera/notes/#what-we-need-to-watch-out","text":"We don\\'t have \\delta^{(1)} \\delta^{(1)} because we have all x x , it is zero. We don\\'t calculate the bias node.","title":"What we need to watch out?"},{"location":"courses/machine-learning-coursera/notes/#backpropagation-intuition","text":"Backpropagation very similar to the forward propagation, we can use a example to calculate the \\lambda_i^{(\\ell)} \\lambda_i^{(\\ell)} or the z^{(\\ell)} z^{(\\ell)} as bellow. To further understand the algorithm, refer to the code in assignment 4. As it is shown in the slide, z_1^{(2)} z_1^{(2)} is calculate by multiply the corresponding \\Theta^{(2)}_{ij} \\Theta^{(2)}_{ij} value. the index i i is the same as in z_i^{(2)} z_i^{(2)} . The index j j means the j j th node in the previous layer.","title":"Backpropagation Intuition"},{"location":"courses/machine-learning-coursera/notes/#unrolling-parameters","text":"With neural networks, we are working with sets of matrices: \\Theta^{(1)} \\Theta^{(1)} , \\Theta^{(2)} \\Theta^{(2)} , \\Theta^{(3)}, \\dots \\Theta^{(3)}, \\dots . D^{(1)}, D^{(1)}, D^{(3)}, \\dots D^{(1)}, D^{(1)}, D^{(3)}, \\dots In order to use optimizing functions such as \"fminunc()\", we will want to \"unroll\" all the elements and put them into one long vector # unroll the matrices thetaVector = [ Theta1 (:); Theta2 (:); Theta3 (:); ] deltaVector = [ D1 (:); D2 (:); D3 (:) ] If the dimensions of Theta1 is 10x11, Theta2 is 10x11 and Theta3 is 1x11, then we can get back our original matrices from the \"unrolled\" versions as follows # using reshape function to build matrix from vectors Theta1 = reshape ( thetaVector ( 1 : 110 ), 10 , 11 ) Theta2 = reshape ( thetaVector ( 111 : 220 ), 10 , 11 ) Theta3 = reshape ( thetaVector ( 221 : 231 ), 1 , 11 ) In Octave, if we want to implement the back propagation algorithm, we have to do the unrolling and reshaping back and forth in order to calculate the gradientVec . Notice that the last two lines are shifted in the picture, it should read \"Use forward propagation and back propagation to compute D^{(1)}, D^{(1)}, D^{(3)} D^{(1)}, D^{(1)}, D^{(3)} and J(\\Theta) J(\\Theta) . Unroll D^{(1)}, D^{(1)}, D^{(3)} D^{(1)}, D^{(1)}, D^{(3)} to get gradientVec .","title":"Unrolling parameters"},{"location":"courses/machine-learning-coursera/notes/#gradient-checking","text":"To ensure a bug free back propagation implementation, one method we can use is gradient checking. It is a way of numerically estimate the gradient value in each computation point. It works as a reference value to check the back propagation is correct. In Octave, we can implement this gradient checking as follows. We then want to check that gradApprox \\approx \\approx deltaVector. Remove the gradient checking when you checking is done, otherwise the code is going to be very slow. epsilon = 1e-4 ; for i = 1 : n , thetaPlus = theta ; thetaPlus ( i ) += epsilon ; thetaMinus = theta ; thetaMinus ( i ) -= epsilon ; gradApprox ( i ) = ( J ( thetaPlus ) - J ( thetaMinus )) / ( 2 * epsilon ) end ;","title":"Gradient Checking"},{"location":"courses/machine-learning-coursera/notes/#random-initialization","text":"Initializing all theta weights to zero does not work with neural networks. When we back propagate, all nodes will update to the same value repeatedly. Instead we can randomly initialize our weights: initialize each \\Theta_{ij}^{(l)} \\Theta_{ij}^{(l)} to a random value between [-\\epsilon, \\epsilon] [-\\epsilon, \\epsilon] . In Octave, we can use the following code to generate the initial theta values. #If the dimensions of Theta1 is 10x11, Theta2 is 10x11 and Theta3 is 1x11. Theta1 = rand ( 10 , 11 ) * ( 2 * INIT_EPSILON ) - INIT_EPSILON ; Theta2 = rand ( 10 , 11 ) * ( 2 * INIT_EPSILON ) - INIT_EPSILON ; Theta3 = rand ( 1 , 11 ) * ( 2 * INIT_EPSILON ) - INIT_EPSILON ; rand(x,y) will initialize a matrix of random real numbers between 0 and 1. (Note: this \\epsilon \\epsilon is unrelated to the \\epsilon \\epsilon from Gradient Checking)","title":"Random Initialization"},{"location":"courses/machine-learning-coursera/notes/#putting-it-together","text":"First, pick a network architecture; choose the layout of your neural network, including how many hidden units in each layer and how many layers total. Number of input units = dimension of features x^{(i)} x^{(i)} Number of output units = number of classes Number of hidden units per layer = usually more the better (must balance with the cost of computation as it increases with more hidden units) Defaults: 1 hidden layer. If more than 1 hidden layer, then the same number of units in every hidden layer.","title":"Putting it Together"},{"location":"courses/machine-learning-coursera/notes/#week-6-advice-for-applying-machine-learning","text":"","title":"Week 6 Advice for applying machine learning"},{"location":"courses/machine-learning-coursera/notes/#deciding-what-to-try-next","text":"Get more training examples Try smaller sets\\'of features Try getting additional features Try adding polynomial features Try decreasing \\lambda \\lambda Try increasing \\lambda \\lambda","title":"Deciding what to try next"},{"location":"courses/machine-learning-coursera/notes/#machine-learning-diagnostic","text":"Diagnostic : A test that you can run to gain insight what is/isn\\'t working with a learning algorithm, and gain guidance as to how best to improve its performance. It can take time to implement, but doing so can be a very good use of your time.","title":"Machine learning diagnostic"},{"location":"courses/machine-learning-coursera/notes/#evaluating-a-hypothesis","text":"Diagnostic over-fitting by splitting the training set and test set Training set (70%) Test set (30%) If the cost function J(\\theta) J(\\theta) calculated for training set is low, it is high for test set, over-fitting happens. This is apply to both logistic regression and linear regression.","title":"Evaluating a hypothesis"},{"location":"courses/machine-learning-coursera/notes/#model-selection-and-trainvalidationtest-sets","text":"Normally we choose a model from a series of different degree of polynomial hypothesis, We choose the model that have the least cost for testing data. More specifically, we learned the parameter \\theta^{(d)} \\theta^{(d)} (d is the degree of polynomial) from the training set and calculate the cost for the test dataset with the learned \\theta \\theta . This is not the best practice for the reason that the hypothesis we selected has been fit to the test set. It is very likely the model will not fit new data very well. We can improve the model selection by introduce the cross validation set. We divide our available data as Training set (60%) Cross validation set (20%) test set (20%) We use the cross validation data to help us to select the model, and using the test data, which the model never see before to evaluate generalization error. Warning Avoid to use the test data to select the model (select the least cost model calculated on the test data set) and then report the generalization error using the test data.","title":"Model selection and Train/Validation/Test sets"},{"location":"courses/machine-learning-coursera/notes/#diagnosing-bias-vs-variance","text":"The important point to understand bias and variance is to understand how the cost for training data, cross validation data (or test data) changes.","title":"Diagnosing bias vs. variance"},{"location":"courses/machine-learning-coursera/notes/#regularization-and-biasvariance","text":"The regularization parameter \\lambda \\lambda could play important role in selecting the best model to fit the data (both cross validation data and test data). This section will study how \\lambda \\lambda affect the cost function both for the training data and cross validation data.","title":"Regularization and bias/variance"},{"location":"courses/machine-learning-coursera/notes/#learning-curve","text":"This section mainly discuss how the sample size affect the cost function. There are two kinds of problem we face, high bias and high variance","title":"Learning curve"},{"location":"courses/machine-learning-coursera/notes/#high-bias","text":"","title":"High bias"},{"location":"courses/machine-learning-coursera/notes/#high-variance","text":"","title":"High variance"},{"location":"courses/machine-learning-coursera/notes/#deciding-what-to-do-next-revisited","text":"bias/variance underfitting/overfitting regularization to improve High bias underfitting bigger lambda lambda more sample will not help much High variance overfitting (high polynomial) smaller lambda lambda more sample will likely to help","title":"Deciding what to do next revisited"},{"location":"courses/machine-learning-coursera/notes/#prioritizing-what-to-work-on","text":"","title":"Prioritizing what to work on"},{"location":"courses/machine-learning-coursera/notes/#building-a-spam-classifier","text":"","title":"Building a spam classifier"},{"location":"courses/machine-learning-coursera/notes/#error-analysis","text":"Start simple algorithm, implement it and test it on cross-validation data. Plot learning curves to decide if more data, more features, etc. are like to help. Error analysis: Manually examine the examples (in cross validation set) that you algorithm make errors on. See if you spot any systematic trend in what type of examples it it making error on.","title":"Error analysis"},{"location":"courses/machine-learning-coursera/notes/#error-metrics-for-skewed-classes","text":"Problem arise in error analysis when only very small(less than 0.5%) negative samples possibly exist. Take the cancer classification as a example.","title":"Error metrics for skewed classes"},{"location":"courses/machine-learning-coursera/notes/#precision-and-recall","text":"","title":"Precision and Recall"},{"location":"courses/machine-learning-coursera/notes/#trading-off-precision-and-recall","text":"","title":"Trading off precision and recall"},{"location":"courses/machine-learning-coursera/notes/#f-score-f_score","text":"","title":"F score {#f_score}"},{"location":"courses/machine-learning-coursera/notes/#data-for-machine-learning","text":"","title":"Data for machine learning"},{"location":"courses/machine-learning-coursera/notes/#week-7-support-vector-machine","text":"","title":"Week 7 Support Vector Machine"},{"location":"courses/machine-learning-coursera/notes/#optimization-objective","text":"SVM will be derived from logistic regression. There are not that much of logic, just following the derivation in the lecture. Let's first review the hypothesis of the logistic regress and its cost function: Notice in the second figure above, we define approximate functions cost_0(z) cost_0(z) and cost_1(z) cost_1(z) . These functions will be used in the SVM cost function we will see next.","title":"Optimization objective"},{"location":"courses/machine-learning-coursera/notes/#svm-cost-function","text":"Compare to logistic regression, SVM use a different parameters C C . As in logistic regression, we use \\lambda \\lambda to control the trade off optimizing the first term and the second term in the objective function. What's more, the SVM hypothesis doesn\\'t give us a probability prediction, it give use either 1 or 0 . Such as h_\\theta = \\begin{cases} 1 \\qquad \\text{if}\\ \\Theta^Tx \\geq 0\\\\ 0 \\qquad \\text{otherwise} \\end{cases} h_\\theta = \\begin{cases} 1 \\qquad \\text{if}\\ \\Theta^Tx \\geq 0\\\\ 0 \\qquad \\text{otherwise} \\end{cases}","title":"SVM cost function"},{"location":"courses/machine-learning-coursera/notes/#large-margin-intuition","text":"Suppose we make the parameter C C very large, we want to make the first term in the summation approximate 0 . We can derive the problem as a optimization problem subject to some condition. This optimization problem will lead us to discuss the decision boundary. when C C is very large, the decision boundary will have a large margin, thus large margin classifier. Later we will see that using different kernel and parameters can control this margin. Question How the large margin classifier related to the optimization problem when C is very large? When C C is very large, we have the magneto decision boundary, if C C is not very large, we will have the black decision boundary, which is having the large margin properties.","title":"Large margin intuition"},{"location":"courses/machine-learning-coursera/notes/#mathematics-behind-large-margin-classification","text":"Using the vector inner product to transform the constrain from \\theta^T x^{(i)} \\geq 1 \\theta^T x^{(i)} \\geq 1 to p^{(i)}\\Vert\\theta \\Vert \\geq 1 p^{(i)}\\Vert\\theta \\Vert \\geq 1 for y^{(i)} = 1 y^{(i)} = 1 The point of this section is to discuss the mathematical insight why SVM choose the large margin decision boundary. (because, the large margin decision boundary will produce a larger p^{(i)} p^{(i)} , thus a smaller \\Vert\\theta \\Vert \\Vert\\theta \\Vert in the constrain of the optimization problem, so as to minimize the objective function \\min_\\theta \\frac{1}{2} \\sum_{j=1}^{n} \\theta_j^2 \\min_\\theta \\frac{1}{2} \\sum_{j=1}^{n} \\theta_j^2 )","title":"Mathematics behind large margin classification"},{"location":"courses/machine-learning-coursera/notes/#kernel","text":"","title":"Kernel"},{"location":"courses/machine-learning-coursera/notes/#non-linear-decision-boundary","text":"Is there a different/better choice of the features f_1, f_2, f_3, \\dots, f_1, f_2, f_3, \\dots,","title":"Non-linear decision boundary"},{"location":"courses/machine-learning-coursera/notes/#landmarks","text":"To respond the above question, we use the following way to generate the new features f_1, f_2, f_3, \\dots, f_1, f_2, f_3, \\dots, . Compare the similarity between the examples and the landmarks using a similarity function.","title":"Landmarks"},{"location":"courses/machine-learning-coursera/notes/#gaussian-kernel","text":"We use the Guassian kernel k(x, \\ell^{(i)}) = \\exp(-\\frac{\\Vert x - \\ell^{(i)}\\Vert^2}{2\\delta^2}) k(x, \\ell^{(i)}) = \\exp(-\\frac{\\Vert x - \\ell^{(i)}\\Vert^2}{2\\delta^2}) as the similarity function to generate the new feature.","title":"Gaussian kernel"},{"location":"courses/machine-learning-coursera/notes/#choosing-the-landmarks","text":"Naturally, we are going to choose the example itself as the landmark, thus we are able to transform the feature x^{(i)} x^{(i)} to feature vector f^{(i)} f^{(i)} for training example (x^{(i)}, y^{(i)}) (x^{(i)}, y^{(i)}) .","title":"Choosing the landmarks"},{"location":"courses/machine-learning-coursera/notes/#using-svm","text":"Choice of parameter C C . Choice of kernel (similarity function): No kernel (\"linear Kernel\") Gaussian kernel (need to choose \\delta^2 \\delta^2 ) Because the similarity function is apply to each feature, we\\'d better to perform feature scaling before using the Gaussian kernel. (take the house price features as an example, the areas and the number of rooms are not in the same scale.) Note Not all similarity functions similarity(x, \\ell) similarity(x, \\ell) make valid kernels. Need to satisfy technical condition called \"Mercer's Theorem\" to make sure SVM packagesl' optimizations run correctly, and do not diverge). Many off-the-shelf kernels available: Polynomial kernel: k(X, \\ell) = (X^T\\ell + c)^{p} k(X, \\ell) = (X^T\\ell + c)^{p} , such as k(x, \\ell) = (X^T\\ell + 1)^2. k(x, \\ell) = (X^T\\ell + 1)^2. More esoteric: String kernel, chi-square kernel, histogram intersection kernel, ... Note Many SVM packages already have built-in multi-class classification functionality. Otherwise, we can use one-vs.-all method. Specifically, train K K SVMs, one to distinguish y = i y = i from the rest, for i = 1, 2, \\dots, K i = 1, 2, \\dots, K , get \\theta^{(1)}, \\theta^{(2)}, \\dots, \\theta^{(K)} \\theta^{(1)}, \\theta^{(2)}, \\dots, \\theta^{(K)} , pick class i i with largest (\\theta^{(i)})^Tx (\\theta^{(i)})^Tx .","title":"Using SVM"},{"location":"courses/machine-learning-coursera/notes/#logistic-regression-vs-smvs","text":"This section mainly discuss when to use logistic regression vs. SVM from the high level. I summarize the content of the slides in the table. n m logistic regression vs. SVMs large (relative to mm) i.e. 10,000 10 - 1000 use logistic regression, or SVM without a kerenl (\"linear kernel\") small (i.e. 1-1000) intermediate (10-10,000) use SVM with Gaussian kernel small (i.e. 1-1000) large (50,000+) create/add more features, then use logistic regression or SVM without a kernel","title":"Logistic regression vs. SMVs"},{"location":"courses/machine-learning-coursera/notes/#week-8-unsupervised-algorithms","text":"","title":"Week 8 Unsupervised Algorithms"},{"location":"courses/machine-learning-coursera/notes/#k-means-algorithm","text":"In the k-means clustering algorithm. We have input K K (number of clusters) and traning set \\{x^{(1)}, x^{(1)}, \\dots, x^{(m)}\\} \\{x^{(1)}, x^{(1)}, \\dots, x^{(m)}\\} , for each of the training example x^{(i)}\\in \\R^n x^{(i)}\\in \\R^n . Note here we drop the x_0 = 1 x_0 = 1 convention. To visualize the algorithm, we have the following three picture. Concretely, the k-means algorithm is: Input: training set \\! \\{x^{(i)}\\}_{i=1}^m, \\! \\{x^{(i)}\\}_{i=1}^m, and number of clusters K K . Algorith: Randomly initialize K K cluster centroids \\mu_1, \\mu_1, \\dots, \\mu_K\\in \\R^n \\mu_1, \\mu_1, \\dots, \\mu_K\\in \\R^n . Repeat: for i = 1 i = 1 to m m : // cluster assignment step c^{(i)} c^{(i)} := index (from 1 to K K ) of cluster centroid closest to x^{(i)} x^{(i)} for k = 1 k = 1 to K K : // move centroid step \\mu_k \\mu_k := average (mean) of points assigned to cluster k k Until convergence","title":"K-Means Algorithm"},{"location":"courses/machine-learning-coursera/notes/#optimization-objective_1","text":"","title":"Optimization objective"},{"location":"courses/machine-learning-coursera/notes/#random-initialization_1","text":"We can random initialize the centroid by randomly pick K K training examples and set \\mu_1, \\dots, \\mu_K \\mu_1, \\dots, \\mu_K equal to these K K examples. Obviously, we should have K < m K < m . However, some of the random initialization would lead the algorithm to achieve local optima, which is shown below in the pictures. To solve this problem, we can random initialize multiple times then to pick clustering that gave lowest cost J(c^{(1)}, \\dots, c^{(m)}, \\mu_1, \\dots \\mu_K) J(c^{(1)}, \\dots, c^{(m)}, \\mu_1, \\dots \\mu_K) .","title":"Random initialization"},{"location":"courses/machine-learning-coursera/notes/#choosing-the-k","text":"It depends on the problem we are trying to solve. One straightforward problem is using a method called the Elbow method as the following picture. Secondly, the K value should be picked to maximize some practical utility function, for example, we cluster the size of different T-shirts, whether we want to have 3 sizes or 5 sizes is depends on the profitability.","title":"Choosing the K"},{"location":"courses/machine-learning-coursera/notes/#dimensionality-reduction","text":"The two motivation of Dimensionality Reduction are: Data compression, for example, reduce data from 2D to 1D. Visualization, we can reduce the high dimensionality data to 2 or 3 dimensions, and visualize them. For example, to visualize some complex economical data for different countries.","title":"Dimensionality reduction"},{"location":"courses/machine-learning-coursera/notes/#principal-component-analysis-pca","text":"To reduce from 2-dimension to 1-dimension: Find a direction (a vector u^{(1)}\\in \\R^n u^{(1)}\\in \\R^n ) onto which to project the data so as to minimize the projection error. To reduce from n-dimension to k-dimension: Find k direction vectors u^{(1)}, u^{(2)}, \\dots, u^{(k)} u^{(1)}, u^{(2)}, \\dots, u^{(k)} onto which to project the data so as to minimize the projection error. Note Note the difference between PCA and linear regression.","title":"Principal Component Analysis (PCA)"},{"location":"courses/machine-learning-coursera/notes/#pca-algorithm","text":"Before carry out the PCA, we\\'d better to do some feature scaling/mean normalization. As in the slides below, the S_i S_i could be the maximum/minimum value or other values such as variance \\sigma \\sigma . once the pre-proccessing steps are done we can run the following algorithm in Octave to implement PCA.","title":"PCA algorithm"},{"location":"courses/machine-learning-coursera/notes/#reconstruction-from-pca-compressed-data","text":"","title":"Reconstruction from PCA compressed data"},{"location":"courses/machine-learning-coursera/notes/#choosing-the-number-of-principal-components","text":"Two concepts we should define here: Average squared projection error: \\frac{1}{m}\\sum_{i=1}^{m}\\lVert x^{(i)} - x_{approx}^{(i)}\\rVert^2 \\frac{1}{m}\\sum_{i=1}^{m}\\lVert x^{(i)} - x_{approx}^{(i)}\\rVert^2 Total variation in the data: \\frac{1}{m}\\sum_{i=1}^{m}\\lVert x^{(i)} \\rVert^2 \\frac{1}{m}\\sum_{i=1}^{m}\\lVert x^{(i)} \\rVert^2 Typically, we choose k k to be smallest value so that ration of the two quality should be less than 1%.","title":"Choosing the number of principal components"},{"location":"courses/machine-learning-coursera/notes/#advice-for-applying-pca","text":"One important point should keep in mind when using PCA: don't try to use PCA to prevent overfitting, use regularization instead.","title":"Advice for applying PCA"},{"location":"courses/machine-learning-coursera/notes/#week-9-anomaly-detection","text":"","title":"Week 9 Anomaly Detection"},{"location":"courses/machine-learning-coursera/notes/#anomaly-detection","text":"Given a model p(x) p(x) , test on the example x^{(i)} x^{(i)} to check whether we have p(x) < \\epsilon p(x) < \\epsilon . Generally, the anomaly detection system apply to the scenario that we don't have much anomaly example, such as a dataset about aircraft engines. In this particular lecture, the p(x) p(x) is multivariate Gaussian distribution . The algorithm is to find the parameter \\mu \\mu and \\sigma^2 \\sigma^2 . Once we fit the data with a multivariate Gaussian distribution, we are able to obtain a probability value, which can be use to detect the anomaly by select a probability threshold \\epsilon \\epsilon . Here is the anomaly detection algorithm:","title":"Anomaly Detection"},{"location":"courses/machine-learning-coursera/notes/#developing-and-evaluating","text":"","title":"Developing and evaluating"},{"location":"courses/machine-learning-coursera/notes/#anomaly-detection-vs-supervised-learning","text":"","title":"Anomaly detection vs. supervised learning"},{"location":"courses/machine-learning-coursera/notes/#multivariate-gaussian-distribution","text":"There are plots of different multivariate Gaussian distributions with different mean and covariance matrix. It intuitively show how changes in the mean and covariance matrix can change the shape of the plot of the distribution. It also compared the original model (multiple single variate Gaussian distribution) to multivariate Gaussian distribution. Generally, if we have multiple features, we tend to use multivariate Gaussian distribution to fit the data, even thought the original model is more computationally cheaper.","title":"Multivariate Gaussian distribution"},{"location":"courses/machine-learning-coursera/notes/#predicting-movie-ratings","text":"","title":"Predicting movie ratings"},{"location":"courses/machine-learning-coursera/notes/#problem-formulation","text":"","title":"Problem formulation"},{"location":"courses/machine-learning-coursera/notes/#content-based-recommendations","text":"Suppose we have a feature vector for each of the movie, combining with the rating values, we can solve the minimization problem to get \\theta^{(j)} \\theta^{(j)} , which is the parameter vector of user j j . With this parameter vector, we can predicting the rating of the movie by (\\theta^{(j)})^T x^{(i)} (\\theta^{(j)})^T x^{(i)} .","title":"Content based recommendations"},{"location":"courses/machine-learning-coursera/notes/#collaborate-filtering","text":"In collaborate filtering, we don\\'t have the feature, we only have the parameter vector \\theta^{(j)} \\theta^{(j)} for user j j . We can solve a optimization problem in regarding to the feature vector x^{(i)} x^{(i)} through the rating values. What interesting about this if we don\\'t have the initial \\theta^{(j)} \\theta^{(j)} , we can generate a small random value of \\theta^{(j)} \\theta^{(j)} , and repetitively to get the feature vector x^{(i)} x^{(i)} . As later we can say, we can solve a optimization problem with regarding to both \\theta^{(j)} \\theta^{(j)} and x^{(i)} x^{(i)} all at once. See the slides for details.","title":"Collaborate filtering"},{"location":"courses/machine-learning-coursera/notes/#collaborative-filtering-algorithm","text":"As discussed above, in practice, we solve the optimization problem respect to both \\theta^{(j)} \\theta^{(j)} and x^{(i)} x^{(i)} . See the slides for the optimization problem we are trying to solve, and the gradient descent algorithm to solve it.","title":"Collaborative filtering algorithm"},{"location":"courses/mining-massive-datasets/notes/","text":"","title":"Mining Massive Data Sets"},{"location":"leetcode/","text":"Leetcode \u00b6 Favorite Problems Array Backtracking Binary Indexed Tree Binary Search Binary Search Tree Breadth First Search Bit Manipulation Depth First Search Design Divide And Conquer Dynamic Programming Geometry Graph Greedy Hash Heap Interval Linked List Math Queue Recursion Reservoir Sampling Segment Tree Sort Stack String Topological Sort Two Pointers Tree Trie Union Find","title":"Index"},{"location":"leetcode/#leetcode","text":"Favorite Problems Array Backtracking Binary Indexed Tree Binary Search Binary Search Tree Breadth First Search Bit Manipulation Depth First Search Design Divide And Conquer Dynamic Programming Geometry Graph Greedy Hash Heap Interval Linked List Math Queue Recursion Reservoir Sampling Segment Tree Sort Stack String Topological Sort Two Pointers Tree Trie Union Find","title":"Leetcode"},{"location":"leetcode/array/notes/","text":"Array \u00b6 Category 1 Remove/Contains Duplidate \u00b6 Contains Duplicate \u00b6 Contains Duplicate II \u00b6 Contains Duplicate III \u00b6 Find the Duplicate Number \u00b6 Remove Duplicates from Sorted Array \u00b6 Remove Duplicates from Sorted Array II \u00b6 Remove Duplicates from Sorted List II \u00b6 Remove Duplicates from Sorted List \u00b6 Move Zeroes \u00b6 Category 2 Matrix problems \u00b6 Spiral Matrix \u00b6 Spiral Matrix II \u00b6 Search a 2D Matrix \u00b6 Search a 2D Matrix II \u00b6 Rotate Image \u00b6 Range Sum Query 2D - Mutable \u00b6 Range Sum Query 2D - Immutable \u00b6 Maximal Square \u00b6 Maximal Rectangle \u00b6 Category 3 Subarray problems \u00b6 Type of subarray problem \u00b6 Find a subarray that fulfills a certain property, i.e maximum size subarray, Longest Substring with At Most Two Distinct Characters Use map or two pointer to solve the problem. Split into subarrays that fulfill certain properties, i.e. sum greater than k. Fixed length subarray indexing To correctly index an array in solving subarray problems are critical. Here is some tips: 1. To iterate through a subarray of certain size, alwasy using the \"one-of-the-end\" pattern. Namely, the iteration index i point to the \"one-off-the-end\" of the subarray. The subarray of size K before the index is started at index i - k . 2. The above convention is especially useful in subarray problems given constrains, such as \"the subarray size greater than k \", \"maximum sum of non-overlapping subarray\", etc. 3. Example problems - Maximum Average Subarray II - Maximum Sum of Two Non-Overlapping Subarrays Two types of prefix sum \u00b6 There are two ways to calculate the prefix sum array. Take which ever conveniece for your when solving a problem. Option 1: sums.resize(n, 0); nums: [1, 2, 3, 4, 5, 6, 7, 8, 9] i j sums: [1, 3, 6, 10, 15, 21, 28, 36, 45] In this case, each element sums[i] in sums represent the cumulative sum for indexes [0, ..., i] . In other words, sum[i] represent cumulative sum up to element i inclusive. When you want to get the range sum rangeSum(i, j) , you can get it in the following way: rangeSum ( i , j ) = sums [ j ] - sums [ i - 1 ] // i > 0 rangeSum ( i , j ) = sums [ j ] // i == 0 work with this option is a little complex, to get the rangeSum(i, j) : rangeSum ( i , j ) = sums [ j ] - sums [ i ] + nums [ i ] // i >= 0 Option 2: sums.resize(n + 1, 0); In this case, each element sums[i] in sums represent the prefix sum of the first i elements in original array nums. When you want to get the range sum by rangeSum(i, j) , you can get it in the following way: rangeSum ( i , j ) = sums [ j + 1 ] - sums [ i ] // i >= 0 Using prefix sum with map \u00b6 One of the core trick in solving the following subarray problems is to build a map from prefix sum to array index for efficient lookup. For example, problems with keywaords \"maximum size equal to K\", \"differ by K\", or \"differ by multiple of k\" are solved using this trick. There are two hints. Hint When a map is used, it need to be initialized using <0, -1> . It is useful for handling some of the corner cases such as [-1, 1], -1 in the problem Maximum Size Subarray Sum Equals k . Hint It is usually easier to work with these problem when adding dummy element at the beginning of the array. For example: using sums[i] to represent the sum of first i element of array nums . Maximum Subarray \u00b6 Kadane's solution This is a DP solution, it reduced the f array to two variables. Making the problem O(1) O(1) in space. Discuss about this solution , where it make use of the idea of global maximum and local maximum. Note Why can not compare to f[i - 1] to find the maximum. If comparing to the f[i - 1] , it will skip elements, the sum will not from a subarray, but sequence of numbers in the array. This is very similar to problems Longest Common Substring and Longest Common Subsequence. The DP \"Choices\" here is NOT to choose or not choose A[i] , but \"Add A[i] to the result of the subproblem or we have to start a subarray from i \" because we cannot skip A[i] . Note The idea of global maximum and local maximum is very useful to solve DP problems. The local maximum is the maximum sum of a continuous subarray, the global maximum is to keep the maximum of the from the local and global maximum. Prefix sum solution The ideas is we have array sums, sums[i] = A[0] +, ... + A[i] , called prefix sum. With one for loop we can find the maxSum so far and the minSum before it. The difference is the possible results, we collect the maximum of those differences. related to Jump Game C++ DP class Solution { public : int maxSubArray ( vector < int >& nums ) { int n = nums . size (); int res = INT_MIN ; int f [ n + 1 ] = { 0 }; // f[i] = maxSubArray of first i elements f [ 0 ] = 0 ; // initial value for ( int i = 1 ; i <= n ; i ++ ) { f [ i ] = max ( f [ i - 1 ] + nums [ i - 1 ], nums [ i - 1 ]); res = max ( f [ i ], res ); } return res ; } }; C++ Kadane's solution class Solution { public : int maxSubArray ( vector < int >& nums ) { int n = nums . size (); int res = INT_MIN ; int curr = 0 ; for ( int i = 0 ; i < n ; i ++ ) { curr = max ( curr + nums [ i ], nums [ i ]); res = max ( curr , res ); } return res ; } }; Java prefix sum solution public class Solution { public int maxSubArray ( int [] A ) { if ( A == null || A . length == 0 ){ return 0 ; } int max = Integer . MIN_VALUE , sum = 0 , minSum = 0 ; for ( int i = 0 ; i < A . length ; i ++ ) { sum += A [ i ] ; max = Math . max ( max , sum - minSum ); minSum = Math . min ( minSum , sum ); } return max ; } }; C++ Greedy solution // why this greedy solution works? class Solution { public : int maxSubArray ( vector < int >& nums ) { int n = nums . size (); int sum = 0 ; int max = 0 ; if ( n == 0 ) return 0 ; max = nums [ 0 ]; for ( int i = 0 ; i < n ; i ++ ) { sum += nums [ i ]; max = sum > max ? sum : max ; sum = sum > 0 ? sum : 0 ; } return max ; } }; Maximum Subarray II* \u00b6 Given an array of integers, find two non-overlapping subarrays which have the largest sum. The number in each subarray should be contiguous. Return the largest sum. Notice The subarray should contain at least one number Example For given [1, 3, -1, 2, -1, 2], the two subarrays are [1, 3] and [2, -1, 2] or [1, 3, -1, 2] and [2], they both have the largest sum 7. Prefix sum solution C++ Prefix sum solution class Solution { public : /* * @param nums: A list of integers * @return: An integer denotes the sum of max two non-overlapping subarrays */ int maxTwoSubArrays ( vector < int > nums ) { int n = nums . size (); int minSum = 0 ; int sums = 0 ; int maxSum = INT_MIN ; int left [ n ] = { 0 }; int right [ n ] = { 0 }; /* calculate the prefix sum */ for ( int i = 0 ; i < n ; i ++ ) { sums += nums [ i ]; maxSum = max ( maxSum , sums - minSum ); // minSum is previous calculated minSum = min ( minSum , sums ); left [ i ] = maxSum ; } /* calculate the postfix sum */ minSum = 0 ; sums = 0 ; maxSum = INT_MIN ; for ( int i = n - 1 ; i >= 0 ; i -- ) { sums += nums [ i ]; maxSum = max ( maxSum , sums - minSum ); minSum = min ( minSum , sums ); right [ i ] = maxSum ; } /* iterate the divider line, left[i] stored the maxSubArraySum * from nums[0] to nums[i], similar for right[i] */ maxSum = INT_MIN ; for ( int i = 0 ; i < n - 1 ; i ++ ) { maxSum = max ( maxSum , left [ i ] + right [ i + 1 ]); } return maxSum ; } }; Warning Cannot swap the highlighted lines. Because the maximum sum is calculated from current sum minus the previous minSum. Maximum Subarray III* \u00b6 Given an array of integers and a number k, find k non-overlapping subarrays which have the largest sum. The number in each subarray should be contiguous. Return the largest sum. Notice The subarray should contain at least one number Example Input: List = [-1,4,-2,3,-2,3] k = 2 Output: 8 Explanation: 4 + (3 + -2 + 3) = 8 DP solution Use the idea of global maximum and local maximum from Maximum Subarray . See this artical for detailed explaination of the solution. class Solution { public : /** * My initial try: O(n^2 k) * Partitioning DP: f[n][k], maximum K subarrays of first n elements. * Last partition: A[j] ,... A[n - 1] * f[i][k] = max_{0 <= j < i}(f[j][k - 1] + MS(A[j] ,... A[i - 1])) * f[0][0] = * * Solution 2, O(nk) * local[i][k]: Max k subarray sum from \"first i elements\" that include nums[i] * global[i][k]: Max k subarray sum from \"first i elements\" that may not include nums[i] * * 2 cases: nums[i - 1] is kth subarray, nums[i - 1] belongs to kth subarray * local[i][k] = max(global[i - 1][k - 1], local[i - 1][k]) + nums[i - 1] * * 2 cases: not include nums[i - 1], include nums[i - 1] * global[i][k] = max(global[i - 1][k], local[i][k]) */ int maxSubArray ( vector < int > nums , int k ) { int n = nums . size (); int local [ n + 1 ][ k + 1 ] = { 0 }; int global [ n + 1 ][ k + 1 ] = { 0 }; for ( int j = 1 ; j <= k ; j ++ ) { // first j - 1 elements cannot form j groups, set to INT_MIN. local [ j - 1 ][ j ] = INT_MIN ; for ( int i = j ; i <= n ; i ++ ) { // must: i >= k. local [ i ][ j ] = max ( global [ i - 1 ][ j - 1 ], local [ i - 1 ][ j ]) + nums [ i - 1 ]; // the case when we divide k elements into k groups. if ( i == j ) { global [ i ][ j ] = local [ i ][ j ]; } else { global [ i ][ j ] = max ( global [ i - 1 ][ j ], local [ i ][ j ]); } } } return global [ n ][ k ]; } }; Maximum Subarray Difference* \u00b6 Given an array with integers. Find two non-overlapping subarrays A and B, which |SUM(A) - SUM(B)| is the largest. Return the largest difference. Notice The subarray should contain at least one number Example For [1, 2, -3, 1], return 6. Prefix sum solution We use the similar idea for problem Maximum Subarray II . We have to maintain four arrays. from forward maximum and minimum subarray sum and backward maximum and minimum subarray sum. C++ Prefix sum solution class Solution { public : /* * @param nums: A list of integers * @return: value of maximum difference between two subarrays */ int maxDiffSubArrays ( vector < int > nums ) { int n = nums . size (); int minSum = 0 ; int sums = 0 ; int maxSum = INT_MIN ; int left_max [ n ] = { 0 }; int left_min [ n ] = { 0 }; int right_max [ n ] = { 0 }; int right_min [ n ] = { 0 }; for ( int i = 0 ; i < n ; i ++ ) { sums += nums [ i ]; maxSum = max ( maxSum , sums - minSum ); minSum = min ( minSum , sums ); left_max [ i ] = maxSum ; //left_min[i] = minSum; } minSum = INT_MAX ; sums = 0 ; maxSum = 0 ; for ( int i = 0 ; i < n ; i ++ ) { sums += nums [ i ]; minSum = min ( minSum , sums - maxSum ); maxSum = max ( maxSum , sums ); //left_max[i] = maxSum; left_min [ i ] = minSum ; } minSum = 0 ; sums = 0 ; maxSum = INT_MIN ; for ( int i = n - 1 ; i >= 0 ; i -- ) { sums += nums [ i ]; maxSum = max ( maxSum , sums - minSum ); minSum = min ( minSum , sums ); right_max [ i ] = maxSum ; //right_min[i] = minSum; } minSum = INT_MAX ; sums = 0 ; maxSum = 0 ; for ( int i = n - 1 ; i >= 0 ; i -- ) { sums += nums [ i ]; minSum = min ( minSum , sums - maxSum ); maxSum = max ( maxSum , sums ); //right_max[i] = maxSum; right_min [ i ] = minSum ; } int diff = INT_MIN ; for ( int i = 0 ; i < n - 1 ; i ++ ) { diff = max ( left_max [ i ] - right_min [ i + 1 ], diff ); diff = max ( right_max [ i + 1 ] - left_min [ i ], diff ); } return diff ; } }; Maximum Product Subarray \u00b6 DP solution It is similar to the problem Maximum Subarray . Notice the negative number, min multiply a minus number could become the largest product. class Solution { public : int maxProduct ( vector < int >& nums ) { int n = nums . size (); int max_pro [ n ] = { 0 }; int min_pro [ n ] = { 0 }; int result = nums [ 0 ]; max_pro [ 0 ] = nums [ 0 ]; min_pro [ 0 ] = nums [ 0 ]; for ( int i = 1 ; i < n ; i ++ ) { if ( nums [ i ] > 0 ) { max_pro [ i ] = max ( max_pro [ i - 1 ] * nums [ i ], nums [ i ]); min_pro [ i ] = min ( min_pro [ i - 1 ] * nums [ i ], nums [ i ]); } else { max_pro [ i ] = max ( min_pro [ i - 1 ] * nums [ i ], nums [ i ]); min_pro [ i ] = min ( max_pro [ i - 1 ] * nums [ i ], nums [ i ]); } result = max ( result , max_pro [ i ]); } return result ; } }; Constant space solution Without need to check whether nums[i] is positive is negative, we can just find the maximum or minium of three cases. class Solution { public : /* * @param nums: An array of integers * @return: An integer */ int maxProduct ( vector < int > nums ) { int n = nums . size (); int res = nums [ 0 ]; int cur_max = nums [ 0 ]; int cur_min = nums [ 0 ]; for ( int i = 1 ; i < n ; i ++ ) { int tmp = cur_max ; cur_max = max ( max ( cur_max * nums [ i ], nums [ i ]), cur_min * nums [ i ]); cur_min = min ( min ( cur_min * nums [ i ], nums [ i ]), tmp * nums [ i ]); res = max ( res , cur_max ); } return res ; } }; Subarray Product Less Than K \u00b6 Subarray Sum* \u00b6 Given an integer array, find a subarray where the sum of numbers is zero. Your code should return the index of the first number and the index of the last number. Notice There is at least one subarray that it's sum equals to zero. Example Given [-3, 1, 2, -3, 4], return [0, 2] or [1, 3]. Hash solution use a hash table to keep the prefix sum. Once we see another prefix sum that exists in the hash table, we discovered the subarray that sums to zero. However, pay attention to the indexing, because it requires to return the original array's index. class Solution { public : /** * @param nums: A list of integers * @return: A list of integers includes the index of the first number * and the index of the last number */ vector < int > subarraySum ( vector < int > nums ){ int n = nums . size (); vector < int > res ( 2 , 0 ); int sum = 0 ; unordered_map < int , int > map ; map [ 0 ] = -1 ; //important for ( int i = 0 ; i < n ; i ++ ) { sum += nums [ i ]; if ( map . count ( sum ) != 0 ) { res [ 0 ] = map [ sum ] + 1 ; res [ 1 ] = i ; break ; } map [ sum ] = i ; } return res ; } }; // test cases // [-1, 2, 3, -3] A // [0,-1, 1, 4, 1] sum // i j Note Pay attention to the initial value and initializethe map[0] = -1 ; This can be validated with an edge case. The time complexity is O(n) Prefix sum solution Calculate the prefix sum first and then use the prefix sum to find the subarray. This solution is O(n^2) O(n^2) class Solution { public : vector < int > subarraySum ( vector < int > nums ){ int n = nums . size (); vector < int > res ( 2 , 0 ); vector < int > sum ( n + 1 , 0 ); sum [ 0 ] = 0 ; for ( int i = 1 ; i <= n ; i ++ ) { sum [ i ] = sum [ i - 1 ] + nums [ i - 1 ]; } for ( int i = 0 ; i < n ; i ++ ) { for ( int j = i ; j <= n ; j ++ ) { if ( j > 1 && sum [ j ] - sum [ i ] == 0 ) { res [ 0 ] = i ; res [ 1 ] = j - 1 ; break ; } } } return res ; } }; Minimum Size Subarray Sum \u00b6 Accumulative sum solution Using accumulative sum and another moving pointer to check both the sum and the length of the subarray. class Solution { public : int minSubArrayLen ( int s , vector < int >& nums ) { int n = nums . size (); if ( n == 0 ) return 0 ; int sum = 0 ; int res = INT_MAX ; int left = 0 ; for ( int i = 0 ; i < n ; i ++ ) { sum += nums [ i ]; while ( sum >= s ) { res = min ( res , i - left + 1 ); sum -= nums [ left ++ ]; } } return res != INT_MAX ? res : 0 ; } }; Maximum Size Subarray Sum Equals k \u00b6 Similar to Continuous Subarray Sum Hash solution Use a hash table to keep <sums, i> entries. Look up using sum - k . We only add to the hash table for the first time a value is appeared. It ensures the length of the found subarray is the largest. Notice you also have to initialize the hash with value <0, -1> to handle the edge case. class Solution { public : int maxSubArrayLen ( vector < int >& nums , int k ) { int n = nums . size (); if ( n == 0 ) return 0 ; unordered_map < int , int > map ; int sum = 0 ; int left = 0 ; int res = INT_MIN ; map [ 0 ] = -1 ; for ( int i = 0 ; i < n ; i ++ ) { sum += nums [ i ]; if ( map . count ( sum - k ) != 0 ) { left = map [ sum - k ] + 1 ; res = max ( res , ( i - left + 1 )); } if ( map . count ( sum ) == 0 ) { map [ sum ] = i ; } } return res != INT_MIN ? res : 0 ; } }; /* test cases: 1. [-1, 1], -1 \u5982\u679c\u6ca1\u6709\u521d\u59cb\u5316hash\uff0c\u8fd9\u4e2acase\u5c31\u4f1a\u9519\u8bef [-1, 0] sums 2. [-1], 0 3. [-1], -1 4. if return the result will be 1 if there is no res variable [1, 1, 0], 1 [1, 2, 2], 1 */ Subarray Sum Equals K \u00b6 Prefix sum solution Use prefix sum to find the subarray sum. Two pointer to check all the possible subarray sum. C++ prefix sum solution public class Solution { public int subarraySum ( int [] nums , int k ) { int count = 0 ; int [] sum = new int [ nums . length + 1 ]; sum [ 0 ] = 0 ; for ( int i = 1 ; i <= nums . length ; i ++ ) sum [ i ] = sum [ i - 1 ] + nums [ i - 1 ]; for ( int start = 0 ; start < nums . length ; start ++ ) { for ( int end = start + 1 ; end <= nums . length ; end ++ ) { if ( sum [ end ] - sum [ start ] == k ) count ++ ; } } return count ; } } Hash solution Use a map to store the prefix sum and a counter. The idea is while calculating prefix sums, if we find an sums - k exist in the map, we found one of target subarray. The subtilty is for a particular prefix sum, there might be multiple earlier prefix sums differ from it by k. We should take this into account. Compare to the hash solution for problem Subarray Sum . /* k = 2 i = 1, 2, 3 sum = 1, 2, 3 cnt = 0, 1, 2 key = 1, 2, 3 val = 1, 1, 1 the reason that the cnt += map[sum - k], not cnt += 1 is that the prefix sum \"sum - k\" has been shown up for total of map[sum - k] times. All those prefix sum could be result of distinct subarrays between current prefix sum and previous prefix sum \"sum - k\" */ class Solution { public : int subarraySum ( vector < int >& nums , int k ) { int n = nums . size (); // key=prefix sum, val=appearance unordered_map < int , int > map ; int cnt = 0 ; int sum = 0 ; map [ 0 ] = 1 ; for ( int i = 0 ; i < n ; i ++ ) { sum += nums [ i ]; if ( map . count ( sum - k ) != 0 ) { cnt += map [ sum - k ]; } map [ sum ] += 1 ; } return cnt ; } }; Warning Notice you have to initialize the map[0] = 1 ; this is because for cases such as [1, 1, 1] , when i = 1 , sum = 2 , [1,1] should be counted as one subarray. Without setting map[0] = 1 at first hand, it will give incorrect results. Subarray Sums Divisible by K \u00b6 preSum is the basis of continuous subarray profblem. One pass solution should explore a property of modulo (preSum[j] - preSum[i]) % K == 0 indicate the when removing K from the larger value preSum[j] n times, we get the smaller value, then preSum[j] % K == preSum[i] % K . This property makes the one pass solution possible. Once the modulo property is found, we can use Hash map to assist our counting. The ideas is to keep counting the remainder, once we have seen the same remainder in the map, the new index and all the found indexes can be used to retrive one solution. The count keep in the map show how many of those can be. One pass solution class Solution { public : int subarraysDivByK ( vector < int >& A , int K ) { int n = A . size (); if ( n == 0 || K == 0 ) { return 0 ; } int preSum = 0 ; unordered_map < int , int > mp ; mp [ 0 ] = 1 ; int count = 0 ; for ( int i = 0 ; i < n ; i ++ ) { preSum += A [ i ]; int reminder = preSum % K ; // deal with negative values if ( reminder < 0 ) reminder += K ; if ( mp . find ( reminder ) != mp . end ()) { count += mp [ reminder ]; } mp [ reminder ] += 1 ; } return count ; } }; ===\"Naive solution O(n^2)\" class Solution { public : int subarraysDivByK ( vector < int >& A , int K ) { int n = A . size (); if ( n == 0 || K == 0 ) { return 0 ; } vector < long > preSum ( n + 1 , 0 ); preSum [ 0 ] = 0 ; for ( int i = 0 ; i < n ; i ++ ) { preSum [ i + 1 ] = preSum [ i ] + A [ i ]; } int cnt = 0 ; for ( int i = 0 ; i < n ; i ++ ) { for ( int j = i + 1 ; j <= n ; j ++ ) { if (( preSum [ j ] - preSum [ i ]) % K == 0 ) { cnt ++ ; } } } return cnt ; } }; Max Sum of Subarry No Larger Than K* \u00b6 This problem in geeksforgeeks as \"Maximum sum subarray having sum less than or equal to given sum\". It has been discussed here. This problem is the basis to solve the problem 363. Max Sum of Rectangle No Larger Than K. Solution 1 using prefix sum and set calculate prefix and using a set to store individual prefix sum, ( vector also works). In each iteration, we lookup the value preSum - k in the set. Notice we can use binary search to find the smallest element that >= preSum - k . We can use lower_bound to achieve that. Notice if it is asking the sum less than k we have to use upper_bound int maxSumSubarryNoLargerThanK ( int A [], int n , int k ) { set < int > preSumSet ; preSumSet . insert ( 0 ); int res = 0 , preSum = 0 ; for ( int i = 0 ; i < n ; ++ i ) { preSum += A [ i ]; set < int >:: iterator siter = preSumSet . lower_bound ( preSum - k ); if ( siter != preSumSet . end () { res = max ( res , preSum - * siter ); } preSumSet . insert ( preSum ); } return res ; } Max Sum of Rectangle No Larger Than K \u00b6 Solution 1 iterate the wide of the matrix and using prefix sum and set lower_bound . To optimize it with the brute force solution, you will find this problem is a combination of the problem Maximum Sum Rectangular Submatrix in Matrix and problem Max Sum of Subarry No Larger Than K. From the problem Max Sum of Subarry No Larger Than K, we have to enumerate the width of the sub-matrix and sum up all row elements and get an array of length m , m is the number of rows of the matrix. Then apply the method. class Solution { public : int maxSumSubmatrix ( vector < vector < int >>& matrix , int k ) { if ( matrix . empty ()) return 0 ; int m = matrix . size (); int n = m ? matrix [ 0 ]. size () : 0 ; int res = INT_MIN ; for ( int l = 0 ; l < n ; ++ l ) { vector < int > sums ( m , 0 ); for ( int r = l ; r < n ; ++ r ) { for ( int i = 0 ; i < m ; ++ i ) { sums [ i ] += matrix [ i ][ r ]; } set < int > preSumSet ; preSumSet . insert ( 0 ); int preSum = 0 , curMax = INT_MIN ; for ( int sum : sums ) { preSum += sum ; set < int >:: iterator it = preSumSet . lower_bound ( preSum - k ); if ( it != preSumSet . end ()) { curMax = max ( curMax , preSum - * it ); } preSumSet . insert ( preSum ); } res = max ( res , curMax ); } } return res ; } }; Note The complexity is n\u22c5n\u22c5(m+m\\log m)=O(n\u22c5n\u22c5m\\log m) n\u22c5n\u22c5(m+m\\log m)=O(n\u22c5n\u22c5m\\log m) Notice the use of lower_bound, this function return iterator point to element greater than or equal to the value curSum - k, if use upper_bound, it will return iterator points to element greater than curSum - k, which would miss the equal to K case. Solution 2 using merge sort The idea is similar that solution 1. Instead of calculate preSum on the fly, we finish calculation and pass it to a mergeSort routine. The use mergeSort here is to find the A[j] - A[i] <= k efficiently, O(nlogn) . The complexity is n\u22c5n\u22c5(m+m\u22c5\\log m)=O(n\u22c5n\u22c5m\u22c5\\log m) class Solution { public : int maxSumSubmatrix ( vector < vector < int >>& matrix , int k ) { int m = matrix . size (); int n = m ? matrix [ 0 ]. size () : 0 ; int res = INT_MIN ; vector < long long > sums ( m + 1 , 0 ); for ( int l = 0 ; l < n ; ++ l ) { vector < long long > sumInRow ( m , 0 ); for ( int r = l ; r < n ; ++ r ) { for ( int i = 0 ; i < m ; ++ i ) { sumInRow [ i ] += matrix [ i ][ r ]; sums [ i + 1 ] = sums [ i ] + sumInRow [ i ]; } res = max ( res , mergeSort ( sums , 0 , m + 1 , k )); if ( res == k ) return k ; } } return res ; } int mergeSort ( vector < long long >& sums , int start , int end , int k ) { if ( end == start + 1 ) return INT_MIN ; int mid = start + ( end - start ) / 2 ; int res = mergeSort ( sums , start , mid , k ); if ( res == k ) return k ; res = max ( res , mergeSort ( sums , mid , end , k )); if ( res == k ) return k ; long long cache [ end - start ]; int j = mid , c = 0 , t = mid ; for ( int i = start ; i < mid ; ++ i ) { /* search first time sums[j] - sums[i] > k */ while ( j < end && sums [ j ] - sums [ i ] <= k ) ++ j ; /* sums[j - 1] - sums[i] <= k, make sure j - 1 is in right side */ if ( j - 1 >= mid ) { res = max ( res , ( int )( sums [ j - 1 ] - sums [ i ])); if ( res == k ) return k ; } /* parallel merge */ while ( t < end && sums [ t ] < sums [ i ]) { cache [ c ++ ] = sums [ t ++ ]; } cache [ c ++ ] = sums [ i ]; } /* parallel merge */ for ( int i = start ; i < t ; ++ i ) { sums [ i ] = cache [ i - start ]; } return res ; } }; Maximum Sum Rectangular Submatrix in Matrix* \u00b6 Subarray Sum Closest* \u00b6 Shortest Unsorted Continuous Subarray (Count inversions) \u00b6 Solution 1 using merge sort This problem is the basic of using merge sort to solve lots of hard problems. long long merge_and_count ( int A [], int start , int end ) { if ( end - start <= 1 ) return 0 ; long long count = 0 ; int mid = start + ( end - start ) / 2 ; count = merge_and_count ( A , start , mid ) + merge_and_count ( A , mid , end ); int j = mid ; int cache [ end - start ]; for ( int i = start , k = 0 ; i < mid ; i ++ ) { while ( j < end && A [ i ] > A [ j ]) { cache [ k ++ ] = A [ j ++ ]; count += mid - i ; } cache [ k ++ ] = A [ i ]; } // copy cache[0, j - start) to A[start, j) // if j < end, then A[j, end) in final place. for ( int i = start ; i < j ; i ++ ) { A [ i ] = cache [ i - start ]; } return count ; } Count Inversion (course assignment) \u00b6 Solution 1 using merge sort This problem is the basic of using merge sort to solve lots of hard problems. long long merge_and_count ( int A [], int start , int end ) { if ( end - start <= 1 ) return 0 ; long long count = 0 ; int mid = start + ( end - start ) / 2 ; count = merge_and_count ( A , start , mid ) + merge_and_count ( A , mid , end ); int j = mid ; int cache [ end - start ]; for ( int i = start , k = 0 ; i < mid ; i ++ ) { while ( j < end && A [ i ] > A [ j ]) { cache [ k ++ ] = A [ j ++ ]; count += mid - i ; } cache [ k ++ ] = A [ i ]; } // copy cache[0, j - start) to A[start, j) // if j < end, then A[j, end) in final place. for ( int i = start ; i < j ; i ++ ) { A [ i ] = cache [ i - start ]; } return count ; } Count of Smaller Numbers After Self \u00b6 Solution 1 Merge sort One important point to remember is you have to create pairs out of the array element and its index, because during merge sort, when we count each value, we don't know where to put those count values in the result vector. The second merge solutions run much faster than the first one. C++ Merge sort class Solution { public : vector < int > countSmaller ( vector < int >& nums ) { int n = nums . size (); vector < int > res ( n , 0 ); vector < pair < int , int > > vp ; for ( int i = 0 ; i < n ; i ++ ) { vp . emplace_back ( nums [ i ], i ); } merge_sort_count ( vp , 0 , n , res ); return res ; } private : void merge_sort_count ( vector < pair < int , int > >& nums , int start , int end , vector < int >& res ) { if ( end - start <= 1 ) return ; int mid = start + ( end - start ) / 2 ; merge_sort_count ( nums , start , mid , res ); merge_sort_count ( nums , mid , end , res ); vector < pair < int , int > > cache ( end - start , pair < int , int > ( 0 , 0 )); int j = mid , k = 0 , t = mid ; for ( int i = start ; i < mid ; i ++ ) { j = mid ; while ( j < end && nums [ i ]. first > nums [ j ]. first ) { // found smaller elements res [ nums [ i ]. second ] ++ ; j ++ ; } while ( t < end && nums [ i ]. first > nums [ t ]. first ) { cache [ k ++ ] = nums [ t ++ ]; } cache [ k ++ ] = nums [ i ]; } for ( int i = start ; i < j ; i ++ ) { nums [ i ] = cache [ i - start ]; } return ; } }; C++ more efficient Merge sort class Solution { public : vector < int > countSmaller ( vector < int >& nums ) { int n = nums . size (); vector < int > res ( n , 0 ); vector < pair < int , int > > vp ; for ( int i = 0 ; i < n ; i ++ ) { vp . emplace_back ( nums [ i ], i ); } mergeSort ( vp , 0 , n , res ); return res ; } void mergeSort ( vector < pair < int , int >>& x , int start , int end , vector < int >& res ) { if ( end - start <= 1 ) return ; int mid = start + ( end - start ) / 2 ; mergeSort ( x , start , mid , res ); mergeSort ( x , mid , end , res ); vector < pair < int , int > > cache ( end - start , pair < int , int > ( 0 , 0 )); int i = start , j = mid , k = 0 ; while ( i < mid && j < end ) { if ( x [ i ]. first <= x [ j ]. first ) { cache [ k ++ ] = x [ i ]; res [ x [ i ]. second ] += j - mid ; ++ i ; } else { cache [ k ++ ] = x [ j ++ ]; } } while ( i < mid ) { cache [ k ++ ] = x [ i ]; res [ x [ i ]. second ] += end - mid ; ++ i ; } while ( j < end ) cache [ k ++ ] = x [ j ++ ]; for ( i = start , k = 0 ; i < end ; ++ i , ++ k ) { x [ i ] = cache [ k ]; } } }; C++ BST class Solution { public : class TreeNode { public : int val , smallerCnt ; TreeNode * left , * right ; TreeNode ( int v , int s ) : left ( NULL ), right ( NULL ), val ( v ), smallerCnt ( s ){} }; vector < int > countSmaller ( vector < int >& nums ) { int n = nums . size (); if ( n == 0 ) return {}; vector < int > res ( n , 0 ); TreeNode * root = NULL ; for ( int i = n - 1 ; i >= 0 ; -- i ) root = insert ( root , nums [ i ], i , 0 , res ); return res ; } private : TreeNode * insert ( TreeNode * node , int val , int idx , int preSum , vector < int >& res ) { if ( node == NULL ) { node = new TreeNode ( val , 0 ); res [ idx ] = preSum ; } else if ( node -> val > val ) { node -> smallerCnt ++ ; node -> left = insert ( node -> left , val , idx , preSum , res ); } else { node -> right = insert ( node -> right , val , idx , preSum + node -> smallerCnt + (( node -> val < val ) ? 1 : 0 ), res ); } return node ; } }; Continuous Subarray Sum \u00b6 Hash solution Once see a multiple of K, you should consider the modulor operation % The values put into to the hash only for the first time, this is similar to the case in the problem Maximum Size Subarray Sum Equals k. C++ Hash soution\" hl_lines=\"11 class Solution { public : bool checkSubarraySum ( vector < int >& nums , int k ) { int n = nums . size (); if ( n == 0 ) return false ; unordered_map < int , int > map ; int sum = 0 ; map [ 0 ] = -1 ; // test case [0, 0], 0 for ( int i = 0 ; i < n ; i ++ ) { sum += nums [ i ]; if ( k != 0 ) sum = sum % k ; if ( map . count ( sum ) != 0 ) { if ( i - map [ sum ] > 1 ) { return true ; } } else { map [ sum ] = i ; } } return false ; } }; Contiguous Array \u00b6 Similar problems: Continuous Subarray Sum Maximum Size Subarray Sum Equals k Hash solution This problem is very similar to the problem Continuous Subarray Sum . However, there is a trick to calculate the cummulative sum, treat 0 as -1 . class Solution { public : int findMaxLength ( vector < int >& nums ) { int n = nums . size (); int res = 0 ; int cnt = 0 ; unordered_map < int , int > map ; map [ 0 ] = -1 ; // test case: [0, 1] for ( int i = 0 ; i < n ; i ++ ) { cnt += nums [ i ] == 0 ? -1 : 1 ; if ( map . count ( cnt ) != 0 ) { res = max ( res , i - map [ cnt ]); } else { map [ cnt ] = i ; } } return res ; } }; Split Array with Equal Sum \u00b6 Cummulative sum soluiton Because of the symetric property of the head subarray and trailing subarray, we can calculate cumulative sum from both direction. This can help to fix the index i and k . we can enumerate the index j in between. C++ cummulateive sum solution class Solution { public : bool splitArray ( vector < int >& nums ) { int n = nums . size (); int sum1 [ n ] = { 0 }; int sum2 [ n ] = { 0 }; sum1 [ 0 ] = nums [ 0 ]; for ( int i = 1 ; i < n ; i ++ ) { sum1 [ i ] = sum1 [ i - 1 ] + nums [ i ]; } sum2 [ n - 1 ] = nums [ n - 1 ]; for ( int i = n - 2 ; i >= 0 ; i -- ) { sum2 [ i ] = sum2 [ i + 1 ] + nums [ i ]; } // notice the index bounds for ( int i = 1 ; i < n - 5 ; i ++ ) { for ( int k = n - 2 ; k > i + 3 ; k -- ) { if ( sum1 [ i ] - nums [ i ] == sum2 [ k ] - nums [ k ]) { for ( int j = i + 2 ; j < k - 1 ; j ++ ) { int sumij = sum1 [ j ] - nums [ j ] - sum1 [ i ]; int sumjk = sum2 [ j ] - nums [ j ] - sum2 [ k ]; if ( sumij == sumjk ) { return true ; } } } } } return false ; } }; 410. Split Array Largest Sum \u00b6 Similar problems: Copy Books (linktcode) . DP solution Notice the edge case: [1, INT_MAX] , use double can avoid integer overflow. Binary Search solution This is a greedy search solution that use binary search to accelerate the search speed The goal is to \"minimize the largest sub-array sum\". It is different from Divide Chocolate , which is maximize the smallest sum. The bisection condition is not A[m] < target any more. It is a function to check whether the constrain can meet given a guess value mid . C++ DP /** * equivalent to the lintcode copy books problem * * last step: mth subarray A[j], ..., A[i - 1]. * State: f[m][n]: minmax sum of m subarrays that include n elements * Equation: f[m][n] = min_{0<=j<n}(max(f[m - 1][j], sum(A[j], ..., A[n - 1]))) * Init: f[0][n] = INT_MAX; * f[0][0] = 0; * NB: notice a special case: [1, 2147483247], 2 * the sum will overflow in the state update, You use a double type */ class Solution { public : int splitArray ( vector < int >& nums , int m ) { int n = nums . size (); double f [ m + 1 ][ n + 1 ]; f [ 0 ][ 0 ] = 0 ; for ( int i = 1 ; i <= n ; i ++ ) { f [ 0 ][ i ] = INT_MAX ; } double sum = 0 ; for ( int k = 1 ; k <= m ; k ++ ) { f [ k ][ 0 ] = 0 ; for ( int i = 1 ; i <= n ; i ++ ) { sum = 0 ; f [ k ][ i ] = INT_MAX ; for ( int j = i ; j >= 0 ; j -- ) { //j = i, mean sum = 0. f [ k ][ i ] = min ( f [ k ][ i ], max ( f [ k - 1 ][ j ], sum )); if ( j > 0 ) { sum += nums [ j - 1 ]; } } } } return f [ m ][ n ]; } }; C++ binary search class Solution { public : int splitArray ( vector < int >& nums , int m ) { int total = 0 ; int mx = 0 ; for ( int num : nums ) { total += num ; mx = max ( mx , num ); } int l = mx , r = total ; while ( l < r ) { int mid = l + ( r - l ) / 2 ; if ( ! canCut ( nums , mid , m - 1 )) { l = mid + 1 ; } else { r = mid ; } } return l ; } // whether m cuts are possible, notice the greedy property of this check // you should notice that if not possible, it is because mid is too small, // not because it is too large. bool canCut ( vector < int >& nums , int mid , int m ) { int sum = 0 ; for ( int num : nums ) { if ( num > mid ) return false ; else if ( sum + num <= mid ) sum += num ; // each cut is greedy else { // cut is ok so far m -- ; if ( m < 0 ) return false ; // more element after all cuts. sum = num ; // init the next group sum } } return true ; } }; C++ binary search (count the # of cuts) class Solution { public : int splitArray ( vector < int >& nums , int m ) { int l = * max_element ( nums . begin (), nums . end ()); int r = accumulate ( nums . begin (), nums . end (), 0 ); while ( l < r ) { int mid = l + ( r - l ) / 2 ; int s = 0 ; int c = 0 ; // count the possible cuts for ( int n : nums ) { if (( s += n ) > mid ) { s = n ; if ( ++ c > m - 1 ) { break ; } } } if ( c > m - 1 ) { l = mid + 1 ; } else { // the \"> mid\" above guarantee the \"no greater than\" // the guess value, if c == m - 1, mid could be the result r = mid ; } } return l ; } }; C++ binary search alternative class Solution { public : int splitArray ( vector < int >& nums , int m ) { int l = * max_element ( nums . begin (), nums . end ()); int r = accumulate ( nums . begin (), nums . end (), 0 ); while ( l < r ) { int mid = l + ( r - l ) / 2 ; int s = 0 ; int c = 1 ; // count the possible cuts for ( int n : nums ) { if ( s + n > mid ) { // voilate the constrains s = 0 ; c ++ ; } s += n ; } if ( c > m ) { l = mid + 1 ; } else { // the \"> mid\" above guarantee the \"no greater than\" // the guess value, if c == m, mid could be the result r = mid ; } } return l ; } }; Copy books (lintcode) \u00b6 Description Given n books and the ith book has A[i] pages. You are given k people to copy the n books. the n books list in a row and each person can claim a continuous range of the n books. For example, one copier can copy the books from ith to jth continuously, but he can not copy the 1st book, 2nd book and 4th book (without the 3rd book). They start copying books at the same time and they all cost 1 minute to copy 1 page of a book. What's the best strategy to assign books so that the slowest copier can finish at the earliest time? Example Given array A = [3,2,4], k = 2. Return 5 (First person spends 5 minutes to copy book 1 and book 2 and the second person spends 4 minutes to copy book 3.) Solution 1 Binary search See the solution for 410. Split Array Largest Sum Solution 2 DP solution There are i books, consider the last copier, he can copy A[j], ..., A[i-1] . The first k-1 copier copy A[0], ..., A[j - 1] . Define state: f[k][i] , meaning the k-th copier copy i books. State transition equation: f[k][i] = \\min_{0 \\le j \\le i} \\max(f[k - 1][j], A[j] + ... + A[i - 1]) f[k][i] = \\min_{0 \\le j \\le i} \\max(f[k - 1][j], A[j] + ... + A[i - 1]) C++ DP solution class Solution { public : /** * last step: last copier copy A[j], ... A[i-1] * first k-1 copier --> A[0], ... A[j - 1]. * f[k][i]: k copier copy i books. * f[k][i] = \\min_{0 \\le j \\le i} \\max(f[k - 1][j], A[j] + ... + A[i - 1]) */ int copyBooks ( vector < int > & pages , int K ) { // write your code here int n = pages . size (); if ( n == 0 ) { return 0 ; } if ( K > n ) { K = n ; } int f [ K + 1 ][ n + 1 ]; /* init */ f [ 0 ][ 0 ] = 0 ; for ( int j = 1 ; j <= n ; j ++ ) { f [ 0 ][ j ] = INT_MAX ; } int sum = 0 ; for ( int k = 1 ; k <= K ; k ++ ) { f [ k ][ 0 ] = 0 ; for ( int i = 1 ; i <= n ; i ++ ) { sum = 0 ; f [ k ][ i ] = INT_MAX ; for ( int j = i ; j >= 0 ; j -- ) { f [ k ][ i ] = min ( f [ k ][ i ], max ( f [ k - 1 ][ j ], sum )); if ( j > 0 ) { sum += pages [ j - 1 ]; } } } } return f [ K ][ n ]; } }; Note We have to enumerate the index j , the highlighted code used a clever technique to optimize this task. It enumerate j backwards. While this seems impossible at the first glance, how can you calculate the states from right to left in DP? Notice the index j is in the upper row (row k-1 ). Once we are in the k -th row, the values in the k-1 -th row are all given. Maximum Average Subarray I \u00b6 Prefix sum solution class Solution { public : double findMaxAverage ( vector < int >& nums , int k ) { int n = nums . size (); double sums [ n ] = { 0 }; double max_avg = INT_MIN ; sums [ 0 ] = nums [ 0 ]; for ( int i = 1 ; i < n ; i ++ ) { sums [ i ] = sums [ i - 1 ] + nums [ i ]; } for ( int i = k - 1 ; i < n ; i ++ ) { double avg = ( sums [ i ] - sums [ i - k + 1 ] + nums [ i - k + 1 ]) / k ; max_avg = max ( max_avg , avg ); } return max_avg ; } }; Maximum Average Subarray II \u00b6 Prefix sum solution This is still a brute force solution. time complexity: O(n^2) O(n^2) space complexity: O(n) O(n) Space optimized solution We could avoid using the prefix sum array and only use two variables to record the prefix sum at any particular instance. One for record prefix sum of exact k elements. Another for the inner loop to check whether removing an element from the beginning will make a new maximum value or not. Space optimized solution We could avoid using the prefix sum array and only use two variables to record the prefix sum at any particular instance. One for record prefix sum of exact k elements. Another for the inner loop to check whether removing an element from the beginning will make a new maximum value or not. Deque solution C++ prefix sum solution class Solution { public : double findMaxAverage ( vector < int >& nums , int k ) { int n = nums . size (); vector < int > sums = nums ; for ( int i = 1 ; i < n ; ++ i ) { sums [ i ] = sums [ i - 1 ] + nums [ i ]; } double res = ( double ) sums [ k - 1 ] / k ; for ( int i = k ; i < n ; ++ i ) { double t = sums [ i ]; if ( t > res * ( i + 1 )) res = t / ( i + 1 ); for ( int j = 0 ; j < i - k + 1 ; ++ j ) { t = sums [ i ] - sums [ j ]; if ( t > res * ( i - j )) res = t / ( i - j ); } } return res ; } }; C++ space optimized class Solution { public : double findMaxAverage ( vector < int >& nums , int k ) { int n = nums . size (); /* range is half open */ double sumsAll = accumulate ( nums . begin (), nums . begin () + k , 0 ); double sums = sumsAll , res = sumsAll / k ; for ( int i = k ; i < n ; ++ i ) { sumsAll += nums [ i ]; sums = sumsAll ; if ( sums > res * ( i + 1 )) res = sums / ( i + 1 ); for ( int j = 0 ; j < i - k + 1 ; ++ j ) { sums -= nums [ j ]; if ( sums > res * ( i - j )) res = sums / ( i - j ); } } return res ; } }; C++ binary search soluiton class Solution { public : double findMaxAverage ( vector < int >& nums , int k ) { int n = nums . size (); double upper = INT_MIN , lower = INT_MAX ; for ( auto num : nums ) { upper = max ( upper , ( double ) num ); lower = min ( lower , ( double ) num ); } while ( lower + 0.00001 < upper ) { double mid = lower + ( upper - lower ) / 2 ; if ( isLarger ( nums , mid , k )) { // is average value >= mid? lower = mid ; } else { upper = mid ; } } return lower ; } /* return true if a greater average value is possible */ bool isLarger ( vector < int >& nums , double mid , int k ) { int n = nums . size (); double sums = 0 , prev = 0 , prev_min = 0 ; for ( int i = 0 ; i < k ; i ++ ) { sums += nums [ i ] - mid ; } if ( sums >= 0 ) { return true ; } /* * we keep looking for whether a subarray sum of length >= k in array * \"sums\" is possible to be greater than zero. If such a subarray exist, * it means that the target average value is greater than the \"mid\" * value. We look at the front part of sums that at least k element * apart from i. If we can find the minimum of the sums[0, 1, ..., i - k] * and check if sums[i] - min(sum[0, 1, ..., i - k]) >= 0. If this is the * case, it indicate there exist a subarray of length >= k with sum * greater than 0 in sums, we can return ture, otherwise, false. */ for ( int i = k ; i < n ; i ++ ) { sums += nums [ i ] - mid ; prev += nums [ i - k ] - mid ; prev_min = min ( prev_min , prev ); if ( sums >= prev_min ) return true ; } return false ; } }; C++ deque solution class Solution { public : double findMaxAverage ( vector < int >& nums , int k ) { int n = nums . size (); vector < double > sums ( n , 0 ); deque < int > q ; sums [ 0 ] = nums [ 0 ]; for ( int i = 1 ; i < n ; ++ i ) sums [ i ] = sums [ i - 1 ] + nums [ i ]; double res = sums [ n - 1 ] / n ; for ( int j = k - 1 ; j < n ; ++ j ) { while ( q . size () >= 2 && density ( sums , q [ q . size () - 2 ], q . back () - 1 ) >= density ( sums , q . back (), j - k )) { q . pop_back (); } q . push_back ( j - k + 1 ); while ( q . size () >= 2 && density ( sums , q [ 0 ], j ) <= density ( sums , q [ 1 ], j )) { q . pop_front (); } res = max ( res , density ( sums , q . front (), j )); } return res ; } private : double density ( vector < double >& sums , int l , int r ) { if ( l == 0 ) return sums [ r ] / ( r + 1 ); return ( sums [ r ] - sums [ l - 1 ]) / ( r - l + 1 ); } }; Note Notice the initial value of prev_min is set to 0 not INT_MAX; Try to understand why set the initial value of prev_min to INT_MAX cannot pass the test case: [8,9,3,1,8,3,0,6,9,2] , 8. Range Sum Query - Immutable \u00b6 Prefix sum solution Use prefix sum to record the accumulative sum of the array in the constructor. The algorithm is O(n) O(n) in space and O(1) O(1) in time. class NumArray { private : vector < int > sums ; int n ; public : NumArray ( vector < int > nums ) { n = nums . size (); sums . resize ( n + 1 , 0 ); sums [ 0 ] = 0 ; for ( int i = 1 ; i <= n ; ++ i ) { sums [ i ] = sums [ i - 1 ] + nums [ i - 1 ]; } } int sumRange ( int i , int j ) { return sums [ j + 1 ] - sums [ i ]; } }; /** * Your NumArray object will be instantiated and called as such: * NumArray obj = new NumArray(nums); * int param_1 = obj.sumRange(i,j); */ Range Sum Query - Mutable \u00b6 Segment tree solution Using segment tree, the solution is given at Leetcode Solution . C++ segment tree class NumArray { private : vector < int > tree ; int n ; public : NumArray ( vector < int > nums ) { n = nums . size (); tree . resize ( 2 * n , 0 ); for ( int i = n , j = 0 ; i < 2 * n ; ++ i , ++ j ) { tree [ i ] = nums [ j ]; } for ( int i = n - 1 ; i > 0 ; -- i ) { tree [ i ] = tree [ 2 * i ] + tree [ 2 * i + 1 ]; } } void update ( int i , int val ) { int pos = n + i ; int left = 0 ; int right = 0 ; tree [ pos ] = val ; while ( pos > 0 ) { left = pos ; right = pos ; if ( pos % 2 == 0 ) { right = pos + 1 ; } if ( pos % 2 == 1 ) { left = pos - 1 ; } tree [ pos / 2 ] = tree [ left ] + tree [ right ]; pos /= 2 ; } } int sumRange ( int i , int j ) { int left = i + n ; int right = j + n ; int sum = 0 ; while ( left <= right ) { if ( left % 2 == 1 ) { sum += tree [ left ]; left ++ ; } if ( right % 2 == 0 ) { sum += tree [ right ]; right -- ; } left /= 2 ; right /= 2 ; } return sum ; } }; /** * Your NumArray object will be instantiated and called as such: * NumArray obj = new NumArray(nums); * obj.update(i,val); * int param_2 = obj.sumRange(i,j); */ Binary Indexed Tree solution I Using Binary indexed tree, we are able to solve it optimally in O(\\log n) O(\\log n) . The solution originally from here class NumArray { private : vector < int > arr ; vector < int > BIT ; int n ; public : NumArray ( vector < int > nums ) { arr = nums ; n = nums . size (); BIT . resize ( n + 1 , 0 ); for ( int i = 0 ; i < n ; i ++ ) { init ( i , arr [ i ]); } } void init ( int i , int val ) { i ++ ; while ( i <= n ) { BIT [ i ] += val ; // BIT[i] = nums[0, i - 1]; i += i & ( - i ); } } void update ( int i , int val ) { int diff = val - arr [ i ]; arr [ i ] = val ; init ( i , diff ); } int sumRange ( int i , int j ) { return getSum ( j ) - getSum ( i - 1 ); } int getSum ( int i ) { i ++ ; int sum = 0 ; while ( i > 0 ) { sum += BIT [ i ]; i -= i & ( - i ); } return sum ; } }; /** * Your NumArray object will be instantiated and called as such: * NumArray obj = new NumArray(nums); * obj.update(i,val); * int param_2 = obj.sumRange(i,j); */ Binary Indexed Tree solution II Similar to the above solution, We have combined the init and update . To make it consistant with the solution with problem Range Sum Query 2D - Mutable class NumArray { private : vector < int > arr ; vector < int > BIT ; int n ; public : NumArray ( vector < int > nums ) { n = nums . size (); BIT . resize ( n + 1 , 0 ); arr . resize ( n , 0 ); for ( int i = 0 ; i < n ; i ++ ) { update ( i , nums [ i ]); } } /* We can combine the init and update like this */ void update ( int i , int val ) { int diff = val - arr [ i ]; arr [ i ] = val ; // here we initialize arr[i] i ++ ; while ( i <= n ) { BIT [ i ] += diff ; // BIT[i] = nums[0, i - 1]; i += i & ( - i ); } } int sumRange ( int i , int j ) { return getSum ( j ) - getSum ( i - 1 ); } int getSum ( int i ) { i ++ ; int sum = 0 ; while ( i > 0 ) { sum += BIT [ i ]; i -= i & ( - i ); } return sum ; } }; /** * Your NumArray object will be instantiated and called as such: * NumArray obj = new NumArray(nums); * obj.update(i,val); * int param_2 = obj.sumRange(i,j); */ Range Sum Query 2D - Immutable \u00b6 Prefix sum solution Extended from the 1d array, we can use the prefix sum of the 2d matrix. we use extra space to store the accumulative sum of the submatrix with upper left coordinate (0, 0) and lower right coordinate (i, j) . C++ prefix sum solution class NumMatrix { private : vector < vector < int > > dp ; public : NumMatrix ( vector < vector < int >> matrix ) { int m = matrix . size (); if ( m == 0 ) return ; int n = matrix [ 0 ]. size (); //dp = vector<vector<int> (m + 1, vector<int>(n + 1, 0)); dp . resize ( m + 1 , vector < int > ( n + 1 , 0 )); for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { dp [ i + 1 ][ j + 1 ] = dp [ i ][ j + 1 ] + dp [ i + 1 ][ j ] + matrix [ i ][ j ] - dp [ i ][ j ]; } } } int sumRegion ( int row1 , int col1 , int row2 , int col2 ) { return dp [ row2 + 1 ][ col2 + 1 ] - dp [ row2 + 1 ][ col1 ] - dp [ row1 ][ col2 + 1 ] + dp [ row1 ][ col1 ]; } }; /** * Your NumMatrix object will be instantiated and called as such: * NumMatrix obj = new NumMatrix(matrix); * int param_1 = obj.sumRegion(row1,col1,row2,col2); */ Note This version create the m + 1 by n + 1 dp array to record the prefix sum. The code is clean and elegant. Alternative prefix sum solution The idea is the same, in the following solution, we have a m by n 2d array to record the accumulative sum. See how complex the code is. C++ prefix sum solution class NumMatrix { private : vector < vector < int >> dp ; int m ; int n ; public : NumMatrix ( vector < vector < int >> matrix ) { m = matrix . size (); if ( m == 0 ) return ; n = matrix [ 0 ]. size (); dp . resize ( m , vector < int > ( n , 0 )); for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { dp [ i ][ j ] = matrix [ i ][ j ]; if ( i > 0 ) { dp [ i ][ j ] += dp [ i - 1 ][ j ]; } if ( j > 0 ) { dp [ i ][ j ] += dp [ i ][ j - 1 ]; } if ( i > 0 && j > 0 ) { dp [ i ][ j ] -= dp [ i - 1 ][ j - 1 ]; } } } } int sumRegion ( int row1 , int col1 , int row2 , int col2 ) { int res = 0 ; res = dp [ row2 ][ col2 ]; if ( row1 > 0 ) { res -= dp [ row1 - 1 ][ col2 ]; } if ( col1 > 0 ) { res -= dp [ row2 ][ col1 - 1 ]; } if ( row1 > 0 && col1 > 0 ) { res += dp [ row1 - 1 ][ col1 - 1 ]; } return res ; } }; /** * Your NumMatrix object will be instantiated and called as such: * NumMatrix obj = new NumMatrix(matrix); * int param_1 = obj.sumRegion(row1,col1,row2,col2); */ Note This version is a little complex. However, the way it was written reflects a very important practice when operating on a 2d array, that is: to check the validation of the array. Range Sum Query 2D - Mutable \u00b6 Binary Indexed Tree solution We use 2D version of Binary Index Tree. Some of the explaination can be found at Topcoder tutorial C++ BIT solution class NumMatrix { private : vector < vector < int > > nums ; vector < vector < int > > tree ; int m ; int n ; public : NumMatrix ( vector < vector < int >> matrix ) { if ( matrix . size () == 0 || matrix [ 0 ]. size () == 0 ) return ; m = matrix . size (); n = matrix [ 0 ]. size (); tree . resize ( m + 1 , vector < int > ( n + 1 , 0 )); nums . resize ( m , vector < int > ( n , 0 )); for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { update ( i , j , matrix [ i ][ j ]); } } } void update ( int row , int col , int val ) { if ( m == 0 || n == 0 ) return ; int diff = val - nums [ row ][ col ]; nums [ row ][ col ] = val ; for ( int i = row + 1 ; i <= m ; i += i & ( - i )) { for ( int j = col + 1 ; j <= n ; j += j & ( - j )) { tree [ i ][ j ] += diff ; } } } int sumRegion ( int row1 , int col1 , int row2 , int col2 ) { if ( m == 0 || n == 0 ) return 0 ; return getSum ( row2 + 1 , col2 + 1 ) - getSum ( row1 , col2 + 1 ) - getSum ( row2 + 1 , col1 ) + getSum ( row1 , col1 ); } int getSum ( int row , int col ) { int sum = 0 ; for ( int i = row ; i > 0 ; i -= i & ( - i )) { for ( int j = col ; j > 0 ; j -= j & ( - j )) { sum += tree [ i ][ j ]; } } return sum ; } }; /** * Your NumMatrix object will be instantiated and called as such: * NumMatrix obj = new NumMatrix(matrix); * obj.update(row,col,val); * int param_2 = obj.sumRegion(row1,col1,row2,col2); */ Count of Range Sum \u00b6 Solution 1 Merge sort using inplace_merge() The core is to figure out how to calculate the result while merging. It is based on the fact that the left half and right half are all sorted. Using the ordering information we are able to locate two points in the right half j and k , between which will fulfill the requirement. Several important points need to be made. 1) calculation of prefix sum of the array. The length is n + 1 not n ? 2) the range passed to the merge subroutine are open-end [start, end) . The base case of the subrouine. It return zero becuase the case has been counted in the for loop, we don't need to count it again. Not because the base case is 0 . class Solution { public : int countRangeSum ( vector < int >& nums , int lower , int upper ) { int n = nums . size (); vector < long > sums ( n + 1 , 0 ); for ( int i = 0 ; i < n ; i ++ ) sums [ i + 1 ] = sums [ i ] + nums [ i ]; return mergeSort ( sums , 0 , n + 1 , lower , upper ); } int mergeSort ( vector < long >& sums , int start , int end , int lower , int upper ) { if ( end - start <= 1 ) return 0 ; // note the meaning of this 0. int mid = start + ( end - start ) / 2 ; count = mergeSort ( sums , start , mid , lower , upper ) + mergeSort ( sums , mid , end , lower , upper ); int m = mid , n = mid , count = 0 ; for ( int i = start ; i < mid ; i ++ ) { while ( m < end && sums [ m ] - sums [ i ] < lower ) m ++ ; while ( n < end && sums [ n ] - sums [ i ] <= upper ) n ++ ; count += n - m ; } inplace_merge ( sums . begin () + start , sums . begin () + mid , sums . begin () + end ); return count ; } }; Solution 2 Merge sort using tmp buffer cache Here is how the count is making sense. |--------------|-------------------| sums: |start |mid |end |---|----------|------|------|-----| i j k Because sums[j] - sums[i] >= lower, and sums[k] - sums[i] > upper, So for the subarray start with i, ending index in [j, k), the range sum is in [lower, upper]. Notice k should not be included. class Solution { public : int countRangeSum ( vector < int >& nums , int lower , int upper ) { int n = nums . size (); long sums [ n + 1 ] = { 0 }; if ( n == 0 ) return 0 ; for ( int i = 0 ; i < n ; i ++ ) { sums [ i + 1 ] = sums [ i ] + nums [ i ]; } /* n + 1 is the one pass the last element of sums */ return countByMergeSort ( sums , 0 , n + 1 , lower , upper ); } /* This function will return sorted array sums[start], ... sums[end - 1] */ int countByMergeSort ( long sums [], int start , int end , int lower , int upper ) { if ( end - start <= 1 ) return 0 ; int mid = start + ( end - start ) / 2 ; int count = countByMergeSort ( sums , start , mid , lower , upper ) + countByMergeSort ( sums , mid , end , lower , upper ); long cache [ end - start ] = { 0 }; int j = mid , k = mid , t = mid ; for ( int i = start , r = 0 ; i < mid ; ++ i , ++ r ) { while ( k < end && sums [ k ] - sums [ i ] < lower ) k ++ ; while ( j < end && sums [ j ] - sums [ i ] <= upper ) j ++ ; count += j - k ; /* calculate the result */ /* Merge left and right to get sorted array {sums[start], .. sums[end - 1]}. * Because left part of sums[start] to sums[mid] are already sorted, * use cache here to merge prefix of the right part: sum[mid] to sums[t] * with left part upto sums[i] for all i = {start, mid - 1}. */ while ( t < end && sums [ t ] < sums [ i ]) cache [ r ++ ] = sums [ t ++ ]; cache [ r ] = sums [ i ]; } /* after this for loop, cache will have partially sorted array * cache = sums_left = {sums[start], ... sums[t - 1]} element * of which will be in their final sorted positions. * array sums_right = {sums[t], sums[end - 1]} is also * in their final sorted positions. */ /* Since the sums_left is sorted, it have size of t - start, * here we copy exactly t - start element from cache to sums. */ for ( int i = start ; i < t ; i ++ ) sums [ i ] = cache [ i - start ]; return count ; } }; Solution 3 BST Solution 4 BIT Maximum Sum of Two Non-Overlapping Subarrays \u00b6 Brute Force Iterate class Solution { public : int maxSumTwoNoOverlap ( vector < int >& A , int L , int M ) { int n = A . size (); if ( L == 0 || M == 0 ) { return 0 ; } vector < int > preSum ( n , 0 ); preSum [ 0 ] = A [ 0 ]; for ( int i = 1 ; i < n ; i ++ ) { preSum [ i ] = preSum [ i - 1 ] + A [ i ]; } int res = 0 ; // iterate the L using index i for ( int i = 0 ; i < n - L + 1 ; i ++ ) { int Lsum = 0 ; if ( i == 0 ) { Lsum = preSum [ i + L - 1 ]; } else { Lsum = preSum [ i + L - 1 ] - preSum [ i - 1 ]; } int Msum = 0 ; // iterate the left M array using index j for ( int j = 0 ; j < i - M ; j ++ ) { int tmp = 0 ; if ( j == 0 ) { tmp = preSum [ j + M - 1 ]; } else { tmp = preSum [ j + M - 1 ] - preSum [ j - 1 ]; } Msum = max ( Msum , tmp ); } // iterate the right M array using index j for ( int j = i + L ; j < n - M + 1 ; j ++ ) { Msum = max ( Msum , preSum [ j + M - 1 ] - preSum [ j - 1 ]); } res = max ( res , Msum + Lsum ); } return res ; } }; One pass class Solution { public : int maxSumTwoNoOverlap ( vector < int >& A , int L , int M ) { int n = A . size (); if ( L == 0 || M == 0 ) { return 0 ; } vector < int > preSum ( n , 0 ); preSum [ 0 ] = A [ 0 ]; for ( int i = 1 ; i < n ; i ++ ) { preSum [ i ] = preSum [ i - 1 ] + A [ i ]; } int res = INT_MIN ; int Lmax = INT_MIN ; int Mmax = INT_MIN ; // for ( int i = L + M ; i <= n ; i ++ ) { // L is front, M is back if ( i == L + M ) { Lmax = preSum [ L - 1 ]; } else { Lmax = max ( Lmax , preSum [ i - M - 1 ] - preSum [ i - L - M - 1 ]); } // M is front, L is back if ( i == L + M ) { Mmax = preSum [ M - 1 ]; } else { Mmax = max ( Mmax , preSum ( i - L - 1 ) - preSum [ i - M - L - 1 ]); } res = max ({ res , Lmax + preSum [ i - 1 ] - preSum [ i - M - 1 ], Mmax + preSum [ i - 1 ] - preSum [ i - L - 1 ]}) } return res ; } }; DP solution //TODO Maximum Sum of 3 Non-Overlapping Subarrays \u00b6 Longest Substring Without Repeating Characters \u00b6 Longest Substring with At Most Two Distinct Characters \u00b6 Longest Substring with At Most K Distinct Characters \u00b6 Subarrays with K Different Integers \u00b6 If you use sliding window to solve this problem, there are lots of corner cases. for example: [1, 1, 2, 1, 2, 1, 3, 1], K = 2 , how can you ensure you count all the subarray? Remember this trick that you can use the atMostKDistinct(A, K) - atMostKDistinct(A, K - 1) . class Solution { public : int subarraysWithKDistinct ( vector < int >& A , int K ) { return subarrayWithAtMostKDistinct ( A , K ) - subarrayWithAtMostKDistinct ( A , K - 1 ); } int subarrayWithAtMostKDistinct ( vector < int >& A , int K ) { unordered_map < int , int > count ; int i = 0 ; int res = 0 ; for ( int j = 0 ; j < A . size (); ++ j ) { if ( ! count [ A [ j ]] ++ ) K -- ; while ( k < 0 ) { if ( !-- count [ A [ j ]]) K ++ ; i ++ ; } res += j - i + 1 ; } return res ; } }; Number of Substrings Containing All Three Characters \u00b6 Count Number of Nice Subarrays \u00b6 Replace the Substring for Balanced String \u00b6 Binary Subarrays With Sum \u00b6 Fruit Into Baskets \u00b6 Shortest Subarray with Sum at Least K \u00b6 Minimum Size Subarray Sum \u00b6 Substring with Concatenation of All Words \u00b6 Max Consecutive Ones II \u00b6 Max Consecutive Ones III \u00b6 Category 4 K Sum problems \u00b6 Two Sum \u00b6 Two Sum II - Input array is sorted \u00b6 Two Sum III - Data structure design \u00b6 Two Sum IV - Input is a BST \u00b6 3Sum \u00b6 3Sum Closest \u00b6 3Sum Smaller \u00b6 4Sum \u00b6 4Sum II \u00b6 K Sum \u00b6 Target Sum \u00b6 Cagegory 5 2D arry (matrix, grid) problems \u00b6 Perfect Rectangle \u00b6 Trapping Rain Water \u00b6 Trapping Rain Water II \u00b6 Container With Most Water \u00b6 Largest Rectangle in Histogram \u00b6 Maximal Rectangle \u00b6 Maximal Square \u00b6 The Skyline Problem \u00b6 Smallest Rectangle Enclosing Black Pixels \u00b6 Rectangle Area \u00b6 Max Sum of Rectangle No Larger Than K \u00b6 Category 6 stock buying problems \u00b6 Most consistent ways of dealing with the series of stock problems 121. Best Time to Buy and Sell Stock \u00b6 Solution 1 O(n) one pass to find the minimum and in the meantime, find the max profit. class Solution { public : int maxProfit ( vector < int > & prices ) { int n = prices . size (); if ( n == 0 ) return 0 ; int res = 0 , low = prices [ 0 ]; for ( int i = 1 ; i < n ; i ++ ) { res = max ( res , prices [ i ] - low ); low = min ( low , prices [ i ]); } return res ; } }; 122. Best Time to Buy and Sell Stock II \u00b6 Solution 1 Greedy since you can buy as many times as you can class Solution { public : int maxProfit ( vector < int > & prices ) { int n = prices . size (); int res = 0 ; for ( int i = 0 ; i < n - 1 ; i ++ ) { if ( prices [ i + 1 ] - prices [ i ]) { res += prices [ i + 1 ] - prices [ i ]; } } return res ; } }; 123. Best Time to Buy and Sell Stock III \u00b6 You can now buy at most twice. how to max the profit. Solution 1 Dynamic programming 5 stages: 1. before buy the first <-- optimal solution could be at this stage 2. hold the first 3. sell the first <-- or at this stage, only bought once, 4. hold the second 5. sell the second <-- or at this stage, bought twice. class Solution { public : int maxProfit ( vector < int > & A ) { //1, 3, 5: // f[i][j] = max{f[i - 1][j], f[i - 1][j - 1] + A[i - 1] - A[i - 2]} // 2, 4: // f[i][j] = max{f[i - 1][j] + A[i - 1] - A[i - 2], // f[i - 1][j - 1], f[i - 1][j - 2] + A[i - 1] - A[i - 2]} int n = A . size (); if ( n == 0 ) { return 0 ; } int f [ n + 1 ][ 6 ]; /* init */ f [ 0 ][ 1 ] = 0 ; f [ 0 ][ 2 ] = f [ 0 ][ 3 ] = f [ 0 ][ 4 ] = f [ 0 ][ 5 ] = INT_MIN ; for ( int i = 1 ; i <= n ; i ++ ) { for ( int j = 1 ; j <= 5 ; j += 2 ) { f [ i ][ j ] = f [ i - 1 ][ j ]; if ( i > 1 && j > 1 && f [ i - 1 ][ j - 1 ] != INT_MIN ) { f [ i ][ j ] = max ( f [ i ][ j ], f [ i - 1 ][ j - 1 ] + A [ i - 1 ] - A [ i - 2 ]); } } for ( int j = 2 ; j < 5 ; j += 2 ) { f [ i ][ j ] = f [ i - 1 ][ j - 1 ]; if ( i > 1 && f [ i - 1 ][ j ] != INT_MIN ) { f [ i ][ j ] = max ( f [ i ][ j ], f [ i - 1 ][ j ] + A [ i - 1 ] - A [ i - 2 ]); } if ( i > 1 && j > 2 && f [ i - 1 ][ j - 2 ] != INT_MIN ) { f [ i ][ j ] = max ( f [ i ][ j ], f [ i - 1 ][ j - 2 ] + A [ i - 1 ] - A [ i - 2 ]); } } } return max ( f [ n ][ 1 ], max ( f [ n ][ 3 ], f [ n ][ 5 ])); } }; Solution 2 Use T[i][k][j] to represent the maximum profit of first i days if we allow at most k transactions and the current number of stocks at hand is j ( j == 0, 1 because hold two stocks at the same time is not allowed). So we have: T[i][2][0] = max(T[i - 1][2][0], T[i - 1][2][1] + prices[i - 1]); T[i][2][1] = max(T[i - 1][2][1], T[i - 1][1][0] - prices[i - 1]); T[i][1][0] = max(T[i - 1][1][0], T[i - 1][1][1] + prices[i - 1]); T[i][1][1] = max(T[i - 1][1][1], T[i - 1][0][0] - prices[i - 1]); Think: How to ensure you fourmular to cover all the possible values? 188. Best Time to Buy and Sell Stock IV \u00b6 309. Best Time to Buy and Sell Stock with Cooldown \u00b6 714. Best Time to Buy and Sell Stock with Transaction Fee \u00b6 Determine the buy data and sell data of maximum profit (DD 139) \u00b6","title":"Array"},{"location":"leetcode/array/notes/#array","text":"","title":"Array"},{"location":"leetcode/array/notes/#category-1-removecontains-duplidate","text":"","title":"Category 1 Remove/Contains Duplidate"},{"location":"leetcode/array/notes/#contains-duplicate","text":"","title":"Contains Duplicate"},{"location":"leetcode/array/notes/#contains-duplicate-ii","text":"","title":"Contains Duplicate II"},{"location":"leetcode/array/notes/#contains-duplicate-iii","text":"","title":"Contains Duplicate III"},{"location":"leetcode/array/notes/#find-the-duplicate-number","text":"","title":"Find the Duplicate Number"},{"location":"leetcode/array/notes/#remove-duplicates-from-sorted-array","text":"","title":"Remove Duplicates from Sorted Array"},{"location":"leetcode/array/notes/#remove-duplicates-from-sorted-array-ii","text":"","title":"Remove Duplicates from Sorted Array II"},{"location":"leetcode/array/notes/#remove-duplicates-from-sorted-list-ii","text":"","title":"Remove Duplicates from Sorted List II"},{"location":"leetcode/array/notes/#remove-duplicates-from-sorted-list","text":"","title":"Remove Duplicates from Sorted List"},{"location":"leetcode/array/notes/#move-zeroes","text":"","title":"Move Zeroes"},{"location":"leetcode/array/notes/#category-2-matrix-problems","text":"","title":"Category 2 Matrix problems"},{"location":"leetcode/array/notes/#spiral-matrix","text":"","title":"Spiral Matrix"},{"location":"leetcode/array/notes/#spiral-matrix-ii","text":"","title":"Spiral Matrix II"},{"location":"leetcode/array/notes/#search-a-2d-matrix","text":"","title":"Search a 2D Matrix"},{"location":"leetcode/array/notes/#search-a-2d-matrix-ii","text":"","title":"Search a 2D Matrix II"},{"location":"leetcode/array/notes/#rotate-image","text":"","title":"Rotate Image"},{"location":"leetcode/array/notes/#range-sum-query-2d-mutable","text":"","title":"Range Sum Query 2D - Mutable"},{"location":"leetcode/array/notes/#range-sum-query-2d-immutable","text":"","title":"Range Sum Query 2D - Immutable"},{"location":"leetcode/array/notes/#maximal-square","text":"","title":"Maximal Square"},{"location":"leetcode/array/notes/#maximal-rectangle","text":"","title":"Maximal Rectangle"},{"location":"leetcode/array/notes/#category-3-subarray-problems","text":"","title":"Category 3 Subarray problems"},{"location":"leetcode/array/notes/#type-of-subarray-problem","text":"Find a subarray that fulfills a certain property, i.e maximum size subarray, Longest Substring with At Most Two Distinct Characters Use map or two pointer to solve the problem. Split into subarrays that fulfill certain properties, i.e. sum greater than k. Fixed length subarray indexing To correctly index an array in solving subarray problems are critical. Here is some tips: 1. To iterate through a subarray of certain size, alwasy using the \"one-of-the-end\" pattern. Namely, the iteration index i point to the \"one-off-the-end\" of the subarray. The subarray of size K before the index is started at index i - k . 2. The above convention is especially useful in subarray problems given constrains, such as \"the subarray size greater than k \", \"maximum sum of non-overlapping subarray\", etc. 3. Example problems - Maximum Average Subarray II - Maximum Sum of Two Non-Overlapping Subarrays","title":"Type of subarray problem"},{"location":"leetcode/array/notes/#two-types-of-prefix-sum","text":"There are two ways to calculate the prefix sum array. Take which ever conveniece for your when solving a problem. Option 1: sums.resize(n, 0); nums: [1, 2, 3, 4, 5, 6, 7, 8, 9] i j sums: [1, 3, 6, 10, 15, 21, 28, 36, 45] In this case, each element sums[i] in sums represent the cumulative sum for indexes [0, ..., i] . In other words, sum[i] represent cumulative sum up to element i inclusive. When you want to get the range sum rangeSum(i, j) , you can get it in the following way: rangeSum ( i , j ) = sums [ j ] - sums [ i - 1 ] // i > 0 rangeSum ( i , j ) = sums [ j ] // i == 0 work with this option is a little complex, to get the rangeSum(i, j) : rangeSum ( i , j ) = sums [ j ] - sums [ i ] + nums [ i ] // i >= 0 Option 2: sums.resize(n + 1, 0); In this case, each element sums[i] in sums represent the prefix sum of the first i elements in original array nums. When you want to get the range sum by rangeSum(i, j) , you can get it in the following way: rangeSum ( i , j ) = sums [ j + 1 ] - sums [ i ] // i >= 0","title":"Two types of prefix sum"},{"location":"leetcode/array/notes/#using-prefix-sum-with-map","text":"One of the core trick in solving the following subarray problems is to build a map from prefix sum to array index for efficient lookup. For example, problems with keywaords \"maximum size equal to K\", \"differ by K\", or \"differ by multiple of k\" are solved using this trick. There are two hints. Hint When a map is used, it need to be initialized using <0, -1> . It is useful for handling some of the corner cases such as [-1, 1], -1 in the problem Maximum Size Subarray Sum Equals k . Hint It is usually easier to work with these problem when adding dummy element at the beginning of the array. For example: using sums[i] to represent the sum of first i element of array nums .","title":"Using prefix sum with map"},{"location":"leetcode/array/notes/#maximum-subarray","text":"Kadane's solution This is a DP solution, it reduced the f array to two variables. Making the problem O(1) O(1) in space. Discuss about this solution , where it make use of the idea of global maximum and local maximum. Note Why can not compare to f[i - 1] to find the maximum. If comparing to the f[i - 1] , it will skip elements, the sum will not from a subarray, but sequence of numbers in the array. This is very similar to problems Longest Common Substring and Longest Common Subsequence. The DP \"Choices\" here is NOT to choose or not choose A[i] , but \"Add A[i] to the result of the subproblem or we have to start a subarray from i \" because we cannot skip A[i] . Note The idea of global maximum and local maximum is very useful to solve DP problems. The local maximum is the maximum sum of a continuous subarray, the global maximum is to keep the maximum of the from the local and global maximum. Prefix sum solution The ideas is we have array sums, sums[i] = A[0] +, ... + A[i] , called prefix sum. With one for loop we can find the maxSum so far and the minSum before it. The difference is the possible results, we collect the maximum of those differences. related to Jump Game C++ DP class Solution { public : int maxSubArray ( vector < int >& nums ) { int n = nums . size (); int res = INT_MIN ; int f [ n + 1 ] = { 0 }; // f[i] = maxSubArray of first i elements f [ 0 ] = 0 ; // initial value for ( int i = 1 ; i <= n ; i ++ ) { f [ i ] = max ( f [ i - 1 ] + nums [ i - 1 ], nums [ i - 1 ]); res = max ( f [ i ], res ); } return res ; } }; C++ Kadane's solution class Solution { public : int maxSubArray ( vector < int >& nums ) { int n = nums . size (); int res = INT_MIN ; int curr = 0 ; for ( int i = 0 ; i < n ; i ++ ) { curr = max ( curr + nums [ i ], nums [ i ]); res = max ( curr , res ); } return res ; } }; Java prefix sum solution public class Solution { public int maxSubArray ( int [] A ) { if ( A == null || A . length == 0 ){ return 0 ; } int max = Integer . MIN_VALUE , sum = 0 , minSum = 0 ; for ( int i = 0 ; i < A . length ; i ++ ) { sum += A [ i ] ; max = Math . max ( max , sum - minSum ); minSum = Math . min ( minSum , sum ); } return max ; } }; C++ Greedy solution // why this greedy solution works? class Solution { public : int maxSubArray ( vector < int >& nums ) { int n = nums . size (); int sum = 0 ; int max = 0 ; if ( n == 0 ) return 0 ; max = nums [ 0 ]; for ( int i = 0 ; i < n ; i ++ ) { sum += nums [ i ]; max = sum > max ? sum : max ; sum = sum > 0 ? sum : 0 ; } return max ; } };","title":"Maximum Subarray"},{"location":"leetcode/array/notes/#maximum-subarray-ii","text":"Given an array of integers, find two non-overlapping subarrays which have the largest sum. The number in each subarray should be contiguous. Return the largest sum. Notice The subarray should contain at least one number Example For given [1, 3, -1, 2, -1, 2], the two subarrays are [1, 3] and [2, -1, 2] or [1, 3, -1, 2] and [2], they both have the largest sum 7. Prefix sum solution C++ Prefix sum solution class Solution { public : /* * @param nums: A list of integers * @return: An integer denotes the sum of max two non-overlapping subarrays */ int maxTwoSubArrays ( vector < int > nums ) { int n = nums . size (); int minSum = 0 ; int sums = 0 ; int maxSum = INT_MIN ; int left [ n ] = { 0 }; int right [ n ] = { 0 }; /* calculate the prefix sum */ for ( int i = 0 ; i < n ; i ++ ) { sums += nums [ i ]; maxSum = max ( maxSum , sums - minSum ); // minSum is previous calculated minSum = min ( minSum , sums ); left [ i ] = maxSum ; } /* calculate the postfix sum */ minSum = 0 ; sums = 0 ; maxSum = INT_MIN ; for ( int i = n - 1 ; i >= 0 ; i -- ) { sums += nums [ i ]; maxSum = max ( maxSum , sums - minSum ); minSum = min ( minSum , sums ); right [ i ] = maxSum ; } /* iterate the divider line, left[i] stored the maxSubArraySum * from nums[0] to nums[i], similar for right[i] */ maxSum = INT_MIN ; for ( int i = 0 ; i < n - 1 ; i ++ ) { maxSum = max ( maxSum , left [ i ] + right [ i + 1 ]); } return maxSum ; } }; Warning Cannot swap the highlighted lines. Because the maximum sum is calculated from current sum minus the previous minSum.","title":"Maximum Subarray II*"},{"location":"leetcode/array/notes/#maximum-subarray-iii","text":"Given an array of integers and a number k, find k non-overlapping subarrays which have the largest sum. The number in each subarray should be contiguous. Return the largest sum. Notice The subarray should contain at least one number Example Input: List = [-1,4,-2,3,-2,3] k = 2 Output: 8 Explanation: 4 + (3 + -2 + 3) = 8 DP solution Use the idea of global maximum and local maximum from Maximum Subarray . See this artical for detailed explaination of the solution. class Solution { public : /** * My initial try: O(n^2 k) * Partitioning DP: f[n][k], maximum K subarrays of first n elements. * Last partition: A[j] ,... A[n - 1] * f[i][k] = max_{0 <= j < i}(f[j][k - 1] + MS(A[j] ,... A[i - 1])) * f[0][0] = * * Solution 2, O(nk) * local[i][k]: Max k subarray sum from \"first i elements\" that include nums[i] * global[i][k]: Max k subarray sum from \"first i elements\" that may not include nums[i] * * 2 cases: nums[i - 1] is kth subarray, nums[i - 1] belongs to kth subarray * local[i][k] = max(global[i - 1][k - 1], local[i - 1][k]) + nums[i - 1] * * 2 cases: not include nums[i - 1], include nums[i - 1] * global[i][k] = max(global[i - 1][k], local[i][k]) */ int maxSubArray ( vector < int > nums , int k ) { int n = nums . size (); int local [ n + 1 ][ k + 1 ] = { 0 }; int global [ n + 1 ][ k + 1 ] = { 0 }; for ( int j = 1 ; j <= k ; j ++ ) { // first j - 1 elements cannot form j groups, set to INT_MIN. local [ j - 1 ][ j ] = INT_MIN ; for ( int i = j ; i <= n ; i ++ ) { // must: i >= k. local [ i ][ j ] = max ( global [ i - 1 ][ j - 1 ], local [ i - 1 ][ j ]) + nums [ i - 1 ]; // the case when we divide k elements into k groups. if ( i == j ) { global [ i ][ j ] = local [ i ][ j ]; } else { global [ i ][ j ] = max ( global [ i - 1 ][ j ], local [ i ][ j ]); } } } return global [ n ][ k ]; } };","title":"Maximum Subarray III*"},{"location":"leetcode/array/notes/#maximum-subarray-difference","text":"Given an array with integers. Find two non-overlapping subarrays A and B, which |SUM(A) - SUM(B)| is the largest. Return the largest difference. Notice The subarray should contain at least one number Example For [1, 2, -3, 1], return 6. Prefix sum solution We use the similar idea for problem Maximum Subarray II . We have to maintain four arrays. from forward maximum and minimum subarray sum and backward maximum and minimum subarray sum. C++ Prefix sum solution class Solution { public : /* * @param nums: A list of integers * @return: value of maximum difference between two subarrays */ int maxDiffSubArrays ( vector < int > nums ) { int n = nums . size (); int minSum = 0 ; int sums = 0 ; int maxSum = INT_MIN ; int left_max [ n ] = { 0 }; int left_min [ n ] = { 0 }; int right_max [ n ] = { 0 }; int right_min [ n ] = { 0 }; for ( int i = 0 ; i < n ; i ++ ) { sums += nums [ i ]; maxSum = max ( maxSum , sums - minSum ); minSum = min ( minSum , sums ); left_max [ i ] = maxSum ; //left_min[i] = minSum; } minSum = INT_MAX ; sums = 0 ; maxSum = 0 ; for ( int i = 0 ; i < n ; i ++ ) { sums += nums [ i ]; minSum = min ( minSum , sums - maxSum ); maxSum = max ( maxSum , sums ); //left_max[i] = maxSum; left_min [ i ] = minSum ; } minSum = 0 ; sums = 0 ; maxSum = INT_MIN ; for ( int i = n - 1 ; i >= 0 ; i -- ) { sums += nums [ i ]; maxSum = max ( maxSum , sums - minSum ); minSum = min ( minSum , sums ); right_max [ i ] = maxSum ; //right_min[i] = minSum; } minSum = INT_MAX ; sums = 0 ; maxSum = 0 ; for ( int i = n - 1 ; i >= 0 ; i -- ) { sums += nums [ i ]; minSum = min ( minSum , sums - maxSum ); maxSum = max ( maxSum , sums ); //right_max[i] = maxSum; right_min [ i ] = minSum ; } int diff = INT_MIN ; for ( int i = 0 ; i < n - 1 ; i ++ ) { diff = max ( left_max [ i ] - right_min [ i + 1 ], diff ); diff = max ( right_max [ i + 1 ] - left_min [ i ], diff ); } return diff ; } };","title":"Maximum Subarray Difference*"},{"location":"leetcode/array/notes/#maximum-product-subarray","text":"DP solution It is similar to the problem Maximum Subarray . Notice the negative number, min multiply a minus number could become the largest product. class Solution { public : int maxProduct ( vector < int >& nums ) { int n = nums . size (); int max_pro [ n ] = { 0 }; int min_pro [ n ] = { 0 }; int result = nums [ 0 ]; max_pro [ 0 ] = nums [ 0 ]; min_pro [ 0 ] = nums [ 0 ]; for ( int i = 1 ; i < n ; i ++ ) { if ( nums [ i ] > 0 ) { max_pro [ i ] = max ( max_pro [ i - 1 ] * nums [ i ], nums [ i ]); min_pro [ i ] = min ( min_pro [ i - 1 ] * nums [ i ], nums [ i ]); } else { max_pro [ i ] = max ( min_pro [ i - 1 ] * nums [ i ], nums [ i ]); min_pro [ i ] = min ( max_pro [ i - 1 ] * nums [ i ], nums [ i ]); } result = max ( result , max_pro [ i ]); } return result ; } }; Constant space solution Without need to check whether nums[i] is positive is negative, we can just find the maximum or minium of three cases. class Solution { public : /* * @param nums: An array of integers * @return: An integer */ int maxProduct ( vector < int > nums ) { int n = nums . size (); int res = nums [ 0 ]; int cur_max = nums [ 0 ]; int cur_min = nums [ 0 ]; for ( int i = 1 ; i < n ; i ++ ) { int tmp = cur_max ; cur_max = max ( max ( cur_max * nums [ i ], nums [ i ]), cur_min * nums [ i ]); cur_min = min ( min ( cur_min * nums [ i ], nums [ i ]), tmp * nums [ i ]); res = max ( res , cur_max ); } return res ; } };","title":"Maximum Product Subarray"},{"location":"leetcode/array/notes/#subarray-product-less-than-k","text":"","title":"Subarray Product Less Than K"},{"location":"leetcode/array/notes/#subarray-sum","text":"Given an integer array, find a subarray where the sum of numbers is zero. Your code should return the index of the first number and the index of the last number. Notice There is at least one subarray that it's sum equals to zero. Example Given [-3, 1, 2, -3, 4], return [0, 2] or [1, 3]. Hash solution use a hash table to keep the prefix sum. Once we see another prefix sum that exists in the hash table, we discovered the subarray that sums to zero. However, pay attention to the indexing, because it requires to return the original array's index. class Solution { public : /** * @param nums: A list of integers * @return: A list of integers includes the index of the first number * and the index of the last number */ vector < int > subarraySum ( vector < int > nums ){ int n = nums . size (); vector < int > res ( 2 , 0 ); int sum = 0 ; unordered_map < int , int > map ; map [ 0 ] = -1 ; //important for ( int i = 0 ; i < n ; i ++ ) { sum += nums [ i ]; if ( map . count ( sum ) != 0 ) { res [ 0 ] = map [ sum ] + 1 ; res [ 1 ] = i ; break ; } map [ sum ] = i ; } return res ; } }; // test cases // [-1, 2, 3, -3] A // [0,-1, 1, 4, 1] sum // i j Note Pay attention to the initial value and initializethe map[0] = -1 ; This can be validated with an edge case. The time complexity is O(n) Prefix sum solution Calculate the prefix sum first and then use the prefix sum to find the subarray. This solution is O(n^2) O(n^2) class Solution { public : vector < int > subarraySum ( vector < int > nums ){ int n = nums . size (); vector < int > res ( 2 , 0 ); vector < int > sum ( n + 1 , 0 ); sum [ 0 ] = 0 ; for ( int i = 1 ; i <= n ; i ++ ) { sum [ i ] = sum [ i - 1 ] + nums [ i - 1 ]; } for ( int i = 0 ; i < n ; i ++ ) { for ( int j = i ; j <= n ; j ++ ) { if ( j > 1 && sum [ j ] - sum [ i ] == 0 ) { res [ 0 ] = i ; res [ 1 ] = j - 1 ; break ; } } } return res ; } };","title":"Subarray Sum*"},{"location":"leetcode/array/notes/#minimum-size-subarray-sum","text":"Accumulative sum solution Using accumulative sum and another moving pointer to check both the sum and the length of the subarray. class Solution { public : int minSubArrayLen ( int s , vector < int >& nums ) { int n = nums . size (); if ( n == 0 ) return 0 ; int sum = 0 ; int res = INT_MAX ; int left = 0 ; for ( int i = 0 ; i < n ; i ++ ) { sum += nums [ i ]; while ( sum >= s ) { res = min ( res , i - left + 1 ); sum -= nums [ left ++ ]; } } return res != INT_MAX ? res : 0 ; } };","title":"Minimum Size Subarray Sum"},{"location":"leetcode/array/notes/#maximum-size-subarray-sum-equals-k","text":"Similar to Continuous Subarray Sum Hash solution Use a hash table to keep <sums, i> entries. Look up using sum - k . We only add to the hash table for the first time a value is appeared. It ensures the length of the found subarray is the largest. Notice you also have to initialize the hash with value <0, -1> to handle the edge case. class Solution { public : int maxSubArrayLen ( vector < int >& nums , int k ) { int n = nums . size (); if ( n == 0 ) return 0 ; unordered_map < int , int > map ; int sum = 0 ; int left = 0 ; int res = INT_MIN ; map [ 0 ] = -1 ; for ( int i = 0 ; i < n ; i ++ ) { sum += nums [ i ]; if ( map . count ( sum - k ) != 0 ) { left = map [ sum - k ] + 1 ; res = max ( res , ( i - left + 1 )); } if ( map . count ( sum ) == 0 ) { map [ sum ] = i ; } } return res != INT_MIN ? res : 0 ; } }; /* test cases: 1. [-1, 1], -1 \u5982\u679c\u6ca1\u6709\u521d\u59cb\u5316hash\uff0c\u8fd9\u4e2acase\u5c31\u4f1a\u9519\u8bef [-1, 0] sums 2. [-1], 0 3. [-1], -1 4. if return the result will be 1 if there is no res variable [1, 1, 0], 1 [1, 2, 2], 1 */","title":"Maximum Size Subarray Sum Equals k"},{"location":"leetcode/array/notes/#subarray-sum-equals-k","text":"Prefix sum solution Use prefix sum to find the subarray sum. Two pointer to check all the possible subarray sum. C++ prefix sum solution public class Solution { public int subarraySum ( int [] nums , int k ) { int count = 0 ; int [] sum = new int [ nums . length + 1 ]; sum [ 0 ] = 0 ; for ( int i = 1 ; i <= nums . length ; i ++ ) sum [ i ] = sum [ i - 1 ] + nums [ i - 1 ]; for ( int start = 0 ; start < nums . length ; start ++ ) { for ( int end = start + 1 ; end <= nums . length ; end ++ ) { if ( sum [ end ] - sum [ start ] == k ) count ++ ; } } return count ; } } Hash solution Use a map to store the prefix sum and a counter. The idea is while calculating prefix sums, if we find an sums - k exist in the map, we found one of target subarray. The subtilty is for a particular prefix sum, there might be multiple earlier prefix sums differ from it by k. We should take this into account. Compare to the hash solution for problem Subarray Sum . /* k = 2 i = 1, 2, 3 sum = 1, 2, 3 cnt = 0, 1, 2 key = 1, 2, 3 val = 1, 1, 1 the reason that the cnt += map[sum - k], not cnt += 1 is that the prefix sum \"sum - k\" has been shown up for total of map[sum - k] times. All those prefix sum could be result of distinct subarrays between current prefix sum and previous prefix sum \"sum - k\" */ class Solution { public : int subarraySum ( vector < int >& nums , int k ) { int n = nums . size (); // key=prefix sum, val=appearance unordered_map < int , int > map ; int cnt = 0 ; int sum = 0 ; map [ 0 ] = 1 ; for ( int i = 0 ; i < n ; i ++ ) { sum += nums [ i ]; if ( map . count ( sum - k ) != 0 ) { cnt += map [ sum - k ]; } map [ sum ] += 1 ; } return cnt ; } }; Warning Notice you have to initialize the map[0] = 1 ; this is because for cases such as [1, 1, 1] , when i = 1 , sum = 2 , [1,1] should be counted as one subarray. Without setting map[0] = 1 at first hand, it will give incorrect results.","title":"Subarray Sum Equals K"},{"location":"leetcode/array/notes/#subarray-sums-divisible-by-k","text":"preSum is the basis of continuous subarray profblem. One pass solution should explore a property of modulo (preSum[j] - preSum[i]) % K == 0 indicate the when removing K from the larger value preSum[j] n times, we get the smaller value, then preSum[j] % K == preSum[i] % K . This property makes the one pass solution possible. Once the modulo property is found, we can use Hash map to assist our counting. The ideas is to keep counting the remainder, once we have seen the same remainder in the map, the new index and all the found indexes can be used to retrive one solution. The count keep in the map show how many of those can be. One pass solution class Solution { public : int subarraysDivByK ( vector < int >& A , int K ) { int n = A . size (); if ( n == 0 || K == 0 ) { return 0 ; } int preSum = 0 ; unordered_map < int , int > mp ; mp [ 0 ] = 1 ; int count = 0 ; for ( int i = 0 ; i < n ; i ++ ) { preSum += A [ i ]; int reminder = preSum % K ; // deal with negative values if ( reminder < 0 ) reminder += K ; if ( mp . find ( reminder ) != mp . end ()) { count += mp [ reminder ]; } mp [ reminder ] += 1 ; } return count ; } }; ===\"Naive solution O(n^2)\" class Solution { public : int subarraysDivByK ( vector < int >& A , int K ) { int n = A . size (); if ( n == 0 || K == 0 ) { return 0 ; } vector < long > preSum ( n + 1 , 0 ); preSum [ 0 ] = 0 ; for ( int i = 0 ; i < n ; i ++ ) { preSum [ i + 1 ] = preSum [ i ] + A [ i ]; } int cnt = 0 ; for ( int i = 0 ; i < n ; i ++ ) { for ( int j = i + 1 ; j <= n ; j ++ ) { if (( preSum [ j ] - preSum [ i ]) % K == 0 ) { cnt ++ ; } } } return cnt ; } };","title":"Subarray Sums Divisible by K"},{"location":"leetcode/array/notes/#max-sum-of-subarry-no-larger-than-k","text":"This problem in geeksforgeeks as \"Maximum sum subarray having sum less than or equal to given sum\". It has been discussed here. This problem is the basis to solve the problem 363. Max Sum of Rectangle No Larger Than K. Solution 1 using prefix sum and set calculate prefix and using a set to store individual prefix sum, ( vector also works). In each iteration, we lookup the value preSum - k in the set. Notice we can use binary search to find the smallest element that >= preSum - k . We can use lower_bound to achieve that. Notice if it is asking the sum less than k we have to use upper_bound int maxSumSubarryNoLargerThanK ( int A [], int n , int k ) { set < int > preSumSet ; preSumSet . insert ( 0 ); int res = 0 , preSum = 0 ; for ( int i = 0 ; i < n ; ++ i ) { preSum += A [ i ]; set < int >:: iterator siter = preSumSet . lower_bound ( preSum - k ); if ( siter != preSumSet . end () { res = max ( res , preSum - * siter ); } preSumSet . insert ( preSum ); } return res ; }","title":"Max Sum of Subarry No Larger Than K*"},{"location":"leetcode/array/notes/#max-sum-of-rectangle-no-larger-than-k","text":"Solution 1 iterate the wide of the matrix and using prefix sum and set lower_bound . To optimize it with the brute force solution, you will find this problem is a combination of the problem Maximum Sum Rectangular Submatrix in Matrix and problem Max Sum of Subarry No Larger Than K. From the problem Max Sum of Subarry No Larger Than K, we have to enumerate the width of the sub-matrix and sum up all row elements and get an array of length m , m is the number of rows of the matrix. Then apply the method. class Solution { public : int maxSumSubmatrix ( vector < vector < int >>& matrix , int k ) { if ( matrix . empty ()) return 0 ; int m = matrix . size (); int n = m ? matrix [ 0 ]. size () : 0 ; int res = INT_MIN ; for ( int l = 0 ; l < n ; ++ l ) { vector < int > sums ( m , 0 ); for ( int r = l ; r < n ; ++ r ) { for ( int i = 0 ; i < m ; ++ i ) { sums [ i ] += matrix [ i ][ r ]; } set < int > preSumSet ; preSumSet . insert ( 0 ); int preSum = 0 , curMax = INT_MIN ; for ( int sum : sums ) { preSum += sum ; set < int >:: iterator it = preSumSet . lower_bound ( preSum - k ); if ( it != preSumSet . end ()) { curMax = max ( curMax , preSum - * it ); } preSumSet . insert ( preSum ); } res = max ( res , curMax ); } } return res ; } }; Note The complexity is n\u22c5n\u22c5(m+m\\log m)=O(n\u22c5n\u22c5m\\log m) n\u22c5n\u22c5(m+m\\log m)=O(n\u22c5n\u22c5m\\log m) Notice the use of lower_bound, this function return iterator point to element greater than or equal to the value curSum - k, if use upper_bound, it will return iterator points to element greater than curSum - k, which would miss the equal to K case. Solution 2 using merge sort The idea is similar that solution 1. Instead of calculate preSum on the fly, we finish calculation and pass it to a mergeSort routine. The use mergeSort here is to find the A[j] - A[i] <= k efficiently, O(nlogn) . The complexity is n\u22c5n\u22c5(m+m\u22c5\\log m)=O(n\u22c5n\u22c5m\u22c5\\log m) class Solution { public : int maxSumSubmatrix ( vector < vector < int >>& matrix , int k ) { int m = matrix . size (); int n = m ? matrix [ 0 ]. size () : 0 ; int res = INT_MIN ; vector < long long > sums ( m + 1 , 0 ); for ( int l = 0 ; l < n ; ++ l ) { vector < long long > sumInRow ( m , 0 ); for ( int r = l ; r < n ; ++ r ) { for ( int i = 0 ; i < m ; ++ i ) { sumInRow [ i ] += matrix [ i ][ r ]; sums [ i + 1 ] = sums [ i ] + sumInRow [ i ]; } res = max ( res , mergeSort ( sums , 0 , m + 1 , k )); if ( res == k ) return k ; } } return res ; } int mergeSort ( vector < long long >& sums , int start , int end , int k ) { if ( end == start + 1 ) return INT_MIN ; int mid = start + ( end - start ) / 2 ; int res = mergeSort ( sums , start , mid , k ); if ( res == k ) return k ; res = max ( res , mergeSort ( sums , mid , end , k )); if ( res == k ) return k ; long long cache [ end - start ]; int j = mid , c = 0 , t = mid ; for ( int i = start ; i < mid ; ++ i ) { /* search first time sums[j] - sums[i] > k */ while ( j < end && sums [ j ] - sums [ i ] <= k ) ++ j ; /* sums[j - 1] - sums[i] <= k, make sure j - 1 is in right side */ if ( j - 1 >= mid ) { res = max ( res , ( int )( sums [ j - 1 ] - sums [ i ])); if ( res == k ) return k ; } /* parallel merge */ while ( t < end && sums [ t ] < sums [ i ]) { cache [ c ++ ] = sums [ t ++ ]; } cache [ c ++ ] = sums [ i ]; } /* parallel merge */ for ( int i = start ; i < t ; ++ i ) { sums [ i ] = cache [ i - start ]; } return res ; } };","title":"Max Sum of Rectangle No Larger Than K"},{"location":"leetcode/array/notes/#maximum-sum-rectangular-submatrix-in-matrix","text":"","title":"Maximum Sum Rectangular Submatrix in Matrix*"},{"location":"leetcode/array/notes/#subarray-sum-closest","text":"","title":"Subarray Sum Closest*"},{"location":"leetcode/array/notes/#shortest-unsorted-continuous-subarray-count-inversions","text":"Solution 1 using merge sort This problem is the basic of using merge sort to solve lots of hard problems. long long merge_and_count ( int A [], int start , int end ) { if ( end - start <= 1 ) return 0 ; long long count = 0 ; int mid = start + ( end - start ) / 2 ; count = merge_and_count ( A , start , mid ) + merge_and_count ( A , mid , end ); int j = mid ; int cache [ end - start ]; for ( int i = start , k = 0 ; i < mid ; i ++ ) { while ( j < end && A [ i ] > A [ j ]) { cache [ k ++ ] = A [ j ++ ]; count += mid - i ; } cache [ k ++ ] = A [ i ]; } // copy cache[0, j - start) to A[start, j) // if j < end, then A[j, end) in final place. for ( int i = start ; i < j ; i ++ ) { A [ i ] = cache [ i - start ]; } return count ; }","title":"Shortest Unsorted Continuous Subarray (Count inversions)"},{"location":"leetcode/array/notes/#count-inversion-course-assignment","text":"Solution 1 using merge sort This problem is the basic of using merge sort to solve lots of hard problems. long long merge_and_count ( int A [], int start , int end ) { if ( end - start <= 1 ) return 0 ; long long count = 0 ; int mid = start + ( end - start ) / 2 ; count = merge_and_count ( A , start , mid ) + merge_and_count ( A , mid , end ); int j = mid ; int cache [ end - start ]; for ( int i = start , k = 0 ; i < mid ; i ++ ) { while ( j < end && A [ i ] > A [ j ]) { cache [ k ++ ] = A [ j ++ ]; count += mid - i ; } cache [ k ++ ] = A [ i ]; } // copy cache[0, j - start) to A[start, j) // if j < end, then A[j, end) in final place. for ( int i = start ; i < j ; i ++ ) { A [ i ] = cache [ i - start ]; } return count ; }","title":"Count Inversion (course assignment)"},{"location":"leetcode/array/notes/#count-of-smaller-numbers-after-self","text":"Solution 1 Merge sort One important point to remember is you have to create pairs out of the array element and its index, because during merge sort, when we count each value, we don't know where to put those count values in the result vector. The second merge solutions run much faster than the first one. C++ Merge sort class Solution { public : vector < int > countSmaller ( vector < int >& nums ) { int n = nums . size (); vector < int > res ( n , 0 ); vector < pair < int , int > > vp ; for ( int i = 0 ; i < n ; i ++ ) { vp . emplace_back ( nums [ i ], i ); } merge_sort_count ( vp , 0 , n , res ); return res ; } private : void merge_sort_count ( vector < pair < int , int > >& nums , int start , int end , vector < int >& res ) { if ( end - start <= 1 ) return ; int mid = start + ( end - start ) / 2 ; merge_sort_count ( nums , start , mid , res ); merge_sort_count ( nums , mid , end , res ); vector < pair < int , int > > cache ( end - start , pair < int , int > ( 0 , 0 )); int j = mid , k = 0 , t = mid ; for ( int i = start ; i < mid ; i ++ ) { j = mid ; while ( j < end && nums [ i ]. first > nums [ j ]. first ) { // found smaller elements res [ nums [ i ]. second ] ++ ; j ++ ; } while ( t < end && nums [ i ]. first > nums [ t ]. first ) { cache [ k ++ ] = nums [ t ++ ]; } cache [ k ++ ] = nums [ i ]; } for ( int i = start ; i < j ; i ++ ) { nums [ i ] = cache [ i - start ]; } return ; } }; C++ more efficient Merge sort class Solution { public : vector < int > countSmaller ( vector < int >& nums ) { int n = nums . size (); vector < int > res ( n , 0 ); vector < pair < int , int > > vp ; for ( int i = 0 ; i < n ; i ++ ) { vp . emplace_back ( nums [ i ], i ); } mergeSort ( vp , 0 , n , res ); return res ; } void mergeSort ( vector < pair < int , int >>& x , int start , int end , vector < int >& res ) { if ( end - start <= 1 ) return ; int mid = start + ( end - start ) / 2 ; mergeSort ( x , start , mid , res ); mergeSort ( x , mid , end , res ); vector < pair < int , int > > cache ( end - start , pair < int , int > ( 0 , 0 )); int i = start , j = mid , k = 0 ; while ( i < mid && j < end ) { if ( x [ i ]. first <= x [ j ]. first ) { cache [ k ++ ] = x [ i ]; res [ x [ i ]. second ] += j - mid ; ++ i ; } else { cache [ k ++ ] = x [ j ++ ]; } } while ( i < mid ) { cache [ k ++ ] = x [ i ]; res [ x [ i ]. second ] += end - mid ; ++ i ; } while ( j < end ) cache [ k ++ ] = x [ j ++ ]; for ( i = start , k = 0 ; i < end ; ++ i , ++ k ) { x [ i ] = cache [ k ]; } } }; C++ BST class Solution { public : class TreeNode { public : int val , smallerCnt ; TreeNode * left , * right ; TreeNode ( int v , int s ) : left ( NULL ), right ( NULL ), val ( v ), smallerCnt ( s ){} }; vector < int > countSmaller ( vector < int >& nums ) { int n = nums . size (); if ( n == 0 ) return {}; vector < int > res ( n , 0 ); TreeNode * root = NULL ; for ( int i = n - 1 ; i >= 0 ; -- i ) root = insert ( root , nums [ i ], i , 0 , res ); return res ; } private : TreeNode * insert ( TreeNode * node , int val , int idx , int preSum , vector < int >& res ) { if ( node == NULL ) { node = new TreeNode ( val , 0 ); res [ idx ] = preSum ; } else if ( node -> val > val ) { node -> smallerCnt ++ ; node -> left = insert ( node -> left , val , idx , preSum , res ); } else { node -> right = insert ( node -> right , val , idx , preSum + node -> smallerCnt + (( node -> val < val ) ? 1 : 0 ), res ); } return node ; } };","title":"Count of Smaller Numbers After Self"},{"location":"leetcode/array/notes/#continuous-subarray-sum","text":"Hash solution Once see a multiple of K, you should consider the modulor operation % The values put into to the hash only for the first time, this is similar to the case in the problem Maximum Size Subarray Sum Equals k. C++ Hash soution\" hl_lines=\"11 class Solution { public : bool checkSubarraySum ( vector < int >& nums , int k ) { int n = nums . size (); if ( n == 0 ) return false ; unordered_map < int , int > map ; int sum = 0 ; map [ 0 ] = -1 ; // test case [0, 0], 0 for ( int i = 0 ; i < n ; i ++ ) { sum += nums [ i ]; if ( k != 0 ) sum = sum % k ; if ( map . count ( sum ) != 0 ) { if ( i - map [ sum ] > 1 ) { return true ; } } else { map [ sum ] = i ; } } return false ; } };","title":"Continuous Subarray Sum"},{"location":"leetcode/array/notes/#contiguous-array","text":"Similar problems: Continuous Subarray Sum Maximum Size Subarray Sum Equals k Hash solution This problem is very similar to the problem Continuous Subarray Sum . However, there is a trick to calculate the cummulative sum, treat 0 as -1 . class Solution { public : int findMaxLength ( vector < int >& nums ) { int n = nums . size (); int res = 0 ; int cnt = 0 ; unordered_map < int , int > map ; map [ 0 ] = -1 ; // test case: [0, 1] for ( int i = 0 ; i < n ; i ++ ) { cnt += nums [ i ] == 0 ? -1 : 1 ; if ( map . count ( cnt ) != 0 ) { res = max ( res , i - map [ cnt ]); } else { map [ cnt ] = i ; } } return res ; } };","title":"Contiguous Array"},{"location":"leetcode/array/notes/#split-array-with-equal-sum","text":"Cummulative sum soluiton Because of the symetric property of the head subarray and trailing subarray, we can calculate cumulative sum from both direction. This can help to fix the index i and k . we can enumerate the index j in between. C++ cummulateive sum solution class Solution { public : bool splitArray ( vector < int >& nums ) { int n = nums . size (); int sum1 [ n ] = { 0 }; int sum2 [ n ] = { 0 }; sum1 [ 0 ] = nums [ 0 ]; for ( int i = 1 ; i < n ; i ++ ) { sum1 [ i ] = sum1 [ i - 1 ] + nums [ i ]; } sum2 [ n - 1 ] = nums [ n - 1 ]; for ( int i = n - 2 ; i >= 0 ; i -- ) { sum2 [ i ] = sum2 [ i + 1 ] + nums [ i ]; } // notice the index bounds for ( int i = 1 ; i < n - 5 ; i ++ ) { for ( int k = n - 2 ; k > i + 3 ; k -- ) { if ( sum1 [ i ] - nums [ i ] == sum2 [ k ] - nums [ k ]) { for ( int j = i + 2 ; j < k - 1 ; j ++ ) { int sumij = sum1 [ j ] - nums [ j ] - sum1 [ i ]; int sumjk = sum2 [ j ] - nums [ j ] - sum2 [ k ]; if ( sumij == sumjk ) { return true ; } } } } } return false ; } };","title":"Split Array with Equal Sum"},{"location":"leetcode/array/notes/#410-split-array-largest-sum","text":"Similar problems: Copy Books (linktcode) . DP solution Notice the edge case: [1, INT_MAX] , use double can avoid integer overflow. Binary Search solution This is a greedy search solution that use binary search to accelerate the search speed The goal is to \"minimize the largest sub-array sum\". It is different from Divide Chocolate , which is maximize the smallest sum. The bisection condition is not A[m] < target any more. It is a function to check whether the constrain can meet given a guess value mid . C++ DP /** * equivalent to the lintcode copy books problem * * last step: mth subarray A[j], ..., A[i - 1]. * State: f[m][n]: minmax sum of m subarrays that include n elements * Equation: f[m][n] = min_{0<=j<n}(max(f[m - 1][j], sum(A[j], ..., A[n - 1]))) * Init: f[0][n] = INT_MAX; * f[0][0] = 0; * NB: notice a special case: [1, 2147483247], 2 * the sum will overflow in the state update, You use a double type */ class Solution { public : int splitArray ( vector < int >& nums , int m ) { int n = nums . size (); double f [ m + 1 ][ n + 1 ]; f [ 0 ][ 0 ] = 0 ; for ( int i = 1 ; i <= n ; i ++ ) { f [ 0 ][ i ] = INT_MAX ; } double sum = 0 ; for ( int k = 1 ; k <= m ; k ++ ) { f [ k ][ 0 ] = 0 ; for ( int i = 1 ; i <= n ; i ++ ) { sum = 0 ; f [ k ][ i ] = INT_MAX ; for ( int j = i ; j >= 0 ; j -- ) { //j = i, mean sum = 0. f [ k ][ i ] = min ( f [ k ][ i ], max ( f [ k - 1 ][ j ], sum )); if ( j > 0 ) { sum += nums [ j - 1 ]; } } } } return f [ m ][ n ]; } }; C++ binary search class Solution { public : int splitArray ( vector < int >& nums , int m ) { int total = 0 ; int mx = 0 ; for ( int num : nums ) { total += num ; mx = max ( mx , num ); } int l = mx , r = total ; while ( l < r ) { int mid = l + ( r - l ) / 2 ; if ( ! canCut ( nums , mid , m - 1 )) { l = mid + 1 ; } else { r = mid ; } } return l ; } // whether m cuts are possible, notice the greedy property of this check // you should notice that if not possible, it is because mid is too small, // not because it is too large. bool canCut ( vector < int >& nums , int mid , int m ) { int sum = 0 ; for ( int num : nums ) { if ( num > mid ) return false ; else if ( sum + num <= mid ) sum += num ; // each cut is greedy else { // cut is ok so far m -- ; if ( m < 0 ) return false ; // more element after all cuts. sum = num ; // init the next group sum } } return true ; } }; C++ binary search (count the # of cuts) class Solution { public : int splitArray ( vector < int >& nums , int m ) { int l = * max_element ( nums . begin (), nums . end ()); int r = accumulate ( nums . begin (), nums . end (), 0 ); while ( l < r ) { int mid = l + ( r - l ) / 2 ; int s = 0 ; int c = 0 ; // count the possible cuts for ( int n : nums ) { if (( s += n ) > mid ) { s = n ; if ( ++ c > m - 1 ) { break ; } } } if ( c > m - 1 ) { l = mid + 1 ; } else { // the \"> mid\" above guarantee the \"no greater than\" // the guess value, if c == m - 1, mid could be the result r = mid ; } } return l ; } }; C++ binary search alternative class Solution { public : int splitArray ( vector < int >& nums , int m ) { int l = * max_element ( nums . begin (), nums . end ()); int r = accumulate ( nums . begin (), nums . end (), 0 ); while ( l < r ) { int mid = l + ( r - l ) / 2 ; int s = 0 ; int c = 1 ; // count the possible cuts for ( int n : nums ) { if ( s + n > mid ) { // voilate the constrains s = 0 ; c ++ ; } s += n ; } if ( c > m ) { l = mid + 1 ; } else { // the \"> mid\" above guarantee the \"no greater than\" // the guess value, if c == m, mid could be the result r = mid ; } } return l ; } };","title":"410. Split Array Largest Sum"},{"location":"leetcode/array/notes/#copy-books-lintcode","text":"Description Given n books and the ith book has A[i] pages. You are given k people to copy the n books. the n books list in a row and each person can claim a continuous range of the n books. For example, one copier can copy the books from ith to jth continuously, but he can not copy the 1st book, 2nd book and 4th book (without the 3rd book). They start copying books at the same time and they all cost 1 minute to copy 1 page of a book. What's the best strategy to assign books so that the slowest copier can finish at the earliest time? Example Given array A = [3,2,4], k = 2. Return 5 (First person spends 5 minutes to copy book 1 and book 2 and the second person spends 4 minutes to copy book 3.) Solution 1 Binary search See the solution for 410. Split Array Largest Sum Solution 2 DP solution There are i books, consider the last copier, he can copy A[j], ..., A[i-1] . The first k-1 copier copy A[0], ..., A[j - 1] . Define state: f[k][i] , meaning the k-th copier copy i books. State transition equation: f[k][i] = \\min_{0 \\le j \\le i} \\max(f[k - 1][j], A[j] + ... + A[i - 1]) f[k][i] = \\min_{0 \\le j \\le i} \\max(f[k - 1][j], A[j] + ... + A[i - 1]) C++ DP solution class Solution { public : /** * last step: last copier copy A[j], ... A[i-1] * first k-1 copier --> A[0], ... A[j - 1]. * f[k][i]: k copier copy i books. * f[k][i] = \\min_{0 \\le j \\le i} \\max(f[k - 1][j], A[j] + ... + A[i - 1]) */ int copyBooks ( vector < int > & pages , int K ) { // write your code here int n = pages . size (); if ( n == 0 ) { return 0 ; } if ( K > n ) { K = n ; } int f [ K + 1 ][ n + 1 ]; /* init */ f [ 0 ][ 0 ] = 0 ; for ( int j = 1 ; j <= n ; j ++ ) { f [ 0 ][ j ] = INT_MAX ; } int sum = 0 ; for ( int k = 1 ; k <= K ; k ++ ) { f [ k ][ 0 ] = 0 ; for ( int i = 1 ; i <= n ; i ++ ) { sum = 0 ; f [ k ][ i ] = INT_MAX ; for ( int j = i ; j >= 0 ; j -- ) { f [ k ][ i ] = min ( f [ k ][ i ], max ( f [ k - 1 ][ j ], sum )); if ( j > 0 ) { sum += pages [ j - 1 ]; } } } } return f [ K ][ n ]; } }; Note We have to enumerate the index j , the highlighted code used a clever technique to optimize this task. It enumerate j backwards. While this seems impossible at the first glance, how can you calculate the states from right to left in DP? Notice the index j is in the upper row (row k-1 ). Once we are in the k -th row, the values in the k-1 -th row are all given.","title":"Copy books (lintcode)"},{"location":"leetcode/array/notes/#maximum-average-subarray-i","text":"Prefix sum solution class Solution { public : double findMaxAverage ( vector < int >& nums , int k ) { int n = nums . size (); double sums [ n ] = { 0 }; double max_avg = INT_MIN ; sums [ 0 ] = nums [ 0 ]; for ( int i = 1 ; i < n ; i ++ ) { sums [ i ] = sums [ i - 1 ] + nums [ i ]; } for ( int i = k - 1 ; i < n ; i ++ ) { double avg = ( sums [ i ] - sums [ i - k + 1 ] + nums [ i - k + 1 ]) / k ; max_avg = max ( max_avg , avg ); } return max_avg ; } };","title":"Maximum Average Subarray I"},{"location":"leetcode/array/notes/#maximum-average-subarray-ii","text":"Prefix sum solution This is still a brute force solution. time complexity: O(n^2) O(n^2) space complexity: O(n) O(n) Space optimized solution We could avoid using the prefix sum array and only use two variables to record the prefix sum at any particular instance. One for record prefix sum of exact k elements. Another for the inner loop to check whether removing an element from the beginning will make a new maximum value or not. Space optimized solution We could avoid using the prefix sum array and only use two variables to record the prefix sum at any particular instance. One for record prefix sum of exact k elements. Another for the inner loop to check whether removing an element from the beginning will make a new maximum value or not. Deque solution C++ prefix sum solution class Solution { public : double findMaxAverage ( vector < int >& nums , int k ) { int n = nums . size (); vector < int > sums = nums ; for ( int i = 1 ; i < n ; ++ i ) { sums [ i ] = sums [ i - 1 ] + nums [ i ]; } double res = ( double ) sums [ k - 1 ] / k ; for ( int i = k ; i < n ; ++ i ) { double t = sums [ i ]; if ( t > res * ( i + 1 )) res = t / ( i + 1 ); for ( int j = 0 ; j < i - k + 1 ; ++ j ) { t = sums [ i ] - sums [ j ]; if ( t > res * ( i - j )) res = t / ( i - j ); } } return res ; } }; C++ space optimized class Solution { public : double findMaxAverage ( vector < int >& nums , int k ) { int n = nums . size (); /* range is half open */ double sumsAll = accumulate ( nums . begin (), nums . begin () + k , 0 ); double sums = sumsAll , res = sumsAll / k ; for ( int i = k ; i < n ; ++ i ) { sumsAll += nums [ i ]; sums = sumsAll ; if ( sums > res * ( i + 1 )) res = sums / ( i + 1 ); for ( int j = 0 ; j < i - k + 1 ; ++ j ) { sums -= nums [ j ]; if ( sums > res * ( i - j )) res = sums / ( i - j ); } } return res ; } }; C++ binary search soluiton class Solution { public : double findMaxAverage ( vector < int >& nums , int k ) { int n = nums . size (); double upper = INT_MIN , lower = INT_MAX ; for ( auto num : nums ) { upper = max ( upper , ( double ) num ); lower = min ( lower , ( double ) num ); } while ( lower + 0.00001 < upper ) { double mid = lower + ( upper - lower ) / 2 ; if ( isLarger ( nums , mid , k )) { // is average value >= mid? lower = mid ; } else { upper = mid ; } } return lower ; } /* return true if a greater average value is possible */ bool isLarger ( vector < int >& nums , double mid , int k ) { int n = nums . size (); double sums = 0 , prev = 0 , prev_min = 0 ; for ( int i = 0 ; i < k ; i ++ ) { sums += nums [ i ] - mid ; } if ( sums >= 0 ) { return true ; } /* * we keep looking for whether a subarray sum of length >= k in array * \"sums\" is possible to be greater than zero. If such a subarray exist, * it means that the target average value is greater than the \"mid\" * value. We look at the front part of sums that at least k element * apart from i. If we can find the minimum of the sums[0, 1, ..., i - k] * and check if sums[i] - min(sum[0, 1, ..., i - k]) >= 0. If this is the * case, it indicate there exist a subarray of length >= k with sum * greater than 0 in sums, we can return ture, otherwise, false. */ for ( int i = k ; i < n ; i ++ ) { sums += nums [ i ] - mid ; prev += nums [ i - k ] - mid ; prev_min = min ( prev_min , prev ); if ( sums >= prev_min ) return true ; } return false ; } }; C++ deque solution class Solution { public : double findMaxAverage ( vector < int >& nums , int k ) { int n = nums . size (); vector < double > sums ( n , 0 ); deque < int > q ; sums [ 0 ] = nums [ 0 ]; for ( int i = 1 ; i < n ; ++ i ) sums [ i ] = sums [ i - 1 ] + nums [ i ]; double res = sums [ n - 1 ] / n ; for ( int j = k - 1 ; j < n ; ++ j ) { while ( q . size () >= 2 && density ( sums , q [ q . size () - 2 ], q . back () - 1 ) >= density ( sums , q . back (), j - k )) { q . pop_back (); } q . push_back ( j - k + 1 ); while ( q . size () >= 2 && density ( sums , q [ 0 ], j ) <= density ( sums , q [ 1 ], j )) { q . pop_front (); } res = max ( res , density ( sums , q . front (), j )); } return res ; } private : double density ( vector < double >& sums , int l , int r ) { if ( l == 0 ) return sums [ r ] / ( r + 1 ); return ( sums [ r ] - sums [ l - 1 ]) / ( r - l + 1 ); } }; Note Notice the initial value of prev_min is set to 0 not INT_MAX; Try to understand why set the initial value of prev_min to INT_MAX cannot pass the test case: [8,9,3,1,8,3,0,6,9,2] , 8.","title":"Maximum Average Subarray II"},{"location":"leetcode/array/notes/#range-sum-query-immutable","text":"Prefix sum solution Use prefix sum to record the accumulative sum of the array in the constructor. The algorithm is O(n) O(n) in space and O(1) O(1) in time. class NumArray { private : vector < int > sums ; int n ; public : NumArray ( vector < int > nums ) { n = nums . size (); sums . resize ( n + 1 , 0 ); sums [ 0 ] = 0 ; for ( int i = 1 ; i <= n ; ++ i ) { sums [ i ] = sums [ i - 1 ] + nums [ i - 1 ]; } } int sumRange ( int i , int j ) { return sums [ j + 1 ] - sums [ i ]; } }; /** * Your NumArray object will be instantiated and called as such: * NumArray obj = new NumArray(nums); * int param_1 = obj.sumRange(i,j); */","title":"Range Sum Query - Immutable"},{"location":"leetcode/array/notes/#range-sum-query-mutable","text":"Segment tree solution Using segment tree, the solution is given at Leetcode Solution . C++ segment tree class NumArray { private : vector < int > tree ; int n ; public : NumArray ( vector < int > nums ) { n = nums . size (); tree . resize ( 2 * n , 0 ); for ( int i = n , j = 0 ; i < 2 * n ; ++ i , ++ j ) { tree [ i ] = nums [ j ]; } for ( int i = n - 1 ; i > 0 ; -- i ) { tree [ i ] = tree [ 2 * i ] + tree [ 2 * i + 1 ]; } } void update ( int i , int val ) { int pos = n + i ; int left = 0 ; int right = 0 ; tree [ pos ] = val ; while ( pos > 0 ) { left = pos ; right = pos ; if ( pos % 2 == 0 ) { right = pos + 1 ; } if ( pos % 2 == 1 ) { left = pos - 1 ; } tree [ pos / 2 ] = tree [ left ] + tree [ right ]; pos /= 2 ; } } int sumRange ( int i , int j ) { int left = i + n ; int right = j + n ; int sum = 0 ; while ( left <= right ) { if ( left % 2 == 1 ) { sum += tree [ left ]; left ++ ; } if ( right % 2 == 0 ) { sum += tree [ right ]; right -- ; } left /= 2 ; right /= 2 ; } return sum ; } }; /** * Your NumArray object will be instantiated and called as such: * NumArray obj = new NumArray(nums); * obj.update(i,val); * int param_2 = obj.sumRange(i,j); */ Binary Indexed Tree solution I Using Binary indexed tree, we are able to solve it optimally in O(\\log n) O(\\log n) . The solution originally from here class NumArray { private : vector < int > arr ; vector < int > BIT ; int n ; public : NumArray ( vector < int > nums ) { arr = nums ; n = nums . size (); BIT . resize ( n + 1 , 0 ); for ( int i = 0 ; i < n ; i ++ ) { init ( i , arr [ i ]); } } void init ( int i , int val ) { i ++ ; while ( i <= n ) { BIT [ i ] += val ; // BIT[i] = nums[0, i - 1]; i += i & ( - i ); } } void update ( int i , int val ) { int diff = val - arr [ i ]; arr [ i ] = val ; init ( i , diff ); } int sumRange ( int i , int j ) { return getSum ( j ) - getSum ( i - 1 ); } int getSum ( int i ) { i ++ ; int sum = 0 ; while ( i > 0 ) { sum += BIT [ i ]; i -= i & ( - i ); } return sum ; } }; /** * Your NumArray object will be instantiated and called as such: * NumArray obj = new NumArray(nums); * obj.update(i,val); * int param_2 = obj.sumRange(i,j); */ Binary Indexed Tree solution II Similar to the above solution, We have combined the init and update . To make it consistant with the solution with problem Range Sum Query 2D - Mutable class NumArray { private : vector < int > arr ; vector < int > BIT ; int n ; public : NumArray ( vector < int > nums ) { n = nums . size (); BIT . resize ( n + 1 , 0 ); arr . resize ( n , 0 ); for ( int i = 0 ; i < n ; i ++ ) { update ( i , nums [ i ]); } } /* We can combine the init and update like this */ void update ( int i , int val ) { int diff = val - arr [ i ]; arr [ i ] = val ; // here we initialize arr[i] i ++ ; while ( i <= n ) { BIT [ i ] += diff ; // BIT[i] = nums[0, i - 1]; i += i & ( - i ); } } int sumRange ( int i , int j ) { return getSum ( j ) - getSum ( i - 1 ); } int getSum ( int i ) { i ++ ; int sum = 0 ; while ( i > 0 ) { sum += BIT [ i ]; i -= i & ( - i ); } return sum ; } }; /** * Your NumArray object will be instantiated and called as such: * NumArray obj = new NumArray(nums); * obj.update(i,val); * int param_2 = obj.sumRange(i,j); */","title":"Range Sum Query - Mutable"},{"location":"leetcode/array/notes/#range-sum-query-2d-immutable_1","text":"Prefix sum solution Extended from the 1d array, we can use the prefix sum of the 2d matrix. we use extra space to store the accumulative sum of the submatrix with upper left coordinate (0, 0) and lower right coordinate (i, j) . C++ prefix sum solution class NumMatrix { private : vector < vector < int > > dp ; public : NumMatrix ( vector < vector < int >> matrix ) { int m = matrix . size (); if ( m == 0 ) return ; int n = matrix [ 0 ]. size (); //dp = vector<vector<int> (m + 1, vector<int>(n + 1, 0)); dp . resize ( m + 1 , vector < int > ( n + 1 , 0 )); for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { dp [ i + 1 ][ j + 1 ] = dp [ i ][ j + 1 ] + dp [ i + 1 ][ j ] + matrix [ i ][ j ] - dp [ i ][ j ]; } } } int sumRegion ( int row1 , int col1 , int row2 , int col2 ) { return dp [ row2 + 1 ][ col2 + 1 ] - dp [ row2 + 1 ][ col1 ] - dp [ row1 ][ col2 + 1 ] + dp [ row1 ][ col1 ]; } }; /** * Your NumMatrix object will be instantiated and called as such: * NumMatrix obj = new NumMatrix(matrix); * int param_1 = obj.sumRegion(row1,col1,row2,col2); */ Note This version create the m + 1 by n + 1 dp array to record the prefix sum. The code is clean and elegant. Alternative prefix sum solution The idea is the same, in the following solution, we have a m by n 2d array to record the accumulative sum. See how complex the code is. C++ prefix sum solution class NumMatrix { private : vector < vector < int >> dp ; int m ; int n ; public : NumMatrix ( vector < vector < int >> matrix ) { m = matrix . size (); if ( m == 0 ) return ; n = matrix [ 0 ]. size (); dp . resize ( m , vector < int > ( n , 0 )); for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { dp [ i ][ j ] = matrix [ i ][ j ]; if ( i > 0 ) { dp [ i ][ j ] += dp [ i - 1 ][ j ]; } if ( j > 0 ) { dp [ i ][ j ] += dp [ i ][ j - 1 ]; } if ( i > 0 && j > 0 ) { dp [ i ][ j ] -= dp [ i - 1 ][ j - 1 ]; } } } } int sumRegion ( int row1 , int col1 , int row2 , int col2 ) { int res = 0 ; res = dp [ row2 ][ col2 ]; if ( row1 > 0 ) { res -= dp [ row1 - 1 ][ col2 ]; } if ( col1 > 0 ) { res -= dp [ row2 ][ col1 - 1 ]; } if ( row1 > 0 && col1 > 0 ) { res += dp [ row1 - 1 ][ col1 - 1 ]; } return res ; } }; /** * Your NumMatrix object will be instantiated and called as such: * NumMatrix obj = new NumMatrix(matrix); * int param_1 = obj.sumRegion(row1,col1,row2,col2); */ Note This version is a little complex. However, the way it was written reflects a very important practice when operating on a 2d array, that is: to check the validation of the array.","title":"Range Sum Query 2D - Immutable"},{"location":"leetcode/array/notes/#range-sum-query-2d-mutable_1","text":"Binary Indexed Tree solution We use 2D version of Binary Index Tree. Some of the explaination can be found at Topcoder tutorial C++ BIT solution class NumMatrix { private : vector < vector < int > > nums ; vector < vector < int > > tree ; int m ; int n ; public : NumMatrix ( vector < vector < int >> matrix ) { if ( matrix . size () == 0 || matrix [ 0 ]. size () == 0 ) return ; m = matrix . size (); n = matrix [ 0 ]. size (); tree . resize ( m + 1 , vector < int > ( n + 1 , 0 )); nums . resize ( m , vector < int > ( n , 0 )); for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { update ( i , j , matrix [ i ][ j ]); } } } void update ( int row , int col , int val ) { if ( m == 0 || n == 0 ) return ; int diff = val - nums [ row ][ col ]; nums [ row ][ col ] = val ; for ( int i = row + 1 ; i <= m ; i += i & ( - i )) { for ( int j = col + 1 ; j <= n ; j += j & ( - j )) { tree [ i ][ j ] += diff ; } } } int sumRegion ( int row1 , int col1 , int row2 , int col2 ) { if ( m == 0 || n == 0 ) return 0 ; return getSum ( row2 + 1 , col2 + 1 ) - getSum ( row1 , col2 + 1 ) - getSum ( row2 + 1 , col1 ) + getSum ( row1 , col1 ); } int getSum ( int row , int col ) { int sum = 0 ; for ( int i = row ; i > 0 ; i -= i & ( - i )) { for ( int j = col ; j > 0 ; j -= j & ( - j )) { sum += tree [ i ][ j ]; } } return sum ; } }; /** * Your NumMatrix object will be instantiated and called as such: * NumMatrix obj = new NumMatrix(matrix); * obj.update(row,col,val); * int param_2 = obj.sumRegion(row1,col1,row2,col2); */","title":"Range Sum Query 2D - Mutable"},{"location":"leetcode/array/notes/#count-of-range-sum","text":"Solution 1 Merge sort using inplace_merge() The core is to figure out how to calculate the result while merging. It is based on the fact that the left half and right half are all sorted. Using the ordering information we are able to locate two points in the right half j and k , between which will fulfill the requirement. Several important points need to be made. 1) calculation of prefix sum of the array. The length is n + 1 not n ? 2) the range passed to the merge subroutine are open-end [start, end) . The base case of the subrouine. It return zero becuase the case has been counted in the for loop, we don't need to count it again. Not because the base case is 0 . class Solution { public : int countRangeSum ( vector < int >& nums , int lower , int upper ) { int n = nums . size (); vector < long > sums ( n + 1 , 0 ); for ( int i = 0 ; i < n ; i ++ ) sums [ i + 1 ] = sums [ i ] + nums [ i ]; return mergeSort ( sums , 0 , n + 1 , lower , upper ); } int mergeSort ( vector < long >& sums , int start , int end , int lower , int upper ) { if ( end - start <= 1 ) return 0 ; // note the meaning of this 0. int mid = start + ( end - start ) / 2 ; count = mergeSort ( sums , start , mid , lower , upper ) + mergeSort ( sums , mid , end , lower , upper ); int m = mid , n = mid , count = 0 ; for ( int i = start ; i < mid ; i ++ ) { while ( m < end && sums [ m ] - sums [ i ] < lower ) m ++ ; while ( n < end && sums [ n ] - sums [ i ] <= upper ) n ++ ; count += n - m ; } inplace_merge ( sums . begin () + start , sums . begin () + mid , sums . begin () + end ); return count ; } }; Solution 2 Merge sort using tmp buffer cache Here is how the count is making sense. |--------------|-------------------| sums: |start |mid |end |---|----------|------|------|-----| i j k Because sums[j] - sums[i] >= lower, and sums[k] - sums[i] > upper, So for the subarray start with i, ending index in [j, k), the range sum is in [lower, upper]. Notice k should not be included. class Solution { public : int countRangeSum ( vector < int >& nums , int lower , int upper ) { int n = nums . size (); long sums [ n + 1 ] = { 0 }; if ( n == 0 ) return 0 ; for ( int i = 0 ; i < n ; i ++ ) { sums [ i + 1 ] = sums [ i ] + nums [ i ]; } /* n + 1 is the one pass the last element of sums */ return countByMergeSort ( sums , 0 , n + 1 , lower , upper ); } /* This function will return sorted array sums[start], ... sums[end - 1] */ int countByMergeSort ( long sums [], int start , int end , int lower , int upper ) { if ( end - start <= 1 ) return 0 ; int mid = start + ( end - start ) / 2 ; int count = countByMergeSort ( sums , start , mid , lower , upper ) + countByMergeSort ( sums , mid , end , lower , upper ); long cache [ end - start ] = { 0 }; int j = mid , k = mid , t = mid ; for ( int i = start , r = 0 ; i < mid ; ++ i , ++ r ) { while ( k < end && sums [ k ] - sums [ i ] < lower ) k ++ ; while ( j < end && sums [ j ] - sums [ i ] <= upper ) j ++ ; count += j - k ; /* calculate the result */ /* Merge left and right to get sorted array {sums[start], .. sums[end - 1]}. * Because left part of sums[start] to sums[mid] are already sorted, * use cache here to merge prefix of the right part: sum[mid] to sums[t] * with left part upto sums[i] for all i = {start, mid - 1}. */ while ( t < end && sums [ t ] < sums [ i ]) cache [ r ++ ] = sums [ t ++ ]; cache [ r ] = sums [ i ]; } /* after this for loop, cache will have partially sorted array * cache = sums_left = {sums[start], ... sums[t - 1]} element * of which will be in their final sorted positions. * array sums_right = {sums[t], sums[end - 1]} is also * in their final sorted positions. */ /* Since the sums_left is sorted, it have size of t - start, * here we copy exactly t - start element from cache to sums. */ for ( int i = start ; i < t ; i ++ ) sums [ i ] = cache [ i - start ]; return count ; } }; Solution 3 BST Solution 4 BIT","title":"Count of Range Sum"},{"location":"leetcode/array/notes/#maximum-sum-of-two-non-overlapping-subarrays","text":"Brute Force Iterate class Solution { public : int maxSumTwoNoOverlap ( vector < int >& A , int L , int M ) { int n = A . size (); if ( L == 0 || M == 0 ) { return 0 ; } vector < int > preSum ( n , 0 ); preSum [ 0 ] = A [ 0 ]; for ( int i = 1 ; i < n ; i ++ ) { preSum [ i ] = preSum [ i - 1 ] + A [ i ]; } int res = 0 ; // iterate the L using index i for ( int i = 0 ; i < n - L + 1 ; i ++ ) { int Lsum = 0 ; if ( i == 0 ) { Lsum = preSum [ i + L - 1 ]; } else { Lsum = preSum [ i + L - 1 ] - preSum [ i - 1 ]; } int Msum = 0 ; // iterate the left M array using index j for ( int j = 0 ; j < i - M ; j ++ ) { int tmp = 0 ; if ( j == 0 ) { tmp = preSum [ j + M - 1 ]; } else { tmp = preSum [ j + M - 1 ] - preSum [ j - 1 ]; } Msum = max ( Msum , tmp ); } // iterate the right M array using index j for ( int j = i + L ; j < n - M + 1 ; j ++ ) { Msum = max ( Msum , preSum [ j + M - 1 ] - preSum [ j - 1 ]); } res = max ( res , Msum + Lsum ); } return res ; } }; One pass class Solution { public : int maxSumTwoNoOverlap ( vector < int >& A , int L , int M ) { int n = A . size (); if ( L == 0 || M == 0 ) { return 0 ; } vector < int > preSum ( n , 0 ); preSum [ 0 ] = A [ 0 ]; for ( int i = 1 ; i < n ; i ++ ) { preSum [ i ] = preSum [ i - 1 ] + A [ i ]; } int res = INT_MIN ; int Lmax = INT_MIN ; int Mmax = INT_MIN ; // for ( int i = L + M ; i <= n ; i ++ ) { // L is front, M is back if ( i == L + M ) { Lmax = preSum [ L - 1 ]; } else { Lmax = max ( Lmax , preSum [ i - M - 1 ] - preSum [ i - L - M - 1 ]); } // M is front, L is back if ( i == L + M ) { Mmax = preSum [ M - 1 ]; } else { Mmax = max ( Mmax , preSum ( i - L - 1 ) - preSum [ i - M - L - 1 ]); } res = max ({ res , Lmax + preSum [ i - 1 ] - preSum [ i - M - 1 ], Mmax + preSum [ i - 1 ] - preSum [ i - L - 1 ]}) } return res ; } }; DP solution //TODO","title":"Maximum Sum of Two Non-Overlapping Subarrays"},{"location":"leetcode/array/notes/#maximum-sum-of-3-non-overlapping-subarrays","text":"","title":"Maximum Sum of 3 Non-Overlapping Subarrays"},{"location":"leetcode/array/notes/#longest-substring-without-repeating-characters","text":"","title":"Longest Substring Without Repeating Characters"},{"location":"leetcode/array/notes/#longest-substring-with-at-most-two-distinct-characters","text":"","title":"Longest Substring with At Most Two Distinct Characters"},{"location":"leetcode/array/notes/#longest-substring-with-at-most-k-distinct-characters","text":"","title":"Longest Substring with At Most K Distinct Characters"},{"location":"leetcode/array/notes/#subarrays-with-k-different-integers","text":"If you use sliding window to solve this problem, there are lots of corner cases. for example: [1, 1, 2, 1, 2, 1, 3, 1], K = 2 , how can you ensure you count all the subarray? Remember this trick that you can use the atMostKDistinct(A, K) - atMostKDistinct(A, K - 1) . class Solution { public : int subarraysWithKDistinct ( vector < int >& A , int K ) { return subarrayWithAtMostKDistinct ( A , K ) - subarrayWithAtMostKDistinct ( A , K - 1 ); } int subarrayWithAtMostKDistinct ( vector < int >& A , int K ) { unordered_map < int , int > count ; int i = 0 ; int res = 0 ; for ( int j = 0 ; j < A . size (); ++ j ) { if ( ! count [ A [ j ]] ++ ) K -- ; while ( k < 0 ) { if ( !-- count [ A [ j ]]) K ++ ; i ++ ; } res += j - i + 1 ; } return res ; } };","title":"Subarrays with K Different Integers"},{"location":"leetcode/array/notes/#number-of-substrings-containing-all-three-characters","text":"","title":"Number of Substrings Containing All Three Characters"},{"location":"leetcode/array/notes/#count-number-of-nice-subarrays","text":"","title":"Count Number of Nice Subarrays"},{"location":"leetcode/array/notes/#replace-the-substring-for-balanced-string","text":"","title":"Replace the Substring for Balanced String"},{"location":"leetcode/array/notes/#binary-subarrays-with-sum","text":"","title":"Binary Subarrays With Sum"},{"location":"leetcode/array/notes/#fruit-into-baskets","text":"","title":"Fruit Into Baskets"},{"location":"leetcode/array/notes/#shortest-subarray-with-sum-at-least-k","text":"","title":"Shortest Subarray with Sum at Least K"},{"location":"leetcode/array/notes/#minimum-size-subarray-sum_1","text":"","title":"Minimum Size Subarray Sum"},{"location":"leetcode/array/notes/#substring-with-concatenation-of-all-words","text":"","title":"Substring with Concatenation of All Words"},{"location":"leetcode/array/notes/#max-consecutive-ones-ii","text":"","title":"Max Consecutive Ones II"},{"location":"leetcode/array/notes/#max-consecutive-ones-iii","text":"","title":"Max Consecutive Ones III"},{"location":"leetcode/array/notes/#category-4-k-sum-problems","text":"","title":"Category 4 K Sum problems"},{"location":"leetcode/array/notes/#two-sum","text":"","title":"Two Sum"},{"location":"leetcode/array/notes/#two-sum-ii-input-array-is-sorted","text":"","title":"Two Sum II - Input array is sorted"},{"location":"leetcode/array/notes/#two-sum-iii-data-structure-design","text":"","title":"Two Sum III - Data structure design"},{"location":"leetcode/array/notes/#two-sum-iv-input-is-a-bst","text":"","title":"Two Sum IV - Input is a BST"},{"location":"leetcode/array/notes/#3sum","text":"","title":"3Sum"},{"location":"leetcode/array/notes/#3sum-closest","text":"","title":"3Sum Closest"},{"location":"leetcode/array/notes/#3sum-smaller","text":"","title":"3Sum Smaller"},{"location":"leetcode/array/notes/#4sum","text":"","title":"4Sum"},{"location":"leetcode/array/notes/#4sum-ii","text":"","title":"4Sum II"},{"location":"leetcode/array/notes/#k-sum","text":"","title":"K Sum"},{"location":"leetcode/array/notes/#target-sum","text":"","title":"Target Sum"},{"location":"leetcode/array/notes/#cagegory-5-2d-arry-matrix-grid-problems","text":"","title":"Cagegory 5 2D arry (matrix, grid) problems"},{"location":"leetcode/array/notes/#perfect-rectangle","text":"","title":"Perfect Rectangle"},{"location":"leetcode/array/notes/#trapping-rain-water","text":"","title":"Trapping Rain Water"},{"location":"leetcode/array/notes/#trapping-rain-water-ii","text":"","title":"Trapping Rain Water II"},{"location":"leetcode/array/notes/#container-with-most-water","text":"","title":"Container With Most Water"},{"location":"leetcode/array/notes/#largest-rectangle-in-histogram","text":"","title":"Largest Rectangle in Histogram"},{"location":"leetcode/array/notes/#maximal-rectangle_1","text":"","title":"Maximal Rectangle"},{"location":"leetcode/array/notes/#maximal-square_1","text":"","title":"Maximal Square"},{"location":"leetcode/array/notes/#the-skyline-problem","text":"","title":"The Skyline Problem"},{"location":"leetcode/array/notes/#smallest-rectangle-enclosing-black-pixels","text":"","title":"Smallest Rectangle Enclosing Black Pixels"},{"location":"leetcode/array/notes/#rectangle-area","text":"","title":"Rectangle Area"},{"location":"leetcode/array/notes/#max-sum-of-rectangle-no-larger-than-k_1","text":"","title":"Max Sum of Rectangle No Larger Than K"},{"location":"leetcode/array/notes/#category-6-stock-buying-problems","text":"Most consistent ways of dealing with the series of stock problems","title":"Category 6 stock buying problems"},{"location":"leetcode/array/notes/#121-best-time-to-buy-and-sell-stock","text":"Solution 1 O(n) one pass to find the minimum and in the meantime, find the max profit. class Solution { public : int maxProfit ( vector < int > & prices ) { int n = prices . size (); if ( n == 0 ) return 0 ; int res = 0 , low = prices [ 0 ]; for ( int i = 1 ; i < n ; i ++ ) { res = max ( res , prices [ i ] - low ); low = min ( low , prices [ i ]); } return res ; } };","title":"121. Best Time to Buy and Sell Stock"},{"location":"leetcode/array/notes/#122-best-time-to-buy-and-sell-stock-ii","text":"Solution 1 Greedy since you can buy as many times as you can class Solution { public : int maxProfit ( vector < int > & prices ) { int n = prices . size (); int res = 0 ; for ( int i = 0 ; i < n - 1 ; i ++ ) { if ( prices [ i + 1 ] - prices [ i ]) { res += prices [ i + 1 ] - prices [ i ]; } } return res ; } };","title":"122. Best Time to Buy and Sell Stock II"},{"location":"leetcode/array/notes/#123-best-time-to-buy-and-sell-stock-iii","text":"You can now buy at most twice. how to max the profit. Solution 1 Dynamic programming 5 stages: 1. before buy the first <-- optimal solution could be at this stage 2. hold the first 3. sell the first <-- or at this stage, only bought once, 4. hold the second 5. sell the second <-- or at this stage, bought twice. class Solution { public : int maxProfit ( vector < int > & A ) { //1, 3, 5: // f[i][j] = max{f[i - 1][j], f[i - 1][j - 1] + A[i - 1] - A[i - 2]} // 2, 4: // f[i][j] = max{f[i - 1][j] + A[i - 1] - A[i - 2], // f[i - 1][j - 1], f[i - 1][j - 2] + A[i - 1] - A[i - 2]} int n = A . size (); if ( n == 0 ) { return 0 ; } int f [ n + 1 ][ 6 ]; /* init */ f [ 0 ][ 1 ] = 0 ; f [ 0 ][ 2 ] = f [ 0 ][ 3 ] = f [ 0 ][ 4 ] = f [ 0 ][ 5 ] = INT_MIN ; for ( int i = 1 ; i <= n ; i ++ ) { for ( int j = 1 ; j <= 5 ; j += 2 ) { f [ i ][ j ] = f [ i - 1 ][ j ]; if ( i > 1 && j > 1 && f [ i - 1 ][ j - 1 ] != INT_MIN ) { f [ i ][ j ] = max ( f [ i ][ j ], f [ i - 1 ][ j - 1 ] + A [ i - 1 ] - A [ i - 2 ]); } } for ( int j = 2 ; j < 5 ; j += 2 ) { f [ i ][ j ] = f [ i - 1 ][ j - 1 ]; if ( i > 1 && f [ i - 1 ][ j ] != INT_MIN ) { f [ i ][ j ] = max ( f [ i ][ j ], f [ i - 1 ][ j ] + A [ i - 1 ] - A [ i - 2 ]); } if ( i > 1 && j > 2 && f [ i - 1 ][ j - 2 ] != INT_MIN ) { f [ i ][ j ] = max ( f [ i ][ j ], f [ i - 1 ][ j - 2 ] + A [ i - 1 ] - A [ i - 2 ]); } } } return max ( f [ n ][ 1 ], max ( f [ n ][ 3 ], f [ n ][ 5 ])); } }; Solution 2 Use T[i][k][j] to represent the maximum profit of first i days if we allow at most k transactions and the current number of stocks at hand is j ( j == 0, 1 because hold two stocks at the same time is not allowed). So we have: T[i][2][0] = max(T[i - 1][2][0], T[i - 1][2][1] + prices[i - 1]); T[i][2][1] = max(T[i - 1][2][1], T[i - 1][1][0] - prices[i - 1]); T[i][1][0] = max(T[i - 1][1][0], T[i - 1][1][1] + prices[i - 1]); T[i][1][1] = max(T[i - 1][1][1], T[i - 1][0][0] - prices[i - 1]); Think: How to ensure you fourmular to cover all the possible values?","title":"123. Best Time to Buy and Sell Stock III"},{"location":"leetcode/array/notes/#188-best-time-to-buy-and-sell-stock-iv","text":"","title":"188. Best Time to Buy and Sell Stock IV"},{"location":"leetcode/array/notes/#309-best-time-to-buy-and-sell-stock-with-cooldown","text":"","title":"309. Best Time to Buy and Sell Stock with Cooldown"},{"location":"leetcode/array/notes/#714-best-time-to-buy-and-sell-stock-with-transaction-fee","text":"","title":"714. Best Time to Buy and Sell Stock with Transaction Fee"},{"location":"leetcode/array/notes/#determine-the-buy-data-and-sell-data-of-maximum-profit-dd-139","text":"","title":"Determine the buy data and sell data of maximum profit (DD 139)"},{"location":"leetcode/backtracking/notes/","text":"Backtracking \u00b6 Introduction \u00b6 Backtracking algorithm can be used to generate all the subsets of a given set, all the permutation of a given sequence, and all the combinations of k elements from a given set with n elements. The algorithms are very similar but differ in some unique property of each problem. Subnets \u00b6 How many subsets there are for a set with n elements? ( 2^n 2^n ) How to generate all the subsets if there are NO duplicates in the set? (See problem Subsets ) How to generate all the UNIQUE subsets if there are duplicates in the set? (See problem Subsets II ) What are the application of subset? Permutation \u00b6 How many permutations (number of ordering) of a sequence with n elements? ( n! n! ) How to generate all the permutations if there are NO duplicates in the set? (See problem Permutations ) How to generate all the UNIQUE permutations if there are duplicates in the set? (See problem Permutations II ) What are the applications of permutation? (upper bound of sorting algorithms) Combination \u00b6 How many combinations of k elements from a set of n elements? ( \\binom{n}{k} \\binom{n}{k} ) How to derive the formula \\binom{n}{k} = \\frac{n!}{k! (n - k)!} \\binom{n}{k} = \\frac{n!}{k! (n - k)!} ? Choose k elements from the set one after another without putting back: n (n - 1) \\dots (n - k + 1) = \\frac{n!}{(n - k)!} n (n - 1) \\dots (n - k + 1) = \\frac{n!}{(n - k)!} Calculate the number of ways to choose k elements as \\binom{n}{k} \\binom{n}{k} , then order the k elements: \\binom{n}{k} \\times k! \\binom{n}{k} \\times k! Equalize the two give us: \\binom{n}{k} = \\frac{n!}{k! (n - k)!} \\binom{n}{k} = \\frac{n!}{k! (n - k)!} What\u2019s the relation between the subset and combination? For k from 0 to n , get all the combinations, all those combinations will form the powerset of the original set of n elements. We have \\sum_{k = 0}^{n}\\binom{n}{k} = \\binom{n}{0} + \\binom{n}{1}, \\dots, \\binom{n}{n} = 2^n\\text{(number of all set)} \\sum_{k = 0}^{n}\\binom{n}{k} = \\binom{n}{0} + \\binom{n}{1}, \\dots, \\binom{n}{n} = 2^n\\text{(number of all set)} How to generate all the combination if there are NO duplicates in the set? (See problem Combinations ) How to generate all the UNIQUE combinations if there are duplicates in the set? (See problem Combination Sum , Combination Sum II , Combination Sum III , Backpack VI ) What are the applications of combination? (calculate the upper bound of sorting algorithms) Partition \u00b6 How many ways can we partition a set of n elements into r groups, with the i -th group have n_i n_i elements. ( \\frac{n!}{n_1! n_2!, \\dots, n_r!} \\frac{n!}{n_1! n_2!, \\dots, n_r!} ) We will look at the problem this way: note the numbfer of different ways of partitioning as C C , each partition have n_1, n_2, \\dots, n_r n_1, n_2, \\dots, n_r elements. We align n_1, n_2, \\dots, n_r n_1, n_2, \\dots, n_r together to form a n elements sequence and there are n! n! of such sequences. Remeber for each partition, there are n_1! n_1! different sequences. So that we have so we have C \\cdot n_1! \\cdot n_2! \\dots n_r! = n! C \\cdot n_1! \\cdot n_2! \\dots n_r! = n! , and \\frac{n!}{n_1! n_2!, \\dots, n_r!} \\frac{n!}{n_1! n_2!, \\dots, n_r!} When the r = 2 , the partition problem essentially becomes a combination problem. What are the applications of partition? (calculate the upper bound of sorting algorithms) Summary \u00b6 Subset , permutation , and combination problems can be solved using a single code template. Other problems that are involving multiple steps, and asking for multiple eligible results that fulfill certain criteria, could also use this template. The subtlety arise when deal with replicates element in the problems. There are some general principles that I summarized for solving it. The recursive helper function prototype: helper ( vector < int > nums , int n , [ int cur ], vector < int > & subset , vector < vector < int >> & result ); The \" select action \" could be done through update the index int cur for each level of recursive call. Each for iteration (invoke helper function) is to \" select \" an element in this level, it recursively adding more elements to the temporary result. To remove duplicate result if duplicate element presented, we first need to be clear about where the duplicate results come from. e.g. In generating the power subsets, the duplicated results due to repeatedly selecting the same element from different original position. (consecutive positions, since it is sorted first). Therefore, the idea to avoid duplicate results is to not select the duplicate elements for a second time to form the same pattern, we use the following template check statement to achieve this. if ( i != cur && nums [ i ] == nums [ i - 1 ]){ continue ; } 4 elements of backtracking What is the iteration? (the for loop, the same level of node in the tree) What is the recursion? (what index will be advanced?) What is counted in the result? (what should be pushed back?) When is the result returned? (the cutting condition) traceability of backtracking By using recursion and iteration together in this backtrack technique, we can imagine the problem as growing a recursive tree. To grow the node at the same level, an index in a for loop can do the job. To grow a child of a node, a recursive function should be called. More specifically, we can advance the index by passing it into the recursive function. This index increment let the tree grow vertically. Problems \u00b6 Remove Invalid Parenthesis \u00b6 Decide what to search Relation with BFS Subsets \u00b6 C++ class Solution { public : vector < vector < int >> subsets ( vector < int >& nums ) { int n = nums . size (); vector < vector < int > > results ; vector < int > subset ; helper ( nums , n , 0 , subset , results ); return results ; } /* helper to get the permutation from curr to n - 1, total is n - curr */ void helper ( vector < int > nums , int n , int curr , vector < int >& subset , vector < vector < int > >& results ) { results . push_back ( subset ); for ( int i = curr ; i < n ; i ++ ) { subset . push_back ( nums [ i ]); helper ( nums , n , i + 1 , subset , results ); subset . pop_back (); } } }; Python class Solution ( object ): def subsets ( self , nums ): \"\"\" :type nums: List[int] :rtype: List[List[int]] \"\"\" n = len ( nums ) results = [] self . helper ( nums , n , 0 , [], results ) return results def helper ( self , nums , n , curr , currSubset , results ): results . append ( list ( currSubset )) for i in range ( curr , n ): currSubset . append ( nums [ i ]) self . helper ( nums , n , i + 1 , currSubset , results ) currSubset . pop () Subsets II \u00b6 C++ class Solution { public : vector < vector < int >> subsetsWithDup ( vector < int >& nums ) { int n = nums . size (); vector < vector < int >> results ; vector < int > subset ; sort ( nums . begin (), nums . end ()); helper ( nums , n , 0 , subset , results ); return results ; } void helper ( vector < int > nums , int n , int curr , vector < int >& subset , vector < vector < int >>& results ) { results . push_back ( subset ); for ( int i = curr ; i < n ; i ++ ) { if ( i > curr && nums [ i ] == nums [ i - 1 ]) { continue ; } subset . push_back ( nums [ i ]); helper ( nums , n , i + 1 , subset , results ); subset . pop_back (); } } }; Python class Solution ( object ): def subsetsWithDup ( self , nums ): \"\"\" :type nums: List[int] :rtype: List[List[int]] \"\"\" n = len ( nums ) results = [] nums . sort () self . helper ( nums , n , 0 , [], results ) return results def helper ( self , nums , n , curr , currSubset , results ): results . append ( list ( currSubset )) for i in range ( curr , n ): if ( i > curr ) and ( nums [ i ] == nums [ i - 1 ]): continue currSubset . append ( nums [ i ]) self . helper ( nums , n , i + 1 , currSubset , results ) currSubset . pop () Permutations \u00b6 C++ Backtracking class Solution { public : vector < vector < int > > permute ( vector < int > nums ) { vector < vector < int > > results ; vector < int > permutation ; int n = nums . size (); if ( n == 0 ) return results ; sort ( nums . begin (), nums . end ()); helper ( nums , n , permutation , results ); return results ; } void helper ( vector < int > nums , int n , vector < int > & permutation , vector < vector < int > > & results ) { if ( permutation . size () == n ) { results . push_back ( permutation ); return ; } for ( int i = 0 ; i < n ; i ++ ) { vector < int >:: iterator it ; it = find ( permutation . begin (), permutation . end (), nums [ i ]); if ( it == permutation . end ()){ permutation . push_back ( nums [ i ]); helper ( nums , n , permutation , results ); permutation . pop_back (); } } } }; Use 'visited' variable class Solution { public : vector < vector < int >> permute ( vector < int >& nums ) { int n = nums . size (); vector < vector < int > > results ; vector < int > permute ; vector < bool > visited ( n , false ); helper ( nums , n , visited , permute , results ); return results ; } void helper ( vector < int > nums , int n , vector < bool > visited , vector < int >& permutation , vector < vector < int > >& results ) { if ( permutation . size () == n ) { results . push_back ( permutation ); return ; } for ( int i = 0 ; i < n ; i ++ ) { if ( visited [ i ]) { continue ; } visited [ i ] = true ; permutation . push_back ( nums [ i ]); helper ( nums , n , visited , permutation , results ); permutation . pop_back (); visited [ i ] = false ; } } }; Permutations II \u00b6 The line 27 is very tricky and important. It can be understand as following: using !visited[i - 1] makes sure when duplicates are selected, the order is ascending (index from small to large). However, using visited[i - 1] means the descending order. You cannot using the find to check whether the element is presetend, you have to use the visited \"bit map\" to record the states. C++ class Solution { public : vector < vector < int >> permuteUnique ( vector < int >& nums ) { int n = nums . size (); vector < vector < int >> results ; vector < int > permutation ; vector < bool > visited ( n , 0 ); if ( n == 0 ) return results ; sort ( nums . begin (), nums . end ()); helper ( nums , n , visited , permutation , results ); return results ; } void helper ( vector < int > nums , int n , vector < bool > visited , vector < int >& permutation , vector < vector < int >>& results ) { if ( permutation . size () == n ) { results . push_back ( permutation ); return ; } for ( int i = 0 ; i < n ; i ++ ) { if ( visited [ i ] || ( i > 0 && nums [ i ] == nums [ i - 1 ] && ! visited [ i - 1 ])) { // 1st //if (visited[i] || (i > 0 && nums[i] == nums[i - 1] && visited[i - 1])) { // 2nd // the condition enfoce the duplicates only added once to the result. // 1st is saying A[i - 1] is added, A[i] is not, then we add A[i]. // 2nd is saying none of A[i - 1] and A[i] is added, then we add A[i]. // duplicated element only add once in all subtrees. // e.g. 1 2 3 4 4 4 5 6 7. the 4, 4, 4 should be only selected once continue ; } visited [ i ] = true ; permutation . push_back ( nums [ i ]); helper ( nums , n , visited , permutation , results ); permutation . pop_back (); visited [ i ] = false ; } } }; Combinations \u00b6 Notice the = in the for loop, here the i is not array index, but number we are enumerating. class Solution { public : vector < vector < int >> combine ( int n , int k ) { vector < vector < int >> results ; vector < int > comb ; helper ( n , k , 1 , comb , results ); return results ; } void helper ( int n , int k , int cur , vector < int >& comb , vector < vector < int >>& results ) { if ( comb . size () == k ) { results . push_back ( comb ); return ; } for ( int i = cur ; i <= n ; i ++ ) { comb . push_back ( i ); helper ( n , k , i + 1 , comb , results ); comb . pop_back (); } } }; Combination Sum \u00b6 Notice in line 23, the recursion is on i again even though it has been selected already. class Solution { public : vector < vector < int >> combinationSum ( vector < int >& candidates , int target ) { vector < vector < int >> results ; vector < int > result ; sort ( candidates . begin (), candidates . end ()); helper ( candidates , 0 , target , result , results ); return results ; } void helper ( vector < int > nums , int cur , int target , vector < int >& result , vector < vector < int >>& results ) { if ( target == 0 ) { results . push_back ( result ); return ; } for ( int i = cur ; i < nums . size (); i ++ ) { if ( nums [ i ] <= target ) { result . push_back ( nums [ i ]); helper ( nums , i , target - nums [ i ], result , results ); result . pop_back (); } } } }; Combination Sum II \u00b6 Notice in this problem, duplicate number could be in the original array, don't worry, we will treat the same element in different position differently. Notice how this code can be changed from the Combinations Sum, compare to the solution we only add the if check in line 21, and call the hlepr function with i + 1 in line 27. class Solution { public : vector < vector < int >> combinationSum2 ( vector < int >& candidates , int target ) { vector < vector < int >> results ; vector < int > result ; sort ( candidates . begin (), candidates . end ()); helper ( candidates , 0 , target , result , results ); return results ; } void helper ( vector < int > nums , int cur , int target , vector < int >& result , vector < vector < int >>& results ) { if ( target == 0 ) { results . push_back ( result ); return ; } for ( int i = cur ; i < nums . size (); i ++ ) { if ( i > cur && nums [ i ] == nums [ i - 1 ]) { continue ; } if ( nums [ i ] <= target ) { result . push_back ( nums [ i ]); helper ( nums , i + 1 , target - nums [ i ], result , results ); result . pop_back (); } } } }; Combination Sum III \u00b6 Very similar to the Combination Sum . Instead of giving an array, this problem gives numbers from 1 to 9 and no duplicate selection allowed. The meaning of this problem is also similar to the problem K Sum , while K Sum is asking \"how many\", so it is solved using Dynamic Programming. differ with Combination Sum in the for loop bounds and the recursive call, remember in Combination Sum , the recursive function called with i instead of i + 1 because duplication is allowed in that problem. class Solution { public : vector < vector < int >> combinationSum2 ( vector < int >& candidates , int target ) { vector < vector < int >> results ; vector < int > result ; sort ( candidates . begin (), candidates . end ()); helper ( candidates , 0 , target , result , results ); return results ; } void helper ( vector < int > nums , int cur , int target , vector < int >& result , vector < vector < int >>& results ) { if ( target == 0 ) { results . push_back ( result ); return ; } for ( int i = cur ; i < nums . size (); i ++ ) { if ( i > cur && nums [ i ] == nums [ i - 1 ]) { continue ; } if ( nums [ i ] <= target ) { result . push_back ( nums [ i ]); helper ( nums , i + 1 , target - nums [ i ], result , results ); result . pop_back (); } } } }; Combination Sum IV (Backpack VI) \u00b6 \u8fd9\u91cc\u53ef\u4ee5\u968f\u4fbf\u53d6\uff0c\u4f3c\u4e4e\u9898\u76ee\u53d8\u5f97\u65e0\u6cd5\u4e0b\u624b\uff0c\u8003\u8651\u201c\u6700\u540e\u4e00\u6b65\u201d\u8fd9\u4e2a\u6280\u5de7\u4e0d\u80fd\u7528\u4e86\uff0c\u56e0\u4e3a\u6700\u540e\u4e00\u6b65\u53ef\u4ee5\u662f\u4efb\u610f\u4e00\u4e2a\u4e86\u3002 \u4f46\u4ecd\u7136\u53ef\u4ee5\u7528\u5b50\u95ee\u9898\u6765\u8003\u8651\u3002\u5148\u4e0d\u7ba1\u6700\u540e\u4e00\u6b65\u662f\u54ea\u4e00\u4e2a\uff0c\u6700\u540e\u4e00\u6b65\u4e4b\u524d\u7684\u76f8\u52a0\u7684\u603b\u548c\u4e00\u5b9a\u662f Target - x . \u8fd9\u6837\u5c31\u8f6c\u5316\u6210\u4e00\u4e2a\u5b50\u95ee\u9898\u53ef\u4ee5\u7528DP\u6765\u505a\u3002 \u5177\u4f53\u505a\u6cd5\u6211\u4eec\u53ef\u4ee5\u5bf9\u4e8e\u6bcf\u4e00\u4e2a\u5c0f\u4e8e\u201c\u603b\u627f\u91cd\u201d\u7684\u91cd\u91cf\u8fdb\u884c\u679a\u4e3e\u6700\u540e\u4e00\u6b65 x \u3002\u53ef\u80fd\u7684 x \u662f A[0], ..., A[i - 1] \u4e2d\u4efb\u610f\u4e00\u4e2a. Must initialize f[i] = 0 . Because some of the state won't be updated indicates that they are not possible to fill). class Solution { public : int backPackVI ( vector < int >& nums , int T ) { int n = nums . size (); if ( n == 0 ) { return 0 ; } int f [ T + 1 ]; f [ 0 ] = 1 ; /* for each sub problem */ for ( int i = 1 ; i <= T ; i ++ ) { f [ i ] = 0 ; /* enumerate the last step */ for ( int j = 0 ; j < n ; j ++ ) { if ( i >= nums [ j ]) { f [ i ] += f [ i - nums [ j ]]; } } } return f [ T ]; } }; Note \u5373\u4f7f\u662f\u7b80\u5355\u7684\u4e00\u7ef4\u80cc\u5305\uff0c\u4f9d\u7136\u662f\u603b\u627f\u91cd\u653e\u5165\u72b6\u6001(\u5373\u6240\u5f00\u6570\u7ec4\u4e0e\u603b\u627f\u91cd\u76f8\u5173) Print one such combination solution Suppose we also interested in print one of the possible solution. How could we change the code? f[i] : \u5b58\u591a\u5c11\u79cd\u65b9\u5f0f pi[i] : \u5982\u679c f[i] >= 1 , \u6700\u540e\u4e00\u4e2a\u6570\u5b57\u53ef\u4ee5\u662f pi[i] Print one such combination class Solution { public : int backPackVI ( vector < int >& nums , int T ) { int n = nums . size (); if ( n == 0 ) { return 0 ; } int f [ T + 1 ]; /* pi[i]: \u5982\u679ci\u53ef\u62fc\u51fa(f[i] >= 1), \u6700\u540e\u4e00\u4e2a\u662fpi[i] */ int pi [ T + 1 ]; f [ 0 ] = 1 ; for ( int i = 1 ; i <= T ; i ++ ) { f [ i ] = 0 ; for ( int j = 0 ; j < n ; j ++ ) { if ( i >= nums [ j ]) { f [ i ] += f [ i - nums [ j ]]; /* \u6700\u540e\u4e00\u4e2a\u662fnums[j]\u7684\u53ef\u62fc\u51fai */ if ( f [ i - nums [ j ]] > 0 ) { /* \u7eaa\u5f55\u4e0b\u6765 */ pi [ i ] = nums [ j ]; } } } } if ( f [ T ] > 0 ) { int i = T ; cout << i << \"=\" << endl ; while ( i != 0 ) { // sum is i now; // last number is pi[i] // previuos sum is i - pi[i] cout << pi [ i ] << endl ; i -= pi [ i ]; } } return f [ T ]; } };","title":"Backtracking"},{"location":"leetcode/backtracking/notes/#backtracking","text":"","title":"Backtracking"},{"location":"leetcode/backtracking/notes/#introduction","text":"Backtracking algorithm can be used to generate all the subsets of a given set, all the permutation of a given sequence, and all the combinations of k elements from a given set with n elements. The algorithms are very similar but differ in some unique property of each problem.","title":"Introduction"},{"location":"leetcode/backtracking/notes/#subnets","text":"How many subsets there are for a set with n elements? ( 2^n 2^n ) How to generate all the subsets if there are NO duplicates in the set? (See problem Subsets ) How to generate all the UNIQUE subsets if there are duplicates in the set? (See problem Subsets II ) What are the application of subset?","title":"Subnets"},{"location":"leetcode/backtracking/notes/#permutation","text":"How many permutations (number of ordering) of a sequence with n elements? ( n! n! ) How to generate all the permutations if there are NO duplicates in the set? (See problem Permutations ) How to generate all the UNIQUE permutations if there are duplicates in the set? (See problem Permutations II ) What are the applications of permutation? (upper bound of sorting algorithms)","title":"Permutation"},{"location":"leetcode/backtracking/notes/#combination","text":"How many combinations of k elements from a set of n elements? ( \\binom{n}{k} \\binom{n}{k} ) How to derive the formula \\binom{n}{k} = \\frac{n!}{k! (n - k)!} \\binom{n}{k} = \\frac{n!}{k! (n - k)!} ? Choose k elements from the set one after another without putting back: n (n - 1) \\dots (n - k + 1) = \\frac{n!}{(n - k)!} n (n - 1) \\dots (n - k + 1) = \\frac{n!}{(n - k)!} Calculate the number of ways to choose k elements as \\binom{n}{k} \\binom{n}{k} , then order the k elements: \\binom{n}{k} \\times k! \\binom{n}{k} \\times k! Equalize the two give us: \\binom{n}{k} = \\frac{n!}{k! (n - k)!} \\binom{n}{k} = \\frac{n!}{k! (n - k)!} What\u2019s the relation between the subset and combination? For k from 0 to n , get all the combinations, all those combinations will form the powerset of the original set of n elements. We have \\sum_{k = 0}^{n}\\binom{n}{k} = \\binom{n}{0} + \\binom{n}{1}, \\dots, \\binom{n}{n} = 2^n\\text{(number of all set)} \\sum_{k = 0}^{n}\\binom{n}{k} = \\binom{n}{0} + \\binom{n}{1}, \\dots, \\binom{n}{n} = 2^n\\text{(number of all set)} How to generate all the combination if there are NO duplicates in the set? (See problem Combinations ) How to generate all the UNIQUE combinations if there are duplicates in the set? (See problem Combination Sum , Combination Sum II , Combination Sum III , Backpack VI ) What are the applications of combination? (calculate the upper bound of sorting algorithms)","title":"Combination"},{"location":"leetcode/backtracking/notes/#partition","text":"How many ways can we partition a set of n elements into r groups, with the i -th group have n_i n_i elements. ( \\frac{n!}{n_1! n_2!, \\dots, n_r!} \\frac{n!}{n_1! n_2!, \\dots, n_r!} ) We will look at the problem this way: note the numbfer of different ways of partitioning as C C , each partition have n_1, n_2, \\dots, n_r n_1, n_2, \\dots, n_r elements. We align n_1, n_2, \\dots, n_r n_1, n_2, \\dots, n_r together to form a n elements sequence and there are n! n! of such sequences. Remeber for each partition, there are n_1! n_1! different sequences. So that we have so we have C \\cdot n_1! \\cdot n_2! \\dots n_r! = n! C \\cdot n_1! \\cdot n_2! \\dots n_r! = n! , and \\frac{n!}{n_1! n_2!, \\dots, n_r!} \\frac{n!}{n_1! n_2!, \\dots, n_r!} When the r = 2 , the partition problem essentially becomes a combination problem. What are the applications of partition? (calculate the upper bound of sorting algorithms)","title":"Partition"},{"location":"leetcode/backtracking/notes/#summary","text":"Subset , permutation , and combination problems can be solved using a single code template. Other problems that are involving multiple steps, and asking for multiple eligible results that fulfill certain criteria, could also use this template. The subtlety arise when deal with replicates element in the problems. There are some general principles that I summarized for solving it. The recursive helper function prototype: helper ( vector < int > nums , int n , [ int cur ], vector < int > & subset , vector < vector < int >> & result ); The \" select action \" could be done through update the index int cur for each level of recursive call. Each for iteration (invoke helper function) is to \" select \" an element in this level, it recursively adding more elements to the temporary result. To remove duplicate result if duplicate element presented, we first need to be clear about where the duplicate results come from. e.g. In generating the power subsets, the duplicated results due to repeatedly selecting the same element from different original position. (consecutive positions, since it is sorted first). Therefore, the idea to avoid duplicate results is to not select the duplicate elements for a second time to form the same pattern, we use the following template check statement to achieve this. if ( i != cur && nums [ i ] == nums [ i - 1 ]){ continue ; } 4 elements of backtracking What is the iteration? (the for loop, the same level of node in the tree) What is the recursion? (what index will be advanced?) What is counted in the result? (what should be pushed back?) When is the result returned? (the cutting condition) traceability of backtracking By using recursion and iteration together in this backtrack technique, we can imagine the problem as growing a recursive tree. To grow the node at the same level, an index in a for loop can do the job. To grow a child of a node, a recursive function should be called. More specifically, we can advance the index by passing it into the recursive function. This index increment let the tree grow vertically.","title":"Summary"},{"location":"leetcode/backtracking/notes/#problems","text":"","title":"Problems"},{"location":"leetcode/backtracking/notes/#remove-invalid-parenthesis","text":"Decide what to search Relation with BFS","title":"Remove Invalid Parenthesis"},{"location":"leetcode/backtracking/notes/#subsets","text":"C++ class Solution { public : vector < vector < int >> subsets ( vector < int >& nums ) { int n = nums . size (); vector < vector < int > > results ; vector < int > subset ; helper ( nums , n , 0 , subset , results ); return results ; } /* helper to get the permutation from curr to n - 1, total is n - curr */ void helper ( vector < int > nums , int n , int curr , vector < int >& subset , vector < vector < int > >& results ) { results . push_back ( subset ); for ( int i = curr ; i < n ; i ++ ) { subset . push_back ( nums [ i ]); helper ( nums , n , i + 1 , subset , results ); subset . pop_back (); } } }; Python class Solution ( object ): def subsets ( self , nums ): \"\"\" :type nums: List[int] :rtype: List[List[int]] \"\"\" n = len ( nums ) results = [] self . helper ( nums , n , 0 , [], results ) return results def helper ( self , nums , n , curr , currSubset , results ): results . append ( list ( currSubset )) for i in range ( curr , n ): currSubset . append ( nums [ i ]) self . helper ( nums , n , i + 1 , currSubset , results ) currSubset . pop ()","title":"Subsets"},{"location":"leetcode/backtracking/notes/#subsets-ii","text":"C++ class Solution { public : vector < vector < int >> subsetsWithDup ( vector < int >& nums ) { int n = nums . size (); vector < vector < int >> results ; vector < int > subset ; sort ( nums . begin (), nums . end ()); helper ( nums , n , 0 , subset , results ); return results ; } void helper ( vector < int > nums , int n , int curr , vector < int >& subset , vector < vector < int >>& results ) { results . push_back ( subset ); for ( int i = curr ; i < n ; i ++ ) { if ( i > curr && nums [ i ] == nums [ i - 1 ]) { continue ; } subset . push_back ( nums [ i ]); helper ( nums , n , i + 1 , subset , results ); subset . pop_back (); } } }; Python class Solution ( object ): def subsetsWithDup ( self , nums ): \"\"\" :type nums: List[int] :rtype: List[List[int]] \"\"\" n = len ( nums ) results = [] nums . sort () self . helper ( nums , n , 0 , [], results ) return results def helper ( self , nums , n , curr , currSubset , results ): results . append ( list ( currSubset )) for i in range ( curr , n ): if ( i > curr ) and ( nums [ i ] == nums [ i - 1 ]): continue currSubset . append ( nums [ i ]) self . helper ( nums , n , i + 1 , currSubset , results ) currSubset . pop ()","title":"Subsets II"},{"location":"leetcode/backtracking/notes/#permutations","text":"C++ Backtracking class Solution { public : vector < vector < int > > permute ( vector < int > nums ) { vector < vector < int > > results ; vector < int > permutation ; int n = nums . size (); if ( n == 0 ) return results ; sort ( nums . begin (), nums . end ()); helper ( nums , n , permutation , results ); return results ; } void helper ( vector < int > nums , int n , vector < int > & permutation , vector < vector < int > > & results ) { if ( permutation . size () == n ) { results . push_back ( permutation ); return ; } for ( int i = 0 ; i < n ; i ++ ) { vector < int >:: iterator it ; it = find ( permutation . begin (), permutation . end (), nums [ i ]); if ( it == permutation . end ()){ permutation . push_back ( nums [ i ]); helper ( nums , n , permutation , results ); permutation . pop_back (); } } } }; Use 'visited' variable class Solution { public : vector < vector < int >> permute ( vector < int >& nums ) { int n = nums . size (); vector < vector < int > > results ; vector < int > permute ; vector < bool > visited ( n , false ); helper ( nums , n , visited , permute , results ); return results ; } void helper ( vector < int > nums , int n , vector < bool > visited , vector < int >& permutation , vector < vector < int > >& results ) { if ( permutation . size () == n ) { results . push_back ( permutation ); return ; } for ( int i = 0 ; i < n ; i ++ ) { if ( visited [ i ]) { continue ; } visited [ i ] = true ; permutation . push_back ( nums [ i ]); helper ( nums , n , visited , permutation , results ); permutation . pop_back (); visited [ i ] = false ; } } };","title":"Permutations"},{"location":"leetcode/backtracking/notes/#permutations-ii","text":"The line 27 is very tricky and important. It can be understand as following: using !visited[i - 1] makes sure when duplicates are selected, the order is ascending (index from small to large). However, using visited[i - 1] means the descending order. You cannot using the find to check whether the element is presetend, you have to use the visited \"bit map\" to record the states. C++ class Solution { public : vector < vector < int >> permuteUnique ( vector < int >& nums ) { int n = nums . size (); vector < vector < int >> results ; vector < int > permutation ; vector < bool > visited ( n , 0 ); if ( n == 0 ) return results ; sort ( nums . begin (), nums . end ()); helper ( nums , n , visited , permutation , results ); return results ; } void helper ( vector < int > nums , int n , vector < bool > visited , vector < int >& permutation , vector < vector < int >>& results ) { if ( permutation . size () == n ) { results . push_back ( permutation ); return ; } for ( int i = 0 ; i < n ; i ++ ) { if ( visited [ i ] || ( i > 0 && nums [ i ] == nums [ i - 1 ] && ! visited [ i - 1 ])) { // 1st //if (visited[i] || (i > 0 && nums[i] == nums[i - 1] && visited[i - 1])) { // 2nd // the condition enfoce the duplicates only added once to the result. // 1st is saying A[i - 1] is added, A[i] is not, then we add A[i]. // 2nd is saying none of A[i - 1] and A[i] is added, then we add A[i]. // duplicated element only add once in all subtrees. // e.g. 1 2 3 4 4 4 5 6 7. the 4, 4, 4 should be only selected once continue ; } visited [ i ] = true ; permutation . push_back ( nums [ i ]); helper ( nums , n , visited , permutation , results ); permutation . pop_back (); visited [ i ] = false ; } } };","title":"Permutations II"},{"location":"leetcode/backtracking/notes/#combinations","text":"Notice the = in the for loop, here the i is not array index, but number we are enumerating. class Solution { public : vector < vector < int >> combine ( int n , int k ) { vector < vector < int >> results ; vector < int > comb ; helper ( n , k , 1 , comb , results ); return results ; } void helper ( int n , int k , int cur , vector < int >& comb , vector < vector < int >>& results ) { if ( comb . size () == k ) { results . push_back ( comb ); return ; } for ( int i = cur ; i <= n ; i ++ ) { comb . push_back ( i ); helper ( n , k , i + 1 , comb , results ); comb . pop_back (); } } };","title":"Combinations"},{"location":"leetcode/backtracking/notes/#combination-sum","text":"Notice in line 23, the recursion is on i again even though it has been selected already. class Solution { public : vector < vector < int >> combinationSum ( vector < int >& candidates , int target ) { vector < vector < int >> results ; vector < int > result ; sort ( candidates . begin (), candidates . end ()); helper ( candidates , 0 , target , result , results ); return results ; } void helper ( vector < int > nums , int cur , int target , vector < int >& result , vector < vector < int >>& results ) { if ( target == 0 ) { results . push_back ( result ); return ; } for ( int i = cur ; i < nums . size (); i ++ ) { if ( nums [ i ] <= target ) { result . push_back ( nums [ i ]); helper ( nums , i , target - nums [ i ], result , results ); result . pop_back (); } } } };","title":"Combination Sum"},{"location":"leetcode/backtracking/notes/#combination-sum-ii","text":"Notice in this problem, duplicate number could be in the original array, don't worry, we will treat the same element in different position differently. Notice how this code can be changed from the Combinations Sum, compare to the solution we only add the if check in line 21, and call the hlepr function with i + 1 in line 27. class Solution { public : vector < vector < int >> combinationSum2 ( vector < int >& candidates , int target ) { vector < vector < int >> results ; vector < int > result ; sort ( candidates . begin (), candidates . end ()); helper ( candidates , 0 , target , result , results ); return results ; } void helper ( vector < int > nums , int cur , int target , vector < int >& result , vector < vector < int >>& results ) { if ( target == 0 ) { results . push_back ( result ); return ; } for ( int i = cur ; i < nums . size (); i ++ ) { if ( i > cur && nums [ i ] == nums [ i - 1 ]) { continue ; } if ( nums [ i ] <= target ) { result . push_back ( nums [ i ]); helper ( nums , i + 1 , target - nums [ i ], result , results ); result . pop_back (); } } } };","title":"Combination Sum II"},{"location":"leetcode/backtracking/notes/#combination-sum-iii","text":"Very similar to the Combination Sum . Instead of giving an array, this problem gives numbers from 1 to 9 and no duplicate selection allowed. The meaning of this problem is also similar to the problem K Sum , while K Sum is asking \"how many\", so it is solved using Dynamic Programming. differ with Combination Sum in the for loop bounds and the recursive call, remember in Combination Sum , the recursive function called with i instead of i + 1 because duplication is allowed in that problem. class Solution { public : vector < vector < int >> combinationSum2 ( vector < int >& candidates , int target ) { vector < vector < int >> results ; vector < int > result ; sort ( candidates . begin (), candidates . end ()); helper ( candidates , 0 , target , result , results ); return results ; } void helper ( vector < int > nums , int cur , int target , vector < int >& result , vector < vector < int >>& results ) { if ( target == 0 ) { results . push_back ( result ); return ; } for ( int i = cur ; i < nums . size (); i ++ ) { if ( i > cur && nums [ i ] == nums [ i - 1 ]) { continue ; } if ( nums [ i ] <= target ) { result . push_back ( nums [ i ]); helper ( nums , i + 1 , target - nums [ i ], result , results ); result . pop_back (); } } } };","title":"Combination Sum III"},{"location":"leetcode/backtracking/notes/#combination-sum-iv-backpack-vi","text":"\u8fd9\u91cc\u53ef\u4ee5\u968f\u4fbf\u53d6\uff0c\u4f3c\u4e4e\u9898\u76ee\u53d8\u5f97\u65e0\u6cd5\u4e0b\u624b\uff0c\u8003\u8651\u201c\u6700\u540e\u4e00\u6b65\u201d\u8fd9\u4e2a\u6280\u5de7\u4e0d\u80fd\u7528\u4e86\uff0c\u56e0\u4e3a\u6700\u540e\u4e00\u6b65\u53ef\u4ee5\u662f\u4efb\u610f\u4e00\u4e2a\u4e86\u3002 \u4f46\u4ecd\u7136\u53ef\u4ee5\u7528\u5b50\u95ee\u9898\u6765\u8003\u8651\u3002\u5148\u4e0d\u7ba1\u6700\u540e\u4e00\u6b65\u662f\u54ea\u4e00\u4e2a\uff0c\u6700\u540e\u4e00\u6b65\u4e4b\u524d\u7684\u76f8\u52a0\u7684\u603b\u548c\u4e00\u5b9a\u662f Target - x . \u8fd9\u6837\u5c31\u8f6c\u5316\u6210\u4e00\u4e2a\u5b50\u95ee\u9898\u53ef\u4ee5\u7528DP\u6765\u505a\u3002 \u5177\u4f53\u505a\u6cd5\u6211\u4eec\u53ef\u4ee5\u5bf9\u4e8e\u6bcf\u4e00\u4e2a\u5c0f\u4e8e\u201c\u603b\u627f\u91cd\u201d\u7684\u91cd\u91cf\u8fdb\u884c\u679a\u4e3e\u6700\u540e\u4e00\u6b65 x \u3002\u53ef\u80fd\u7684 x \u662f A[0], ..., A[i - 1] \u4e2d\u4efb\u610f\u4e00\u4e2a. Must initialize f[i] = 0 . Because some of the state won't be updated indicates that they are not possible to fill). class Solution { public : int backPackVI ( vector < int >& nums , int T ) { int n = nums . size (); if ( n == 0 ) { return 0 ; } int f [ T + 1 ]; f [ 0 ] = 1 ; /* for each sub problem */ for ( int i = 1 ; i <= T ; i ++ ) { f [ i ] = 0 ; /* enumerate the last step */ for ( int j = 0 ; j < n ; j ++ ) { if ( i >= nums [ j ]) { f [ i ] += f [ i - nums [ j ]]; } } } return f [ T ]; } }; Note \u5373\u4f7f\u662f\u7b80\u5355\u7684\u4e00\u7ef4\u80cc\u5305\uff0c\u4f9d\u7136\u662f\u603b\u627f\u91cd\u653e\u5165\u72b6\u6001(\u5373\u6240\u5f00\u6570\u7ec4\u4e0e\u603b\u627f\u91cd\u76f8\u5173) Print one such combination solution Suppose we also interested in print one of the possible solution. How could we change the code? f[i] : \u5b58\u591a\u5c11\u79cd\u65b9\u5f0f pi[i] : \u5982\u679c f[i] >= 1 , \u6700\u540e\u4e00\u4e2a\u6570\u5b57\u53ef\u4ee5\u662f pi[i] Print one such combination class Solution { public : int backPackVI ( vector < int >& nums , int T ) { int n = nums . size (); if ( n == 0 ) { return 0 ; } int f [ T + 1 ]; /* pi[i]: \u5982\u679ci\u53ef\u62fc\u51fa(f[i] >= 1), \u6700\u540e\u4e00\u4e2a\u662fpi[i] */ int pi [ T + 1 ]; f [ 0 ] = 1 ; for ( int i = 1 ; i <= T ; i ++ ) { f [ i ] = 0 ; for ( int j = 0 ; j < n ; j ++ ) { if ( i >= nums [ j ]) { f [ i ] += f [ i - nums [ j ]]; /* \u6700\u540e\u4e00\u4e2a\u662fnums[j]\u7684\u53ef\u62fc\u51fai */ if ( f [ i - nums [ j ]] > 0 ) { /* \u7eaa\u5f55\u4e0b\u6765 */ pi [ i ] = nums [ j ]; } } } } if ( f [ T ] > 0 ) { int i = T ; cout << i << \"=\" << endl ; while ( i != 0 ) { // sum is i now; // last number is pi[i] // previuos sum is i - pi[i] cout << pi [ i ] << endl ; i -= pi [ i ]; } } return f [ T ]; } };","title":"Combination Sum IV (Backpack VI)"},{"location":"leetcode/binary-indexed-tree/notes/","text":"Binary Indexed Tree \u00b6 \u00b6 Problems \u00b6 \u00b6","title":"Binary Indexed Tree"},{"location":"leetcode/binary-indexed-tree/notes/#binary-indexed-tree","text":"","title":"Binary Indexed Tree"},{"location":"leetcode/binary-indexed-tree/notes/#_1","text":"","title":""},{"location":"leetcode/binary-indexed-tree/notes/#problems","text":"","title":"Problems"},{"location":"leetcode/binary-indexed-tree/notes/#_2","text":"","title":""},{"location":"leetcode/binary-search/notes/","text":"Binary Search \u00b6 Binary search problem characteristics \u00b6 Ordered binary search. You need to find an index or array element where ordering info is available, either explicitly (sorted array) or implicitly (partially sorted or other special info). monotony pattern. If the ordering info isn't available, but you can exclude \"all\" the possible cases from left or right by a condition comparing f(mid) to the target . Binary search problem solving techniques \u00b6 Clarify that you are trying to find the first one or find the last one. Clarify that you are trying to move the index or move the value (i.e. kth smallest number in a multiplicative table). Use an \"ordering abstraction\" vless(target, f(mid)) . This ordering abstraction will produce a boolean array that indicate the ordering information between the target value and the f(mid) . Decide whether the left or right part the f(mid) should fall into. The principle to determine the predicate is simple: don't rule out the possible result (maintain the loop invariant). Shrink the range accordingly based on the predicate decided in step 3. Test the case that the search range is small, such as only have one or two elements. Binary search practical use case \u00b6 Find whether the given target is in the array. Find the position of the first value equal to the given target. Find the insertion position of the given target in the array. Find the position of the last value equal to the given target. Find the total number of x in a sorted array. Find the last element less than the target. Find the first element greater than the target. Binary search in C++ STL \u00b6 lower_bound : return iterator point to the element no less than the target. upper_bound : return iterator point to the element greater than the target. equal_range : return a pair of iterators, the first of which is lower_bound , the second is upper_bound . binary_search : return true if an element equivalent to val is found, and false otherwise. Caveat of binary search implementation \u00b6 Specify the range: [start, end) or [start, end] ? C++ STL used [start, end) to denote a range, which bring in many conveniences. We will stick on this convention. Which while loop condition? start < end ? start <= end ? start != end ? start + 1 < end ? The calculation of the mid . mid = start + (end - start) / 2 or mid = (start + end) / 2 ? To proof mid is always in the range [begin, end) . The \"bisection\": start = mid + 1 , start = mid , or end = mid - 1 or end = mid ? Where is the result? start ? end ? How to make sure? A \"universal\" binary search implementation \u00b6 Despite the above caveats, just remember that there are two versions of binary search one can write based on the range [begin, end) and [begin, end] . Iterator type in C++ using the former, it have many benefits in reduce the code complexity. Among all the binary search implementation you have seen, the following one is the most powerful version and it equivalent to C++ STL lower_bound algorithm. /** * return an index to an element no less than x. Be more specifically, if there is * an element in the given array equal to x, it returns the index of first such * element; if there is no element that is equal to x, it returns the index * where x can be inserted into the position without changing the ordering of * the elements. * * All possible return value for calling this function with array.size() == n is * [0, 1, ..., n - 1, n] * */ size_t binary_search ( int x , vector < int >& array , size_t n ) { size_t begin = 0 , end = n ; while ( begin != end ) { size_t mid = begin + ( end - begin ) / 2 ; if ( array [ mid ] < x ) { begin = mid + 1 ; } else { end = mid ; } } return begin ; } Important observations about this implementation \u00b6 mid cannot less than begin , they can be equal. This will ensure begin = mid + 1 in the if statement at least reduce the size of [begin, end] by 1. Informal proof: if (array[mid] < x) , it indicate x can only possible be in array[mid + 1, mid + 2, ... n - 1]. mid + 1 is at least 1 greater than begin. mid and end never equal inside the while loop, mid < end is always hold. This will ensure end = mid in the else statement at least reduce the size of [begin, end] by 1. Informal proof: we have begain < end , so begin + end < 2 * end , thus (begin + end) / 2 < end , because integer divisioin truncate down, mid = (begin + end) / 2 always less than end. begin and end never cross. Informal proof: Inside the while loop, at the begining, we have begin < end . If the current iteration executes the if condition, begin = mid + 1 at most advance begin to end but not exceed end . If it execute the else condition, end = mid would at worst case change end point to the minimum value of mid , because we have begin <= mid . Thus, we can conclude that executing the statement end = mid will not change end less than begin , at worst equal to begin . Claims regarding this binary search routine \u00b6 The range [begin, end) is used, which comply to the convention used in C++ iterator. It is impossible that mid == end . If they are equal, array[n] is invalid memory access. We use the loop condition while (begin != end) to indicate that once the loop terminates, we have begin == end . By checking whether begin is a valid index to the array or not, we can know whether x is greater than all the elements in the array or not. If we want to check whether x is found in the array, we simply check array[begin] == x . However, this condition is based on the assumption that begin < end initially. Considering that, using while (begin < end) is better if you cannot ensure begin < end before the loop. Setting begin = mid + 1 reduces the size of the remaining interested sub-array and maintains the invariant, which is if x in the array, x is in [begin, end) . Setting end = mid reduces the size of the remaining interested sub-array (mid never equal to end) and maintains the invariant, which is if x in the array, x is in [begin, end) . This claim is a little hard to absorb. On way to understand is like the following: ~~Because we need keep searching x in the range [begin, mid] if we get in the else statement. In the else case there are two possibilities: 1) array[mid] > x . 2) array[mid] = x . For 1) it indicates x is in [begin, mid) , setting end = mid maintains the loop invariant correctly, which is that x is in the shrinked range. For the 2) it is a little complex. If array[mid] is the only element equal to x, setting end = mid appears violate the loop invariant by exclude x from the range [begin, end) . however, remember array[mid] is the only element equal to x, after the while loop, begin = end , we have the x found by begin even though theoretically [begin, end) is already an empty range since begin = end and array[begin] = array[end] = x . If there are more values are equal to x before and after the element array[mid] the loop will always end up finding the first x value in the array. If we use end = mid + 1 . Try test case [1, 3, 5, 7] , with x = 0 . deadloop will accur. i.e. begin = 0, mid = 1, end = 2 . Category 1 Binary search on sorted arrays \u00b6 To solve this type of binary search problem. You should focus on the following: Come up test cases to verify your solution. Be able to find which side to drop for each iteration. Be extremly careful \"off by 1\" bugs. (1. reasoning: is mid value possible to be the solution or not. 2. exercise test cases: especially short ones) 34. Search for a Range \u00b6 C++ Use STL function class Solution { public : vector < int > searchRange ( vector < int >& nums , int target ) { vector < int > res ( 2 , -1 ); int low = lower_bound ( nums . begin (), nums . end (), target ) - nums . begin (); int high = upper_bound ( nums . begin (), nums . end (), target ) - nums . begin (); if ( low == high ) return res ; return { low , hight - 1 }; } }; C++ Implementation of binary search class Solution { public : vector < int > searchRange ( vector < int >& nums , int target ) { vector < int > res ( 2 , -1 ); int low = lower_bound ( nums , target ); //int high = lower_bound(nums, target + 1); // also works. int high = upper_bound ( nums , target ); if ( low == high ) { return res ; } return { low , high - 1 }; } int lower_bound ( vector < int >& nums , int target ) { if ( nums . size () == 0 ) return 0 ; int l = 0 , r = nums . size (); while ( l < r ) { int m = l + ( r - l ) / 2 ; if ( nums [ m ] < target ) { l = m + 1 ; } else { r = m ; } } return l ; } int upper_bound ( vector < int >& nums , int target ) { if ( nums . size () == 0 ) return 0 ; int l = 0 , r = nums . size (); while ( l < r ) { int m = l + ( r - l ) / 2 ; if ( nums [ m ] <= target ) { l = m + 1 ; } else { r = m ; } } return l ; } }; Python solution class Solution ( object ): def searchRange ( self , nums , target ): \"\"\" :type nums: List[int] :type target: int :rtype: List[int] \"\"\" if len ( nums ) == 0 : return [ - 1 , - 1 ] begin = 0 end = len ( nums ) while begin != end : mid = begin + ( end - begin ) / 2 if nums [ mid ] < target : begin = mid + 1 else : end = mid if begin == len ( nums ): return [ - 1 , - 1 ] if nums [ begin ] == target : lower = begin else : lower = - 1 begin = 0 end = len ( nums ) while begin != end : mid = begin + ( end - begin ) / 2 if nums [ mid ] <= target : begin = mid + 1 else : end = mid if nums [ begin - 1 ] == target : upper = begin - 1 else : upper = - 1 return [ lower , upper ] 35. Search Insert Position \u00b6 C++ solution lower_bound class Solution { public : int searchInsert ( vector < int >& nums , int target ) { if ( nums . size () == 0 ) return 0 ; int l = 0 , r = nums . size (); while ( l < r ) { int m = l + ( r - l ) / 2 ; if ( nums [ m ] < target ) { l = m + 1 ; } else { r = m ; } } return l ; } }; 33. Search in Rotated Sorted Array \u00b6 How to locate the sorted half? If left half is sorted, check where the target t is like be. else if right half is sorted, check where the target t is like to be. else if mid element is equal to left or right. Remove one of them. Although no duplicate, should consider short input like [3 1], 1 will have the equal case. C++ /** t = 1 t = 3 t = 5 t = 4 t = -1 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 3 4 5 1 1 3 5 4 1 <--need check */ class Solution { public : int search ( vector < int >& A , int t ) { if ( A . empty ()) return -1 ; int l = 0 , r = A . size () - 1 ; while ( l < r ) { int m = l + ( r - l ) / 2 ; if ( A [ m ] == t ) return m ; if ( A [ l ] < A [ m ]) { // left is sorted if ( A [ l ] <= t && t < A [ m ]) { r = m - 1 ; } else { l = m + 1 ; } } else if ( A [ m ] < A [ r ]) { // right is sorted if ( A [ m ] < t && t <= A [ r ]) { l = m + 1 ; } else { r = m - 1 ; } } else { // if equal, remove one. case: [3, 1], 1 if ( A [ l ] == A [ m ]) l ++ ; if ( A [ m ] == A [ r ]) r -- ; } } return A [ l ] == t ? l : -1 ; } }; Python class Solution ( object ): def search ( self , nums , target ): \"\"\" :type nums: List[int] :type target: int :rtype: int \"\"\" if len ( nums ) == 0 : return - 1 left = 0 right = len ( nums ) - 1 while left < right : mid = left + ( right - left ) / 2 if nums [ mid ] == target : return mid ; if nums [ left ] < nums [ mid ]: if nums [ left ] <= target and target < nums [ mid ]: right = mid - 1 else : left = mid + 1 elif nums [ mid ] < nums [ right ]: if nums [ mid ] < target and target <= nums [ right ]: left = mid + 1 else : right = mid - 1 else : if nums [ left ] == nums [ mid ]: left += 1 if nums [ right ] == nums [ mid ]: right -= 1 if nums [ left ] == target : return left return - 1 81. Search in Rotated Sorted Array II \u00b6 How to locate the sorted half? C++ class Solution { public : bool search ( vector < int >& A , int t ) { if ( A . empty ()) return false ; int l = 0 , r = A . size () - 1 ; while ( l < r ) { int m = l + ( r - l ) / 2 ; if ( A [ m ] == t ) return true ; if ( A [ l ] < A [ m ]) { if ( A [ l ] <= t && t < A [ m ]) { r = m - 1 ; } else { l = m + 1 ; } } else if ( A [ m ] < A [ r ]) { if ( A [ m ] < t && t <= A [ r ]) { l = m + 1 ; } else { r = m - 1 ; } } else { if ( A [ l ] == A [ m ]) l ++ ; if ( A [ m ] == A [ r ]) r -- ; } } return A [ l ] == t ? true : false ; } }; Python class Solution ( object ): def search ( self , nums , target ): \"\"\" :type nums: List[int] :type target: int :rtype: int \"\"\" if len ( nums ) == 0 : return False left = 0 right = len ( nums ) - 1 while left < right : mid = left + ( right - left ) / 2 if nums [ mid ] == target : return True if nums [ left ] < nums [ mid ]: if nums [ left ] <= target and target < nums [ mid ]: right = mid - 1 else : left = mid + 1 elif nums [ mid ] < nums [ right ]: if nums [ mid ] < target and target <= nums [ right ]: left = mid + 1 else : right = mid - 1 else : if nums [ left ] == nums [ mid ]: left += 1 if nums [ right ] == nums [ mid ]: right -= 1 if nums [ left ] == target : return True return False 153. Find Minimum in Rotated Sorted Array \u00b6 Try to locate the valley which contains the min. Notice when A[0] < A[n - 1] , return A[0] . Draw a monotonic curve and then split the curve into two half, swith the order. This can help you to write the code. C++ class Solution { public : int findMin ( vector < int >& A ) { int l = 0 , r = A . size () - 1 ; while ( l < r ) { if ( A [ l ] < A [ r ]) // serve as base case. return A [ l ]; int m = l + ( r - l ) / 2 ; if ( A [ m ] > A [ r ]) { // also works. looking for not sorted half l = m + 1 ; } else if ( A [ m ] < A [ r ]) { // don't really need if statement r = m ; } } return A [ l ]; } }; Python class Solution ( object ): def findMin ( self , nums ): \"\"\" :type nums: List[int] :rtype: int \"\"\" if len ( nums ) == 0 : return - 1 left = 0 right = len ( nums ) - 1 while left < right : if nums [ left ] == nums [ right ]: return nums [ left ] mid = left + ( right - left ) / 2 if nums [ mid ] > nums [ right ]: left = mid + 1 else : right = mid return nums [ left ] 154. Find Minimum in Rotated Sorted Array II \u00b6 Locate the valley which contains the min. Since duplicates exist. we cannot use the observation A[l] == A[r] . Here we deal with duplicates using decrease by one step. C++ class Solution { public : int findMin ( vector < int >& A ) { int l = 0 , r = A . size () - 1 ; while ( l < r ) { int m = l + ( r - l ) / 2 ; if ( A [ m ] > A [ r ]) { l = m + 1 ; } else if ( A [ m ] < A [ r ]) { r = m ; } else { r -- ; } } return A [ l ]; } }; Python class Solution ( object ): def findMin ( self , nums ): \"\"\" :type nums: List[int] :rtype: int \"\"\" if len ( nums ) == 0 : return - 1 left = 0 right = len ( nums ) - 1 while left < right : mid = left + ( right - left ) / 2 if nums [ mid ] > nums [ right ]: left = mid + 1 elif nums [ mid ] < nums [ right ]: right = mid else : right -= 1 return nums [ left ] 162 Find Peak Element \u00b6 Use Binary search Use the neighboring relation to determin which side a peak value may occur then eliminate the other side. class Solution { public : int findPeakElement ( vector < int >& A ) { int l = 0 , r = A . size () - 1 ; while ( l < r ) { int m = l + ( r - l ) / 2 ; if ( A [ m ] < A [ m + 1 ]) { l = m + 1 ; } else if ( A [ m ] > A [ m + 1 ]) { r = m ; } } return l ; } }; 278. First Bad Version \u00b6 Binary search Notice how this can be related to the ordering abstraction. // Forward declaration of isBadVersion API. bool isBadVersion ( int version ); class Solution { public : int firstBadVersion ( int n ) { int l = 0 , r = n ; while ( l < r ) { int m = l + ( r - l ) / 2 ; if ( ! isBadVersion ( m )) { l = m + 1 ; } else { r = m ; } } return l ; } }; 74. Search a 2D Matrix \u00b6 Binary search We can view the matrix as a big sorted array and then binary search the target. Notice test your finished routine using edge cases. (i.e. the initial value of end) class Solution { public : bool searchMatrix ( vector < vector < int >>& matrix , int target ) { int m = matrix . size (); int n = m ? matrix [ 0 ]. size () : 0 ; if ( m == 0 || n == 0 ) return false ; int start = 0 , end = m * n - 1 ; while ( start < end ) { int mid = start + ( end - start ) / 2 ; int i = mid / n , j = mid % n ; if ( matrix [ i ][ j ] < target ) { start = mid + 1 ; } else { end = mid ; } } return matrix [ start / n ][ start % n ] == target ? true : false ; } }; 240. Search a 2D Matrix II \u00b6 Binary search to exclude whole column or whole row the key is you decide where to start the compare. If you start from left bottom or right top, the solution should be abvious. Notice the idea is from binary search, if ordering info available, we want to exclude as many as impossible values as we can. class Solution { public : bool searchMatrix ( vector < vector < int >>& matrix , int target ) { int m = matrix . size (); int n = m ? matrix [ 0 ]. size () : 0 ; if ( m == 0 || n == 0 ) return false ; int x = m - 1 , y = 0 ; while ( x >= 0 && y < n ) { if ( matrix [ x ][ y ] == target ) { return true ; } if ( matrix [ x ][ y ] < target ) { y ++ ; } else { x -- ; } } return false ; } }; 302. Smallest Rectangle Enclosing Black Pixels \u00b6 C++ Brute Force class Solution { public : int minArea ( vector < vector < char >>& image , int x , int y ) { int m = image . size (); int n = m ? image [ 0 ]. size () : 0 ; int top = m , bottom = 0 , left = n , right = 0 ; for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { if ( image [ i ][ j ] == '1' ) { top = min ( top , i ); bottom = max ( bottom , i + 1 ); left = min ( left , j ); right = max ( right , j + 1 ); } } } return ( right - left ) * ( bottom - top ); } }; Binary search Notice the binary search idea is related to the problem Smallest Good Base and Wood Cut. The basic idea is to search each of 1 from 4 directions. First, make sure you can search one boundary and the others are similar. For example, to search the first row that contains 1 , we can look at the whole column/row to see whether this col/row have 1 . Because we are searching the first row that have 1 top down, bisec based on the count of 1 on each row we can know whether we ignore the upper half or the lower half. C++ Binary Search class Solution { public : int minArea ( vector < vector < char >>& image , int x , int y ) { int m = image . size (); int n = m ? image [ 0 ]. size () : 0 ; int top = bsearch_byrows ( image , 0 , x , 0 , n , true ); // search top int bottom = bsearch_byrows ( image , x + 1 , m , 0 , n , false ); int left = bsearch_bycols ( image , 0 , y , top , bottom , true ); int right = bsearch_bycols ( image , y + 1 , n , top , bottom , false ); return ( bottom - top ) * ( right - left ); } int bsearch_byrows ( vector < vector < char >>& image , int x , int y , int left , int right , bool white2black ) { while ( x < y ) { int m = ( x + y ) / 2 ; int k = left ; while ( k < right && image [ m ][ k ] == '0' ) k ++ ; if ( k < right == white2black ) { // mth row have '1' y = m ; } else { x = m + 1 ; } } return x ; } int bsearch_bycols ( vector < vector < char >>& image , int x , int y , int top , int bottom , bool white2black ) { while ( x < y ) { int m = ( x + y ) / 2 ; int k = top ; while ( k < bottom && image [ k ][ m ] == '0' ) k ++ ; if ( k < bottom == white2black ) { // mth column have '1' y = m ; } else { x = m + 1 ; } } return x ; } }; Python Binary Search class Solution ( object ): def minArea ( self , image , x , y ): \"\"\" :type image: List[List[str]] :type x: int :type y: int :rtype: int \"\"\" m = len ( image ) n = 0 if m != 0 : n = len ( image [ 0 ]) top = self . bsearch_row ( image , 0 , x , 0 , n , True ) bottom = self . bsearch_row ( image , x + 1 , m , 0 , n , False ) left = self . bsearch_col ( image , 0 , y , top , bottom , True ) right = self . bsearch_col ( image , y + 1 , n , top , bottom , False ) return ( bottom - top ) * ( right - left ) def bsearch_row ( self , image , start , end , lower , upper , white2black ): while start < end : m = ( start + end ) / 2 k = lower while k < upper and image [ m ][ k ] == '0' : k += 1 if ( k < upper ) == white2black : end = m else : start = m + 1 return start def bsearch_col ( self , image , start , end , lower , upper , white2black ): while start < end : m = ( start + end ) / 2 k = lower while k < upper and image [ k ][ m ] == '0' : k += 1 if ( k < upper ) == white2black : end = m else : start = m + 1 return start C++ BFS class Solution { public : int minArea ( vector < vector < char >>& image , int x , int y ) { int m = image . size (); int n = m ? image [ 0 ]. size () : 0 ; int top = m , bottom = 0 , left = n , right = 0 ; int xx [ 4 ] = { -1 , 0 , 1 , 0 }; int yy [ 4 ] = { 0 , 1 , 0 , -1 }; queue < pair < int , int >> q ; q . push ({ x , y }); image [ x ][ y ] = '0' ; while ( ! q . empty ()) { pair < int , int > t = q . front (); q . pop (); top = min ( top , t . first ); bottom = max ( bottom , t . first + 1 ); left = min ( left , t . second ); right = max ( right , t . second + 1 ); for ( int k = 0 ; k < 4 ; ++ k ) { int a = t . first + xx [ k ]; int b = t . second + yy [ k ]; if ( a >= 0 && a < m && b >= 0 && b < n && image [ a ][ b ] == '1' ) { q . push ({ a , b }); image [ a ][ b ] = '0' ; } } } return ( right - left ) * ( bottom - top ); } }; Python BFS from collections import deque class Solution ( object ): def minArea ( self , image , x , y ): \"\"\" :type image: List[List[str]] :type x: int :type y: int :rtype: int \"\"\" m = len ( image ) n = 0 if m != 0 : n = len ( image [ 0 ]) xx = [ - 1 , 0 , 1 , 0 ] yy = [ 0 , - 1 , 0 , 1 ] top = m bottom = 0 left = n right = 0 q = deque () q . append ([ x , y ]) image [ x ][ y ] = '0' while len ( q ) > 0 : t = q . popleft () top = min ( top , t [ 0 ]) bottom = max ( bottom , t [ 0 ] + 1 ) left = min ( left , t [ 1 ]) right = max ( right , t [ 1 ] + 1 ) for k in range ( 4 ): a = t [ 0 ] + xx [ k ] b = t [ 1 ] + yy [ k ] if a >= 0 and a < m and b >= 0 and b < n and image [ a ][ b ] == '1' : q . append ([ a , b ]) image [ a ][ b ] = '0' return ( right - left ) * ( bottom - top ) C++ DFS class Solution { private : int m , n ; int top , bottom , left , right ; public : int minArea ( vector < vector < char >>& image , int x , int y ) { m = image . size (); n = m ? image [ 0 ]. size () : 0 ; top = m , bottom = 0 , left = n , right = 0 ; dfs_helper ( image , x , y ); return ( right - left ) * ( bottom - top ); } void dfs_helper ( vector < vector < char >>& image , int x , int y ) { if ( x < 0 || x >= m || y < 0 || y >= n || image [ x ][ y ] == '0' ) { return ; } image [ x ][ y ] = '0' ; top = min ( top , x ); bottom = max ( bottom , x + 1 ); left = min ( left , y ); right = max ( right , y + 1 ); dfs_helper ( image , x - 1 , y ); dfs_helper ( image , x , y + 1 ); dfs_helper ( image , x + 1 , y ); dfs_helper ( image , x , y - 1 ); } }; 363. Max Sum of Rectangle No Larger Than K \u00b6 Iterate the wide of the matrix and using prefix sum and set lower_bound . From the problem Max Sum of Subarry No Larger Than K, we have to enumerate the width of the sub-matrix and sum up all row elements and get an array of length m , m is the number of rows of the matrix. Then apply the method. C++ presum class Solution { public : int maxSumSubmatrix ( vector < vector < int >>& matrix , int k ) { if ( matrix . empty ()) return 0 ; int m = matrix . size (); int n = m ? matrix [ 0 ]. size () : 0 ; int res = INT_MIN ; for ( int l = 0 ; l < n ; ++ l ) { vector < int > sums ( m , 0 ); for ( int r = l ; r < n ; ++ r ) { for ( int i = 0 ; i < m ; ++ i ) { sums [ i ] += matrix [ i ][ r ]; } set < int > preSumSet ; preSumSet . insert ( 0 ); int preSum = 0 , curMax = INT_MIN ; for ( int sum : sums ) { preSum += sum ; set < int >:: iterator it = preSumSet . lower_bound ( preSum - k ); if ( it != preSumSet . end ()) { curMax = max ( curMax , preSum - * it ); } preSumSet . insert ( preSum ); } res = max ( res , curMax ); } } return res ; } }; merge sort The idea is similar that solution 1. Instead of calculate preSum on the fly, we finish calculation and pass it to a mergeSort routine. The use mergeSort here is to find the A[j] - A[i] <= k efficiently, O(nlogn) . C++ Merge Sort class Solution { public : int maxSumSubmatrix ( vector < vector < int >>& matrix , int k ) { int m = matrix . size (); int n = m ? matrix [ 0 ]. size () : 0 ; int res = INT_MIN ; vector < long long > sums ( m + 1 , 0 ); for ( int l = 0 ; l < n ; ++ l ) { vector < long long > sumInRow ( m , 0 ); for ( int r = l ; r < n ; ++ r ) { for ( int i = 0 ; i < m ; ++ i ) { sumInRow [ i ] += matrix [ i ][ r ]; sums [ i + 1 ] = sums [ i ] + sumInRow [ i ]; } res = max ( res , mergeSort ( sums , 0 , m + 1 , k )); if ( res == k ) return k ; } } return res ; } int mergeSort ( vector < long long >& sums , int start , int end , int k ) { if ( end == start + 1 ) return INT_MIN ; int mid = start + ( end - start ) / 2 ; int res = mergeSort ( sums , start , mid , k ); if ( res == k ) return k ; res = max ( res , mergeSort ( sums , mid , end , k )); if ( res == k ) return k ; long long cache [ end - start ]; int j = mid , c = 0 , t = mid ; for ( int i = start ; i < mid ; ++ i ) { while ( j < end && sums [ j ] - sums [ i ] <= k ) ++ j ; // search first time sums[j] - sums[i] > k if ( j - 1 >= mid ) { // sums[j - 1] - sums[i] <= k, make sure j - 1 is still in right side res = max ( res , ( int )( sums [ j - 1 ] - sums [ i ])); if ( res == k ) return k ; } while ( t < end && sums [ t ] < sums [ i ]) { cache [ c ++ ] = sums [ t ++ ]; } cache [ c ++ ] = sums [ i ]; } for ( int i = start ; i < t ; ++ i ) { sums [ i ] = cache [ i - start ]; } return res ; } }; Category 2 Using ordering abstraction \u00b6 69. Sqrt(x) \u00b6 Solution 1 using ordering abstraction definition To find a square root of a integer x using binary search. We need to first determin the range [left, right] that the target value sqrt(x) may in. The potential range we can search is [0, x/2 + 1] . Then we should clarify this binary search is the \"find the first one\" type or the \"find the last one\" type. Basically, we want to determine our ordering abstraction f(target, g(i)) that is able to produce a boolean array. The boolean array have true part and false part seperated. Here target = sqrt(x) and g(i) = i . We define f(sqrt(x), i) = true when i <= sqrt(x) and f(sqrt(x), i) = false when i > sqrt(x) . This came from the following intuition: We are looking for the \"last\" integer whose square is less than x . Why not the otherwise? Because if you change to find the \"first\" integer whose square is greater than the x from right section of the boolean array, it is hard to define our ordering abstraction f . Of cause, we can search the \"first\" integer whose square is greater than x and find the previous integer next to it as the solution, but this later solution is a bit complex and counter intuitive. We prefer the first definition of ordering abstraction. Although a workable solution following the second ordering abstraction is also given below. For example: to solve the sqrt(8) and sqrt(9) using our definition: k, i = 0 1 2 3 4 5 6 7 8 9 10 n = 11 A = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] f(sqrt(8), k) = [T T T F F] f(sqrt(9), k) = [T T T T F] The binary search routine will be: class Solution { public : int mySqrt ( int x ) { int l = 0 , r = x / 2 + 1 ; while ( l < r ) { // int m = l + (r - l) / 2; // will deadloop for 4, why? int m = r - ( r - l ) / 2 ; if ( m <= x / m ) { l = m ; } else { r = m - 1 ; } } return l ; } }; Solution 2 using the alternative ordering abstraction definition Second ordering abstraction (find first value whose square is greater than x) k, i = 0 1 2 3 4 5 6 7 8 9 10 n = 11 A = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] f(sqrt(8), k) = [F F F T T] f(sqrt(9), k) = [F F F F T] class Solution { public : int mySqrt ( int x ) { if ( x == 0 ) return 0 ; // should handle, but will got division by zero in line 9. int l = 0 , r = x / 2 + 2 ; // r = x / 2 + 1 will not working for x = 1, have to have the one past last; while ( l < r ) { //int m = r - (r - l) / 2; // will dead loop for 4 int m = l + ( r - l ) / 2 ; if ( m > x / m ) { r = m ; } else { l = m + 1 ; } } return l - 1 ; } }; 367. Valid Perfect Square \u00b6 Solution 1 Binary search using ordering abstraction Notice you have to run tests for cases from 1 to 5. C++ class Solution { public : bool isPerfectSquare ( int num ) { if ( num == 1 ) return true ; int begin = 1 , end = num / 2 ; while ( begin < end ) { //long long mid = begin + (end - begin) / 2; // not working, deadloop for 5 long long mid = end - ( end - begin ) / 2 ; if ( mid * mid == num ) return true ; if ( mid * mid < num ) { begin = mid ; } else { end = mid - 1 ; } } return false ; } }; Python class Solution ( object ): def isPerfectSquare ( self , num ): \"\"\" :type num: int :rtype: bool \"\"\" if num == 1 : return True lower = 1 upper = num / 2 while lower < upper : mid = upper - ( upper - lower ) / 2 if mid * mid == num : return True if mid * mid < num : lower = mid else : upper = mid - 1 return False 441. Arranging Coins \u00b6 Solution 1 Biinary search Notice the integer overflow possibility, we use long to deal with it. class Solution { public : int arrangeCoins ( int n ) { if ( n < 2 ) { return n ; } long l = 0 ; long r = n ; while ( l < r ) { long m = l + ( r - l ) / 2 ; long t = m * ( m + 1 ) / 2 ; if ( t == n ) return m ; if ( t < n ) { l = m + 1 ; } else { r = m ; } } return l - 1 ; } }; 633. Sum of Square Numbers \u00b6 Solution 1 Binary search Once you have derived value b from a and c , you can binary search b . class Solution { public : bool judgeSquareSum ( int c ) { if ( c == 0 ) return true ; for ( long long a = 0 ; a * a <= c ; ++ a ) { int b = c - ( int ) ( a * a ); int l = 0 , r = b / 2 + 1 ; while ( l < r ) { long long m = r - ( r - l ) / 2 ; if ( m * m == b ) return true ; if ( m * m < b ) { l = m ; } else { r = m - 1 ; } } } return false ; } }; Solution 2 Two pointers Notice this square sum can be found efficiently using two pointers. class Solution { public : bool judgeSquareSum ( int c ) { int a = 0 , b = sqrt ( c ); while ( a <= b ){ int sum = a * a + b * b ; if ( sum < c ) a ++ ; else if ( sum > c ) b -- ; else return true ; } return false ; } }; Solution 3 Using set Keep inserting the value into a set, in the meantime also look up the other class Solution { public : bool judgeSquareSum ( int c ) { set < int > s ; for ( int i = 0 ; i <= sqrt ( c ); ++ i ) { s . insert ( c - i * i ); if ( s . count ( i * i )) return true ; } return false ; } }; 658. Find K Closest Elements \u00b6 Solution 1 Binary search Compare to problem 475. Heaters Our search target is to find the starting index of the subarray of length K. class Solution { public : vector < int > findClosestElements ( vector < int >& arr , int k , int x ) { int start = 0 , end = arr . size () - k ; while ( start < end ) { int mid = start + ( end - start ) / 2 ; // looking for a \"mid\" that if ( x - arr [ mid ] > arr [ mid + k ] - x ) { start = mid + 1 ; } else { end = mid ; } } return vector < int > ( arr . begin () + start , arr . begin () + start + k ); } }; Solution 2 Binary search and Two pointers We first use binary search to locate the x value then expand to left and right looking for the k closest elements Notice the i < 0 in the if condition, it is very important to be there. otherwise the array index will be out of bound. class Solution { public : vector < int > findClosestElements ( vector < int >& arr , int k , int x ) { int index = lower_bound ( arr . begin (), arr . end (), x ) - arr . begin (); int i = index - 1 , j = index ; while ( k -- ) { if ( i < 0 || j < arr . size () && abs ( arr [ j ] - x ) < abs ( arr [ i ] - x )) { j ++ ; } else { i -- ; } } return vector < int > ( arr . begin () + i + 1 , arr . begin () + j ); } }; 611. Valid Triangle Number \u00b6 The main idea comes from the triangle lateral property, in which the triple should fullfil: a + b > c , a + c > b , and b + c > a . Once we sort it. We are able to gain some advantages that we don't have to check all these 3 relations. Instead, we should only take care of A[i] + A[j] > A[k] , in which i < j < k . Because we sorted the array, we can also fix the i and j , using binary search to find the k in the ragne of A[j + 1] ~ A[n - 1] . We can use our classic binary search template to achieve the goal. class Solution { public : int triangleNumber ( vector < int >& nums ) { int n = nums . size (); int res = 0 ; sort ( nums . begin (), nums . end ()); for ( int i = 0 ; i < n - 2 ; ++ i ) { for ( int j = i + 1 ; j < n - 1 ; ++ j ) { int l = j + 1 , r = n ; // range of all possible k, notice l start with j + 1 int t = nums [ i ] + nums [ j ]; while ( l < r ) { int m = l + ( r - l ) / 2 ; if ( nums [ m ] < t ) { l = m + 1 ; } else { r = m ; } } res += l - j - 1 ; // notice the count start from j + 1 to l - 1. } } return res ; } }; Category 3 Using ordering abstration (monotonicity) \u00b6 287. Find the Duplicate Number \u00b6 Solution 1 Binary search The problem asking for better than O(n^2) we could check to see whether binary search will work. If you count how many values <= the mid elements of [1, ..., n-1] , it will give you enough information to discard part of the array. Here you should distinguish what will be split and what will be searched. The answer is the [1, ..., n-1] sequence, not the given array. The simple proof of why it works can be put in this the following way. If the count of elements that <=mid in the array is less than mid , we can learn that the duplicate is in the higher end. If the count is greater, we can know that the duplicate element is in the lower end of the sequence [1, ..., n-1] . class Solution { public : int findDuplicate ( vector < int >& nums ) { int begin = 1 , end = nums . size () - 1 ; while ( begin < end ) { int mid = begin + ( end - begin ) / 2 ; int count = 0 ; for ( int a : nums ) { if ( a <= mid ) ++ count ; } if ( count <= mid ) // \"=\" for [1,2,2] begin = mid + 1 ; else end = mid ; } return begin ; } }; Solution 2 tortoise and hare algorithm This problem is very similar to the the find circle in linked list. Generally, if you repeate A[A[i]] , the out put will show some periodic patterns. In fact you can imagine a rho shaped sequence. Image there is a function f(i) = A[i] , it mapping from 1, 2, 3, ... n to 1, 2, 3, ... n . Try to traverse A[i] , you will finally get circle through some same sequence of elements again and again, thus you obtain a rho shaped sequency like a circle in a linked list. The reason of it being a rho shape is becuase at least one element you will not come back to it if you leave it. Find Duplicate class Solution { public : int findDuplicate ( vector < int >& nums ) { int n = nums . size (); if ( n == 0 ) return 0 ; int slow = 0 , fast = 0 , find = 0 ; while ( slow != fast || ( slow == 0 && fast == 0 )) { slow = nums [ slow ]; fast = nums [ nums [ fast ]]; } while ( slow != find ) { slow = nums [ slow ]; find = nums [ find ]; } return find ; } }; 360. Sort Transformed Array \u00b6 374. Guess Number Higher or Lower \u00b6 C++ binary search // Forward declaration of guess API. // @param num, your guess // @return -1 if my number is lower, 1 if my number is higher, otherwise return 0 int guess ( int num ); class Solution { public : int guessNumber ( int n ) { int start = 1 , end = n ; while ( start < end ) { int mid = start + ( end - start ) / 2 ; if ( guess ( mid ) == 0 ) return mid ; if ( guess ( mid ) == 1 ) { start = mid + 1 ; } else { end = mid ; } } return start ; } }; Python # The guess API is already defined for you. # @param num, your guess # @return -1 if my number is lower, 1 if my number is higher, otherwise return 0 # def guess(num): class Solution ( object ): def guessNumber ( self , n ): \"\"\" :type n: int :rtype: int \"\"\" begin = 0 end = n while begin != end : mid = begin + ( end - begin ) / 2 if guess ( mid ) == 0 : return mid if guess ( mid ) == 1 : begin = mid + 1 else : end = mid return begin 475. Heaters \u00b6 Sort then brute force The solution we are looking for is the max value of the smallest house-heater distance. Think through what is the distance you want to keep, min or max C++ class Solution { public : int findRadius ( vector < int >& houses , vector < int >& heaters ) { int m = houses . size (); int n = heaters . size (); sort ( houses . begin (), houses . end ()); sort ( heaters . begin (), heaters . end ()); int res = INT_MIN ; int i , j = 0 ; for ( i = 0 ; i < m ; ++ i ) { while ( j < n - 1 && abs ( heaters [ j + 1 ] - houses [ i ]) <= abs ( heaters [ j ] - houses [ i ])) { j ++ ; } res = max ( res , abs ( houses [ i ] - heaters [ j ])); } return res ; } }; Python class Solution ( object ): def findRadius ( self , houses , heaters ): \"\"\" :type houses: List[int] :type heaters: List[int] :rtype: int \"\"\" m = len ( houses ) n = len ( heaters ) houses . sort () heaters . sort () i = 0 j = 0 res = 0 for i in range ( m ): while j < n - 1 and abs ( heaters [ j + 1 ] - houses [ i ]) <= abs ( heaters [ j ] - houses [ i ]): j += 1 res = max ( res , abs ( houses [ i ] - heaters [ j ])) return res Binary search the neighboring heaters get max of min Notice we cannot sort hourses and then search each heater's position. A special cases [1, 2, 3] 2 , the result is 0 whereis it should be 1 . C++ class Solution { public : int findRadius ( vector < int >& houses , vector < int >& heaters ) { int n = heaters . size (); sort ( heaters . begin (), heaters . end ()); int res = INT_MIN ; for ( int house : houses ) { int start = 0 , end = n ; while ( start < end ) { int mid = start + ( end - start ) / 2 ; if ( heaters [ mid ] < house ) start = mid + 1 ; else end = mid ; } int dist1 = ( start == n ) ? INT_MAX : heaters [ start ] - house ; int dist2 = ( start == 0 ) ? INT_MAX : house - heaters [ start - 1 ]; res = max ( res , min ( dist1 , dist2 )); } return res ; } }; Python class Solution ( object ): def findRadius ( self , houses , heaters ): \"\"\" :type houses: List[int] :type heaters: List[int] :rtype: int \"\"\" m = len ( houses ) n = len ( heaters ) heaters . sort () i = 0 j = 0 res = float ( '-inf' ) for i in range ( m ): start = 0 end = n while start != end : mid = start + ( end - start ) / 2 if heaters [ mid ] < houses [ i ]: start = mid + 1 else : end = mid dist1 = float ( 'inf' ) dist2 = float ( 'inf' ) if start != n : dist1 = heaters [ start ] - houses [ i ] if start != 0 : dist2 = houses [ i ] - heaters [ start - 1 ] res = max ( res , min ( dist1 , dist2 )) return res 410. Split Array Largest Sum \u00b6 1011. Capacity To Ship Packages Within D Days \u00b6 Binary solution Same as the 410. Split Array Largest Sum class Solution { public : int shipWithinDays ( vector < int >& weights , int D ) { int n = weights . size (); if ( n < D ) return 0 ; int l = * max_element ( weights . begin (), weights . end ()); int h = accumulate ( weights . begin (), weights . end (), 0 ); while ( l < h ) { int m = ( l + h ) / 2 ; int c = 1 ; // need cut D-1 times int sum = 0 ; for ( int w : weights ) { if ( sum + w > m ) { sum = 0 ; c ++ ; } sum += w ; } if ( c > D ) { l = m + 1 ; } else { h = m ; } } return l ; } }; 875. Koko Eating Bananas \u00b6 Binary search Using the monotonic guessing approach, notice the trick in counting whether the given guess value is possible. C++ binary search class Solution { public : int minEatingSpeed ( vector < int >& piles , int H ) { int N = piles . size (); if ( N > H ) return 0 ; int l = 1 ; long r = 10e9 ; while ( l < r ) { int k = l + ( r - l ) / 2 ; int hour = 0 ; for ( int p : piles ) { if ( k >= p ) { hour ++ ; } else { hour += ( p + k - 1 ) / k ; } } if ( hour > H ) { // K is too large, l = k + 1 ; } else { r = k ; } } return l ; } }; C++ binary search simplified class Solution { public : int minEatingSpeed ( vector < int >& piles , int H ) { int N = piles . size (); if ( N > H ) return 0 ; int l = 1 ; long r = 10e9 ; while ( l < r ) { int k = l + ( r - l ) / 2 ; int hour = 0 ; for ( int p : piles ) { hour += ( p + k - 1 ) / k ; } if ( hour > H ) { // K is too large, l = k + 1 ; } else { r = k ; } } return l ; } }; 1539. Kth Missing Positive Number \u00b6 Naive Solution using multiple variables and keep loop invariant. Binary Search Observe the relation: total missing positives before A[m] is A[m] - 1 - m because the index m and A[m] is related to the missing positives thus can be used for counting. the bisection condition can be interpreted as a boolean predicate: \"whether the number of missing positives before A[m] is no less than k ?\" Naive Solution class Solution { public : int findKthPositive ( vector < int >& arr , int k ) { if ( arr . empty ()) return k ; int missing_cnt = arr [ 0 ] - 1 ; if ( missing_cnt >= k ) return k ; int prev = arr [ 0 ]; for ( int i = 1 ; i < arr . size (); ++ i ) { if ( ! ( arr [ i ] == prev || arr [ i ] == prev + 1 )) { int skip = arr [ i ] - prev - 1 ; if ( missing_cnt + skip >= k ) { return prev + k - missing_cnt ; } missing_cnt += skip ; } prev = arr [ i ]; } return ( prev + k - missing_cnt ); } }; Binary Search class Solution { public : int findKthPositive ( vector < int >& arr , int k ) { int l = 0 , r = arr . size (); while ( l < r ) { int m = l + ( r - l ) / 2 ; if ( arr [ m ] - 1 - m < k ) { l = m + 1 ; } else { r = m ; } } return l + k ; } }; 1482. Minimum Number of Days to Make m Bouquets \u00b6 Solution Binary Search Use a subroutine to compute whether the constrain can be meet or not. The search is looking for the whether m bouquets is possible, meet the binary pattern \"no less than\". So that we use if(cnt_m < m) and the return values is l. C++ binary search class Solution { public : int minDays ( vector < int >& bloomDay , int m , int k ) { int l = * min_element ( bloomDay . begin (), bloomDay . end ()); int r = * max_element ( bloomDay . begin (), bloomDay . end ()); if ( bloomDay . size () < m * k ) return -1 ; while ( l < r ) { int mid = l + ( r - l ) / 2 ; int cnt_k = 0 ; int cnt_m = 0 ; for ( int d : bloomDay ) { if ( d > mid ) { cnt_k = 0 ; } else { cnt_k ++ ; if ( cnt_k == k ) { cnt_m ++ ; cnt_k = 0 ; } } } if ( cnt_m < m ) { l = mid + 1 ; } else { r = mid ; } } return l ; } }; 1283. Find the Smallest Divisor Given a Threshold \u00b6 Solution Binary search Notice the specific divisor calculation. under this divisor operation, no matter how large the divisor is, the sum is always greater than nums.size() , if not, the solution is not guaranteed. so the threshold cannot smaller than nums.size() . This also indicate that the minimum divisor is less than or eaual to max(nums) . in the bsection predicate, notice this time the condition becomes if (res > target) essentially, the if (f(mid) < target) in the binary search templates is saying mid and f(mid) are positive correlation. here the mid and res are negative correlation. C++ binary search class Solution { public : int smallestDivisor ( vector < int >& nums , int threshold ) { int l = 1 ; int r = * max_element ( nums . begin (), nums . end ()); while ( l < r ) { int m = l + ( r - l ) / 2 ; int res = 0 ; for ( int num : nums ) { res += ( num + m - 1 ) / m ; } if ( res > threshold ) { l = m + 1 ; } else { r = m ; } } return l ; } }; 1231. Divide Chocolate \u00b6 Solution Binary search The key difference between this problem and 410. Split Array Largest Sum is this problem asks for maximizing the smallest sweetness of the pieces, whereas the 410. Split Array Largest Sum asks minimizing the largest piece. Support k cuts generate m outcomes S = \\{s_1^{|k|}, s_2^{|k|}, \\cdots, s_m^{|k|}\\} S = \\{s_1^{|k|}, s_2^{|k|}, \\cdots, s_m^{|k|}\\} , this problem is to find the value of \\operatorname*{argmax}_m (\\operatorname*{argmax}_k (S)) \\operatorname*{argmax}_m (\\operatorname*{argmax}_k (S)) . Imagine you guessed a value m , which is the maximum sweetness you can get from the smallest sweetness piece of all cuts. How to test whether the value m is possible? If possible, we will increase it to maximize it; if not, we will still keep it a candidate. Same problem as 183. Wood cut . Binary Search class Solution { public : int maximizeSweetness ( vector < int >& sweetness , int K ) { int start = * min_element ( sweetness . begin (), sweetness . end ()); int end = accumulate ( sweetness . begin (), sweetness . end (), 0 ); while ( start < end ) { int mid = ( start + end + 1 ) / 2 ; int sum = 0 ; int cuts = 0 ; for ( int s : sweetness ) { if (( sum += s ) >= mid ) { sum = 0 ; if ( ++ cuts > K ) break ; } } if ( cuts > K ) { // because >= mid above guarentee the \"no less than\" the guess. // if cuts > K, mid could be the right answer and should be returned. // Remember the binary search invariance requies not miss any // answer in each iteration. start = mid ; } else { end = mid - 1 ; } } return start ; } }; Note Compare the binary search solution of problem of 1231. Divide Chocolate and 410. Split Array Largest Sum . Notice how different in checking the number of cuts. It exceeds the limit K , for max and min case, it indicate a very trivial difference in meaning. Copy books (lintcode) \u00b6 183. Wood cut (lintcode) \u00b6 Description Given n pieces of wood with length L[i] (integer array). Cut them into small pieces to guarantee you could have equal or more than k pieces with the same length. What is the longest length you can get from the n pieces of wood? Given L & k, return the maximum length of the small pieces. You couldn't cut wood into float length. If you couldn't get >= k pieces, return 0. Solution 1 Binary search It requires getting equal or more than k pieces of wood with the same length. So you have to cut the wood to fulfill the requirement. However, you need to promise that each of the k wood is the longest that is possible. Imagine that you are given a bunch of wood to cut. How would you do it? You probably want to try to make one cut and see whether you can make it or not. If not, you may want to make two cuts, and so on. But how could you program such a solution? It is very hard. Start thinking about the length seems a good option. Suppose you know your final maximum length. You would be able to make the cut accordingly. Now given a length out of guessing, can you verify whether it going to work or not? Yes, you can! That's the core idea of this solution. class Solution { public : int woodCut ( vector < int > & L , int k ) { if ( L . empty ()) return 0 ; int maxlen = * max_element ( L . begin (), L . end ()); if ( k == 0 ) return maxlen ; int start = max ( 1 , maxlen / k ), end = maxlen ; while ( start < end ) { int mid = start + ( end - start ) / 2 ; int count = 0 ; for ( int len : L ) { count += len / ( mid + 1 ); } if ( count >= k ) start = mid + 1 ; else end = mid ; } int count = 0 ; for ( int len : L ) count += len / start ; return count >= k ? start : 0 ; } }; 774. Minimize Max Distance to Gas Station \u00b6 Solution 1 Binary search It is very similar to the problem Wood cut . You just need to take care of the accuracy of the results, namely also the int/double casts. It is also the hard part of the problem. Notice the count variable is int type, you should test your solution expecially for the line count += dist[i] / mid ; class Solution { public : double minmaxGasDist ( vector < int >& stations , int K ) { int n = stations . size (); vector < int > dist ( n , 0 ); // dist[0] = null; int d = 0 ; for ( int i = 1 ; i < n ; ++ i ) { dist [ i ] = stations [ i ] - stations [ i - 1 ]; d = max ( d , dist [ i ]); } double low = 0 , high = d ; while ( low + 0.000001 < high ) { double mid = low + ( high - low ) / 2 ; int count = 0 ; for ( int i = 1 ; i < n ; ++ i ) { count += dist [ i ] / mid ; } if ( count > K ) { // mid is too small low = mid ; } else { high = mid ; } } return low ; } }; 644. Maximum Average Subarray II \u00b6 First try to understand why the constrain is \"greater than or equal than k\". You find that this constrain will ensure the solution exists and the problem is interesting. Notice the monotonicity of the \"average sum\" values, namely for a given value, if it doesn't fulfill the constrain (length >= k & max(avg)), you can eliminate half of the values from the solution space. Your binary search predicate will be to test whether there exists a subarray with length greater than k and an average value is larger than the mid . We use a trick to verify the constrains. The tricky thing is that the two constrains are not checked separately, they need to be work together in order to achieve better complexity. The length constrain is ensured partly by the \"skip\" indexing ( i - k ), partly by keeping the smallest average before the current considered subarray. Key Math Insight \\begin{align*} \\mu_k = \\frac{a_i + a_{i+1} + \\cdots, + a_j}{j-i+1} & >= Mid \\\\ a_i + a_{i+1} + \\cdots, + a_j & >= Mid \\times (j-i+1) \\\\ (a_i - Mid) + (a_{i+1} - Mid) + \\cdots, + (a_j - Mid) & >= 0 \\end{align*} \\begin{align*} \\mu_k = \\frac{a_i + a_{i+1} + \\cdots, + a_j}{j-i+1} & >= Mid \\\\ a_i + a_{i+1} + \\cdots, + a_j & >= Mid \\times (j-i+1) \\\\ (a_i - Mid) + (a_{i+1} - Mid) + \\cdots, + (a_j - Mid) & >= 0 \\end{align*} class Solution { public : double findMaxAverage ( vector < int >& nums , int k ) { double start = * min_element ( nums . begin (), nums . end ()); double end = * max_element ( nums . begin (), nums . end ()); while ( lower + 0.00001 < upper ) { double mid = lower + ( upper - lower ) / 2 ; if ( isLarger ( nums , mid , k )) { // is average value >= mid? lower = mid ; } else { upper = mid ; } } return lower ; } /* return true if a greater average value is possible */ bool isLarger ( vector < int >& nums , double mid , int k ) { int n = nums . size (); double sums = 0 , prev = 0 , prev_min = 0 ; for ( int i = 0 ; i < k ; i ++ ) { sums += nums [ i ] - mid ; } // we keep looking for whether a subarray sum of length >= k in array // \"sums\" is possible to be greater than zero. If such a subarray exist, // it means that the target average value is greater than the \"mid\" value. if ( sums >= 0 ) { return true ; } // we look at the front part of sums that at least k element apart from i. // If we can find the minimum of the sums[0], sums[1], ..., sums[i - k] // and check if sums[i] - min(sums[0], sums[1], ..., sums[i - k]) >= 0. // If this is the case, it indicate, there exist a subarray of length >= k // with sum greater than 0 in sums. we can return ture. for ( int i = k ; i < n ; i ++ ) { sums += nums [ i ] - mid ; prev += nums [ i - k ] - mid ; prev_min = min ( prev_min , prev ); if ( sums >= prev_min ) return true ; } return false ; } }; 778. Swim in Rising Water \u00b6 In This problem we are trying to find a path, in which the maximum element in the path among all paths is minimum. Meaning we look for a target value in the grid, such that there exist a path from grid[0][0] to grid[n-1][n-1] which includes this value and it is the maximum value in the path. class Solution { int x [ 4 ] = { 0 , -1 , 0 , 1 }; int y [ 4 ] = { -1 , 0 , 1 , 0 }; public : int swimInWater ( vector < vector < int >>& grid ) { int n = grid . size (); int begin = grid [ 0 ][ 0 ], end = n * n - 1 ; // binary search find a path with mini elevation while ( begin < end ) { int mid = begin + ( end - begin ) / 2 ; if ( pathExist ( grid , mid )) { end = mid ; } else { begin = mid + 1 ; } } return begin ; } bool pathExist ( vector < vector < int >> & grid , int mid ) { int n = grid . size (); vector < vector < int >> visited ( n , vector < int > ( n , 0 )); return dfs_helper ( grid , visited , n , mid , 0 , 0 ); } bool dfs_helper ( vector < vector < int >> & grid , vector < vector < int >>& visited , int n , int mid , int i , int j ) { visited [ i ][ j ] = 1 ; for ( int k = 0 ; k < 4 ; ++ k ) { int a = i + x [ k ]; int b = j + y [ k ]; if ( a < 0 || a >= n || b < 0 || b >= n || visited [ a ][ b ] == 1 || grid [ a ][ b ] > mid ) continue ; if ( a == n - 1 && b == n - 1 ) return true ; if ( dfs_helper ( grid , visited , n , mid , a , b )) return true ; } return false ; } }; 483 Smallest Good Base \u00b6 Solution 1 Binary search This problem requires a bit reasoning to achieve the solution. The starting point is realy mean what's asking by the problem. Here it is asking a minimum base that represent the given number n in a representation like binary representation. For example: 13 = 3^0 + 3^1 + 3^2 so 13 can be representd as 111 (base 3). First of all, there is a special case that such a base may not exist. (precisely, we should seperate the special case when n = (n-1)^0 + (n-1)^1 ; With this special case in mind, we can use binary search to iterate through each m from largest to smallest and check whether the corresponding k is a good base of the given value n . Because when m is the largest, k is the smallest, so if the bianry search find one it must be the smallest k we are looking for. If binary search found nothing, we simpley return the special case n-1 . class Solution { public : string smallestGoodBase ( string n ) { long long num = stoll ( n ); /* for each lenght of the potentional representation, * n = 1 + k + ... + k^{i-1} = (k^i-1)/(k-1), lower bound k is 2, * we have 2^i-1 = n ==> upper bound i = log2(n+1). */ for ( int i = log2 ( num + 1 ); i >= 2 ; -- i ) { /* upper bound is obtained by n = 1 + k + k^2 ... + k^(i-1) > k^(i-1), * n > k^(i-1) ==> k < n^(1/(i-1)); */ long long left = 2 , right = pow ( num , 1.0 / ( i - 1 )) + 1 ; while ( left < right ) { long long mid = left + ( right - left ) / 2 ; long long sum = 0 ; /* calculate i digits value with base \"mid\" */ for ( int j = 0 ; j < i ; ++ j ) { sum = sum * mid + 1 ; } /* binary search for the mid (good base) */ if ( sum == num ) return to_string ( mid ); if ( sum < num ) left = mid + 1 ; else right = mid ; } } return to_string ( num - 1 ); } }; 658. Find K Closest Elements \u00b6 373. Find K Pairs with Smallest Sums \u00b6 378. Kth Smallest Element in a Sorted Matrix \u00b6 Solution 1 Binary Search The idea of using binary search for this problem my not be straightforward. But the method is very important. The idea is very similar to the problem Search in a rotated sorted array. Because the matrix is sorted row wise and column wise, there are some ordering information we can make use of. Notice we are not try to search using the matrix index, we are searching the matrix element value. Compare to the problem 287. Find the Duplicate Number. The comparison if (count < k) isn't include mid explicitly. but the count is some function f(mid) , with the current mid , the count value is unique and can be use to test a condition that decide which side we can go to shrink the range the target value is possible in. class Solution { public : int kthSmallest ( vector < vector < int >>& matrix , int k ) { int m = matrix . size (); int n = m ? matrix [ 0 ]. size () : 0 ; int start = matrix [ 0 ][ 0 ], end = matrix [ m - 1 ][ n - 1 ]; while ( start < end ) { int mid = start + ( end - start ) / 2 ; int count = 0 ; for ( int i = 0 ; i < m ; ++ i ) { count += upper_bound ( matrix [ i ]. begin (), matrix [ i ]. end (), mid ) - matrix [ i ]. begin (); } if ( count < k ) { // notice no mid here, but count is a function of mid. start = mid + 1 ; } else { end = mid ; } } return start ; } }; Solution 2 Priority Queue Notice when the k <= n^2 , index j < matrix.size() will also make it work. class Solution { public : int kthSmallest ( vector < vector < int >>& matrix , int k ) { priority_queue < int > pq ; for ( int i = 0 ; i < matrix . size (); ++ i ) { for ( int j = 0 ; j < matrix [ 0 ]. size (); ++ j ) { pq . push ( matrix [ i ][ j ]); if ( pq . size () > k ) pq . pop (); } } return pq . top (); } }; 668. Kth Smallest Number in Multiplication Table \u00b6 Solution 1 Binary search While this problem looks simple. But it really isn't unless you observed the following. The condition used for binary search is \"whether there are k smaller elements in the range [start, mid]\". You are looking for the smallest number that has k elements less than or equal to it. Like in the problem Kth Smallest Element in a Sorted Matrix, we will move the number not the index. We move the start or end appropriately based on this condition, if there are more than k, we shrink the range by reduce end: end = mid . If there are less than k numbers, we increase begin hopefully to make mid larger so as to have close to k numbers in the range of [1, mid] . When being == end , we've located the kth number desired. In case k > m*n , we will got begin == end < k , which is not a solution. In counting how many elements less than mid, you have to be clever a bit by using the feature that this matrix is a multiplicative table. That is for row i , you can at most have x/i number smaller than x , why? Follow up: Does the kth element will be in the range of [1, m*n] ? class Solution { public : int findKthNumber ( int m , int n , int k ) { int begin = 1 , end = m * n ; while ( begin < end ) { int mid = begin + ( end - begin ) / 2 ; int count = 0 ; for ( int i = 1 ; i <= m ; ++ i ) { count += min ( mid / i , n ); } if ( count < k ) begin = mid + 1 ; else end = mid ; } return begin ; } }; 719. Find K-th Smallest Pair Distance \u00b6 Solution 1 Priority Queue TLE class Solution { public : int smallestDistancePair ( vector < int >& nums , int k ) { priority_queue < int > pq ; for ( int i = 0 ; i < nums . size (); ++ i ) { for ( int j = i + 1 ; j < nums . size (); ++ j ) { int dist = abs ( nums [ i ] - nums [ j ]); if ( pq . size () < k ) { pq . push ( dist ); } else if ( dist < pq . top ()) { pq . push ( dist ), pq . pop (); } } } return pq . top (); } }; Solution 2 Binary search Similar to Problem 668. Kth Smallest Number in Multiplication Table . The problem is complicated at the first glance. A brute force solution generates all the absolute distances and then sort to find the kth smallest one. We found it is potentially a searchable scenario if we sort the elements. We have range [min_distance, max_distance] . We search a distance in this range such that there are exactly k pairs distance including itself. If the count of pair distance less than k, we try to increase it buy start = mid + 1 , vice versa. When the binary search loop stops, if the result exist, start point to the distance we are searching. Since this problem guarrantee solution exist, we return start . class Solution { public : int smallestDistancePair ( vector < int >& nums , int k ) { sort ( nums . begin (), nums . end ()); int start = nums [ 1 ] - nums [ 0 ]; for ( int i = 2 ; i < nums . size (); ++ i ) { start = min ( start , nums [ i ] - nums [ i - 1 ]); } int end = nums . back () - nums [ 0 ]; while ( start < end ) { int mid = start + ( end - start ) / 2 ; // count how many absolute differences that <= mid; int count = 0 ; for ( int i = 0 ; i < nums . size (); ++ i ) { int j = i ; while ( j < nums . size () && nums [ j ] - nums [ i ] <= mid ) j ++ ; count += j - i - 1 ; } if ( count < k ) { start = mid + 1 ; } else { end = mid ; } } return start ; } }; Solution 3 Using binary search to optimize the counting You can also write your own binary search routine upper_bound . class Solution { public : int smallestDistancePair ( vector < int >& nums , int k ) { sort ( nums . begin (), nums . end ()); int start = nums [ 1 ] - nums [ 0 ]; for ( int i = 2 ; i < nums . size (); ++ i ) { start = min ( start , nums [ i ] - nums [ i - 1 ]); } int end = nums . back () - nums [ 0 ]; while ( start < end ) { int mid = start + ( end - start ) / 2 ; // count how many absolute differences that <= mid; int count = 0 ; /* for (int i = 0; i < nums.size(); ++i) { int j = i; while (j < nums.size() && nums[j] - nums[i] <= mid) j++; count += j - i - 1; } */ // optimize the counting use binary search (nested binary search) for ( int i = 0 ; i < nums . size (); ++ i ) { auto iter = upper_bound ( nums . begin () + i , nums . end (), nums [ i ] + mid ); count += iter - ( nums . begin () + i ) - 1 ; } if ( count < k ) { start = mid + 1 ; } else { end = mid ; } } return start ; } }; 786. K-th Smallest Prime Fraction \u00b6 You should find seek to find the monotonic pattern of the fraction and think how to search it effectively. To use binary search you need to draw an imaginary matrix to consider how can you search effectively. Binary search O(nlog(max_frac)) class Solution { public : vector < int > kthSmallestPrimeFraction ( vector < int >& A , int K ) { int n = A . size (); double l = 0 , r = 1.0 ; while ( l < r ) { double m = ( l + r ) / 2 ; // calculate how many smaller on the right int cnt = 0 ; double mx = 0 ; int p , q ; int j = 1 ; for ( int i = 0 ; i < n - 1 ; ++ i ) { while ( j < n && A [ i ] > A [ j ] * m ) ++ j ; // int j = upper_bound(A.begin() + i, A.end(), A[i] / m) - A.begin(); cnt += ( n - j ); if ( n == j ) break ; double fraction = ( double ) A [ i ] / ( double ) A [ j ]; if ( fraction > mx ) { p = A [ i ]; q = A [ j ]; mx = fraction ; } } if ( cnt == K ) { return { p , q }; } if ( cnt > K ) { r = m ; } else if ( cnt < K ) { l = m ; } } return {}; } }; 1631. Path With Minimum Effort \u00b6 Solution 1 Binary search + BFS Because we are searching for the smallest effort of all paths. If the proposed solution is not possible, namely, all paths have effort greater than the proposed solution (the proposed value is too small). We need to increase the start in binary search. We are looking for the no-less-than x in the binary search. Solution 2 Dijkstra If you can change the problem into searching a weighted graph with edge weights, which are the absolute differences (effort). Since the weights are all positives, using Dijkstra algorithm can find the shortest path in the measure of effort. C++ Binary search + BFS class Solution { vector < int > dx = { 0 , 1 , 0 , -1 }; vector < int > dy = { -1 , 0 , 1 , 0 }; public : int minimumEffortPath ( vector < vector < int >>& heights ) { int m = heights . size (); int n = m == 0 ? 0 : heights [ 0 ]. size (); int start = 0 , end = 10e6 ; while ( start < end ) { int mid = ( start + end ) / 2 ; if ( ! pathPossible ( heights , mid )) { start = mid + 1 ; } else { end = mid ; } } return start ; } bool pathPossible ( vector < vector < int >>& heights , int val ) { int m = heights . size (); int n = m == 0 ? 0 : heights [ 0 ]. size (); queue < vector < int >> q ; q . push ({ 0 , 0 }); set < int > visited ; visited . insert ( 0 ); while ( ! q . empty ()) { vector < int > t = q . front (); int x = t [ 0 ]; int y = t [ 1 ]; q . pop (); if ( x == m - 1 && y == n - 1 ) return true ; for ( int k = 0 ; k < 4 ; k ++ ) { int a = x + dx [ k ]; int b = y + dy [ k ]; if ( a < 0 || a >= m || b < 0 || b >= n ) continue ; if ( val < abs ( heights [ a ][ b ] - heights [ x ][ y ])) continue ; if ( visited . count ( a * n + b ) > 0 ) continue ; q . push ({ a , b }); visited . insert ( a * n + b ); } } return false ; } }; Java Binary search + BFS class Solution { private int [] d = { 0 , 1 , 0 , - 1 , 0 }; public int minimumEffortPath ( int [][] heights ) { int lo = 0 , hi = 1_000_000 ; while ( lo < hi ) { int effort = lo + ( hi - lo ) / 2 ; if ( isPath ( heights , effort )) { hi = effort ; } else { lo = effort + 1 ; } } return lo ; } private boolean isPath ( int [][] h , int effort ) { int m = h . length , n = h [ 0 ] . length ; Queue < int []> q = new LinkedList <> (); q . offer ( new int [ 2 ] ); Set < Integer > seen = new HashSet <> (); seen . add ( 0 ); while ( ! q . isEmpty ()) { int [] cur = q . poll (); int x = cur [ 0 ] , y = cur [ 1 ] ; if ( x == m - 1 && y == n - 1 ) { return true ; } for ( int k = 0 ; k < 4 ; ++ k ) { int r = x + d [ k ] , c = y + d [ k + 1 ] ; if ( 0 <= r && r < m && 0 <= c && c < n && effort >= Math . abs ( h [ r ][ c ] - h [ x ][ y ] ) && seen . add ( r * n + c )) { q . offer ( new int [] { r , c }); } } } return false ; } } C++ Dijkstra class Solution { public : int minimumEffortPath ( vector < vector < int >>& heights ) { int m = heights . size (); int n = heights [ 0 ]. size (); vector < vector < int >> dist ( m , vector < int > ( n , INT_MAX )); // min distance found so far. priority_queue < pair < int , int > , vector < pair < int , int >> , greater < pair < int , int >>> pq ; int d [ 5 ] = { 0 , 1 , 0 , -1 , 0 }; pq . push ({ 0 , 0 }); // first: min effort, second: encoded (x, y) (=x * n + y); while ( ! pq . empty ()) { pair < int , int > t = pq . top (), pq . pop (); int effort = t . first ; int x = t . second / n ; int y = t . second % n ; if ( x == m - 1 && y == n - 1 ) return effort ; for ( int k = 0 ; k < 4 ; ++ k ) { int a = x + d [ k ]; int b = y + d [ k + 1 if ( a < 0 || a >= m || b < 0 || b >= n ) continue ; // update neighboring node, effort=min effort before visit node(a,b) int currEffort = max ( effort , abs ( heights [ a ][ b ] - heights [ x ][ y ])); if ( dist [ a ][ b ] > currEffort ) { dist [ a ][ b ] = currEffort ; pq . push ({ currEffort , a * n + b }); } } } return -1 ; } }; Python Dijkastra class Solution : def minimumEffortPath ( self , heights : List [ List [ int ]]) -> int : m , n = map ( len , [ heights , heights [ 0 ]]) efforts = [[ math . inf ] * n for _ in range ( m )] efforts [ 0 ][ 0 ] = 0 heap = [( 0 , 0 , 0 )] while heap : effort , x , y = heapq . heappop ( heap ); if ( x , y ) == ( m - 1 , n - 1 ) : return effort for i , j in ( x , y - 1 ), ( x , y + 1 ), ( x - 1 , y ), ( x + 1 , y ) : if i < 0 or i >= m or j < 0 or j >= n : continue currEffort = max ( effort , abs ( heights [ x ][ y ] - heights [ i ][ j ])) if efforts [ i ][ j ] > currEffort : efforts [ i ][ j ] = currEffort heapq . heappush ( heap , ( currEffort , i , j )) 1102. Path With Maximum Minimum Value \u00b6 Solution 1 Binary Search + BFS Again, propose a possible value and use a isValid function to check the validity of the proposed solution. Solution 2 BFS + PQ This solution can be thought as a variant of Dijkastra, but not the same. Solution 3 Union Find We need to sort all the vertices by their values in descending order, then choose element from the vertices and use Union-Find to check connectivity of A[0][0] and A[m - 1][n - 1] . C++ Binary Search + BFS class Solution { public : int maximumMinimumPath ( vector < vector < int >>& A ) { int m = A . size (); int n = A [ 0 ]. size (); int start = 0 , end = max ( A [ 0 ][ 0 ], A [ m - 1 ][ n - 1 ]); int res = 0 , mid = 0 ; while ( start < end ) { mid = start + ( end - start ) / 2 ; if ( pathPossible ( A , mid )) { start = mid + 1 ; } else { end = mid ; } } return start ; } bool pathPossible ( vector < vector < int >>& A , int mid ) { int m = A . size (); int n = A [ 0 ]. size (); queue < pair < int , int >> q ; q . emplace ( 0 , 0 ); vector < vector < int >> v ( m , vector < int > ( n , 0 )); v [ 0 ][ 0 ] = 1 ; int d [ 5 ] = { 0 , 1 , 0 , -1 , 0 }; while ( ! q . empty ()) { int x = q . front (). first ; int y = q . front (). second ; q . pop (); if ( x == m - 1 && y == n - 1 ) return true ; for ( int k = 0 ; k < 4 ; ++ k ) { int a = x + d [ k ]; int b = y + d [ k + 1 ]; if ( a < 0 || a >= m || b < 0 || b >= n || v [ a ][ b ] == 1 ) continue ; if ( mid > A [ a ][ b ]) continue ; q . emplace ( a , b ); v [ a ][ b ] = 1 ; } } return false ; } }; C++ BFS + PQ class Solution { public : int maximumMinimumPath ( vector < vector < int >>& A ) { int m = A . size (); int n = A [ 0 ]. size (); int res = INT_MAX ; priority_queue < pair < int , int > , vector < pair < int , int >>> pq ; // max heap. pq . emplace ( A [ 0 ][ 0 ], 0 ); vector < vector < int >> visited ( m , vector < int > ( n , 0 )); visited [ 0 ][ 0 ] = -1 ; int d [ 5 ] = { 0 , 1 , 0 , -1 , 0 }; while ( ! pq . empty ()) { pair < int , int > t = pq . top (); pq . pop (); int cost = t . first ; int x = t . second / n ; int y = t . second % n ; res = min ( res , cost ); if ( x == m - 1 && y == n - 1 ) break ; for ( int k = 0 ; k < 4 ; k ++ ) { int r = x + d [ k ]; int c = y + d [ k + 1 ]; if ( r < 0 || r >= m || c < 0 || c >= n || visited [ r ][ c ] < 0 ) continue ; pq . emplace ( A [ r ][ c ], r * n + c ); visited [ r ][ c ] = -1 ; } } return res ; } }; C++ Union-Find Category 4 Binary search as an optimization routine \u00b6 300 Longest Increasing Subsequence \u00b6 Solution 1 DP The base case is single char. f[i] is the length of LIS from the begining. class Solution { public : int lengthOfLIS ( vector < int >& nums ) { if ( n == nums . size ()) return 0 ; int f [ n ] = { 0 }; int res = 0 ; for ( int j = 0 ; j < n ; j ++ ) { f [ j ] = 1 ; for ( int i = 0 ; i < j ; i ++ ) { if ( nums [ i ] < nums [ j ] && f [ i ] + 1 > f [ j ]) f [ j ] = f [ i ] + 1 ; } res = max ( res , f [ j ]); } return res ; } }; Solution 2 Using binary search The DP solution is O(n^2) . Using binary search could reduce to O(nlogn) . Binary search solution analysis. For each i , we are looking for the largest f value that has smallest A value. For example, A[0] = 5 could be ignored because of its f value is same as A[1] = 1 , which is smaller. In searching for the LIS, we prefer a small ending value when the length is the same. The following solution using a vector b to record the minimum A value for each length of LIS ( f value), we use binary search to find the last value in b that smaller that current value A[i] . If we found such a value in b , we use A[i] to replace the value next to the found value in b ). i 0 1 2 3 4 5 6 7 A 5 1 3 7 6 4 2 10 f 1 1 2 3 3 3 2 4 f[1] = 1, a[1] = 1 f[6] = 2, a[6] = 2 f[5] = 3, a[5] = 4 f[7] = 4, a[7] = 10 class Solution { public : int lengthOfLIS ( vector < int >& nums ) { vector < int > b ; for ( int i = 0 ; i < nums . size (); ++ i ) { int l = 0 , r = b . size (); while ( l < r ) { int m = l + ( r - l ) / 2 ; if ( b [ m ] < nums [ i ]) { // nums[i] is the target l = m + 1 ; } else { r = m ; } } if ( l == b . size ()) // nums[i] greater than all element in b b . push_back ( nums [ i ]); else // begin point to next element no less than the target nums[i]. b [ l ] = nums [ i ]; } return b . size (); } }; Alternatively, we could use lower_bound . class Solution { public : int lengthOfLIS ( vector < int >& nums ) { vector < int > b ; for ( int i = 0 ; i < nums . size (); ++ i ) { int l = lower_bound ( b . begin (), b . end (), nums [ i ]) - b . begin (); if ( l == b . size ()) // nums[i] greater than all element in b b . push_back ( nums [ i ]); else // begin point to next element no less than the target nums[i]. b [ l ] = nums [ i ]; } return b . size (); } }; 354. Russian Doll Envelopes \u00b6 174. Dungeon Game \u00b6","title":"Binary Search"},{"location":"leetcode/binary-search/notes/#binary-search","text":"","title":"Binary Search"},{"location":"leetcode/binary-search/notes/#binary-search-problem-characteristics","text":"Ordered binary search. You need to find an index or array element where ordering info is available, either explicitly (sorted array) or implicitly (partially sorted or other special info). monotony pattern. If the ordering info isn't available, but you can exclude \"all\" the possible cases from left or right by a condition comparing f(mid) to the target .","title":"Binary search problem characteristics"},{"location":"leetcode/binary-search/notes/#binary-search-problem-solving-techniques","text":"Clarify that you are trying to find the first one or find the last one. Clarify that you are trying to move the index or move the value (i.e. kth smallest number in a multiplicative table). Use an \"ordering abstraction\" vless(target, f(mid)) . This ordering abstraction will produce a boolean array that indicate the ordering information between the target value and the f(mid) . Decide whether the left or right part the f(mid) should fall into. The principle to determine the predicate is simple: don't rule out the possible result (maintain the loop invariant). Shrink the range accordingly based on the predicate decided in step 3. Test the case that the search range is small, such as only have one or two elements.","title":"Binary search problem solving techniques"},{"location":"leetcode/binary-search/notes/#binary-search-practical-use-case","text":"Find whether the given target is in the array. Find the position of the first value equal to the given target. Find the insertion position of the given target in the array. Find the position of the last value equal to the given target. Find the total number of x in a sorted array. Find the last element less than the target. Find the first element greater than the target.","title":"Binary search practical use case"},{"location":"leetcode/binary-search/notes/#binary-search-in-c-stl","text":"lower_bound : return iterator point to the element no less than the target. upper_bound : return iterator point to the element greater than the target. equal_range : return a pair of iterators, the first of which is lower_bound , the second is upper_bound . binary_search : return true if an element equivalent to val is found, and false otherwise.","title":"Binary search in C++ STL"},{"location":"leetcode/binary-search/notes/#caveat-of-binary-search-implementation","text":"Specify the range: [start, end) or [start, end] ? C++ STL used [start, end) to denote a range, which bring in many conveniences. We will stick on this convention. Which while loop condition? start < end ? start <= end ? start != end ? start + 1 < end ? The calculation of the mid . mid = start + (end - start) / 2 or mid = (start + end) / 2 ? To proof mid is always in the range [begin, end) . The \"bisection\": start = mid + 1 , start = mid , or end = mid - 1 or end = mid ? Where is the result? start ? end ? How to make sure?","title":"Caveat of binary search implementation"},{"location":"leetcode/binary-search/notes/#a-universal-binary-search-implementation","text":"Despite the above caveats, just remember that there are two versions of binary search one can write based on the range [begin, end) and [begin, end] . Iterator type in C++ using the former, it have many benefits in reduce the code complexity. Among all the binary search implementation you have seen, the following one is the most powerful version and it equivalent to C++ STL lower_bound algorithm. /** * return an index to an element no less than x. Be more specifically, if there is * an element in the given array equal to x, it returns the index of first such * element; if there is no element that is equal to x, it returns the index * where x can be inserted into the position without changing the ordering of * the elements. * * All possible return value for calling this function with array.size() == n is * [0, 1, ..., n - 1, n] * */ size_t binary_search ( int x , vector < int >& array , size_t n ) { size_t begin = 0 , end = n ; while ( begin != end ) { size_t mid = begin + ( end - begin ) / 2 ; if ( array [ mid ] < x ) { begin = mid + 1 ; } else { end = mid ; } } return begin ; }","title":"A \"universal\" binary search implementation"},{"location":"leetcode/binary-search/notes/#important-observations-about-this-implementation","text":"mid cannot less than begin , they can be equal. This will ensure begin = mid + 1 in the if statement at least reduce the size of [begin, end] by 1. Informal proof: if (array[mid] < x) , it indicate x can only possible be in array[mid + 1, mid + 2, ... n - 1]. mid + 1 is at least 1 greater than begin. mid and end never equal inside the while loop, mid < end is always hold. This will ensure end = mid in the else statement at least reduce the size of [begin, end] by 1. Informal proof: we have begain < end , so begin + end < 2 * end , thus (begin + end) / 2 < end , because integer divisioin truncate down, mid = (begin + end) / 2 always less than end. begin and end never cross. Informal proof: Inside the while loop, at the begining, we have begin < end . If the current iteration executes the if condition, begin = mid + 1 at most advance begin to end but not exceed end . If it execute the else condition, end = mid would at worst case change end point to the minimum value of mid , because we have begin <= mid . Thus, we can conclude that executing the statement end = mid will not change end less than begin , at worst equal to begin .","title":"Important observations about this implementation"},{"location":"leetcode/binary-search/notes/#claims-regarding-this-binary-search-routine","text":"The range [begin, end) is used, which comply to the convention used in C++ iterator. It is impossible that mid == end . If they are equal, array[n] is invalid memory access. We use the loop condition while (begin != end) to indicate that once the loop terminates, we have begin == end . By checking whether begin is a valid index to the array or not, we can know whether x is greater than all the elements in the array or not. If we want to check whether x is found in the array, we simply check array[begin] == x . However, this condition is based on the assumption that begin < end initially. Considering that, using while (begin < end) is better if you cannot ensure begin < end before the loop. Setting begin = mid + 1 reduces the size of the remaining interested sub-array and maintains the invariant, which is if x in the array, x is in [begin, end) . Setting end = mid reduces the size of the remaining interested sub-array (mid never equal to end) and maintains the invariant, which is if x in the array, x is in [begin, end) . This claim is a little hard to absorb. On way to understand is like the following: ~~Because we need keep searching x in the range [begin, mid] if we get in the else statement. In the else case there are two possibilities: 1) array[mid] > x . 2) array[mid] = x . For 1) it indicates x is in [begin, mid) , setting end = mid maintains the loop invariant correctly, which is that x is in the shrinked range. For the 2) it is a little complex. If array[mid] is the only element equal to x, setting end = mid appears violate the loop invariant by exclude x from the range [begin, end) . however, remember array[mid] is the only element equal to x, after the while loop, begin = end , we have the x found by begin even though theoretically [begin, end) is already an empty range since begin = end and array[begin] = array[end] = x . If there are more values are equal to x before and after the element array[mid] the loop will always end up finding the first x value in the array. If we use end = mid + 1 . Try test case [1, 3, 5, 7] , with x = 0 . deadloop will accur. i.e. begin = 0, mid = 1, end = 2 .","title":"Claims regarding this binary search routine"},{"location":"leetcode/binary-search/notes/#category-1-binary-search-on-sorted-arrays","text":"To solve this type of binary search problem. You should focus on the following: Come up test cases to verify your solution. Be able to find which side to drop for each iteration. Be extremly careful \"off by 1\" bugs. (1. reasoning: is mid value possible to be the solution or not. 2. exercise test cases: especially short ones)","title":"Category 1 Binary search on sorted arrays"},{"location":"leetcode/binary-search/notes/#34-search-for-a-range","text":"C++ Use STL function class Solution { public : vector < int > searchRange ( vector < int >& nums , int target ) { vector < int > res ( 2 , -1 ); int low = lower_bound ( nums . begin (), nums . end (), target ) - nums . begin (); int high = upper_bound ( nums . begin (), nums . end (), target ) - nums . begin (); if ( low == high ) return res ; return { low , hight - 1 }; } }; C++ Implementation of binary search class Solution { public : vector < int > searchRange ( vector < int >& nums , int target ) { vector < int > res ( 2 , -1 ); int low = lower_bound ( nums , target ); //int high = lower_bound(nums, target + 1); // also works. int high = upper_bound ( nums , target ); if ( low == high ) { return res ; } return { low , high - 1 }; } int lower_bound ( vector < int >& nums , int target ) { if ( nums . size () == 0 ) return 0 ; int l = 0 , r = nums . size (); while ( l < r ) { int m = l + ( r - l ) / 2 ; if ( nums [ m ] < target ) { l = m + 1 ; } else { r = m ; } } return l ; } int upper_bound ( vector < int >& nums , int target ) { if ( nums . size () == 0 ) return 0 ; int l = 0 , r = nums . size (); while ( l < r ) { int m = l + ( r - l ) / 2 ; if ( nums [ m ] <= target ) { l = m + 1 ; } else { r = m ; } } return l ; } }; Python solution class Solution ( object ): def searchRange ( self , nums , target ): \"\"\" :type nums: List[int] :type target: int :rtype: List[int] \"\"\" if len ( nums ) == 0 : return [ - 1 , - 1 ] begin = 0 end = len ( nums ) while begin != end : mid = begin + ( end - begin ) / 2 if nums [ mid ] < target : begin = mid + 1 else : end = mid if begin == len ( nums ): return [ - 1 , - 1 ] if nums [ begin ] == target : lower = begin else : lower = - 1 begin = 0 end = len ( nums ) while begin != end : mid = begin + ( end - begin ) / 2 if nums [ mid ] <= target : begin = mid + 1 else : end = mid if nums [ begin - 1 ] == target : upper = begin - 1 else : upper = - 1 return [ lower , upper ]","title":"34. Search for a Range"},{"location":"leetcode/binary-search/notes/#35-search-insert-position","text":"C++ solution lower_bound class Solution { public : int searchInsert ( vector < int >& nums , int target ) { if ( nums . size () == 0 ) return 0 ; int l = 0 , r = nums . size (); while ( l < r ) { int m = l + ( r - l ) / 2 ; if ( nums [ m ] < target ) { l = m + 1 ; } else { r = m ; } } return l ; } };","title":"35. Search Insert Position"},{"location":"leetcode/binary-search/notes/#33-search-in-rotated-sorted-array","text":"How to locate the sorted half? If left half is sorted, check where the target t is like be. else if right half is sorted, check where the target t is like to be. else if mid element is equal to left or right. Remove one of them. Although no duplicate, should consider short input like [3 1], 1 will have the equal case. C++ /** t = 1 t = 3 t = 5 t = 4 t = -1 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 3 4 5 1 1 3 5 4 1 <--need check */ class Solution { public : int search ( vector < int >& A , int t ) { if ( A . empty ()) return -1 ; int l = 0 , r = A . size () - 1 ; while ( l < r ) { int m = l + ( r - l ) / 2 ; if ( A [ m ] == t ) return m ; if ( A [ l ] < A [ m ]) { // left is sorted if ( A [ l ] <= t && t < A [ m ]) { r = m - 1 ; } else { l = m + 1 ; } } else if ( A [ m ] < A [ r ]) { // right is sorted if ( A [ m ] < t && t <= A [ r ]) { l = m + 1 ; } else { r = m - 1 ; } } else { // if equal, remove one. case: [3, 1], 1 if ( A [ l ] == A [ m ]) l ++ ; if ( A [ m ] == A [ r ]) r -- ; } } return A [ l ] == t ? l : -1 ; } }; Python class Solution ( object ): def search ( self , nums , target ): \"\"\" :type nums: List[int] :type target: int :rtype: int \"\"\" if len ( nums ) == 0 : return - 1 left = 0 right = len ( nums ) - 1 while left < right : mid = left + ( right - left ) / 2 if nums [ mid ] == target : return mid ; if nums [ left ] < nums [ mid ]: if nums [ left ] <= target and target < nums [ mid ]: right = mid - 1 else : left = mid + 1 elif nums [ mid ] < nums [ right ]: if nums [ mid ] < target and target <= nums [ right ]: left = mid + 1 else : right = mid - 1 else : if nums [ left ] == nums [ mid ]: left += 1 if nums [ right ] == nums [ mid ]: right -= 1 if nums [ left ] == target : return left return - 1","title":"33. Search in Rotated Sorted Array"},{"location":"leetcode/binary-search/notes/#81-search-in-rotated-sorted-array-ii","text":"How to locate the sorted half? C++ class Solution { public : bool search ( vector < int >& A , int t ) { if ( A . empty ()) return false ; int l = 0 , r = A . size () - 1 ; while ( l < r ) { int m = l + ( r - l ) / 2 ; if ( A [ m ] == t ) return true ; if ( A [ l ] < A [ m ]) { if ( A [ l ] <= t && t < A [ m ]) { r = m - 1 ; } else { l = m + 1 ; } } else if ( A [ m ] < A [ r ]) { if ( A [ m ] < t && t <= A [ r ]) { l = m + 1 ; } else { r = m - 1 ; } } else { if ( A [ l ] == A [ m ]) l ++ ; if ( A [ m ] == A [ r ]) r -- ; } } return A [ l ] == t ? true : false ; } }; Python class Solution ( object ): def search ( self , nums , target ): \"\"\" :type nums: List[int] :type target: int :rtype: int \"\"\" if len ( nums ) == 0 : return False left = 0 right = len ( nums ) - 1 while left < right : mid = left + ( right - left ) / 2 if nums [ mid ] == target : return True if nums [ left ] < nums [ mid ]: if nums [ left ] <= target and target < nums [ mid ]: right = mid - 1 else : left = mid + 1 elif nums [ mid ] < nums [ right ]: if nums [ mid ] < target and target <= nums [ right ]: left = mid + 1 else : right = mid - 1 else : if nums [ left ] == nums [ mid ]: left += 1 if nums [ right ] == nums [ mid ]: right -= 1 if nums [ left ] == target : return True return False","title":"81. Search in Rotated Sorted Array II"},{"location":"leetcode/binary-search/notes/#153-find-minimum-in-rotated-sorted-array","text":"Try to locate the valley which contains the min. Notice when A[0] < A[n - 1] , return A[0] . Draw a monotonic curve and then split the curve into two half, swith the order. This can help you to write the code. C++ class Solution { public : int findMin ( vector < int >& A ) { int l = 0 , r = A . size () - 1 ; while ( l < r ) { if ( A [ l ] < A [ r ]) // serve as base case. return A [ l ]; int m = l + ( r - l ) / 2 ; if ( A [ m ] > A [ r ]) { // also works. looking for not sorted half l = m + 1 ; } else if ( A [ m ] < A [ r ]) { // don't really need if statement r = m ; } } return A [ l ]; } }; Python class Solution ( object ): def findMin ( self , nums ): \"\"\" :type nums: List[int] :rtype: int \"\"\" if len ( nums ) == 0 : return - 1 left = 0 right = len ( nums ) - 1 while left < right : if nums [ left ] == nums [ right ]: return nums [ left ] mid = left + ( right - left ) / 2 if nums [ mid ] > nums [ right ]: left = mid + 1 else : right = mid return nums [ left ]","title":"153. Find Minimum in Rotated Sorted Array"},{"location":"leetcode/binary-search/notes/#154-find-minimum-in-rotated-sorted-array-ii","text":"Locate the valley which contains the min. Since duplicates exist. we cannot use the observation A[l] == A[r] . Here we deal with duplicates using decrease by one step. C++ class Solution { public : int findMin ( vector < int >& A ) { int l = 0 , r = A . size () - 1 ; while ( l < r ) { int m = l + ( r - l ) / 2 ; if ( A [ m ] > A [ r ]) { l = m + 1 ; } else if ( A [ m ] < A [ r ]) { r = m ; } else { r -- ; } } return A [ l ]; } }; Python class Solution ( object ): def findMin ( self , nums ): \"\"\" :type nums: List[int] :rtype: int \"\"\" if len ( nums ) == 0 : return - 1 left = 0 right = len ( nums ) - 1 while left < right : mid = left + ( right - left ) / 2 if nums [ mid ] > nums [ right ]: left = mid + 1 elif nums [ mid ] < nums [ right ]: right = mid else : right -= 1 return nums [ left ]","title":"154. Find Minimum in Rotated Sorted Array II"},{"location":"leetcode/binary-search/notes/#162-find-peak-element","text":"Use Binary search Use the neighboring relation to determin which side a peak value may occur then eliminate the other side. class Solution { public : int findPeakElement ( vector < int >& A ) { int l = 0 , r = A . size () - 1 ; while ( l < r ) { int m = l + ( r - l ) / 2 ; if ( A [ m ] < A [ m + 1 ]) { l = m + 1 ; } else if ( A [ m ] > A [ m + 1 ]) { r = m ; } } return l ; } };","title":"162 Find Peak Element"},{"location":"leetcode/binary-search/notes/#278-first-bad-version","text":"Binary search Notice how this can be related to the ordering abstraction. // Forward declaration of isBadVersion API. bool isBadVersion ( int version ); class Solution { public : int firstBadVersion ( int n ) { int l = 0 , r = n ; while ( l < r ) { int m = l + ( r - l ) / 2 ; if ( ! isBadVersion ( m )) { l = m + 1 ; } else { r = m ; } } return l ; } };","title":"278. First Bad Version"},{"location":"leetcode/binary-search/notes/#74-search-a-2d-matrix","text":"Binary search We can view the matrix as a big sorted array and then binary search the target. Notice test your finished routine using edge cases. (i.e. the initial value of end) class Solution { public : bool searchMatrix ( vector < vector < int >>& matrix , int target ) { int m = matrix . size (); int n = m ? matrix [ 0 ]. size () : 0 ; if ( m == 0 || n == 0 ) return false ; int start = 0 , end = m * n - 1 ; while ( start < end ) { int mid = start + ( end - start ) / 2 ; int i = mid / n , j = mid % n ; if ( matrix [ i ][ j ] < target ) { start = mid + 1 ; } else { end = mid ; } } return matrix [ start / n ][ start % n ] == target ? true : false ; } };","title":"74. Search a 2D Matrix"},{"location":"leetcode/binary-search/notes/#240-search-a-2d-matrix-ii","text":"Binary search to exclude whole column or whole row the key is you decide where to start the compare. If you start from left bottom or right top, the solution should be abvious. Notice the idea is from binary search, if ordering info available, we want to exclude as many as impossible values as we can. class Solution { public : bool searchMatrix ( vector < vector < int >>& matrix , int target ) { int m = matrix . size (); int n = m ? matrix [ 0 ]. size () : 0 ; if ( m == 0 || n == 0 ) return false ; int x = m - 1 , y = 0 ; while ( x >= 0 && y < n ) { if ( matrix [ x ][ y ] == target ) { return true ; } if ( matrix [ x ][ y ] < target ) { y ++ ; } else { x -- ; } } return false ; } };","title":"240. Search a 2D Matrix II"},{"location":"leetcode/binary-search/notes/#302-smallest-rectangle-enclosing-black-pixels","text":"C++ Brute Force class Solution { public : int minArea ( vector < vector < char >>& image , int x , int y ) { int m = image . size (); int n = m ? image [ 0 ]. size () : 0 ; int top = m , bottom = 0 , left = n , right = 0 ; for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { if ( image [ i ][ j ] == '1' ) { top = min ( top , i ); bottom = max ( bottom , i + 1 ); left = min ( left , j ); right = max ( right , j + 1 ); } } } return ( right - left ) * ( bottom - top ); } }; Binary search Notice the binary search idea is related to the problem Smallest Good Base and Wood Cut. The basic idea is to search each of 1 from 4 directions. First, make sure you can search one boundary and the others are similar. For example, to search the first row that contains 1 , we can look at the whole column/row to see whether this col/row have 1 . Because we are searching the first row that have 1 top down, bisec based on the count of 1 on each row we can know whether we ignore the upper half or the lower half. C++ Binary Search class Solution { public : int minArea ( vector < vector < char >>& image , int x , int y ) { int m = image . size (); int n = m ? image [ 0 ]. size () : 0 ; int top = bsearch_byrows ( image , 0 , x , 0 , n , true ); // search top int bottom = bsearch_byrows ( image , x + 1 , m , 0 , n , false ); int left = bsearch_bycols ( image , 0 , y , top , bottom , true ); int right = bsearch_bycols ( image , y + 1 , n , top , bottom , false ); return ( bottom - top ) * ( right - left ); } int bsearch_byrows ( vector < vector < char >>& image , int x , int y , int left , int right , bool white2black ) { while ( x < y ) { int m = ( x + y ) / 2 ; int k = left ; while ( k < right && image [ m ][ k ] == '0' ) k ++ ; if ( k < right == white2black ) { // mth row have '1' y = m ; } else { x = m + 1 ; } } return x ; } int bsearch_bycols ( vector < vector < char >>& image , int x , int y , int top , int bottom , bool white2black ) { while ( x < y ) { int m = ( x + y ) / 2 ; int k = top ; while ( k < bottom && image [ k ][ m ] == '0' ) k ++ ; if ( k < bottom == white2black ) { // mth column have '1' y = m ; } else { x = m + 1 ; } } return x ; } }; Python Binary Search class Solution ( object ): def minArea ( self , image , x , y ): \"\"\" :type image: List[List[str]] :type x: int :type y: int :rtype: int \"\"\" m = len ( image ) n = 0 if m != 0 : n = len ( image [ 0 ]) top = self . bsearch_row ( image , 0 , x , 0 , n , True ) bottom = self . bsearch_row ( image , x + 1 , m , 0 , n , False ) left = self . bsearch_col ( image , 0 , y , top , bottom , True ) right = self . bsearch_col ( image , y + 1 , n , top , bottom , False ) return ( bottom - top ) * ( right - left ) def bsearch_row ( self , image , start , end , lower , upper , white2black ): while start < end : m = ( start + end ) / 2 k = lower while k < upper and image [ m ][ k ] == '0' : k += 1 if ( k < upper ) == white2black : end = m else : start = m + 1 return start def bsearch_col ( self , image , start , end , lower , upper , white2black ): while start < end : m = ( start + end ) / 2 k = lower while k < upper and image [ k ][ m ] == '0' : k += 1 if ( k < upper ) == white2black : end = m else : start = m + 1 return start C++ BFS class Solution { public : int minArea ( vector < vector < char >>& image , int x , int y ) { int m = image . size (); int n = m ? image [ 0 ]. size () : 0 ; int top = m , bottom = 0 , left = n , right = 0 ; int xx [ 4 ] = { -1 , 0 , 1 , 0 }; int yy [ 4 ] = { 0 , 1 , 0 , -1 }; queue < pair < int , int >> q ; q . push ({ x , y }); image [ x ][ y ] = '0' ; while ( ! q . empty ()) { pair < int , int > t = q . front (); q . pop (); top = min ( top , t . first ); bottom = max ( bottom , t . first + 1 ); left = min ( left , t . second ); right = max ( right , t . second + 1 ); for ( int k = 0 ; k < 4 ; ++ k ) { int a = t . first + xx [ k ]; int b = t . second + yy [ k ]; if ( a >= 0 && a < m && b >= 0 && b < n && image [ a ][ b ] == '1' ) { q . push ({ a , b }); image [ a ][ b ] = '0' ; } } } return ( right - left ) * ( bottom - top ); } }; Python BFS from collections import deque class Solution ( object ): def minArea ( self , image , x , y ): \"\"\" :type image: List[List[str]] :type x: int :type y: int :rtype: int \"\"\" m = len ( image ) n = 0 if m != 0 : n = len ( image [ 0 ]) xx = [ - 1 , 0 , 1 , 0 ] yy = [ 0 , - 1 , 0 , 1 ] top = m bottom = 0 left = n right = 0 q = deque () q . append ([ x , y ]) image [ x ][ y ] = '0' while len ( q ) > 0 : t = q . popleft () top = min ( top , t [ 0 ]) bottom = max ( bottom , t [ 0 ] + 1 ) left = min ( left , t [ 1 ]) right = max ( right , t [ 1 ] + 1 ) for k in range ( 4 ): a = t [ 0 ] + xx [ k ] b = t [ 1 ] + yy [ k ] if a >= 0 and a < m and b >= 0 and b < n and image [ a ][ b ] == '1' : q . append ([ a , b ]) image [ a ][ b ] = '0' return ( right - left ) * ( bottom - top ) C++ DFS class Solution { private : int m , n ; int top , bottom , left , right ; public : int minArea ( vector < vector < char >>& image , int x , int y ) { m = image . size (); n = m ? image [ 0 ]. size () : 0 ; top = m , bottom = 0 , left = n , right = 0 ; dfs_helper ( image , x , y ); return ( right - left ) * ( bottom - top ); } void dfs_helper ( vector < vector < char >>& image , int x , int y ) { if ( x < 0 || x >= m || y < 0 || y >= n || image [ x ][ y ] == '0' ) { return ; } image [ x ][ y ] = '0' ; top = min ( top , x ); bottom = max ( bottom , x + 1 ); left = min ( left , y ); right = max ( right , y + 1 ); dfs_helper ( image , x - 1 , y ); dfs_helper ( image , x , y + 1 ); dfs_helper ( image , x + 1 , y ); dfs_helper ( image , x , y - 1 ); } };","title":"302. Smallest Rectangle Enclosing Black Pixels"},{"location":"leetcode/binary-search/notes/#363-max-sum-of-rectangle-no-larger-than-k","text":"Iterate the wide of the matrix and using prefix sum and set lower_bound . From the problem Max Sum of Subarry No Larger Than K, we have to enumerate the width of the sub-matrix and sum up all row elements and get an array of length m , m is the number of rows of the matrix. Then apply the method. C++ presum class Solution { public : int maxSumSubmatrix ( vector < vector < int >>& matrix , int k ) { if ( matrix . empty ()) return 0 ; int m = matrix . size (); int n = m ? matrix [ 0 ]. size () : 0 ; int res = INT_MIN ; for ( int l = 0 ; l < n ; ++ l ) { vector < int > sums ( m , 0 ); for ( int r = l ; r < n ; ++ r ) { for ( int i = 0 ; i < m ; ++ i ) { sums [ i ] += matrix [ i ][ r ]; } set < int > preSumSet ; preSumSet . insert ( 0 ); int preSum = 0 , curMax = INT_MIN ; for ( int sum : sums ) { preSum += sum ; set < int >:: iterator it = preSumSet . lower_bound ( preSum - k ); if ( it != preSumSet . end ()) { curMax = max ( curMax , preSum - * it ); } preSumSet . insert ( preSum ); } res = max ( res , curMax ); } } return res ; } }; merge sort The idea is similar that solution 1. Instead of calculate preSum on the fly, we finish calculation and pass it to a mergeSort routine. The use mergeSort here is to find the A[j] - A[i] <= k efficiently, O(nlogn) . C++ Merge Sort class Solution { public : int maxSumSubmatrix ( vector < vector < int >>& matrix , int k ) { int m = matrix . size (); int n = m ? matrix [ 0 ]. size () : 0 ; int res = INT_MIN ; vector < long long > sums ( m + 1 , 0 ); for ( int l = 0 ; l < n ; ++ l ) { vector < long long > sumInRow ( m , 0 ); for ( int r = l ; r < n ; ++ r ) { for ( int i = 0 ; i < m ; ++ i ) { sumInRow [ i ] += matrix [ i ][ r ]; sums [ i + 1 ] = sums [ i ] + sumInRow [ i ]; } res = max ( res , mergeSort ( sums , 0 , m + 1 , k )); if ( res == k ) return k ; } } return res ; } int mergeSort ( vector < long long >& sums , int start , int end , int k ) { if ( end == start + 1 ) return INT_MIN ; int mid = start + ( end - start ) / 2 ; int res = mergeSort ( sums , start , mid , k ); if ( res == k ) return k ; res = max ( res , mergeSort ( sums , mid , end , k )); if ( res == k ) return k ; long long cache [ end - start ]; int j = mid , c = 0 , t = mid ; for ( int i = start ; i < mid ; ++ i ) { while ( j < end && sums [ j ] - sums [ i ] <= k ) ++ j ; // search first time sums[j] - sums[i] > k if ( j - 1 >= mid ) { // sums[j - 1] - sums[i] <= k, make sure j - 1 is still in right side res = max ( res , ( int )( sums [ j - 1 ] - sums [ i ])); if ( res == k ) return k ; } while ( t < end && sums [ t ] < sums [ i ]) { cache [ c ++ ] = sums [ t ++ ]; } cache [ c ++ ] = sums [ i ]; } for ( int i = start ; i < t ; ++ i ) { sums [ i ] = cache [ i - start ]; } return res ; } };","title":"363. Max Sum of Rectangle No Larger Than K"},{"location":"leetcode/binary-search/notes/#category-2-using-ordering-abstraction","text":"","title":"Category 2 Using ordering abstraction"},{"location":"leetcode/binary-search/notes/#69-sqrtx","text":"Solution 1 using ordering abstraction definition To find a square root of a integer x using binary search. We need to first determin the range [left, right] that the target value sqrt(x) may in. The potential range we can search is [0, x/2 + 1] . Then we should clarify this binary search is the \"find the first one\" type or the \"find the last one\" type. Basically, we want to determine our ordering abstraction f(target, g(i)) that is able to produce a boolean array. The boolean array have true part and false part seperated. Here target = sqrt(x) and g(i) = i . We define f(sqrt(x), i) = true when i <= sqrt(x) and f(sqrt(x), i) = false when i > sqrt(x) . This came from the following intuition: We are looking for the \"last\" integer whose square is less than x . Why not the otherwise? Because if you change to find the \"first\" integer whose square is greater than the x from right section of the boolean array, it is hard to define our ordering abstraction f . Of cause, we can search the \"first\" integer whose square is greater than x and find the previous integer next to it as the solution, but this later solution is a bit complex and counter intuitive. We prefer the first definition of ordering abstraction. Although a workable solution following the second ordering abstraction is also given below. For example: to solve the sqrt(8) and sqrt(9) using our definition: k, i = 0 1 2 3 4 5 6 7 8 9 10 n = 11 A = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] f(sqrt(8), k) = [T T T F F] f(sqrt(9), k) = [T T T T F] The binary search routine will be: class Solution { public : int mySqrt ( int x ) { int l = 0 , r = x / 2 + 1 ; while ( l < r ) { // int m = l + (r - l) / 2; // will deadloop for 4, why? int m = r - ( r - l ) / 2 ; if ( m <= x / m ) { l = m ; } else { r = m - 1 ; } } return l ; } }; Solution 2 using the alternative ordering abstraction definition Second ordering abstraction (find first value whose square is greater than x) k, i = 0 1 2 3 4 5 6 7 8 9 10 n = 11 A = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] f(sqrt(8), k) = [F F F T T] f(sqrt(9), k) = [F F F F T] class Solution { public : int mySqrt ( int x ) { if ( x == 0 ) return 0 ; // should handle, but will got division by zero in line 9. int l = 0 , r = x / 2 + 2 ; // r = x / 2 + 1 will not working for x = 1, have to have the one past last; while ( l < r ) { //int m = r - (r - l) / 2; // will dead loop for 4 int m = l + ( r - l ) / 2 ; if ( m > x / m ) { r = m ; } else { l = m + 1 ; } } return l - 1 ; } };","title":"69. Sqrt(x)"},{"location":"leetcode/binary-search/notes/#367-valid-perfect-square","text":"Solution 1 Binary search using ordering abstraction Notice you have to run tests for cases from 1 to 5. C++ class Solution { public : bool isPerfectSquare ( int num ) { if ( num == 1 ) return true ; int begin = 1 , end = num / 2 ; while ( begin < end ) { //long long mid = begin + (end - begin) / 2; // not working, deadloop for 5 long long mid = end - ( end - begin ) / 2 ; if ( mid * mid == num ) return true ; if ( mid * mid < num ) { begin = mid ; } else { end = mid - 1 ; } } return false ; } }; Python class Solution ( object ): def isPerfectSquare ( self , num ): \"\"\" :type num: int :rtype: bool \"\"\" if num == 1 : return True lower = 1 upper = num / 2 while lower < upper : mid = upper - ( upper - lower ) / 2 if mid * mid == num : return True if mid * mid < num : lower = mid else : upper = mid - 1 return False","title":"367. Valid Perfect Square"},{"location":"leetcode/binary-search/notes/#441-arranging-coins","text":"Solution 1 Biinary search Notice the integer overflow possibility, we use long to deal with it. class Solution { public : int arrangeCoins ( int n ) { if ( n < 2 ) { return n ; } long l = 0 ; long r = n ; while ( l < r ) { long m = l + ( r - l ) / 2 ; long t = m * ( m + 1 ) / 2 ; if ( t == n ) return m ; if ( t < n ) { l = m + 1 ; } else { r = m ; } } return l - 1 ; } };","title":"441. Arranging Coins"},{"location":"leetcode/binary-search/notes/#633-sum-of-square-numbers","text":"Solution 1 Binary search Once you have derived value b from a and c , you can binary search b . class Solution { public : bool judgeSquareSum ( int c ) { if ( c == 0 ) return true ; for ( long long a = 0 ; a * a <= c ; ++ a ) { int b = c - ( int ) ( a * a ); int l = 0 , r = b / 2 + 1 ; while ( l < r ) { long long m = r - ( r - l ) / 2 ; if ( m * m == b ) return true ; if ( m * m < b ) { l = m ; } else { r = m - 1 ; } } } return false ; } }; Solution 2 Two pointers Notice this square sum can be found efficiently using two pointers. class Solution { public : bool judgeSquareSum ( int c ) { int a = 0 , b = sqrt ( c ); while ( a <= b ){ int sum = a * a + b * b ; if ( sum < c ) a ++ ; else if ( sum > c ) b -- ; else return true ; } return false ; } }; Solution 3 Using set Keep inserting the value into a set, in the meantime also look up the other class Solution { public : bool judgeSquareSum ( int c ) { set < int > s ; for ( int i = 0 ; i <= sqrt ( c ); ++ i ) { s . insert ( c - i * i ); if ( s . count ( i * i )) return true ; } return false ; } };","title":"633. Sum of Square Numbers"},{"location":"leetcode/binary-search/notes/#658-find-k-closest-elements","text":"Solution 1 Binary search Compare to problem 475. Heaters Our search target is to find the starting index of the subarray of length K. class Solution { public : vector < int > findClosestElements ( vector < int >& arr , int k , int x ) { int start = 0 , end = arr . size () - k ; while ( start < end ) { int mid = start + ( end - start ) / 2 ; // looking for a \"mid\" that if ( x - arr [ mid ] > arr [ mid + k ] - x ) { start = mid + 1 ; } else { end = mid ; } } return vector < int > ( arr . begin () + start , arr . begin () + start + k ); } }; Solution 2 Binary search and Two pointers We first use binary search to locate the x value then expand to left and right looking for the k closest elements Notice the i < 0 in the if condition, it is very important to be there. otherwise the array index will be out of bound. class Solution { public : vector < int > findClosestElements ( vector < int >& arr , int k , int x ) { int index = lower_bound ( arr . begin (), arr . end (), x ) - arr . begin (); int i = index - 1 , j = index ; while ( k -- ) { if ( i < 0 || j < arr . size () && abs ( arr [ j ] - x ) < abs ( arr [ i ] - x )) { j ++ ; } else { i -- ; } } return vector < int > ( arr . begin () + i + 1 , arr . begin () + j ); } };","title":"658. Find K Closest Elements"},{"location":"leetcode/binary-search/notes/#611-valid-triangle-number","text":"The main idea comes from the triangle lateral property, in which the triple should fullfil: a + b > c , a + c > b , and b + c > a . Once we sort it. We are able to gain some advantages that we don't have to check all these 3 relations. Instead, we should only take care of A[i] + A[j] > A[k] , in which i < j < k . Because we sorted the array, we can also fix the i and j , using binary search to find the k in the ragne of A[j + 1] ~ A[n - 1] . We can use our classic binary search template to achieve the goal. class Solution { public : int triangleNumber ( vector < int >& nums ) { int n = nums . size (); int res = 0 ; sort ( nums . begin (), nums . end ()); for ( int i = 0 ; i < n - 2 ; ++ i ) { for ( int j = i + 1 ; j < n - 1 ; ++ j ) { int l = j + 1 , r = n ; // range of all possible k, notice l start with j + 1 int t = nums [ i ] + nums [ j ]; while ( l < r ) { int m = l + ( r - l ) / 2 ; if ( nums [ m ] < t ) { l = m + 1 ; } else { r = m ; } } res += l - j - 1 ; // notice the count start from j + 1 to l - 1. } } return res ; } };","title":"611. Valid Triangle Number"},{"location":"leetcode/binary-search/notes/#category-3-using-ordering-abstration-monotonicity","text":"","title":"Category 3 Using ordering abstration (monotonicity)"},{"location":"leetcode/binary-search/notes/#287-find-the-duplicate-number","text":"Solution 1 Binary search The problem asking for better than O(n^2) we could check to see whether binary search will work. If you count how many values <= the mid elements of [1, ..., n-1] , it will give you enough information to discard part of the array. Here you should distinguish what will be split and what will be searched. The answer is the [1, ..., n-1] sequence, not the given array. The simple proof of why it works can be put in this the following way. If the count of elements that <=mid in the array is less than mid , we can learn that the duplicate is in the higher end. If the count is greater, we can know that the duplicate element is in the lower end of the sequence [1, ..., n-1] . class Solution { public : int findDuplicate ( vector < int >& nums ) { int begin = 1 , end = nums . size () - 1 ; while ( begin < end ) { int mid = begin + ( end - begin ) / 2 ; int count = 0 ; for ( int a : nums ) { if ( a <= mid ) ++ count ; } if ( count <= mid ) // \"=\" for [1,2,2] begin = mid + 1 ; else end = mid ; } return begin ; } }; Solution 2 tortoise and hare algorithm This problem is very similar to the the find circle in linked list. Generally, if you repeate A[A[i]] , the out put will show some periodic patterns. In fact you can imagine a rho shaped sequence. Image there is a function f(i) = A[i] , it mapping from 1, 2, 3, ... n to 1, 2, 3, ... n . Try to traverse A[i] , you will finally get circle through some same sequence of elements again and again, thus you obtain a rho shaped sequency like a circle in a linked list. The reason of it being a rho shape is becuase at least one element you will not come back to it if you leave it. Find Duplicate class Solution { public : int findDuplicate ( vector < int >& nums ) { int n = nums . size (); if ( n == 0 ) return 0 ; int slow = 0 , fast = 0 , find = 0 ; while ( slow != fast || ( slow == 0 && fast == 0 )) { slow = nums [ slow ]; fast = nums [ nums [ fast ]]; } while ( slow != find ) { slow = nums [ slow ]; find = nums [ find ]; } return find ; } };","title":"287. Find the Duplicate Number"},{"location":"leetcode/binary-search/notes/#360-sort-transformed-array","text":"","title":"360. Sort Transformed Array"},{"location":"leetcode/binary-search/notes/#374-guess-number-higher-or-lower","text":"C++ binary search // Forward declaration of guess API. // @param num, your guess // @return -1 if my number is lower, 1 if my number is higher, otherwise return 0 int guess ( int num ); class Solution { public : int guessNumber ( int n ) { int start = 1 , end = n ; while ( start < end ) { int mid = start + ( end - start ) / 2 ; if ( guess ( mid ) == 0 ) return mid ; if ( guess ( mid ) == 1 ) { start = mid + 1 ; } else { end = mid ; } } return start ; } }; Python # The guess API is already defined for you. # @param num, your guess # @return -1 if my number is lower, 1 if my number is higher, otherwise return 0 # def guess(num): class Solution ( object ): def guessNumber ( self , n ): \"\"\" :type n: int :rtype: int \"\"\" begin = 0 end = n while begin != end : mid = begin + ( end - begin ) / 2 if guess ( mid ) == 0 : return mid if guess ( mid ) == 1 : begin = mid + 1 else : end = mid return begin","title":"374. Guess Number Higher or Lower"},{"location":"leetcode/binary-search/notes/#475-heaters","text":"Sort then brute force The solution we are looking for is the max value of the smallest house-heater distance. Think through what is the distance you want to keep, min or max C++ class Solution { public : int findRadius ( vector < int >& houses , vector < int >& heaters ) { int m = houses . size (); int n = heaters . size (); sort ( houses . begin (), houses . end ()); sort ( heaters . begin (), heaters . end ()); int res = INT_MIN ; int i , j = 0 ; for ( i = 0 ; i < m ; ++ i ) { while ( j < n - 1 && abs ( heaters [ j + 1 ] - houses [ i ]) <= abs ( heaters [ j ] - houses [ i ])) { j ++ ; } res = max ( res , abs ( houses [ i ] - heaters [ j ])); } return res ; } }; Python class Solution ( object ): def findRadius ( self , houses , heaters ): \"\"\" :type houses: List[int] :type heaters: List[int] :rtype: int \"\"\" m = len ( houses ) n = len ( heaters ) houses . sort () heaters . sort () i = 0 j = 0 res = 0 for i in range ( m ): while j < n - 1 and abs ( heaters [ j + 1 ] - houses [ i ]) <= abs ( heaters [ j ] - houses [ i ]): j += 1 res = max ( res , abs ( houses [ i ] - heaters [ j ])) return res Binary search the neighboring heaters get max of min Notice we cannot sort hourses and then search each heater's position. A special cases [1, 2, 3] 2 , the result is 0 whereis it should be 1 . C++ class Solution { public : int findRadius ( vector < int >& houses , vector < int >& heaters ) { int n = heaters . size (); sort ( heaters . begin (), heaters . end ()); int res = INT_MIN ; for ( int house : houses ) { int start = 0 , end = n ; while ( start < end ) { int mid = start + ( end - start ) / 2 ; if ( heaters [ mid ] < house ) start = mid + 1 ; else end = mid ; } int dist1 = ( start == n ) ? INT_MAX : heaters [ start ] - house ; int dist2 = ( start == 0 ) ? INT_MAX : house - heaters [ start - 1 ]; res = max ( res , min ( dist1 , dist2 )); } return res ; } }; Python class Solution ( object ): def findRadius ( self , houses , heaters ): \"\"\" :type houses: List[int] :type heaters: List[int] :rtype: int \"\"\" m = len ( houses ) n = len ( heaters ) heaters . sort () i = 0 j = 0 res = float ( '-inf' ) for i in range ( m ): start = 0 end = n while start != end : mid = start + ( end - start ) / 2 if heaters [ mid ] < houses [ i ]: start = mid + 1 else : end = mid dist1 = float ( 'inf' ) dist2 = float ( 'inf' ) if start != n : dist1 = heaters [ start ] - houses [ i ] if start != 0 : dist2 = houses [ i ] - heaters [ start - 1 ] res = max ( res , min ( dist1 , dist2 )) return res","title":"475. Heaters"},{"location":"leetcode/binary-search/notes/#410-split-array-largest-sum","text":"","title":"410. Split Array Largest Sum"},{"location":"leetcode/binary-search/notes/#1011-capacity-to-ship-packages-within-d-days","text":"Binary solution Same as the 410. Split Array Largest Sum class Solution { public : int shipWithinDays ( vector < int >& weights , int D ) { int n = weights . size (); if ( n < D ) return 0 ; int l = * max_element ( weights . begin (), weights . end ()); int h = accumulate ( weights . begin (), weights . end (), 0 ); while ( l < h ) { int m = ( l + h ) / 2 ; int c = 1 ; // need cut D-1 times int sum = 0 ; for ( int w : weights ) { if ( sum + w > m ) { sum = 0 ; c ++ ; } sum += w ; } if ( c > D ) { l = m + 1 ; } else { h = m ; } } return l ; } };","title":"1011. Capacity To Ship Packages Within D Days"},{"location":"leetcode/binary-search/notes/#875-koko-eating-bananas","text":"Binary search Using the monotonic guessing approach, notice the trick in counting whether the given guess value is possible. C++ binary search class Solution { public : int minEatingSpeed ( vector < int >& piles , int H ) { int N = piles . size (); if ( N > H ) return 0 ; int l = 1 ; long r = 10e9 ; while ( l < r ) { int k = l + ( r - l ) / 2 ; int hour = 0 ; for ( int p : piles ) { if ( k >= p ) { hour ++ ; } else { hour += ( p + k - 1 ) / k ; } } if ( hour > H ) { // K is too large, l = k + 1 ; } else { r = k ; } } return l ; } }; C++ binary search simplified class Solution { public : int minEatingSpeed ( vector < int >& piles , int H ) { int N = piles . size (); if ( N > H ) return 0 ; int l = 1 ; long r = 10e9 ; while ( l < r ) { int k = l + ( r - l ) / 2 ; int hour = 0 ; for ( int p : piles ) { hour += ( p + k - 1 ) / k ; } if ( hour > H ) { // K is too large, l = k + 1 ; } else { r = k ; } } return l ; } };","title":"875. Koko Eating Bananas"},{"location":"leetcode/binary-search/notes/#1539-kth-missing-positive-number","text":"Naive Solution using multiple variables and keep loop invariant. Binary Search Observe the relation: total missing positives before A[m] is A[m] - 1 - m because the index m and A[m] is related to the missing positives thus can be used for counting. the bisection condition can be interpreted as a boolean predicate: \"whether the number of missing positives before A[m] is no less than k ?\" Naive Solution class Solution { public : int findKthPositive ( vector < int >& arr , int k ) { if ( arr . empty ()) return k ; int missing_cnt = arr [ 0 ] - 1 ; if ( missing_cnt >= k ) return k ; int prev = arr [ 0 ]; for ( int i = 1 ; i < arr . size (); ++ i ) { if ( ! ( arr [ i ] == prev || arr [ i ] == prev + 1 )) { int skip = arr [ i ] - prev - 1 ; if ( missing_cnt + skip >= k ) { return prev + k - missing_cnt ; } missing_cnt += skip ; } prev = arr [ i ]; } return ( prev + k - missing_cnt ); } }; Binary Search class Solution { public : int findKthPositive ( vector < int >& arr , int k ) { int l = 0 , r = arr . size (); while ( l < r ) { int m = l + ( r - l ) / 2 ; if ( arr [ m ] - 1 - m < k ) { l = m + 1 ; } else { r = m ; } } return l + k ; } };","title":"1539. Kth Missing Positive Number"},{"location":"leetcode/binary-search/notes/#1482-minimum-number-of-days-to-make-m-bouquets","text":"Solution Binary Search Use a subroutine to compute whether the constrain can be meet or not. The search is looking for the whether m bouquets is possible, meet the binary pattern \"no less than\". So that we use if(cnt_m < m) and the return values is l. C++ binary search class Solution { public : int minDays ( vector < int >& bloomDay , int m , int k ) { int l = * min_element ( bloomDay . begin (), bloomDay . end ()); int r = * max_element ( bloomDay . begin (), bloomDay . end ()); if ( bloomDay . size () < m * k ) return -1 ; while ( l < r ) { int mid = l + ( r - l ) / 2 ; int cnt_k = 0 ; int cnt_m = 0 ; for ( int d : bloomDay ) { if ( d > mid ) { cnt_k = 0 ; } else { cnt_k ++ ; if ( cnt_k == k ) { cnt_m ++ ; cnt_k = 0 ; } } } if ( cnt_m < m ) { l = mid + 1 ; } else { r = mid ; } } return l ; } };","title":"1482. Minimum Number of Days to Make m Bouquets"},{"location":"leetcode/binary-search/notes/#1283-find-the-smallest-divisor-given-a-threshold","text":"Solution Binary search Notice the specific divisor calculation. under this divisor operation, no matter how large the divisor is, the sum is always greater than nums.size() , if not, the solution is not guaranteed. so the threshold cannot smaller than nums.size() . This also indicate that the minimum divisor is less than or eaual to max(nums) . in the bsection predicate, notice this time the condition becomes if (res > target) essentially, the if (f(mid) < target) in the binary search templates is saying mid and f(mid) are positive correlation. here the mid and res are negative correlation. C++ binary search class Solution { public : int smallestDivisor ( vector < int >& nums , int threshold ) { int l = 1 ; int r = * max_element ( nums . begin (), nums . end ()); while ( l < r ) { int m = l + ( r - l ) / 2 ; int res = 0 ; for ( int num : nums ) { res += ( num + m - 1 ) / m ; } if ( res > threshold ) { l = m + 1 ; } else { r = m ; } } return l ; } };","title":"1283. Find the Smallest Divisor Given a Threshold"},{"location":"leetcode/binary-search/notes/#1231-divide-chocolate","text":"Solution Binary search The key difference between this problem and 410. Split Array Largest Sum is this problem asks for maximizing the smallest sweetness of the pieces, whereas the 410. Split Array Largest Sum asks minimizing the largest piece. Support k cuts generate m outcomes S = \\{s_1^{|k|}, s_2^{|k|}, \\cdots, s_m^{|k|}\\} S = \\{s_1^{|k|}, s_2^{|k|}, \\cdots, s_m^{|k|}\\} , this problem is to find the value of \\operatorname*{argmax}_m (\\operatorname*{argmax}_k (S)) \\operatorname*{argmax}_m (\\operatorname*{argmax}_k (S)) . Imagine you guessed a value m , which is the maximum sweetness you can get from the smallest sweetness piece of all cuts. How to test whether the value m is possible? If possible, we will increase it to maximize it; if not, we will still keep it a candidate. Same problem as 183. Wood cut . Binary Search class Solution { public : int maximizeSweetness ( vector < int >& sweetness , int K ) { int start = * min_element ( sweetness . begin (), sweetness . end ()); int end = accumulate ( sweetness . begin (), sweetness . end (), 0 ); while ( start < end ) { int mid = ( start + end + 1 ) / 2 ; int sum = 0 ; int cuts = 0 ; for ( int s : sweetness ) { if (( sum += s ) >= mid ) { sum = 0 ; if ( ++ cuts > K ) break ; } } if ( cuts > K ) { // because >= mid above guarentee the \"no less than\" the guess. // if cuts > K, mid could be the right answer and should be returned. // Remember the binary search invariance requies not miss any // answer in each iteration. start = mid ; } else { end = mid - 1 ; } } return start ; } }; Note Compare the binary search solution of problem of 1231. Divide Chocolate and 410. Split Array Largest Sum . Notice how different in checking the number of cuts. It exceeds the limit K , for max and min case, it indicate a very trivial difference in meaning.","title":"1231. Divide Chocolate"},{"location":"leetcode/binary-search/notes/#copy-books-lintcode","text":"","title":"Copy books (lintcode)"},{"location":"leetcode/binary-search/notes/#183-wood-cut-lintcode","text":"Description Given n pieces of wood with length L[i] (integer array). Cut them into small pieces to guarantee you could have equal or more than k pieces with the same length. What is the longest length you can get from the n pieces of wood? Given L & k, return the maximum length of the small pieces. You couldn't cut wood into float length. If you couldn't get >= k pieces, return 0. Solution 1 Binary search It requires getting equal or more than k pieces of wood with the same length. So you have to cut the wood to fulfill the requirement. However, you need to promise that each of the k wood is the longest that is possible. Imagine that you are given a bunch of wood to cut. How would you do it? You probably want to try to make one cut and see whether you can make it or not. If not, you may want to make two cuts, and so on. But how could you program such a solution? It is very hard. Start thinking about the length seems a good option. Suppose you know your final maximum length. You would be able to make the cut accordingly. Now given a length out of guessing, can you verify whether it going to work or not? Yes, you can! That's the core idea of this solution. class Solution { public : int woodCut ( vector < int > & L , int k ) { if ( L . empty ()) return 0 ; int maxlen = * max_element ( L . begin (), L . end ()); if ( k == 0 ) return maxlen ; int start = max ( 1 , maxlen / k ), end = maxlen ; while ( start < end ) { int mid = start + ( end - start ) / 2 ; int count = 0 ; for ( int len : L ) { count += len / ( mid + 1 ); } if ( count >= k ) start = mid + 1 ; else end = mid ; } int count = 0 ; for ( int len : L ) count += len / start ; return count >= k ? start : 0 ; } };","title":"183. Wood cut (lintcode)"},{"location":"leetcode/binary-search/notes/#774-minimize-max-distance-to-gas-station","text":"Solution 1 Binary search It is very similar to the problem Wood cut . You just need to take care of the accuracy of the results, namely also the int/double casts. It is also the hard part of the problem. Notice the count variable is int type, you should test your solution expecially for the line count += dist[i] / mid ; class Solution { public : double minmaxGasDist ( vector < int >& stations , int K ) { int n = stations . size (); vector < int > dist ( n , 0 ); // dist[0] = null; int d = 0 ; for ( int i = 1 ; i < n ; ++ i ) { dist [ i ] = stations [ i ] - stations [ i - 1 ]; d = max ( d , dist [ i ]); } double low = 0 , high = d ; while ( low + 0.000001 < high ) { double mid = low + ( high - low ) / 2 ; int count = 0 ; for ( int i = 1 ; i < n ; ++ i ) { count += dist [ i ] / mid ; } if ( count > K ) { // mid is too small low = mid ; } else { high = mid ; } } return low ; } };","title":"774. Minimize Max Distance to Gas Station"},{"location":"leetcode/binary-search/notes/#644-maximum-average-subarray-ii","text":"First try to understand why the constrain is \"greater than or equal than k\". You find that this constrain will ensure the solution exists and the problem is interesting. Notice the monotonicity of the \"average sum\" values, namely for a given value, if it doesn't fulfill the constrain (length >= k & max(avg)), you can eliminate half of the values from the solution space. Your binary search predicate will be to test whether there exists a subarray with length greater than k and an average value is larger than the mid . We use a trick to verify the constrains. The tricky thing is that the two constrains are not checked separately, they need to be work together in order to achieve better complexity. The length constrain is ensured partly by the \"skip\" indexing ( i - k ), partly by keeping the smallest average before the current considered subarray. Key Math Insight \\begin{align*} \\mu_k = \\frac{a_i + a_{i+1} + \\cdots, + a_j}{j-i+1} & >= Mid \\\\ a_i + a_{i+1} + \\cdots, + a_j & >= Mid \\times (j-i+1) \\\\ (a_i - Mid) + (a_{i+1} - Mid) + \\cdots, + (a_j - Mid) & >= 0 \\end{align*} \\begin{align*} \\mu_k = \\frac{a_i + a_{i+1} + \\cdots, + a_j}{j-i+1} & >= Mid \\\\ a_i + a_{i+1} + \\cdots, + a_j & >= Mid \\times (j-i+1) \\\\ (a_i - Mid) + (a_{i+1} - Mid) + \\cdots, + (a_j - Mid) & >= 0 \\end{align*} class Solution { public : double findMaxAverage ( vector < int >& nums , int k ) { double start = * min_element ( nums . begin (), nums . end ()); double end = * max_element ( nums . begin (), nums . end ()); while ( lower + 0.00001 < upper ) { double mid = lower + ( upper - lower ) / 2 ; if ( isLarger ( nums , mid , k )) { // is average value >= mid? lower = mid ; } else { upper = mid ; } } return lower ; } /* return true if a greater average value is possible */ bool isLarger ( vector < int >& nums , double mid , int k ) { int n = nums . size (); double sums = 0 , prev = 0 , prev_min = 0 ; for ( int i = 0 ; i < k ; i ++ ) { sums += nums [ i ] - mid ; } // we keep looking for whether a subarray sum of length >= k in array // \"sums\" is possible to be greater than zero. If such a subarray exist, // it means that the target average value is greater than the \"mid\" value. if ( sums >= 0 ) { return true ; } // we look at the front part of sums that at least k element apart from i. // If we can find the minimum of the sums[0], sums[1], ..., sums[i - k] // and check if sums[i] - min(sums[0], sums[1], ..., sums[i - k]) >= 0. // If this is the case, it indicate, there exist a subarray of length >= k // with sum greater than 0 in sums. we can return ture. for ( int i = k ; i < n ; i ++ ) { sums += nums [ i ] - mid ; prev += nums [ i - k ] - mid ; prev_min = min ( prev_min , prev ); if ( sums >= prev_min ) return true ; } return false ; } };","title":"644. Maximum Average Subarray II"},{"location":"leetcode/binary-search/notes/#778-swim-in-rising-water","text":"In This problem we are trying to find a path, in which the maximum element in the path among all paths is minimum. Meaning we look for a target value in the grid, such that there exist a path from grid[0][0] to grid[n-1][n-1] which includes this value and it is the maximum value in the path. class Solution { int x [ 4 ] = { 0 , -1 , 0 , 1 }; int y [ 4 ] = { -1 , 0 , 1 , 0 }; public : int swimInWater ( vector < vector < int >>& grid ) { int n = grid . size (); int begin = grid [ 0 ][ 0 ], end = n * n - 1 ; // binary search find a path with mini elevation while ( begin < end ) { int mid = begin + ( end - begin ) / 2 ; if ( pathExist ( grid , mid )) { end = mid ; } else { begin = mid + 1 ; } } return begin ; } bool pathExist ( vector < vector < int >> & grid , int mid ) { int n = grid . size (); vector < vector < int >> visited ( n , vector < int > ( n , 0 )); return dfs_helper ( grid , visited , n , mid , 0 , 0 ); } bool dfs_helper ( vector < vector < int >> & grid , vector < vector < int >>& visited , int n , int mid , int i , int j ) { visited [ i ][ j ] = 1 ; for ( int k = 0 ; k < 4 ; ++ k ) { int a = i + x [ k ]; int b = j + y [ k ]; if ( a < 0 || a >= n || b < 0 || b >= n || visited [ a ][ b ] == 1 || grid [ a ][ b ] > mid ) continue ; if ( a == n - 1 && b == n - 1 ) return true ; if ( dfs_helper ( grid , visited , n , mid , a , b )) return true ; } return false ; } };","title":"778. Swim in Rising Water"},{"location":"leetcode/binary-search/notes/#483-smallest-good-base","text":"Solution 1 Binary search This problem requires a bit reasoning to achieve the solution. The starting point is realy mean what's asking by the problem. Here it is asking a minimum base that represent the given number n in a representation like binary representation. For example: 13 = 3^0 + 3^1 + 3^2 so 13 can be representd as 111 (base 3). First of all, there is a special case that such a base may not exist. (precisely, we should seperate the special case when n = (n-1)^0 + (n-1)^1 ; With this special case in mind, we can use binary search to iterate through each m from largest to smallest and check whether the corresponding k is a good base of the given value n . Because when m is the largest, k is the smallest, so if the bianry search find one it must be the smallest k we are looking for. If binary search found nothing, we simpley return the special case n-1 . class Solution { public : string smallestGoodBase ( string n ) { long long num = stoll ( n ); /* for each lenght of the potentional representation, * n = 1 + k + ... + k^{i-1} = (k^i-1)/(k-1), lower bound k is 2, * we have 2^i-1 = n ==> upper bound i = log2(n+1). */ for ( int i = log2 ( num + 1 ); i >= 2 ; -- i ) { /* upper bound is obtained by n = 1 + k + k^2 ... + k^(i-1) > k^(i-1), * n > k^(i-1) ==> k < n^(1/(i-1)); */ long long left = 2 , right = pow ( num , 1.0 / ( i - 1 )) + 1 ; while ( left < right ) { long long mid = left + ( right - left ) / 2 ; long long sum = 0 ; /* calculate i digits value with base \"mid\" */ for ( int j = 0 ; j < i ; ++ j ) { sum = sum * mid + 1 ; } /* binary search for the mid (good base) */ if ( sum == num ) return to_string ( mid ); if ( sum < num ) left = mid + 1 ; else right = mid ; } } return to_string ( num - 1 ); } };","title":"483 Smallest Good Base"},{"location":"leetcode/binary-search/notes/#658-find-k-closest-elements_1","text":"","title":"658. Find K Closest Elements"},{"location":"leetcode/binary-search/notes/#373-find-k-pairs-with-smallest-sums","text":"","title":"373. Find K Pairs with Smallest Sums"},{"location":"leetcode/binary-search/notes/#378-kth-smallest-element-in-a-sorted-matrix","text":"Solution 1 Binary Search The idea of using binary search for this problem my not be straightforward. But the method is very important. The idea is very similar to the problem Search in a rotated sorted array. Because the matrix is sorted row wise and column wise, there are some ordering information we can make use of. Notice we are not try to search using the matrix index, we are searching the matrix element value. Compare to the problem 287. Find the Duplicate Number. The comparison if (count < k) isn't include mid explicitly. but the count is some function f(mid) , with the current mid , the count value is unique and can be use to test a condition that decide which side we can go to shrink the range the target value is possible in. class Solution { public : int kthSmallest ( vector < vector < int >>& matrix , int k ) { int m = matrix . size (); int n = m ? matrix [ 0 ]. size () : 0 ; int start = matrix [ 0 ][ 0 ], end = matrix [ m - 1 ][ n - 1 ]; while ( start < end ) { int mid = start + ( end - start ) / 2 ; int count = 0 ; for ( int i = 0 ; i < m ; ++ i ) { count += upper_bound ( matrix [ i ]. begin (), matrix [ i ]. end (), mid ) - matrix [ i ]. begin (); } if ( count < k ) { // notice no mid here, but count is a function of mid. start = mid + 1 ; } else { end = mid ; } } return start ; } }; Solution 2 Priority Queue Notice when the k <= n^2 , index j < matrix.size() will also make it work. class Solution { public : int kthSmallest ( vector < vector < int >>& matrix , int k ) { priority_queue < int > pq ; for ( int i = 0 ; i < matrix . size (); ++ i ) { for ( int j = 0 ; j < matrix [ 0 ]. size (); ++ j ) { pq . push ( matrix [ i ][ j ]); if ( pq . size () > k ) pq . pop (); } } return pq . top (); } };","title":"378. Kth Smallest Element in a Sorted Matrix"},{"location":"leetcode/binary-search/notes/#668-kth-smallest-number-in-multiplication-table","text":"Solution 1 Binary search While this problem looks simple. But it really isn't unless you observed the following. The condition used for binary search is \"whether there are k smaller elements in the range [start, mid]\". You are looking for the smallest number that has k elements less than or equal to it. Like in the problem Kth Smallest Element in a Sorted Matrix, we will move the number not the index. We move the start or end appropriately based on this condition, if there are more than k, we shrink the range by reduce end: end = mid . If there are less than k numbers, we increase begin hopefully to make mid larger so as to have close to k numbers in the range of [1, mid] . When being == end , we've located the kth number desired. In case k > m*n , we will got begin == end < k , which is not a solution. In counting how many elements less than mid, you have to be clever a bit by using the feature that this matrix is a multiplicative table. That is for row i , you can at most have x/i number smaller than x , why? Follow up: Does the kth element will be in the range of [1, m*n] ? class Solution { public : int findKthNumber ( int m , int n , int k ) { int begin = 1 , end = m * n ; while ( begin < end ) { int mid = begin + ( end - begin ) / 2 ; int count = 0 ; for ( int i = 1 ; i <= m ; ++ i ) { count += min ( mid / i , n ); } if ( count < k ) begin = mid + 1 ; else end = mid ; } return begin ; } };","title":"668. Kth Smallest Number in Multiplication Table"},{"location":"leetcode/binary-search/notes/#719-find-k-th-smallest-pair-distance","text":"Solution 1 Priority Queue TLE class Solution { public : int smallestDistancePair ( vector < int >& nums , int k ) { priority_queue < int > pq ; for ( int i = 0 ; i < nums . size (); ++ i ) { for ( int j = i + 1 ; j < nums . size (); ++ j ) { int dist = abs ( nums [ i ] - nums [ j ]); if ( pq . size () < k ) { pq . push ( dist ); } else if ( dist < pq . top ()) { pq . push ( dist ), pq . pop (); } } } return pq . top (); } }; Solution 2 Binary search Similar to Problem 668. Kth Smallest Number in Multiplication Table . The problem is complicated at the first glance. A brute force solution generates all the absolute distances and then sort to find the kth smallest one. We found it is potentially a searchable scenario if we sort the elements. We have range [min_distance, max_distance] . We search a distance in this range such that there are exactly k pairs distance including itself. If the count of pair distance less than k, we try to increase it buy start = mid + 1 , vice versa. When the binary search loop stops, if the result exist, start point to the distance we are searching. Since this problem guarrantee solution exist, we return start . class Solution { public : int smallestDistancePair ( vector < int >& nums , int k ) { sort ( nums . begin (), nums . end ()); int start = nums [ 1 ] - nums [ 0 ]; for ( int i = 2 ; i < nums . size (); ++ i ) { start = min ( start , nums [ i ] - nums [ i - 1 ]); } int end = nums . back () - nums [ 0 ]; while ( start < end ) { int mid = start + ( end - start ) / 2 ; // count how many absolute differences that <= mid; int count = 0 ; for ( int i = 0 ; i < nums . size (); ++ i ) { int j = i ; while ( j < nums . size () && nums [ j ] - nums [ i ] <= mid ) j ++ ; count += j - i - 1 ; } if ( count < k ) { start = mid + 1 ; } else { end = mid ; } } return start ; } }; Solution 3 Using binary search to optimize the counting You can also write your own binary search routine upper_bound . class Solution { public : int smallestDistancePair ( vector < int >& nums , int k ) { sort ( nums . begin (), nums . end ()); int start = nums [ 1 ] - nums [ 0 ]; for ( int i = 2 ; i < nums . size (); ++ i ) { start = min ( start , nums [ i ] - nums [ i - 1 ]); } int end = nums . back () - nums [ 0 ]; while ( start < end ) { int mid = start + ( end - start ) / 2 ; // count how many absolute differences that <= mid; int count = 0 ; /* for (int i = 0; i < nums.size(); ++i) { int j = i; while (j < nums.size() && nums[j] - nums[i] <= mid) j++; count += j - i - 1; } */ // optimize the counting use binary search (nested binary search) for ( int i = 0 ; i < nums . size (); ++ i ) { auto iter = upper_bound ( nums . begin () + i , nums . end (), nums [ i ] + mid ); count += iter - ( nums . begin () + i ) - 1 ; } if ( count < k ) { start = mid + 1 ; } else { end = mid ; } } return start ; } };","title":"719. Find K-th Smallest Pair Distance"},{"location":"leetcode/binary-search/notes/#786-k-th-smallest-prime-fraction","text":"You should find seek to find the monotonic pattern of the fraction and think how to search it effectively. To use binary search you need to draw an imaginary matrix to consider how can you search effectively. Binary search O(nlog(max_frac)) class Solution { public : vector < int > kthSmallestPrimeFraction ( vector < int >& A , int K ) { int n = A . size (); double l = 0 , r = 1.0 ; while ( l < r ) { double m = ( l + r ) / 2 ; // calculate how many smaller on the right int cnt = 0 ; double mx = 0 ; int p , q ; int j = 1 ; for ( int i = 0 ; i < n - 1 ; ++ i ) { while ( j < n && A [ i ] > A [ j ] * m ) ++ j ; // int j = upper_bound(A.begin() + i, A.end(), A[i] / m) - A.begin(); cnt += ( n - j ); if ( n == j ) break ; double fraction = ( double ) A [ i ] / ( double ) A [ j ]; if ( fraction > mx ) { p = A [ i ]; q = A [ j ]; mx = fraction ; } } if ( cnt == K ) { return { p , q }; } if ( cnt > K ) { r = m ; } else if ( cnt < K ) { l = m ; } } return {}; } };","title":"786. K-th Smallest Prime Fraction"},{"location":"leetcode/binary-search/notes/#1631-path-with-minimum-effort","text":"Solution 1 Binary search + BFS Because we are searching for the smallest effort of all paths. If the proposed solution is not possible, namely, all paths have effort greater than the proposed solution (the proposed value is too small). We need to increase the start in binary search. We are looking for the no-less-than x in the binary search. Solution 2 Dijkstra If you can change the problem into searching a weighted graph with edge weights, which are the absolute differences (effort). Since the weights are all positives, using Dijkstra algorithm can find the shortest path in the measure of effort. C++ Binary search + BFS class Solution { vector < int > dx = { 0 , 1 , 0 , -1 }; vector < int > dy = { -1 , 0 , 1 , 0 }; public : int minimumEffortPath ( vector < vector < int >>& heights ) { int m = heights . size (); int n = m == 0 ? 0 : heights [ 0 ]. size (); int start = 0 , end = 10e6 ; while ( start < end ) { int mid = ( start + end ) / 2 ; if ( ! pathPossible ( heights , mid )) { start = mid + 1 ; } else { end = mid ; } } return start ; } bool pathPossible ( vector < vector < int >>& heights , int val ) { int m = heights . size (); int n = m == 0 ? 0 : heights [ 0 ]. size (); queue < vector < int >> q ; q . push ({ 0 , 0 }); set < int > visited ; visited . insert ( 0 ); while ( ! q . empty ()) { vector < int > t = q . front (); int x = t [ 0 ]; int y = t [ 1 ]; q . pop (); if ( x == m - 1 && y == n - 1 ) return true ; for ( int k = 0 ; k < 4 ; k ++ ) { int a = x + dx [ k ]; int b = y + dy [ k ]; if ( a < 0 || a >= m || b < 0 || b >= n ) continue ; if ( val < abs ( heights [ a ][ b ] - heights [ x ][ y ])) continue ; if ( visited . count ( a * n + b ) > 0 ) continue ; q . push ({ a , b }); visited . insert ( a * n + b ); } } return false ; } }; Java Binary search + BFS class Solution { private int [] d = { 0 , 1 , 0 , - 1 , 0 }; public int minimumEffortPath ( int [][] heights ) { int lo = 0 , hi = 1_000_000 ; while ( lo < hi ) { int effort = lo + ( hi - lo ) / 2 ; if ( isPath ( heights , effort )) { hi = effort ; } else { lo = effort + 1 ; } } return lo ; } private boolean isPath ( int [][] h , int effort ) { int m = h . length , n = h [ 0 ] . length ; Queue < int []> q = new LinkedList <> (); q . offer ( new int [ 2 ] ); Set < Integer > seen = new HashSet <> (); seen . add ( 0 ); while ( ! q . isEmpty ()) { int [] cur = q . poll (); int x = cur [ 0 ] , y = cur [ 1 ] ; if ( x == m - 1 && y == n - 1 ) { return true ; } for ( int k = 0 ; k < 4 ; ++ k ) { int r = x + d [ k ] , c = y + d [ k + 1 ] ; if ( 0 <= r && r < m && 0 <= c && c < n && effort >= Math . abs ( h [ r ][ c ] - h [ x ][ y ] ) && seen . add ( r * n + c )) { q . offer ( new int [] { r , c }); } } } return false ; } } C++ Dijkstra class Solution { public : int minimumEffortPath ( vector < vector < int >>& heights ) { int m = heights . size (); int n = heights [ 0 ]. size (); vector < vector < int >> dist ( m , vector < int > ( n , INT_MAX )); // min distance found so far. priority_queue < pair < int , int > , vector < pair < int , int >> , greater < pair < int , int >>> pq ; int d [ 5 ] = { 0 , 1 , 0 , -1 , 0 }; pq . push ({ 0 , 0 }); // first: min effort, second: encoded (x, y) (=x * n + y); while ( ! pq . empty ()) { pair < int , int > t = pq . top (), pq . pop (); int effort = t . first ; int x = t . second / n ; int y = t . second % n ; if ( x == m - 1 && y == n - 1 ) return effort ; for ( int k = 0 ; k < 4 ; ++ k ) { int a = x + d [ k ]; int b = y + d [ k + 1 if ( a < 0 || a >= m || b < 0 || b >= n ) continue ; // update neighboring node, effort=min effort before visit node(a,b) int currEffort = max ( effort , abs ( heights [ a ][ b ] - heights [ x ][ y ])); if ( dist [ a ][ b ] > currEffort ) { dist [ a ][ b ] = currEffort ; pq . push ({ currEffort , a * n + b }); } } } return -1 ; } }; Python Dijkastra class Solution : def minimumEffortPath ( self , heights : List [ List [ int ]]) -> int : m , n = map ( len , [ heights , heights [ 0 ]]) efforts = [[ math . inf ] * n for _ in range ( m )] efforts [ 0 ][ 0 ] = 0 heap = [( 0 , 0 , 0 )] while heap : effort , x , y = heapq . heappop ( heap ); if ( x , y ) == ( m - 1 , n - 1 ) : return effort for i , j in ( x , y - 1 ), ( x , y + 1 ), ( x - 1 , y ), ( x + 1 , y ) : if i < 0 or i >= m or j < 0 or j >= n : continue currEffort = max ( effort , abs ( heights [ x ][ y ] - heights [ i ][ j ])) if efforts [ i ][ j ] > currEffort : efforts [ i ][ j ] = currEffort heapq . heappush ( heap , ( currEffort , i , j ))","title":"1631. Path With Minimum Effort"},{"location":"leetcode/binary-search/notes/#1102-path-with-maximum-minimum-value","text":"Solution 1 Binary Search + BFS Again, propose a possible value and use a isValid function to check the validity of the proposed solution. Solution 2 BFS + PQ This solution can be thought as a variant of Dijkastra, but not the same. Solution 3 Union Find We need to sort all the vertices by their values in descending order, then choose element from the vertices and use Union-Find to check connectivity of A[0][0] and A[m - 1][n - 1] . C++ Binary Search + BFS class Solution { public : int maximumMinimumPath ( vector < vector < int >>& A ) { int m = A . size (); int n = A [ 0 ]. size (); int start = 0 , end = max ( A [ 0 ][ 0 ], A [ m - 1 ][ n - 1 ]); int res = 0 , mid = 0 ; while ( start < end ) { mid = start + ( end - start ) / 2 ; if ( pathPossible ( A , mid )) { start = mid + 1 ; } else { end = mid ; } } return start ; } bool pathPossible ( vector < vector < int >>& A , int mid ) { int m = A . size (); int n = A [ 0 ]. size (); queue < pair < int , int >> q ; q . emplace ( 0 , 0 ); vector < vector < int >> v ( m , vector < int > ( n , 0 )); v [ 0 ][ 0 ] = 1 ; int d [ 5 ] = { 0 , 1 , 0 , -1 , 0 }; while ( ! q . empty ()) { int x = q . front (). first ; int y = q . front (). second ; q . pop (); if ( x == m - 1 && y == n - 1 ) return true ; for ( int k = 0 ; k < 4 ; ++ k ) { int a = x + d [ k ]; int b = y + d [ k + 1 ]; if ( a < 0 || a >= m || b < 0 || b >= n || v [ a ][ b ] == 1 ) continue ; if ( mid > A [ a ][ b ]) continue ; q . emplace ( a , b ); v [ a ][ b ] = 1 ; } } return false ; } }; C++ BFS + PQ class Solution { public : int maximumMinimumPath ( vector < vector < int >>& A ) { int m = A . size (); int n = A [ 0 ]. size (); int res = INT_MAX ; priority_queue < pair < int , int > , vector < pair < int , int >>> pq ; // max heap. pq . emplace ( A [ 0 ][ 0 ], 0 ); vector < vector < int >> visited ( m , vector < int > ( n , 0 )); visited [ 0 ][ 0 ] = -1 ; int d [ 5 ] = { 0 , 1 , 0 , -1 , 0 }; while ( ! pq . empty ()) { pair < int , int > t = pq . top (); pq . pop (); int cost = t . first ; int x = t . second / n ; int y = t . second % n ; res = min ( res , cost ); if ( x == m - 1 && y == n - 1 ) break ; for ( int k = 0 ; k < 4 ; k ++ ) { int r = x + d [ k ]; int c = y + d [ k + 1 ]; if ( r < 0 || r >= m || c < 0 || c >= n || visited [ r ][ c ] < 0 ) continue ; pq . emplace ( A [ r ][ c ], r * n + c ); visited [ r ][ c ] = -1 ; } } return res ; } }; C++ Union-Find","title":"1102. Path With Maximum Minimum Value"},{"location":"leetcode/binary-search/notes/#category-4-binary-search-as-an-optimization-routine","text":"","title":"Category 4 Binary search as an optimization routine"},{"location":"leetcode/binary-search/notes/#300-longest-increasing-subsequence","text":"Solution 1 DP The base case is single char. f[i] is the length of LIS from the begining. class Solution { public : int lengthOfLIS ( vector < int >& nums ) { if ( n == nums . size ()) return 0 ; int f [ n ] = { 0 }; int res = 0 ; for ( int j = 0 ; j < n ; j ++ ) { f [ j ] = 1 ; for ( int i = 0 ; i < j ; i ++ ) { if ( nums [ i ] < nums [ j ] && f [ i ] + 1 > f [ j ]) f [ j ] = f [ i ] + 1 ; } res = max ( res , f [ j ]); } return res ; } }; Solution 2 Using binary search The DP solution is O(n^2) . Using binary search could reduce to O(nlogn) . Binary search solution analysis. For each i , we are looking for the largest f value that has smallest A value. For example, A[0] = 5 could be ignored because of its f value is same as A[1] = 1 , which is smaller. In searching for the LIS, we prefer a small ending value when the length is the same. The following solution using a vector b to record the minimum A value for each length of LIS ( f value), we use binary search to find the last value in b that smaller that current value A[i] . If we found such a value in b , we use A[i] to replace the value next to the found value in b ). i 0 1 2 3 4 5 6 7 A 5 1 3 7 6 4 2 10 f 1 1 2 3 3 3 2 4 f[1] = 1, a[1] = 1 f[6] = 2, a[6] = 2 f[5] = 3, a[5] = 4 f[7] = 4, a[7] = 10 class Solution { public : int lengthOfLIS ( vector < int >& nums ) { vector < int > b ; for ( int i = 0 ; i < nums . size (); ++ i ) { int l = 0 , r = b . size (); while ( l < r ) { int m = l + ( r - l ) / 2 ; if ( b [ m ] < nums [ i ]) { // nums[i] is the target l = m + 1 ; } else { r = m ; } } if ( l == b . size ()) // nums[i] greater than all element in b b . push_back ( nums [ i ]); else // begin point to next element no less than the target nums[i]. b [ l ] = nums [ i ]; } return b . size (); } }; Alternatively, we could use lower_bound . class Solution { public : int lengthOfLIS ( vector < int >& nums ) { vector < int > b ; for ( int i = 0 ; i < nums . size (); ++ i ) { int l = lower_bound ( b . begin (), b . end (), nums [ i ]) - b . begin (); if ( l == b . size ()) // nums[i] greater than all element in b b . push_back ( nums [ i ]); else // begin point to next element no less than the target nums[i]. b [ l ] = nums [ i ]; } return b . size (); } };","title":"300 Longest Increasing Subsequence"},{"location":"leetcode/binary-search/notes/#354-russian-doll-envelopes","text":"","title":"354. Russian Doll Envelopes"},{"location":"leetcode/binary-search/notes/#174-dungeon-game","text":"","title":"174. Dungeon Game"},{"location":"leetcode/breadth-first-search/notes/","text":"Breadth-First Search \u00b6 Algorithm \u00b6 BFS search a graph by first explore all the vertices that are adjacent to the source vertex s , then explore the adjacent vertices of those already examined in layer-by-layer fashion until all the vertices are visited. The psudo code BFS (graph G, start vertex s) [ all nodes initially unexplored ] -- mark s as explored -- let Q = queue data structure, initialized with s -- while Q not empty: -- remove the first node of Q, call it v -- for each edge(v,w) : -- if w unexplored -- mark w as explored -- add w to Q (at the end) BFS properties \u00b6 Runtime complexity \\Theta(|V| + |E|) \\Theta(|V| + |E|) . It calculate shortest path from the source s s to the vertex v v . It build a breadth-first tree, CLRS called predecessor subgraph. If edge (u, v) (u, v) in the BFS tree, the simple path from source vertext s s to u u is equal to the simple path from s s to v v , or differ by 1. Simple path from a source vertex s s to a destination vertex v v is equal to the later\u2019s level label . If v_1, v_2, \\cdots, v_n v_1, v_2, \\cdots, v_n is the vertex enqueued in a certain point, v_1 v_1 is the head and v_n v_n is the back, then the level label of v_1 v_1 is either equal to level label of v_n v_n or less by 1. Also the level label of all the vertices in the queue is non-decreasing from head to tail. In other words, at a certain time the enqueued vertices at most comes from two different levels. Search \u00b6 unweighted edges Bi-directional search, (Speed up Dijkastra of 6.006) At all time, the level of nodes in queue at most differ by 1 The relationship between BFS and DFS (The Maze II) BFS tips for board and grid problems When push a pair of coordinate to a BFS queue, one way of doing it is using pair<int, int> . Another way is to convert to a number by p = i * n + j , and retrieve the coordinate by i = p / n and j = p % n . The BFS starting point and \"target\" are important, give a second thought about where should the search start and how it terminate. visitted[m][n] is not necessary sometimes. See whether you can modify the original borad/matrix/grid/maze to reflect whether the element has already been accessed or not. In a borad or maze like input, searching neighbor cells requires you to manipulate indexes for accessing its neighbors. We do this by defining two \"offset\" coordinates arrays. int x[4] = {-1, 0, 1, 0}; int y[4] = {0, 1, 0, -1}; The BFS queue doesn't necessarily start with one element pushed. It could be \"multi-end\" BFS . Like in problme Pacific Atlantic Water Flow and Longest Increasing Path in a Matrix , one could init the queue with all possible starting node then in use a while to visit all the node towards finding the optimal path. The \"multi-end\" BFS not only can make the BFS level by level (each round of for loop visited one level of nodes), but also can make use some optimal feature of BFS, like the BFS solution of Walls and Gates. The BFS while loop could include another while loop to interate throught total q.size() nodes. This is different from the \"oracle\" BFS solution from CRLS, but sometime could be useful. For example: Longest Increasing Path in a Matrix . The Maze \u00b6 DFS class Solution { public : bool hasPath ( vector < vector < int >>& maze , vector < int >& start , vector < int >& destination ) { int m = maze . size (); int n = m ? maze [ 0 ]. size () : 0 ; vector < vector < bool >> visited ( m , vector < bool > ( n , false )); return dfs_helper ( maze , start , destination , visited ); } void dfs_helper ( vector < vector < int >>& maze , vector < int >& start , vector < int >& destination , vector < vector < bool >>& visited ) { if ( start [ 0 ] == destination [ 0 ] && start [ 1 ] == destination [ 1 ]) { return true ; } int x [ 4 ] = { 0 , -1 , 0 , 1 }; int y [ 4 ] = { -1 , 0 , 1 , 0 }; bool res = false ; for ( int k = 0 ; k < 4 ; k ++ ) { int a = start [ 0 ] + x [ k ]; int b = start [ 1 ] + y [ k ]; // walk untill hit the wall while ( a >= 0 && b >= 0 && a < maze . size () && b < maze [ 0 ]. size90 && maze [ a ][ b ] == 0 ) { a += x [ k ]; b += y [ k ]; } if ( ! visited [ a - x [ k ]][ b - y [ k ]]) { visited [ a - x [ k ]][ b - y [ k ]] = true ; vector < int > new_start ({ a - x [ k ], b - y [ k ]}); res != dfs_helper ( maze , new_start , destination , visited ); } } return res ; } }; === \"BFS\" ```c++ class Solution { public: bool hasPath(vector<vector<int>>&maze, vector<int>& start, vector<int>& destination) { int m = maze.size(); int n = m ? maze[0].size() : 0; vector<vector<bool>> visited(m, vector<bool>(n, false)); queue<vector<int>> q; q.push(start); int x[4] = {0, -1, 0, 1}; int y[4] = {-1, 0, 1, 0}; while (!q.empty()) { vector<int> s = q.front(); q.pop(); if (s[0] == destination[0] && s[1] == destination[1]) { return true; } for (int k = 0; k < 4; k++) { int a = s[0] + x[k]; int b = s[1] + y[k]; while (a >= 0 && b >= 0 && a < m && b < n && maze[a][b] == 0) { a += x[k]; b += y[k]; } if (!visited[a - x[k]][b - y[k]]) { q.push(vector<int>({a - x[k], b - y[k]})); visited[a - x[k]][b - y[k]] = true; } } } return false; } }; ``` The Maze II \u00b6 BFS can computer shortest path, we need additional memory to keep the optimal values. we update the corresponding destination cell in distance vector when we found a smaller distane can reach it from any direction. the final result will be in distance[destination[0]][destination[1]] . If we don't want to use so much memory, we can use a flag to mark when the BFS switch levels. This solution is essentially to use BFS to solve the weighted graph problem as Victor mentioned in 6006 R15. Compare this solution to the Dijkastra solution. The update distance matrix is essentially the relaxiation step. class Solution { public : int shortestDistance ( vector < vector < int >>& maze , vector < int >& start , vector < int >& destination ) { int m = maze . size (); int n = m ? maze [ 0 ]. size () : 0 ; if ( m == 0 ) return 0 ; vector < vector < int >> distance ( m , vector < int > ( n , INT_MAX )); queue < vector < int >> q ; q . push ( start ); distance [ start [ 0 ]][ start [ 1 ]] = 0 ; int x [ 4 ] = { -1 , 0 , 1 , 0 }; int y [ 4 ] = { 0 , 1 , 0 , -1 }; while ( ! q . empty ()) { vector < int > s = q . front (); q . pop (); for ( int k = 0 ; k < 4 ; k ++ ) { int a = s [ 0 ] + x [ k ]; int b = s [ 1 ] + y [ k ]; int count = 0 ; while ( a >= 0 && b >= 0 && a < m && b < n && maze [ a ][ b ] == 0 ) { a += x [ k ]; b += y [ k ]; count ++ ; } // notice here is a optimize if ( distance [ s [ 0 ]][ s [ 1 ]] + count < distance [ a - x [ k ]][ b - y [ k ]]) { distance [ a - x [ k ]][ b - y [ k ]] = distance [ s [ 0 ]][ s [ 1 ]] + count ; q . push ( vector < int > ({ a - x [ k ], b - y [ k ]})); /* could use emplace({a - x[k], b - y[k]}) */ } } } return distance [ destination [ 0 ]][ destination [ 1 ]] == INT_MAX ? -1 : distance [ destination [ 0 ]][ destination [ 1 ]]; } }; Note Based on the BFS solution, we can use a priority queue. Then the problem is very similar to the Dijkastra algorithm. The discussion about the improvement in leetcode forum is very interesting. The Maze III \u00b6 This problem is aksing for output the shortest path to reach the destination. The idea is to use a distance[i][i] and result[i][j] . we should keep update the distance[i][j] and result[i][j] with smaller distance and lexicographical order. The hardest part is that there is two constrains. While using BFS, How could we ensure the two constrains are met, and also we are not missing any possible path. (i.e. missed to push a path to the BFS queue.) A trick we can make use of is we searching in the lexicographically order. such as down , left , right , up . However, we face a problem: the smallest lexicon order result may not be the shortest distance . Think this through. We need to used additional memory to record that piece of information beside the memory used to keep the minimum distance. BFS class Solution { public : string findShortestWay ( vector < vector < int >>& maze , vector < int >& start , vector < int >& destination ) { int m = maze . size (); int n = m ? maze [ 0 ]. size () : 0 ; if ( m == 0 ) return 0 ; vector < vector < int >> distance ( m , vector < int > ( n , INT_MAX )); vector < vector < string >> result ( m , vector < string > ( n , \"\" )); queue < vector < int >> q ; q . push ( start ); distance [ start [ 0 ]][ start [ 1 ]] = 0 ; char path [ 4 ] = { 'd' , 'l' , 'r' , 'u' }; int x [ 4 ] = { 1 , 0 , 0 , -1 }; int y [ 4 ] = { 0 , -1 , 1 , 0 }; while ( ! q . empty ()) { vector < int > s = q . front (); q . pop (); for ( int k = 0 ; k < 4 ; k ++ ) { int a = s [ 0 ] + x [ k ]; int b = s [ 1 ] + y [ k ]; int count = 0 ; /* continue if no wall */ while ( a >= 0 && b >= 0 && a < m && b < n && maze [ a ][ b ] == 0 ) { /* run over the destination */ if ( a == destination [ 0 ] && b == destination [ 1 ]){ if ( distance [ s [ 0 ]][ s [ 1 ]] + count < distance [ a ][ b ]) { distance [ a ][ b ] = distance [ s [ 0 ]][ s [ 1 ]] + count ; result [ a ][ b ] = result [ s [ 0 ]][ s [ 1 ]] + path [ k ]; } else if ( distance [ s [ 0 ]][ s [ 1 ]] + count == distance [ a ][ b ]) { string tmp = result [ s [ 0 ]][ s [ 1 ]] + path [ k ]; /* same distance, smaller lexicon */ if ( tmp . compare ( result [ a ][ b ]) < 0 ) { result [ a ][ b ] = tmp ; } } } a += x [ k ]; b += y [ k ]; count ++ ; } /* hit a wall, have to retreat \"0\", current a, b is in wall or off maze */ if ( distance [ s [ 0 ]][ s [ 1 ]] + count < distance [ a - x [ k ]][ b - y [ k ]]) { distance [ a - x [ k ]][ b - y [ k ]] = distance [ s [ 0 ]][ s [ 1 ]] + count ; result [ a - x [ k ]][ b - y [ k ]] = result [ s [ 0 ]][ s [ 1 ]] + path [ k ]; q . push ( vector < int > ({ a - x [ k ], b - y [ k ]})); } else if ( distance [ s [ 0 ]][ s [ 1 ]] + count == distance [ a - x [ k ]][ b - y [ k ]]) { string tmp = result [ s [ 0 ]][ s [ 1 ]] + path [ k ]; /* same distance, smaller lexicon */ if ( tmp . compare ( result [ a - x [ k ]][ b - y [ k ]]) < 0 ) { result [ a - x [ k ]][ b - y [ k ]] = tmp ; /* possible path, need to search it again */ q . push ( vector < int > ({ a - x [ k ], b - y [ k ]})); } } } } return distance [ destination [ 0 ]][ destination [ 1 ]] == INT_MAX ? \"impossible\" : result [ destination [ 0 ]][ destination [ 1 ]]; } }; DFS class Solution { public : string findShortestWay ( vector < vector < int >>& maze , vector < int >& ball , vector < int >& hole ) { return roll ( maze , ball [ 0 ], ball [ 1 ], hole , 0 , 0 , 0 , \"\" , pair < string , int > () = { \"impossible\" , INT_MAX }); } string roll ( vector < vector < int >>& maze , int rowBall , int colBall , const vector < int >& hole , int distRow , int distCol , int steps , const string & path , pair < string , int >& res ) { if ( steps < res . second ) { if ( distRow != 0 || distCol != 0 ) { while (( rowBall + distRow ) >= 0 && ( rowBall + distRow ) < maze . size () && ( colBall + distCol ) >= 0 && ( colBall + distCol ) < maze [ 0 ]. size () && maze [ rowBall + distRow ][ colBall + distCol ] != 1 ) { rowBall += distRow ; colBall += distCol ; ++ steps ; if ( rowBall == hole [ 0 ] && colBall == hole [ 1 ] && steps < res . second ) { res = { path , steps }; } } } if ( maze [ rowBall ][ colBall ] == 0 || steps + 2 < maze [ rowBall ][ colBall ]) { maze [ rowBall ][ colBall ] = steps + 2 ; if ( distRow == 0 ) roll ( maze , rowBall , colBall , hole , 1 , 0 , steps , path + \"d\" , res ); if ( distCol == 0 ) roll ( maze , rowBall , colBall , hole , 0 , -1 , steps , path + \"l\" , res ); if ( distCol == 0 ) roll ( maze , rowBall , colBall , hole , 0 , 1 , steps , path + \"r\" , res ); if ( distRow == 0 ) roll ( maze , rowBall , colBall , hole , -1 , 0 , steps , path + \"u\" , res ); } } return res . first ; } }; Dijkstra algoirthm class Vertex { public : int x , y , dist ; std :: string path ; Vertex ( int x , int y , int dist , std :: string path ) { this -> x = x ; this -> y = y ; this -> dist = dist ; this -> path = path ; } Vertex ( int x , int y ) { this -> x = x ; this -> y = y ; this -> dist = INT_MAX ; this -> path = \"\" ; } bool equals ( Vertex v ) { return this -> x == v . x && this -> y == v . y ; } bool equals ( int x , int y ) { return this -> x == x && this -> y == y ; } }; class Solution { private : // coordinates and path strings for the four directions. std :: vector < int > directions = { -1 , 0 , 1 , 0 , -1 }; std :: vector < std :: string > paths = { \"u\" , \"r\" , \"d\" , \"l\" }; // can't roll if it's boundry, wall or hole. bool canRoll ( vector < vector < int >> & maze , Vertex hole , int x , int y ) { int m = maze . size (), n = maze [ 0 ]. size (); if ( x >= 0 && x < m && y >= 0 && y < n ) return maze [ x ][ y ] != 1 && ! hole . equals ( x , y ); else return false ; } Vertex roll ( vector < vector < int >> & maze , Vertex curr , Vertex hole , int dx , int dy , std :: string path ) { int x = curr . x , y = curr . y , steps = curr . dist ; while ( canRoll ( maze , hole , x + dx , y + dy )) { x += dx ; y += dy ; steps ++ ; } // if couldn't roll due to hole, then return vertex corresponding to the hole. if ( hole . equals ( x + dx , y + dy )) { x += dx ; y += dy ; } return Vertex ( x , y , steps , curr . path + path ); } public : /* * to form a pq based on the increasing order of distances, and if same * distance then sorting based on lexicographic order. */ struct Compare { bool operator ()( const Vertex & a , const Vertex & b ) { return a . dist > b . dist || a . dist == b . dist && a . path > b . path ; } }; /* * Applying Dijkstra's algorithm here. * We treat hole as a gate here. The trick is look for holes while rolling * the back in a particular direction. So, the pq here contains the vertices * before gates as well as the holes. So, using dijkstra, when we pop a hole * from pq, we know its that. */ string findShortestWay ( vector < vector < int >>& maze , vector < int >& ball , vector < int >& hole ) { int m = maze . size (), n = maze [ 0 ]. size (); std :: priority_queue < Vertex , std :: vector < Vertex > , Compare > pq ; Vertex startVertex ( ball [ 0 ], ball [ 1 ], 0 , \"\" ), holeVertex ( hole [ 0 ], hole [ 1 ]); std :: unordered_set < int > visited ; pq . push ( startVertex ); while ( ! pq . empty ()) { Vertex curr = pq . top (); visited . insert ( curr . x * n + curr . y ); pq . pop (); if ( curr . equals ( holeVertex )) { return curr . path ; } for ( int i = 0 ; i < directions . size () - 1 ; i ++ ) { Vertex newStart = roll ( maze , curr , holeVertex , directions [ i ], directions [ i + 1 ], paths [ i ]); if ( visited . find ( newStart . x * n + newStart . y ) == visited . end ()) pq . push ( newStart ); } } return \"impossible\" ; } }; Pacific Atlantic Water Flow \u00b6 Using DFS and two matrices to record the states and then to loop through the result check for both possible flow cases. The key is how to start the DFS? should we initiate the DFS in each (i, j) ? Here we can see it is more efficient if that we start from the edges. Notice we mark visited[i][j] = true immediately enter the helper function. DFS solution class Solution { public : vector < pair < int , int >> pacificAtlantic ( vector < vector < int >>& matrix ) { int m = matrix . size (); int n = m ? matrix [ 0 ]. size () : 0 ; vector < vector < int >> p_visited ( m , vector < int > ( n , 0 )); vector < vector < int >> a_visited ( m , vector < int > ( n , 0 )); vector < pair < int , int >> res ; for ( int i = 0 ; i < m ; i ++ ) { helper ( matrix , i , 0 , p_visited ); helper ( matrix , i , n - 1 , a_visited ); } for ( int j = 0 ; j < n ; j ++ ) { helper ( matrix , 0 , j , p_visited ); helper ( matrix , m - 1 , j , a_visited ); } for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { if ( p_visited [ i ][ j ] && a_visited [ i ][ j ]) { //res.push_back(pair<int, int>(i, j)); res . push_back ({ i , j }); } } } return res ; } void helper ( vector < vector < int >>& mat , int i , int j , vector < vector < int >>& visited ) { int m = mat . size (); int n = m ? mat [ 0 ]. size () : 0 ; int x [] = { 0 , -1 , 0 , 1 }; int y [] = { -1 , 0 , 1 , 0 }; visited [ i ][ j ] = true ; /* visite mat[i][j] first */ /* and then explore its neighbors */ for ( int k = 0 ; k < 4 ; k ++ ) { int p = i + x [ k ]; int q = j + y [ k ]; if ( p >= 0 && p < m && q >= 0 && q < n && ! visited [ p ][ q ] && mat [ i ][ j ] <= mat [ p ][ q ]) { helper ( mat , p , q , visited ); } } } }; BFS solution class Solution { public : vector < pair < int , int >> pacificAtlantic ( vector < vector < int >>& matrix ) { int m = matrix . size (); int n = m ? matrix [ 0 ]. size () : 0 ; vector < vector < bool >> vPacific ( m , vector < bool > ( n , false )); vector < vector < bool >> vAtlantic ( m , vector < bool > ( n , false )); queue < pair < int , int >> qPacific ; queue < pair < int , int >> qAtlantic ; for ( int i = 0 ; i < m ; ++ i ) { qPacific . push ({ i , 0 }); vPacific [ i ][ 0 ] = true ; qAtlantic . push ({ i , n - 1 }); vAtlantic [ i ][ n - 1 ] = true ; } for ( int i = 0 ; i < n ; ++ i ) { qPacific . push ({ 0 , i }); vPacific [ 0 ][ i ] = true ; qAtlantic . push ({ m - 1 , i }); vAtlantic [ m - 1 ][ i ] = true ; } bfs_helper ( matrix , qPacific , vPacific ); bfs_helper ( matrix , qAtlantic , vAtlantic ); vector < pair < int , int >> res ; for ( int i = 0 ; i < m ; ++ i ) { for ( int j = 0 ; j < n ; ++ j ) { if ( vPacific [ i ][ j ] && vAtlantic [ i ][ j ]) { res . push_back ({ i , j }); } } } return res ; } void bfs_helper ( vector < vector < int >>& matrix , queue < pair < int , int >>& q , vector < vector < bool >>& visited ) { int m = matrix . size (); int n = m ? matrix [ 0 ]. size () : 0 ; int x [ 4 ] = { 0 , -1 , 0 , 1 }; int y [ 4 ] = { -1 , 0 , 1 , 0 }; while ( ! q . empty ()) { int a = q . front (). first ; int b = q . front (). second ; q . pop (); for ( int k = 0 ; k < 4 ; ++ k ) { int u = a + x [ k ]; int v = b + y [ k ]; if ( u < 0 || u >= m || v < 0 || v >= n || visited [ u ][ v ] || matrix [ a ][ b ] > matrix [ u ][ v ]) continue ; visited [ u ][ v ] = true ; q . push ({ u , v }); } } } }; Warning Notice your terminate condition matrix[a][b] > matrix[u][v] in the DFS solution indicates your search direction cannot not go from high to low. The search direction is from low to high (or equal). This is to optimize the search since we don't have to revisite a node later. Longest Increasing Path in a Matrix \u00b6 BFS solution Each cell should possibly be the starting cell of the longest increasing path. So we have to search starting from each one of the cells. It seems the running time is quadratic. How to optimize? use markers matrix to mark whether the matrix[i][j] have greater neighbors, left , top , right , bottom -> 1, 2, 4, 8. The do BFS only looking for the directions that have greater neighbors. The idea is very similar to the \"branch cutting\" in DFS. class Solution { public : int longestIncreasingPath ( vector < vector < int >>& matrix ) { int m = matrix . size (); int n = m ? matrix [ 0 ]. size () : 0 ; vector < vector < int >> markers ( m , vector < int > ( n , 0 )); int x [ 4 ] = { -1 , 0 , 1 , 0 }; int y [ 4 ] = { 0 , 1 , 0 , -1 }; int res = 0 ; for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { for ( int k = 0 ; k < 4 ; k ++ ) { int p = i + x [ k ]; int q = j + y [ k ]; if ( p < 0 || p >= m || q < 0 || q >= m ) continue ; if ( matrix [ p ][ q ] > matrix [ i ][ j ]) markers [ i ][ j ] ^= ( 1 << k ); } } } for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { int level = 0 ; if ( markers [ i ][ j ] > 0 ) { level = 0 ; queue < int > q ; q . push ( i * n + j ); while ( ! q . empty ()) { int a = q . front () / n ; int b = q . front () % n ; q . pop (); bool flag = true ; for ( int k = 0 ; k < 4 ; k ++ ) { int u = a + x [ k ]; int v = b + y [ k ]; if ( u < 0 || u >= m || v < 0 || v >= m ) continue ; if ( has_greater_neighbor ( markers [ a ][ b ], k )) { q . push ( u * n + v ); if ( flag ) { level ++ ; flag = false ; } } } } } res = max ( res , level ); } } return res ; } bool has_greater_neighbor ( int flags , int k ) { return ( flags >> k ) & 1 ; } }; BFS solution II BFS can find the shortest path. How can you use BFS to find the longest path? In this problem, we are not use BFS's shortest path properpy, we make use of BFS's property of finding the connected components. We can imagine each increasing path is a connected component, we need to find out the largest one. Notice how the solution construct the search structure (for loop inside the while loop). By construct the search structure in this way, the len is actually the level of nodes from all the peaks identified in the first nested for loops. Java BFS class Solution { public int longestIncreasingPath ( int [][] matrix ) { private final int [][] dirs = {{ 1 , 0 }, { - 1 , 0 },{ 0 , 1 }, { 0 , - 1 }}; private boolean ispeak ( int [][] matrix , boolean [][] marked , int i , int j ) { if ( i > 0 && ! marked [ i - 1 ][ j ] && matrix [ i - 1 ][ j ] > matrix [ i ][ j ] ) return false ; if ( i < matrix . length - 1 && ! marked [ i + 1 ][ j ] && matrix [ i + 1 ][ j ] > matrix [ i ][ j ] ) return false ; if ( j > 0 && ! marked [ i ][ j - 1 ] && matrix [ i ][ j - 1 ] > matrix [ i ][ j ] ) return false ; if ( j < matrix [ 0 ] . length - 1 && ! marked [ i ][ j + 1 ] && matrix [ i ][ j + 1 ] > matrix [ i ][ j ] ) return false ; return true ; } public int longestIncreasingPath ( int [][] matrix ) { if ( matrix . length == 0 || matrix [ 0 ] . length == 0 ) return 0 ; int len = 0 ; LinkedList < int []> queue = new LinkedList <> (); boolean [][] marked = new boolean [ matrix . length ][ matrix [ 0 ] . length ] ; for ( int i = 0 ; i < matrix . length ; i ++ ) { for ( int j = 0 ; j < matrix [ 0 ] . length ; j ++ ) { if ( ispeak ( matrix , marked , i , j )) queue . add ( new int [] { i , j }); } } while ( ! queue . isEmpty ()) { len ++ ; int size = queue . size (); for ( int i = 0 ; i < size ; i ++ ) { int [] p = queue . poll (); marked [ p [ 0 ]][ p [ 1 ]] = true ; for ( int j = 0 ; j < 4 ; j ++ ) { int r = p [ 0 ]+ dirs [ j ][ 0 ] , c = p [ 1 ]+ dirs [ j ][ 1 ] ; if ( r >= 0 && r < matrix . length && c >= 0 && c < matrix [ 0 ] . length && ! marked [ r ][ c ] && ispeak ( matrix , marked , r , c )) { if ( matrix [ r ][ c ] != matrix [ p [ 0 ]][ p [ 1 ]] ) queue . add ( new int [] { r , c }); } } } } return len ; } } } Note This BFS is not the same as the classic BFS routine from CLRS. It is a generalized BFS approach, which can be transformed to a problem to find the connected components. DFS solution we can also use DFS for this problem, it turns out DFS is the simpliest solution. DFS solution class Solution { public : int longestIncreasingPath ( vector < vector < int >>& matrix ) { int m = matrix . size (); int n = m ? matrix [ 0 ]. size () : 0 ; if ( n == 0 ) return 0 ; vector < vector < int >> cache ( m , vector < int > ( n , 0 )); int res = 1 ; for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { int len = dfs_helper ( matrix , i , j , m , n , cache ); res = max ( res , len ); } } return res ; } int dfs_helper ( vector < vector < int >>& matrix , int i , int j , int m , int n , vector < vector < int >>& cache ) { if ( cache [ i ][ j ]) return cache [ i ][ j ]; int x [ 4 ] = { -1 , 0 , 1 , 0 }; int y [ 4 ] = { 0 , 1 , 0 , -1 }; int res = 1 ; for ( int k = 0 ; k < 4 ; k ++ ) { int p = i + x [ k ]; int q = j + y [ k ]; if ( p < 0 || p >= m || q < 0 || q >= n || matrix [ p ][ q ] <= matrix [ i ][ j ]) continue ; int len = 1 + dfs_helper ( matrix , p , q , m , n , cache ); res = max ( res , len ); } cache [ i ][ j ] = res ; return res ; } }; Walls and Gates \u00b6 BFS solution We push all the gate to the queue at once, then BFS to update the room. This makes use of the optimal character of BFS and bring us some optimization. Proof correctness: If init the queue with all gates, BFS alternate between gates, once BFS explore one level of rooms for a particular gate, it switches to explore the same level or next level rooms for another gate. The level is the distance from room to gate. Suppose room R1 is visited at the first time from G1, then if R1 is visited again via G2, the distance from G2 to R1 is possible to be the same as G1 to R1, or greater than G1 to R1 by 1. The assigned nearest distance from room to gate never be able to reduce further in future. Then the first assigned distance value for a room will be the nearest one. DFS solution I initially start with each INF cell and search for 0. the issue is we have to pass the original i , j , in case we reach the base case to update the INF cell. Another trick is how to handle the repeated searching the same empty room. Extra space like visited[m][n] will work. but a better way to handle it is what we did in the below solution. We check if d > ma[i][j] , if d > 0 , the d > mat[i][j] also make sure we update mat[i][j] with a smaller value because the initial value of INF cell is INT_MAX . This is different from DFS because DFS can ensure the shortest path. The time complexity is O(m^2n^2) O(m^2n^2) . The space complexity is from the stack. It is definitely not constant. C++ BFS class Solution { public : void wallsAndGates ( vector < vector < int >>& rooms ) { int m = rooms . size (); int n = m ? rooms [ 0 ]. size () : 0 ; int x [ 4 ] = { -1 , 0 , 1 , 0 }; int y [ 4 ] = { 0 , 1 , 0 , -1 }; queue < pair < int , int >> q ; for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { if ( rooms [ i ][ j ] == 0 ) { // start from each gate q . push ({ i , j }); } } } while ( ! q . empty ()) { int a = q . front (). first ; int b = q . front (). second ; q . pop (); for ( int k = 0 ; k < 4 ; k ++ ) { int u = a + x [ k ]; int v = b + y [ k ]; if ( u < 0 || u >= m || v < 0 || v >= n || rooms [ u ][ v ] != INT_MAX ) { continue ; } rooms [ u ][ v ] = rooms [ a ][ b ] + 1 ; q . push ({ u , v }); } } } }; C++ DFS class Solution { public : void wallsAndGates ( vector < vector < int >>& rooms ) { int m = rooms . size (); int n = m ? rooms [ 0 ]. size () : 0 ; for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { if ( rooms [ i ][ j ] == 0 ) { // start from each gate, not room helper ( rooms , i , j , 0 ); } } } } /* i, j are the starting point. */ void helper ( vector < vector < int >>& mat , int i , int j , int d ) { int m = mat . size (); int n = m ? mat [ 0 ]. size () : 0 ; int x [ 4 ] = { -1 , 0 , 1 , 0 }; int y [ 4 ] = { 0 , 1 , 0 , -1 }; if ( i < 0 || i >= m || j < 0 || j >= n || mat [ i ][ j ] < d ) { return ; } // have already made sure d < mat[i][j] mat [ i ][ j ] = d ; for ( int k = 0 ; k < 4 ; k ++ ) { int a = i + x [ k ]; int b = j + y [ k ]; helper ( mat , a , b , d + 1 ); } } }; multi-end BFS Notice this implementation of BFS is different from triditional BFS. It start with a queue that have more than one element. Some people call this multi-end BFS . It is perform better than the original BFS, The time complexity is O(mn) O(mn) 01 Matrix \u00b6 Surrounded Regions \u00b6 We first mark the boundary O , then start BFS from each O . If it cannot reach the special marks, we capture the cell. This solution is starting from each of inner O and search the mark at the boundary. The solution is TLE due to more inner O 's than boundary O 's. class Solution { public : void solve ( vector < vector < char >>& board ) { int m = board . size (); int n = m ? board [ 0 ]. size () : 0 ; int x [ 4 ] = { -1 , 0 , 1 , 0 }; int y [ 4 ] = { 0 , 1 , 0 , -1 }; for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { if ( i == 0 || j == 0 || i == m - 1 || j == n - 1 ) { board [ i ][ j ] = board [ i ][ j ] == 'O' ? '-' : 'X' ; } } } for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { if ( board [ i ][ j ] == 'O' ) { bool flag = true ; queue < pair < int , int >> q ; q . push ({ i , j }); while ( ! q . empty ()) { int a = q . front (). first , b = q . front (). second ; q . pop (); for ( int k = 0 ; k < 4 ; k ++ ) { int u = a + x [ k ]; int v = b + y [ k ]; if ( u < 0 || u >= m || v < 0 || v >= n || board [ u ][ v ] == 'X' ) { continue ; } if ( board [ u ][ v ] == 'O' ) { q . push ({ u , v }); } if ( board [ u ][ v ] == '-' ) { flag = false ; break ; } } if ( ! flag ) break ; } if ( flag ) board [ i ][ j ] = 'X' ; } } } for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { if ( i == 0 || j == 0 || i == m - 1 || j == n - 1 ) { board [ i ][ j ] = board [ i ][ j ] == '-' ? 'O' : 'X' ; } } } } }; Instead searching whether a 'O' have an outlet. We can discover all 'O's that lead to edge 'O's and mark them, then all the rest 'O's are captured. Specifically, starting from every 'O' at the edge, using BFS to search all the 'O's outside the 'X's, all these 'O's that cannot be captured. Compare to solution 1, this ideas is thinking \u201cout of the box\u201d, instead of thinking inside 'O's, we take care outside 'O's first. Similar problems: Walls and Gates , Shortest Distance from All Buildings . class Solution { public : void solve ( vector < vector < char >>& board ) { int m = board . size (); int n = m ? board [ 0 ]. size () : 0 ; // bfs search from boundary 'O's for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { if ( i == 0 || j == 0 || i == m - 1 || j == n - 1 ) { if ( board [ i ][ j ] == 'O' ) bfs_helper ( board , i , j ); } } } for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { if ( board [ i ][ j ] == 'O' ) { board [ i ][ j ] = 'X' ; // capture all inner 'O's } else if ( board [ i ][ j ] == '-' ) { board [ i ][ j ] = 'O' ; // flip back the marks } } } } // helper function to mark \"outside\" 'O' as '-' void bfs_helper ( vector < vector < char >>& board , int i , int j ) { int m = board . size (); int n = m ? board [ 0 ]. size () : 0 ; int x [ 4 ] = { -1 , 0 , 1 , 0 }; int y [ 4 ] = { 0 , 1 , 0 , -1 }; board [ i ][ j ] = '-' ; queue < pair < int , int >> q ; q . push ({ i , j }); while ( ! q . empty ()) { int a = q . front (). first , b = q . front (). second ; q . pop (); for ( int k = 0 ; k < 4 ; k ++ ) { int u = a + x [ k ]; int v = b + y [ k ]; if ( u >= 0 && u < m && v >= 0 && v < n && board [ u ][ v ] == 'O' ) { q . push ({ u , v }); board [ u ][ v ] = '-' ; } } } } }; Tips \u4ee5\u4e0a\u4e24\u4e2a solution \u8db3\u4ee5\u8bf4\u660eBFS\u7684\u641c\u7d22\u65b9\u5411\u548c\u641c\u7d22\u76ee\u6807\u975e\u5e38\u91cd\u8981\u3002\u5de7\u5999\u8f6c\u5316\u641c\u7d22\u76ee\u6807\u53ef\u4ee5\u4f7f\u5f97code\u5f88\u597d\u5199\uff0c\u66f4\u7b80\u6d01\u3002 Best Meeting Point \u00b6 At the first attempt, I tried to use BFS to do search level by level, which can exhaust all the possible distances from 1 to 0. Unfortunately, it is TLE in the OJ. Following from the idea in problem Minimum Moves to Equal Array Elements II , we can use math method to calculate the median of the x and y coordinates. The worst case time complexity is O(mn\\log mn) O(mn\\log mn) , since there are at most m\\times n m\\times n \"1\" in the grid. C++ Median solution class Solution { public : int minTotalDistance ( vector < vector < int >>& grid ) { int m = grid . size (); int n = m ? grid [ 0 ]. size () : 0 ; vector < int > rows , cols ; for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { if ( grid [ i ][ j ] == 1 ) { rows . push_back ( i ); cols . push_back ( j ); } } } return minDistance ( rows ) + minDistance ( cols ); } int minDistance ( vector < int > vec ) { int n = vec . size (); sort ( vec . begin (), vec . end ()); int l = 0 , r = n - 1 ; int res = 0 ; while ( l < r ) res += vec [ r -- ] - vec [ l ++ ]; return res ; } }; C++ Median solution II class Solution { public : int minTotalDistance ( vector < vector < int >>& grid ) { int m = grid . size (); int n = m ? grid [ 0 ]. size () : 0 ; vector < int > rows , cols ; for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { if ( grid [ i ][ j ] == 1 ) { rows . push_back ( i ); cols . push_back ( j ); } } } sort ( cols . begin (), cols . end ()); int l = 0 , r = rows . size () - 1 ; int res = 0 ; while ( l < r ) res += rows [ r ] - rows [ l ] + cols [ r -- ] - cols [ l ++ ]; return res ; } }; BFS class Solution { public : int minTotalDistance ( vector < vector < int >>& grid ) { int m = grid . size (); int n = m ? grid [ 0 ]. size () : 0 ; int res = INT_MAX ; vector < vector < int >> dist ( m , vector < int > ( n , 0 )); for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { if ( grid [ i ][ j ] == 1 ) { bfs_helper ( grid , i , j , dist ); } } } for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { if ( res > dist [ i ][ j ]) { res = dist [ i ][ j ]; } } } return res ; } void bfs_helper ( vector < vector < int >>& grid , int i , int j , vector < vector < int >> & dist ) { int m = grid . size (); int n = m ? grid [ 0 ]. size () : 0 ; int x [ 4 ] = { -1 , 0 , 1 , 0 }; int y [ 4 ] = { 0 , 1 , 0 , -1 }; vector < vector < bool >> visited ( m , vector < bool > ( n , false )); int level = 1 ; queue < pair < int , int >> q ; q . push ({ i , j }); visited [ i ][ j ] = true ; while ( ! q . empty ()) { int len = q . size (); for ( int l = 0 ; l < len ; l ++ ) { int a = q . front (). first ; int b = q . front (). second ; q . pop (); for ( int k = 0 ; k < 4 ; k ++ ) { int u = a + x [ k ]; int v = b + y [ k ]; if ( u >= 0 && u < m && v >= 0 && v < n && ! visited [ u ][ v ]) { dist [ u ][ v ] += level ; visited [ u ][ v ] = true ; q . push ({ u , v }); } } } level ++ ; } } }; Number of Islands \u00b6 Battleships in a Board \u00b6 Bus Routes \u00b6 Word Ladder \u00b6 Minimum Genetic Mutation \u00b6 Word Ladder II \u00b6 Open the Lock \u00b6 Evaluate Devision \u00b6 399. Evaluate Division \u00b6 C++ BFS class Solution { public : vector < double > calcEquation ( vector < vector < string >>& equations , vector < double >& values , vector < vector < string >>& queries ) { unordered_map < string , vector < pair < string , double >>> graph ; for ( int i = 0 ; i < equations . size (); i ++ ) { graph [ equations [ i ][ 0 ]]. push_back ({ equations [ i ][ 1 ], values [ i ]}); graph [ equations [ i ][ 0 ]]. push_back ({ equations [ i ][ 0 ], 1.0 }); graph [ equations [ i ][ 1 ]]. push_back ({ equations [ i ][ 0 ], 1.0 / values [ i ]}); graph [ equations [ i ][ 1 ]]. push_back ({ equations [ i ][ 1 ], 1.0 }); } vector < double > res ; for ( auto & qr : queries ) { if ( graph . find ( qr [ 0 ]) == graph . end () || graph . find ( qr [ 1 ]) == graph . end ()) { res . push_back ( -1 ); continue ; } queue < pair < string , double >> q ; unordered_set < string > visited ; q . push ({ qr [ 0 ], 1.0 }); visited . insert ( qr [ 0 ]); bool find = false ; while ( ! q . empty () && ! find ) { auto t = q . front (); q . pop (); if ( t . first == qr [ 1 ]) { res . push_back ( t . second ); find = true ; break ; } for ( auto a : graph [ t . first ]) { if ( visited . find ( a . first ) == visited . end ()) { a . second *= t . second ; q . push ( a ); visited . insert ( a . first ); } } } if ( ! find ) res . push_back ( -1.0 ); } return res ; } }; C++ DFS class Solution { public : vector < double > calcEquation ( vector < pair < string , string >> equations , vector < double >& values , vector < pair < string , string >> queries ) { int m = equations . size (); int n = queries . size (); vector < double > res ( n , -1 ); set < string > s ; for ( auto equation : equations ) { s . insert ( equation . first ); s . insert ( equation . second ); } for ( int i = 0 ; i < n ; i ++ ) { vector < string > query ({ queries [ i ]. first , queries [ i ]. second }); if ( s . count ( query [ 0 ]) && s . count ( query [ 1 ])) { vector < bool > visited ( m , 0 ); res [ i ] = helper ( equations , values , query , visited ); } } return res ; } // Parameter visited here is to help the search. double helper ( vector < pair < string , string >> equations , vector < double >& values , vector < string > query , vector < bool > visited ) { int m = equations . size (); //base case, writing in seperate loop is more efficient O(n) for ( int i = 0 ; i < m ; i ++ ) { if ( equations [ i ]. first == query [ 0 ] && equations [ i ]. second == query [ 1 ]) { return values [ i ]; } if ( equations [ i ]. first == query [ 1 ] && equations [ i ]. second == query [ 0 ]) { return 1 / values [ i ]; } } // not found do DFS for ( int i = 0 ; i < m ; i ++ ) { if ( equations [ i ]. first == query [ 0 ] && ! visited [ i ]) { visited [ i ] = true ; double t = values [ i ] * helper ( equations , values , { equations [ i ]. second , query [ 1 ]}, visited ); if ( t > 0 ) return t ; visited [ i ] = false ; } if ( equations [ i ]. second == query [ 0 ] && ! visited [ i ]) { visited [ i ] = true ; double t = ( 1 / values [ i ]) * helper ( equations , values , { equations [ i ]. first , query [ 1 ]}, visited ); if ( t > 0 ) return t ; visited [ i ] = false ; } } return -1.0 ; } }; C++ Floyd-Warshall Compute shortest path for undirected graphs \u00b6 The Maze II \u00b6 Shortest Distance from All Buildings \u00b6 Similar to Walls and Gates , We should search from the \"dst\" (buildings) to \"src\" (empty land), while the problem is asking search from empty land to buildings. We need a 2D array to record the distance obtained by searching. The result is obtained at last after we finish the searching. Attention: we have also need to check whether the new building can reach all buildings. So use another 2D array and a counter num_buildings to check it can reach all. class Solution { public : int shortestDistance ( vector < vector < int >>& grid ) { int m = grid . size (); int n = m ? grid [ 0 ]. size () : 0 ; int num_buildings = 0 ; vector < vector < int >> dist1 ( m , vector < int > ( n , 0 )); vector < vector < int >> reach ( m , vector < int > ( n , 0 )); int x [ 4 ] = { -1 , 0 , 1 , 0 }; int y [ 4 ] = { 0 , 1 , 0 , -1 }; for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { if ( grid [ i ][ j ] == 1 ) { num_buildings ++ ; queue < int > q ; vector < vector < bool >> visited ( m , vector < bool > ( n , false )); q . push ( i * n + j ); int level = 1 ; // grid[a][b] is level 0, grid[u][v] is level 1 while ( ! q . empty ()) { int l = q . size (); for ( int s = 0 ; s < l ; s ++ ) { // BFS level by level int a = q . front () / n ; int b = q . front () % n ; q . pop (); for ( int k = 0 ; k < 4 ; k ++ ) { int u = a + x [ k ]; int v = b + y [ k ]; if ( u >= 0 && u < m && v >= 0 && v < n && grid [ u ][ v ] == 0 && ! visited [ u ][ v ]) { dist1 [ u ][ v ] += level ; reach [ u ][ v ] ++ ; // use to count num of 1s that reach this 0. visited [ u ][ v ] = true ; q . push ( u * n + v ); } } } level ++ ; } } } } int res = INT_MAX ; for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { if ( grid [ i ][ j ] == 0 && reach [ i ][ j ] == num_buildings ) { res = min ( res , dist1 [ i ][ j ]); } } } return res == INT_MAX ? -1 : res ; } }; We can modify the original 2D array to record which cell is visited and which isn't so as to optimize the space complexity. Each time we start BFS from a building grid[i][j]=1 , the land will be decreased by 1 . So after the first BFS, all grid[i][j]=0 becomes grid[i][j]=-1 . The decreasing of grid[i][j] is equivalent to use the visited variables. similar to Solution 1, we use a 2D array dist[i][j] to record the distances. This distance is dist[i][j] used to record each BFS started from a building ( grid[i][j]=1 ), not the global distances. To accumulate all the distance from 1s, we have to use another 2D array sum to record it. C++ Optimized BFS solution class Solution { public : int shortestDistance ( vector < vector < int >>& grid ) { int m = grid . size (); int n = m ? grid [ 0 ]. size () : 0 ; int res = INT_MAX ; int counter = 0 ; vector < vector < int >> sum = grid ; int x [ 4 ] = { 0 , -1 , 0 , 1 }; int y [ 4 ] = { -1 , 0 , 1 , 0 }; for ( int i = 0 ; i < m ; ++ i ) { for ( int j = 0 ; j < n ; ++ j ) { if ( grid [ i ][ j ] == 1 ) { vector < vector < int >> dist = grid ; queue < int > q ; q . push ( i * n + j ); res = INT_MAX ; while ( ! q . empty ()) { int a = q . front () / n ; int b = q . front () % n ; q . pop (); for ( int k = 0 ; k < 4 ; ++ k ) { int u = a + x [ k ]; int v = b + y [ k ]; if ( u < m && u >= 0 && v < n && v >= 0 && grid [ u ][ v ] == counter ) { grid [ u ][ v ] -- ; // Mark this potential building position is visited in prev round dist [ u ][ v ] = dist [ a ][ b ] + 1 ; sum [ u ][ v ] += dist [ u ][ v ] - 1 ; res = min ( res , sum [ u ][ v ]); q . push ( u * n + v ); } } } counter -- ; // mark as visited } } } return res == INT_MAX ? -1 : res ; } }; Shortest Path Visiting All Nodes \u00b6 Shortest Path to Get All Keys \u00b6 Compute connected components for undirected graphs \u00b6 Longest Increasing Path in a Matrix \u00b6 Number of Connected Components in an Undirected Graph \u00b6 Build the undirected graph using adjacency list from the edge list representation. Call BFS in a for loop and use a visited vector to mark the status of the vertex explorations. Direct transform the above BFS solution to a DFS solution. C++ BFS solution class Solution { public : int countComponents ( int n , vector < pair < int , int >>& edges ) { vector < vector < int >> graph ( n , vector < int > ( 0 )); vector < bool > visited ( n , false ); for ( auto edge : edges ) { graph [ edge . first ]. push_back ( edge . second ); graph [ edge . second ]. push_back ( edge . first ); } queue < int > q ; int count = 0 ; for ( int i = 0 ; i < n ; ++ i ) { if ( ! visited [ i ]) { q . push ( i ); visited [ i ] = true ; while ( ! q . empty ()) { int t = q . front (); q . pop (); for ( auto a : graph [ t ]) { if ( ! visited [ a ]) { visited [ a ] = true ; q . push ( a ); } } } count ++ ; } } return count ; } }; C++ DFS solution class Solution { public : int countComponents ( int n , vector < pair < int , int >>& edges ) { vector < vector < int >> graph ( n , vector < int > ( 0 )); vector < bool > visited ( n , false ); int count = 0 ; for ( auto edge : edges ) { graph [ edge . first ]. push_back ( edge . second ); graph [ edge . second ]. push_back ( edge . first ); } for ( int i = 0 ; i < n ; ++ i ) { if ( ! visited [ i ]) { dfs_helper ( graph , visited , i ); count ++ ; } } return count ; } void dfs_helper ( vector < vector < int >>& graph , vector < bool >& visited , int i ) { visited [ i ] = true ; for ( auto a : graph [ i ]) { if ( ! visited [ a ]) { dfs_helper ( graph , visited , a ); } } } }; Union-Find solution class Solution { vector < int > parent ; public : int countComponents ( int n , vector < pair < int , int >>& edges ) { parent = vector < int > ( n , -1 ); for ( int i = 0 ; i < n ; ++ i ) parent [ i ] = i ; for ( auto edge : edges ) { int p = root ( edge . first ); int q = root ( edge . second ); if ( p != q ) parent [ q ] = p ; } int count = 0 ; for ( int i = 0 ; i < n ; ++ i ) { if ( parent [ i ] == i ) { count ++ ; } } return count ; } int root ( int i ) { while ( i != parent [ i ]) i = parent [ i ]; return i ; } };","title":"Breadth-First Search (BFS)"},{"location":"leetcode/breadth-first-search/notes/#breadth-first-search","text":"","title":"Breadth-First Search"},{"location":"leetcode/breadth-first-search/notes/#algorithm","text":"BFS search a graph by first explore all the vertices that are adjacent to the source vertex s , then explore the adjacent vertices of those already examined in layer-by-layer fashion until all the vertices are visited. The psudo code BFS (graph G, start vertex s) [ all nodes initially unexplored ] -- mark s as explored -- let Q = queue data structure, initialized with s -- while Q not empty: -- remove the first node of Q, call it v -- for each edge(v,w) : -- if w unexplored -- mark w as explored -- add w to Q (at the end)","title":"Algorithm"},{"location":"leetcode/breadth-first-search/notes/#bfs-properties","text":"Runtime complexity \\Theta(|V| + |E|) \\Theta(|V| + |E|) . It calculate shortest path from the source s s to the vertex v v . It build a breadth-first tree, CLRS called predecessor subgraph. If edge (u, v) (u, v) in the BFS tree, the simple path from source vertext s s to u u is equal to the simple path from s s to v v , or differ by 1. Simple path from a source vertex s s to a destination vertex v v is equal to the later\u2019s level label . If v_1, v_2, \\cdots, v_n v_1, v_2, \\cdots, v_n is the vertex enqueued in a certain point, v_1 v_1 is the head and v_n v_n is the back, then the level label of v_1 v_1 is either equal to level label of v_n v_n or less by 1. Also the level label of all the vertices in the queue is non-decreasing from head to tail. In other words, at a certain time the enqueued vertices at most comes from two different levels.","title":"BFS properties"},{"location":"leetcode/breadth-first-search/notes/#search","text":"unweighted edges Bi-directional search, (Speed up Dijkastra of 6.006) At all time, the level of nodes in queue at most differ by 1 The relationship between BFS and DFS (The Maze II) BFS tips for board and grid problems When push a pair of coordinate to a BFS queue, one way of doing it is using pair<int, int> . Another way is to convert to a number by p = i * n + j , and retrieve the coordinate by i = p / n and j = p % n . The BFS starting point and \"target\" are important, give a second thought about where should the search start and how it terminate. visitted[m][n] is not necessary sometimes. See whether you can modify the original borad/matrix/grid/maze to reflect whether the element has already been accessed or not. In a borad or maze like input, searching neighbor cells requires you to manipulate indexes for accessing its neighbors. We do this by defining two \"offset\" coordinates arrays. int x[4] = {-1, 0, 1, 0}; int y[4] = {0, 1, 0, -1}; The BFS queue doesn't necessarily start with one element pushed. It could be \"multi-end\" BFS . Like in problme Pacific Atlantic Water Flow and Longest Increasing Path in a Matrix , one could init the queue with all possible starting node then in use a while to visit all the node towards finding the optimal path. The \"multi-end\" BFS not only can make the BFS level by level (each round of for loop visited one level of nodes), but also can make use some optimal feature of BFS, like the BFS solution of Walls and Gates. The BFS while loop could include another while loop to interate throught total q.size() nodes. This is different from the \"oracle\" BFS solution from CRLS, but sometime could be useful. For example: Longest Increasing Path in a Matrix .","title":"Search"},{"location":"leetcode/breadth-first-search/notes/#the-maze","text":"DFS class Solution { public : bool hasPath ( vector < vector < int >>& maze , vector < int >& start , vector < int >& destination ) { int m = maze . size (); int n = m ? maze [ 0 ]. size () : 0 ; vector < vector < bool >> visited ( m , vector < bool > ( n , false )); return dfs_helper ( maze , start , destination , visited ); } void dfs_helper ( vector < vector < int >>& maze , vector < int >& start , vector < int >& destination , vector < vector < bool >>& visited ) { if ( start [ 0 ] == destination [ 0 ] && start [ 1 ] == destination [ 1 ]) { return true ; } int x [ 4 ] = { 0 , -1 , 0 , 1 }; int y [ 4 ] = { -1 , 0 , 1 , 0 }; bool res = false ; for ( int k = 0 ; k < 4 ; k ++ ) { int a = start [ 0 ] + x [ k ]; int b = start [ 1 ] + y [ k ]; // walk untill hit the wall while ( a >= 0 && b >= 0 && a < maze . size () && b < maze [ 0 ]. size90 && maze [ a ][ b ] == 0 ) { a += x [ k ]; b += y [ k ]; } if ( ! visited [ a - x [ k ]][ b - y [ k ]]) { visited [ a - x [ k ]][ b - y [ k ]] = true ; vector < int > new_start ({ a - x [ k ], b - y [ k ]}); res != dfs_helper ( maze , new_start , destination , visited ); } } return res ; } }; === \"BFS\" ```c++ class Solution { public: bool hasPath(vector<vector<int>>&maze, vector<int>& start, vector<int>& destination) { int m = maze.size(); int n = m ? maze[0].size() : 0; vector<vector<bool>> visited(m, vector<bool>(n, false)); queue<vector<int>> q; q.push(start); int x[4] = {0, -1, 0, 1}; int y[4] = {-1, 0, 1, 0}; while (!q.empty()) { vector<int> s = q.front(); q.pop(); if (s[0] == destination[0] && s[1] == destination[1]) { return true; } for (int k = 0; k < 4; k++) { int a = s[0] + x[k]; int b = s[1] + y[k]; while (a >= 0 && b >= 0 && a < m && b < n && maze[a][b] == 0) { a += x[k]; b += y[k]; } if (!visited[a - x[k]][b - y[k]]) { q.push(vector<int>({a - x[k], b - y[k]})); visited[a - x[k]][b - y[k]] = true; } } } return false; } }; ```","title":"The Maze"},{"location":"leetcode/breadth-first-search/notes/#the-maze-ii","text":"BFS can computer shortest path, we need additional memory to keep the optimal values. we update the corresponding destination cell in distance vector when we found a smaller distane can reach it from any direction. the final result will be in distance[destination[0]][destination[1]] . If we don't want to use so much memory, we can use a flag to mark when the BFS switch levels. This solution is essentially to use BFS to solve the weighted graph problem as Victor mentioned in 6006 R15. Compare this solution to the Dijkastra solution. The update distance matrix is essentially the relaxiation step. class Solution { public : int shortestDistance ( vector < vector < int >>& maze , vector < int >& start , vector < int >& destination ) { int m = maze . size (); int n = m ? maze [ 0 ]. size () : 0 ; if ( m == 0 ) return 0 ; vector < vector < int >> distance ( m , vector < int > ( n , INT_MAX )); queue < vector < int >> q ; q . push ( start ); distance [ start [ 0 ]][ start [ 1 ]] = 0 ; int x [ 4 ] = { -1 , 0 , 1 , 0 }; int y [ 4 ] = { 0 , 1 , 0 , -1 }; while ( ! q . empty ()) { vector < int > s = q . front (); q . pop (); for ( int k = 0 ; k < 4 ; k ++ ) { int a = s [ 0 ] + x [ k ]; int b = s [ 1 ] + y [ k ]; int count = 0 ; while ( a >= 0 && b >= 0 && a < m && b < n && maze [ a ][ b ] == 0 ) { a += x [ k ]; b += y [ k ]; count ++ ; } // notice here is a optimize if ( distance [ s [ 0 ]][ s [ 1 ]] + count < distance [ a - x [ k ]][ b - y [ k ]]) { distance [ a - x [ k ]][ b - y [ k ]] = distance [ s [ 0 ]][ s [ 1 ]] + count ; q . push ( vector < int > ({ a - x [ k ], b - y [ k ]})); /* could use emplace({a - x[k], b - y[k]}) */ } } } return distance [ destination [ 0 ]][ destination [ 1 ]] == INT_MAX ? -1 : distance [ destination [ 0 ]][ destination [ 1 ]]; } }; Note Based on the BFS solution, we can use a priority queue. Then the problem is very similar to the Dijkastra algorithm. The discussion about the improvement in leetcode forum is very interesting.","title":"The Maze II"},{"location":"leetcode/breadth-first-search/notes/#the-maze-iii","text":"This problem is aksing for output the shortest path to reach the destination. The idea is to use a distance[i][i] and result[i][j] . we should keep update the distance[i][j] and result[i][j] with smaller distance and lexicographical order. The hardest part is that there is two constrains. While using BFS, How could we ensure the two constrains are met, and also we are not missing any possible path. (i.e. missed to push a path to the BFS queue.) A trick we can make use of is we searching in the lexicographically order. such as down , left , right , up . However, we face a problem: the smallest lexicon order result may not be the shortest distance . Think this through. We need to used additional memory to record that piece of information beside the memory used to keep the minimum distance. BFS class Solution { public : string findShortestWay ( vector < vector < int >>& maze , vector < int >& start , vector < int >& destination ) { int m = maze . size (); int n = m ? maze [ 0 ]. size () : 0 ; if ( m == 0 ) return 0 ; vector < vector < int >> distance ( m , vector < int > ( n , INT_MAX )); vector < vector < string >> result ( m , vector < string > ( n , \"\" )); queue < vector < int >> q ; q . push ( start ); distance [ start [ 0 ]][ start [ 1 ]] = 0 ; char path [ 4 ] = { 'd' , 'l' , 'r' , 'u' }; int x [ 4 ] = { 1 , 0 , 0 , -1 }; int y [ 4 ] = { 0 , -1 , 1 , 0 }; while ( ! q . empty ()) { vector < int > s = q . front (); q . pop (); for ( int k = 0 ; k < 4 ; k ++ ) { int a = s [ 0 ] + x [ k ]; int b = s [ 1 ] + y [ k ]; int count = 0 ; /* continue if no wall */ while ( a >= 0 && b >= 0 && a < m && b < n && maze [ a ][ b ] == 0 ) { /* run over the destination */ if ( a == destination [ 0 ] && b == destination [ 1 ]){ if ( distance [ s [ 0 ]][ s [ 1 ]] + count < distance [ a ][ b ]) { distance [ a ][ b ] = distance [ s [ 0 ]][ s [ 1 ]] + count ; result [ a ][ b ] = result [ s [ 0 ]][ s [ 1 ]] + path [ k ]; } else if ( distance [ s [ 0 ]][ s [ 1 ]] + count == distance [ a ][ b ]) { string tmp = result [ s [ 0 ]][ s [ 1 ]] + path [ k ]; /* same distance, smaller lexicon */ if ( tmp . compare ( result [ a ][ b ]) < 0 ) { result [ a ][ b ] = tmp ; } } } a += x [ k ]; b += y [ k ]; count ++ ; } /* hit a wall, have to retreat \"0\", current a, b is in wall or off maze */ if ( distance [ s [ 0 ]][ s [ 1 ]] + count < distance [ a - x [ k ]][ b - y [ k ]]) { distance [ a - x [ k ]][ b - y [ k ]] = distance [ s [ 0 ]][ s [ 1 ]] + count ; result [ a - x [ k ]][ b - y [ k ]] = result [ s [ 0 ]][ s [ 1 ]] + path [ k ]; q . push ( vector < int > ({ a - x [ k ], b - y [ k ]})); } else if ( distance [ s [ 0 ]][ s [ 1 ]] + count == distance [ a - x [ k ]][ b - y [ k ]]) { string tmp = result [ s [ 0 ]][ s [ 1 ]] + path [ k ]; /* same distance, smaller lexicon */ if ( tmp . compare ( result [ a - x [ k ]][ b - y [ k ]]) < 0 ) { result [ a - x [ k ]][ b - y [ k ]] = tmp ; /* possible path, need to search it again */ q . push ( vector < int > ({ a - x [ k ], b - y [ k ]})); } } } } return distance [ destination [ 0 ]][ destination [ 1 ]] == INT_MAX ? \"impossible\" : result [ destination [ 0 ]][ destination [ 1 ]]; } }; DFS class Solution { public : string findShortestWay ( vector < vector < int >>& maze , vector < int >& ball , vector < int >& hole ) { return roll ( maze , ball [ 0 ], ball [ 1 ], hole , 0 , 0 , 0 , \"\" , pair < string , int > () = { \"impossible\" , INT_MAX }); } string roll ( vector < vector < int >>& maze , int rowBall , int colBall , const vector < int >& hole , int distRow , int distCol , int steps , const string & path , pair < string , int >& res ) { if ( steps < res . second ) { if ( distRow != 0 || distCol != 0 ) { while (( rowBall + distRow ) >= 0 && ( rowBall + distRow ) < maze . size () && ( colBall + distCol ) >= 0 && ( colBall + distCol ) < maze [ 0 ]. size () && maze [ rowBall + distRow ][ colBall + distCol ] != 1 ) { rowBall += distRow ; colBall += distCol ; ++ steps ; if ( rowBall == hole [ 0 ] && colBall == hole [ 1 ] && steps < res . second ) { res = { path , steps }; } } } if ( maze [ rowBall ][ colBall ] == 0 || steps + 2 < maze [ rowBall ][ colBall ]) { maze [ rowBall ][ colBall ] = steps + 2 ; if ( distRow == 0 ) roll ( maze , rowBall , colBall , hole , 1 , 0 , steps , path + \"d\" , res ); if ( distCol == 0 ) roll ( maze , rowBall , colBall , hole , 0 , -1 , steps , path + \"l\" , res ); if ( distCol == 0 ) roll ( maze , rowBall , colBall , hole , 0 , 1 , steps , path + \"r\" , res ); if ( distRow == 0 ) roll ( maze , rowBall , colBall , hole , -1 , 0 , steps , path + \"u\" , res ); } } return res . first ; } }; Dijkstra algoirthm class Vertex { public : int x , y , dist ; std :: string path ; Vertex ( int x , int y , int dist , std :: string path ) { this -> x = x ; this -> y = y ; this -> dist = dist ; this -> path = path ; } Vertex ( int x , int y ) { this -> x = x ; this -> y = y ; this -> dist = INT_MAX ; this -> path = \"\" ; } bool equals ( Vertex v ) { return this -> x == v . x && this -> y == v . y ; } bool equals ( int x , int y ) { return this -> x == x && this -> y == y ; } }; class Solution { private : // coordinates and path strings for the four directions. std :: vector < int > directions = { -1 , 0 , 1 , 0 , -1 }; std :: vector < std :: string > paths = { \"u\" , \"r\" , \"d\" , \"l\" }; // can't roll if it's boundry, wall or hole. bool canRoll ( vector < vector < int >> & maze , Vertex hole , int x , int y ) { int m = maze . size (), n = maze [ 0 ]. size (); if ( x >= 0 && x < m && y >= 0 && y < n ) return maze [ x ][ y ] != 1 && ! hole . equals ( x , y ); else return false ; } Vertex roll ( vector < vector < int >> & maze , Vertex curr , Vertex hole , int dx , int dy , std :: string path ) { int x = curr . x , y = curr . y , steps = curr . dist ; while ( canRoll ( maze , hole , x + dx , y + dy )) { x += dx ; y += dy ; steps ++ ; } // if couldn't roll due to hole, then return vertex corresponding to the hole. if ( hole . equals ( x + dx , y + dy )) { x += dx ; y += dy ; } return Vertex ( x , y , steps , curr . path + path ); } public : /* * to form a pq based on the increasing order of distances, and if same * distance then sorting based on lexicographic order. */ struct Compare { bool operator ()( const Vertex & a , const Vertex & b ) { return a . dist > b . dist || a . dist == b . dist && a . path > b . path ; } }; /* * Applying Dijkstra's algorithm here. * We treat hole as a gate here. The trick is look for holes while rolling * the back in a particular direction. So, the pq here contains the vertices * before gates as well as the holes. So, using dijkstra, when we pop a hole * from pq, we know its that. */ string findShortestWay ( vector < vector < int >>& maze , vector < int >& ball , vector < int >& hole ) { int m = maze . size (), n = maze [ 0 ]. size (); std :: priority_queue < Vertex , std :: vector < Vertex > , Compare > pq ; Vertex startVertex ( ball [ 0 ], ball [ 1 ], 0 , \"\" ), holeVertex ( hole [ 0 ], hole [ 1 ]); std :: unordered_set < int > visited ; pq . push ( startVertex ); while ( ! pq . empty ()) { Vertex curr = pq . top (); visited . insert ( curr . x * n + curr . y ); pq . pop (); if ( curr . equals ( holeVertex )) { return curr . path ; } for ( int i = 0 ; i < directions . size () - 1 ; i ++ ) { Vertex newStart = roll ( maze , curr , holeVertex , directions [ i ], directions [ i + 1 ], paths [ i ]); if ( visited . find ( newStart . x * n + newStart . y ) == visited . end ()) pq . push ( newStart ); } } return \"impossible\" ; } };","title":"The Maze III"},{"location":"leetcode/breadth-first-search/notes/#pacific-atlantic-water-flow","text":"Using DFS and two matrices to record the states and then to loop through the result check for both possible flow cases. The key is how to start the DFS? should we initiate the DFS in each (i, j) ? Here we can see it is more efficient if that we start from the edges. Notice we mark visited[i][j] = true immediately enter the helper function. DFS solution class Solution { public : vector < pair < int , int >> pacificAtlantic ( vector < vector < int >>& matrix ) { int m = matrix . size (); int n = m ? matrix [ 0 ]. size () : 0 ; vector < vector < int >> p_visited ( m , vector < int > ( n , 0 )); vector < vector < int >> a_visited ( m , vector < int > ( n , 0 )); vector < pair < int , int >> res ; for ( int i = 0 ; i < m ; i ++ ) { helper ( matrix , i , 0 , p_visited ); helper ( matrix , i , n - 1 , a_visited ); } for ( int j = 0 ; j < n ; j ++ ) { helper ( matrix , 0 , j , p_visited ); helper ( matrix , m - 1 , j , a_visited ); } for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { if ( p_visited [ i ][ j ] && a_visited [ i ][ j ]) { //res.push_back(pair<int, int>(i, j)); res . push_back ({ i , j }); } } } return res ; } void helper ( vector < vector < int >>& mat , int i , int j , vector < vector < int >>& visited ) { int m = mat . size (); int n = m ? mat [ 0 ]. size () : 0 ; int x [] = { 0 , -1 , 0 , 1 }; int y [] = { -1 , 0 , 1 , 0 }; visited [ i ][ j ] = true ; /* visite mat[i][j] first */ /* and then explore its neighbors */ for ( int k = 0 ; k < 4 ; k ++ ) { int p = i + x [ k ]; int q = j + y [ k ]; if ( p >= 0 && p < m && q >= 0 && q < n && ! visited [ p ][ q ] && mat [ i ][ j ] <= mat [ p ][ q ]) { helper ( mat , p , q , visited ); } } } }; BFS solution class Solution { public : vector < pair < int , int >> pacificAtlantic ( vector < vector < int >>& matrix ) { int m = matrix . size (); int n = m ? matrix [ 0 ]. size () : 0 ; vector < vector < bool >> vPacific ( m , vector < bool > ( n , false )); vector < vector < bool >> vAtlantic ( m , vector < bool > ( n , false )); queue < pair < int , int >> qPacific ; queue < pair < int , int >> qAtlantic ; for ( int i = 0 ; i < m ; ++ i ) { qPacific . push ({ i , 0 }); vPacific [ i ][ 0 ] = true ; qAtlantic . push ({ i , n - 1 }); vAtlantic [ i ][ n - 1 ] = true ; } for ( int i = 0 ; i < n ; ++ i ) { qPacific . push ({ 0 , i }); vPacific [ 0 ][ i ] = true ; qAtlantic . push ({ m - 1 , i }); vAtlantic [ m - 1 ][ i ] = true ; } bfs_helper ( matrix , qPacific , vPacific ); bfs_helper ( matrix , qAtlantic , vAtlantic ); vector < pair < int , int >> res ; for ( int i = 0 ; i < m ; ++ i ) { for ( int j = 0 ; j < n ; ++ j ) { if ( vPacific [ i ][ j ] && vAtlantic [ i ][ j ]) { res . push_back ({ i , j }); } } } return res ; } void bfs_helper ( vector < vector < int >>& matrix , queue < pair < int , int >>& q , vector < vector < bool >>& visited ) { int m = matrix . size (); int n = m ? matrix [ 0 ]. size () : 0 ; int x [ 4 ] = { 0 , -1 , 0 , 1 }; int y [ 4 ] = { -1 , 0 , 1 , 0 }; while ( ! q . empty ()) { int a = q . front (). first ; int b = q . front (). second ; q . pop (); for ( int k = 0 ; k < 4 ; ++ k ) { int u = a + x [ k ]; int v = b + y [ k ]; if ( u < 0 || u >= m || v < 0 || v >= n || visited [ u ][ v ] || matrix [ a ][ b ] > matrix [ u ][ v ]) continue ; visited [ u ][ v ] = true ; q . push ({ u , v }); } } } }; Warning Notice your terminate condition matrix[a][b] > matrix[u][v] in the DFS solution indicates your search direction cannot not go from high to low. The search direction is from low to high (or equal). This is to optimize the search since we don't have to revisite a node later.","title":"Pacific Atlantic Water Flow"},{"location":"leetcode/breadth-first-search/notes/#longest-increasing-path-in-a-matrix","text":"BFS solution Each cell should possibly be the starting cell of the longest increasing path. So we have to search starting from each one of the cells. It seems the running time is quadratic. How to optimize? use markers matrix to mark whether the matrix[i][j] have greater neighbors, left , top , right , bottom -> 1, 2, 4, 8. The do BFS only looking for the directions that have greater neighbors. The idea is very similar to the \"branch cutting\" in DFS. class Solution { public : int longestIncreasingPath ( vector < vector < int >>& matrix ) { int m = matrix . size (); int n = m ? matrix [ 0 ]. size () : 0 ; vector < vector < int >> markers ( m , vector < int > ( n , 0 )); int x [ 4 ] = { -1 , 0 , 1 , 0 }; int y [ 4 ] = { 0 , 1 , 0 , -1 }; int res = 0 ; for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { for ( int k = 0 ; k < 4 ; k ++ ) { int p = i + x [ k ]; int q = j + y [ k ]; if ( p < 0 || p >= m || q < 0 || q >= m ) continue ; if ( matrix [ p ][ q ] > matrix [ i ][ j ]) markers [ i ][ j ] ^= ( 1 << k ); } } } for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { int level = 0 ; if ( markers [ i ][ j ] > 0 ) { level = 0 ; queue < int > q ; q . push ( i * n + j ); while ( ! q . empty ()) { int a = q . front () / n ; int b = q . front () % n ; q . pop (); bool flag = true ; for ( int k = 0 ; k < 4 ; k ++ ) { int u = a + x [ k ]; int v = b + y [ k ]; if ( u < 0 || u >= m || v < 0 || v >= m ) continue ; if ( has_greater_neighbor ( markers [ a ][ b ], k )) { q . push ( u * n + v ); if ( flag ) { level ++ ; flag = false ; } } } } } res = max ( res , level ); } } return res ; } bool has_greater_neighbor ( int flags , int k ) { return ( flags >> k ) & 1 ; } }; BFS solution II BFS can find the shortest path. How can you use BFS to find the longest path? In this problem, we are not use BFS's shortest path properpy, we make use of BFS's property of finding the connected components. We can imagine each increasing path is a connected component, we need to find out the largest one. Notice how the solution construct the search structure (for loop inside the while loop). By construct the search structure in this way, the len is actually the level of nodes from all the peaks identified in the first nested for loops. Java BFS class Solution { public int longestIncreasingPath ( int [][] matrix ) { private final int [][] dirs = {{ 1 , 0 }, { - 1 , 0 },{ 0 , 1 }, { 0 , - 1 }}; private boolean ispeak ( int [][] matrix , boolean [][] marked , int i , int j ) { if ( i > 0 && ! marked [ i - 1 ][ j ] && matrix [ i - 1 ][ j ] > matrix [ i ][ j ] ) return false ; if ( i < matrix . length - 1 && ! marked [ i + 1 ][ j ] && matrix [ i + 1 ][ j ] > matrix [ i ][ j ] ) return false ; if ( j > 0 && ! marked [ i ][ j - 1 ] && matrix [ i ][ j - 1 ] > matrix [ i ][ j ] ) return false ; if ( j < matrix [ 0 ] . length - 1 && ! marked [ i ][ j + 1 ] && matrix [ i ][ j + 1 ] > matrix [ i ][ j ] ) return false ; return true ; } public int longestIncreasingPath ( int [][] matrix ) { if ( matrix . length == 0 || matrix [ 0 ] . length == 0 ) return 0 ; int len = 0 ; LinkedList < int []> queue = new LinkedList <> (); boolean [][] marked = new boolean [ matrix . length ][ matrix [ 0 ] . length ] ; for ( int i = 0 ; i < matrix . length ; i ++ ) { for ( int j = 0 ; j < matrix [ 0 ] . length ; j ++ ) { if ( ispeak ( matrix , marked , i , j )) queue . add ( new int [] { i , j }); } } while ( ! queue . isEmpty ()) { len ++ ; int size = queue . size (); for ( int i = 0 ; i < size ; i ++ ) { int [] p = queue . poll (); marked [ p [ 0 ]][ p [ 1 ]] = true ; for ( int j = 0 ; j < 4 ; j ++ ) { int r = p [ 0 ]+ dirs [ j ][ 0 ] , c = p [ 1 ]+ dirs [ j ][ 1 ] ; if ( r >= 0 && r < matrix . length && c >= 0 && c < matrix [ 0 ] . length && ! marked [ r ][ c ] && ispeak ( matrix , marked , r , c )) { if ( matrix [ r ][ c ] != matrix [ p [ 0 ]][ p [ 1 ]] ) queue . add ( new int [] { r , c }); } } } } return len ; } } } Note This BFS is not the same as the classic BFS routine from CLRS. It is a generalized BFS approach, which can be transformed to a problem to find the connected components. DFS solution we can also use DFS for this problem, it turns out DFS is the simpliest solution. DFS solution class Solution { public : int longestIncreasingPath ( vector < vector < int >>& matrix ) { int m = matrix . size (); int n = m ? matrix [ 0 ]. size () : 0 ; if ( n == 0 ) return 0 ; vector < vector < int >> cache ( m , vector < int > ( n , 0 )); int res = 1 ; for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { int len = dfs_helper ( matrix , i , j , m , n , cache ); res = max ( res , len ); } } return res ; } int dfs_helper ( vector < vector < int >>& matrix , int i , int j , int m , int n , vector < vector < int >>& cache ) { if ( cache [ i ][ j ]) return cache [ i ][ j ]; int x [ 4 ] = { -1 , 0 , 1 , 0 }; int y [ 4 ] = { 0 , 1 , 0 , -1 }; int res = 1 ; for ( int k = 0 ; k < 4 ; k ++ ) { int p = i + x [ k ]; int q = j + y [ k ]; if ( p < 0 || p >= m || q < 0 || q >= n || matrix [ p ][ q ] <= matrix [ i ][ j ]) continue ; int len = 1 + dfs_helper ( matrix , p , q , m , n , cache ); res = max ( res , len ); } cache [ i ][ j ] = res ; return res ; } };","title":"Longest Increasing Path in a Matrix"},{"location":"leetcode/breadth-first-search/notes/#walls-and-gates","text":"BFS solution We push all the gate to the queue at once, then BFS to update the room. This makes use of the optimal character of BFS and bring us some optimization. Proof correctness: If init the queue with all gates, BFS alternate between gates, once BFS explore one level of rooms for a particular gate, it switches to explore the same level or next level rooms for another gate. The level is the distance from room to gate. Suppose room R1 is visited at the first time from G1, then if R1 is visited again via G2, the distance from G2 to R1 is possible to be the same as G1 to R1, or greater than G1 to R1 by 1. The assigned nearest distance from room to gate never be able to reduce further in future. Then the first assigned distance value for a room will be the nearest one. DFS solution I initially start with each INF cell and search for 0. the issue is we have to pass the original i , j , in case we reach the base case to update the INF cell. Another trick is how to handle the repeated searching the same empty room. Extra space like visited[m][n] will work. but a better way to handle it is what we did in the below solution. We check if d > ma[i][j] , if d > 0 , the d > mat[i][j] also make sure we update mat[i][j] with a smaller value because the initial value of INF cell is INT_MAX . This is different from DFS because DFS can ensure the shortest path. The time complexity is O(m^2n^2) O(m^2n^2) . The space complexity is from the stack. It is definitely not constant. C++ BFS class Solution { public : void wallsAndGates ( vector < vector < int >>& rooms ) { int m = rooms . size (); int n = m ? rooms [ 0 ]. size () : 0 ; int x [ 4 ] = { -1 , 0 , 1 , 0 }; int y [ 4 ] = { 0 , 1 , 0 , -1 }; queue < pair < int , int >> q ; for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { if ( rooms [ i ][ j ] == 0 ) { // start from each gate q . push ({ i , j }); } } } while ( ! q . empty ()) { int a = q . front (). first ; int b = q . front (). second ; q . pop (); for ( int k = 0 ; k < 4 ; k ++ ) { int u = a + x [ k ]; int v = b + y [ k ]; if ( u < 0 || u >= m || v < 0 || v >= n || rooms [ u ][ v ] != INT_MAX ) { continue ; } rooms [ u ][ v ] = rooms [ a ][ b ] + 1 ; q . push ({ u , v }); } } } }; C++ DFS class Solution { public : void wallsAndGates ( vector < vector < int >>& rooms ) { int m = rooms . size (); int n = m ? rooms [ 0 ]. size () : 0 ; for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { if ( rooms [ i ][ j ] == 0 ) { // start from each gate, not room helper ( rooms , i , j , 0 ); } } } } /* i, j are the starting point. */ void helper ( vector < vector < int >>& mat , int i , int j , int d ) { int m = mat . size (); int n = m ? mat [ 0 ]. size () : 0 ; int x [ 4 ] = { -1 , 0 , 1 , 0 }; int y [ 4 ] = { 0 , 1 , 0 , -1 }; if ( i < 0 || i >= m || j < 0 || j >= n || mat [ i ][ j ] < d ) { return ; } // have already made sure d < mat[i][j] mat [ i ][ j ] = d ; for ( int k = 0 ; k < 4 ; k ++ ) { int a = i + x [ k ]; int b = j + y [ k ]; helper ( mat , a , b , d + 1 ); } } }; multi-end BFS Notice this implementation of BFS is different from triditional BFS. It start with a queue that have more than one element. Some people call this multi-end BFS . It is perform better than the original BFS, The time complexity is O(mn) O(mn)","title":"Walls and Gates"},{"location":"leetcode/breadth-first-search/notes/#01-matrix","text":"","title":"01 Matrix"},{"location":"leetcode/breadth-first-search/notes/#surrounded-regions","text":"We first mark the boundary O , then start BFS from each O . If it cannot reach the special marks, we capture the cell. This solution is starting from each of inner O and search the mark at the boundary. The solution is TLE due to more inner O 's than boundary O 's. class Solution { public : void solve ( vector < vector < char >>& board ) { int m = board . size (); int n = m ? board [ 0 ]. size () : 0 ; int x [ 4 ] = { -1 , 0 , 1 , 0 }; int y [ 4 ] = { 0 , 1 , 0 , -1 }; for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { if ( i == 0 || j == 0 || i == m - 1 || j == n - 1 ) { board [ i ][ j ] = board [ i ][ j ] == 'O' ? '-' : 'X' ; } } } for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { if ( board [ i ][ j ] == 'O' ) { bool flag = true ; queue < pair < int , int >> q ; q . push ({ i , j }); while ( ! q . empty ()) { int a = q . front (). first , b = q . front (). second ; q . pop (); for ( int k = 0 ; k < 4 ; k ++ ) { int u = a + x [ k ]; int v = b + y [ k ]; if ( u < 0 || u >= m || v < 0 || v >= n || board [ u ][ v ] == 'X' ) { continue ; } if ( board [ u ][ v ] == 'O' ) { q . push ({ u , v }); } if ( board [ u ][ v ] == '-' ) { flag = false ; break ; } } if ( ! flag ) break ; } if ( flag ) board [ i ][ j ] = 'X' ; } } } for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { if ( i == 0 || j == 0 || i == m - 1 || j == n - 1 ) { board [ i ][ j ] = board [ i ][ j ] == '-' ? 'O' : 'X' ; } } } } }; Instead searching whether a 'O' have an outlet. We can discover all 'O's that lead to edge 'O's and mark them, then all the rest 'O's are captured. Specifically, starting from every 'O' at the edge, using BFS to search all the 'O's outside the 'X's, all these 'O's that cannot be captured. Compare to solution 1, this ideas is thinking \u201cout of the box\u201d, instead of thinking inside 'O's, we take care outside 'O's first. Similar problems: Walls and Gates , Shortest Distance from All Buildings . class Solution { public : void solve ( vector < vector < char >>& board ) { int m = board . size (); int n = m ? board [ 0 ]. size () : 0 ; // bfs search from boundary 'O's for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { if ( i == 0 || j == 0 || i == m - 1 || j == n - 1 ) { if ( board [ i ][ j ] == 'O' ) bfs_helper ( board , i , j ); } } } for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { if ( board [ i ][ j ] == 'O' ) { board [ i ][ j ] = 'X' ; // capture all inner 'O's } else if ( board [ i ][ j ] == '-' ) { board [ i ][ j ] = 'O' ; // flip back the marks } } } } // helper function to mark \"outside\" 'O' as '-' void bfs_helper ( vector < vector < char >>& board , int i , int j ) { int m = board . size (); int n = m ? board [ 0 ]. size () : 0 ; int x [ 4 ] = { -1 , 0 , 1 , 0 }; int y [ 4 ] = { 0 , 1 , 0 , -1 }; board [ i ][ j ] = '-' ; queue < pair < int , int >> q ; q . push ({ i , j }); while ( ! q . empty ()) { int a = q . front (). first , b = q . front (). second ; q . pop (); for ( int k = 0 ; k < 4 ; k ++ ) { int u = a + x [ k ]; int v = b + y [ k ]; if ( u >= 0 && u < m && v >= 0 && v < n && board [ u ][ v ] == 'O' ) { q . push ({ u , v }); board [ u ][ v ] = '-' ; } } } } }; Tips \u4ee5\u4e0a\u4e24\u4e2a solution \u8db3\u4ee5\u8bf4\u660eBFS\u7684\u641c\u7d22\u65b9\u5411\u548c\u641c\u7d22\u76ee\u6807\u975e\u5e38\u91cd\u8981\u3002\u5de7\u5999\u8f6c\u5316\u641c\u7d22\u76ee\u6807\u53ef\u4ee5\u4f7f\u5f97code\u5f88\u597d\u5199\uff0c\u66f4\u7b80\u6d01\u3002","title":"Surrounded Regions"},{"location":"leetcode/breadth-first-search/notes/#best-meeting-point","text":"At the first attempt, I tried to use BFS to do search level by level, which can exhaust all the possible distances from 1 to 0. Unfortunately, it is TLE in the OJ. Following from the idea in problem Minimum Moves to Equal Array Elements II , we can use math method to calculate the median of the x and y coordinates. The worst case time complexity is O(mn\\log mn) O(mn\\log mn) , since there are at most m\\times n m\\times n \"1\" in the grid. C++ Median solution class Solution { public : int minTotalDistance ( vector < vector < int >>& grid ) { int m = grid . size (); int n = m ? grid [ 0 ]. size () : 0 ; vector < int > rows , cols ; for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { if ( grid [ i ][ j ] == 1 ) { rows . push_back ( i ); cols . push_back ( j ); } } } return minDistance ( rows ) + minDistance ( cols ); } int minDistance ( vector < int > vec ) { int n = vec . size (); sort ( vec . begin (), vec . end ()); int l = 0 , r = n - 1 ; int res = 0 ; while ( l < r ) res += vec [ r -- ] - vec [ l ++ ]; return res ; } }; C++ Median solution II class Solution { public : int minTotalDistance ( vector < vector < int >>& grid ) { int m = grid . size (); int n = m ? grid [ 0 ]. size () : 0 ; vector < int > rows , cols ; for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { if ( grid [ i ][ j ] == 1 ) { rows . push_back ( i ); cols . push_back ( j ); } } } sort ( cols . begin (), cols . end ()); int l = 0 , r = rows . size () - 1 ; int res = 0 ; while ( l < r ) res += rows [ r ] - rows [ l ] + cols [ r -- ] - cols [ l ++ ]; return res ; } }; BFS class Solution { public : int minTotalDistance ( vector < vector < int >>& grid ) { int m = grid . size (); int n = m ? grid [ 0 ]. size () : 0 ; int res = INT_MAX ; vector < vector < int >> dist ( m , vector < int > ( n , 0 )); for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { if ( grid [ i ][ j ] == 1 ) { bfs_helper ( grid , i , j , dist ); } } } for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { if ( res > dist [ i ][ j ]) { res = dist [ i ][ j ]; } } } return res ; } void bfs_helper ( vector < vector < int >>& grid , int i , int j , vector < vector < int >> & dist ) { int m = grid . size (); int n = m ? grid [ 0 ]. size () : 0 ; int x [ 4 ] = { -1 , 0 , 1 , 0 }; int y [ 4 ] = { 0 , 1 , 0 , -1 }; vector < vector < bool >> visited ( m , vector < bool > ( n , false )); int level = 1 ; queue < pair < int , int >> q ; q . push ({ i , j }); visited [ i ][ j ] = true ; while ( ! q . empty ()) { int len = q . size (); for ( int l = 0 ; l < len ; l ++ ) { int a = q . front (). first ; int b = q . front (). second ; q . pop (); for ( int k = 0 ; k < 4 ; k ++ ) { int u = a + x [ k ]; int v = b + y [ k ]; if ( u >= 0 && u < m && v >= 0 && v < n && ! visited [ u ][ v ]) { dist [ u ][ v ] += level ; visited [ u ][ v ] = true ; q . push ({ u , v }); } } } level ++ ; } } };","title":"Best Meeting Point"},{"location":"leetcode/breadth-first-search/notes/#number-of-islands","text":"","title":"Number of Islands"},{"location":"leetcode/breadth-first-search/notes/#battleships-in-a-board","text":"","title":"Battleships in a Board"},{"location":"leetcode/breadth-first-search/notes/#bus-routes","text":"","title":"Bus Routes"},{"location":"leetcode/breadth-first-search/notes/#word-ladder","text":"","title":"Word Ladder"},{"location":"leetcode/breadth-first-search/notes/#minimum-genetic-mutation","text":"","title":"Minimum Genetic Mutation"},{"location":"leetcode/breadth-first-search/notes/#word-ladder-ii","text":"","title":"Word Ladder II"},{"location":"leetcode/breadth-first-search/notes/#open-the-lock","text":"","title":"Open the Lock"},{"location":"leetcode/breadth-first-search/notes/#evaluate-devision","text":"","title":"Evaluate Devision"},{"location":"leetcode/breadth-first-search/notes/#399-evaluate-division","text":"C++ BFS class Solution { public : vector < double > calcEquation ( vector < vector < string >>& equations , vector < double >& values , vector < vector < string >>& queries ) { unordered_map < string , vector < pair < string , double >>> graph ; for ( int i = 0 ; i < equations . size (); i ++ ) { graph [ equations [ i ][ 0 ]]. push_back ({ equations [ i ][ 1 ], values [ i ]}); graph [ equations [ i ][ 0 ]]. push_back ({ equations [ i ][ 0 ], 1.0 }); graph [ equations [ i ][ 1 ]]. push_back ({ equations [ i ][ 0 ], 1.0 / values [ i ]}); graph [ equations [ i ][ 1 ]]. push_back ({ equations [ i ][ 1 ], 1.0 }); } vector < double > res ; for ( auto & qr : queries ) { if ( graph . find ( qr [ 0 ]) == graph . end () || graph . find ( qr [ 1 ]) == graph . end ()) { res . push_back ( -1 ); continue ; } queue < pair < string , double >> q ; unordered_set < string > visited ; q . push ({ qr [ 0 ], 1.0 }); visited . insert ( qr [ 0 ]); bool find = false ; while ( ! q . empty () && ! find ) { auto t = q . front (); q . pop (); if ( t . first == qr [ 1 ]) { res . push_back ( t . second ); find = true ; break ; } for ( auto a : graph [ t . first ]) { if ( visited . find ( a . first ) == visited . end ()) { a . second *= t . second ; q . push ( a ); visited . insert ( a . first ); } } } if ( ! find ) res . push_back ( -1.0 ); } return res ; } }; C++ DFS class Solution { public : vector < double > calcEquation ( vector < pair < string , string >> equations , vector < double >& values , vector < pair < string , string >> queries ) { int m = equations . size (); int n = queries . size (); vector < double > res ( n , -1 ); set < string > s ; for ( auto equation : equations ) { s . insert ( equation . first ); s . insert ( equation . second ); } for ( int i = 0 ; i < n ; i ++ ) { vector < string > query ({ queries [ i ]. first , queries [ i ]. second }); if ( s . count ( query [ 0 ]) && s . count ( query [ 1 ])) { vector < bool > visited ( m , 0 ); res [ i ] = helper ( equations , values , query , visited ); } } return res ; } // Parameter visited here is to help the search. double helper ( vector < pair < string , string >> equations , vector < double >& values , vector < string > query , vector < bool > visited ) { int m = equations . size (); //base case, writing in seperate loop is more efficient O(n) for ( int i = 0 ; i < m ; i ++ ) { if ( equations [ i ]. first == query [ 0 ] && equations [ i ]. second == query [ 1 ]) { return values [ i ]; } if ( equations [ i ]. first == query [ 1 ] && equations [ i ]. second == query [ 0 ]) { return 1 / values [ i ]; } } // not found do DFS for ( int i = 0 ; i < m ; i ++ ) { if ( equations [ i ]. first == query [ 0 ] && ! visited [ i ]) { visited [ i ] = true ; double t = values [ i ] * helper ( equations , values , { equations [ i ]. second , query [ 1 ]}, visited ); if ( t > 0 ) return t ; visited [ i ] = false ; } if ( equations [ i ]. second == query [ 0 ] && ! visited [ i ]) { visited [ i ] = true ; double t = ( 1 / values [ i ]) * helper ( equations , values , { equations [ i ]. first , query [ 1 ]}, visited ); if ( t > 0 ) return t ; visited [ i ] = false ; } } return -1.0 ; } }; C++ Floyd-Warshall","title":"399. Evaluate Division"},{"location":"leetcode/breadth-first-search/notes/#compute-shortest-path-for-undirected-graphs","text":"","title":"Compute shortest path for undirected graphs"},{"location":"leetcode/breadth-first-search/notes/#the-maze-ii_1","text":"","title":"The Maze II"},{"location":"leetcode/breadth-first-search/notes/#shortest-distance-from-all-buildings","text":"Similar to Walls and Gates , We should search from the \"dst\" (buildings) to \"src\" (empty land), while the problem is asking search from empty land to buildings. We need a 2D array to record the distance obtained by searching. The result is obtained at last after we finish the searching. Attention: we have also need to check whether the new building can reach all buildings. So use another 2D array and a counter num_buildings to check it can reach all. class Solution { public : int shortestDistance ( vector < vector < int >>& grid ) { int m = grid . size (); int n = m ? grid [ 0 ]. size () : 0 ; int num_buildings = 0 ; vector < vector < int >> dist1 ( m , vector < int > ( n , 0 )); vector < vector < int >> reach ( m , vector < int > ( n , 0 )); int x [ 4 ] = { -1 , 0 , 1 , 0 }; int y [ 4 ] = { 0 , 1 , 0 , -1 }; for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { if ( grid [ i ][ j ] == 1 ) { num_buildings ++ ; queue < int > q ; vector < vector < bool >> visited ( m , vector < bool > ( n , false )); q . push ( i * n + j ); int level = 1 ; // grid[a][b] is level 0, grid[u][v] is level 1 while ( ! q . empty ()) { int l = q . size (); for ( int s = 0 ; s < l ; s ++ ) { // BFS level by level int a = q . front () / n ; int b = q . front () % n ; q . pop (); for ( int k = 0 ; k < 4 ; k ++ ) { int u = a + x [ k ]; int v = b + y [ k ]; if ( u >= 0 && u < m && v >= 0 && v < n && grid [ u ][ v ] == 0 && ! visited [ u ][ v ]) { dist1 [ u ][ v ] += level ; reach [ u ][ v ] ++ ; // use to count num of 1s that reach this 0. visited [ u ][ v ] = true ; q . push ( u * n + v ); } } } level ++ ; } } } } int res = INT_MAX ; for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { if ( grid [ i ][ j ] == 0 && reach [ i ][ j ] == num_buildings ) { res = min ( res , dist1 [ i ][ j ]); } } } return res == INT_MAX ? -1 : res ; } }; We can modify the original 2D array to record which cell is visited and which isn't so as to optimize the space complexity. Each time we start BFS from a building grid[i][j]=1 , the land will be decreased by 1 . So after the first BFS, all grid[i][j]=0 becomes grid[i][j]=-1 . The decreasing of grid[i][j] is equivalent to use the visited variables. similar to Solution 1, we use a 2D array dist[i][j] to record the distances. This distance is dist[i][j] used to record each BFS started from a building ( grid[i][j]=1 ), not the global distances. To accumulate all the distance from 1s, we have to use another 2D array sum to record it. C++ Optimized BFS solution class Solution { public : int shortestDistance ( vector < vector < int >>& grid ) { int m = grid . size (); int n = m ? grid [ 0 ]. size () : 0 ; int res = INT_MAX ; int counter = 0 ; vector < vector < int >> sum = grid ; int x [ 4 ] = { 0 , -1 , 0 , 1 }; int y [ 4 ] = { -1 , 0 , 1 , 0 }; for ( int i = 0 ; i < m ; ++ i ) { for ( int j = 0 ; j < n ; ++ j ) { if ( grid [ i ][ j ] == 1 ) { vector < vector < int >> dist = grid ; queue < int > q ; q . push ( i * n + j ); res = INT_MAX ; while ( ! q . empty ()) { int a = q . front () / n ; int b = q . front () % n ; q . pop (); for ( int k = 0 ; k < 4 ; ++ k ) { int u = a + x [ k ]; int v = b + y [ k ]; if ( u < m && u >= 0 && v < n && v >= 0 && grid [ u ][ v ] == counter ) { grid [ u ][ v ] -- ; // Mark this potential building position is visited in prev round dist [ u ][ v ] = dist [ a ][ b ] + 1 ; sum [ u ][ v ] += dist [ u ][ v ] - 1 ; res = min ( res , sum [ u ][ v ]); q . push ( u * n + v ); } } } counter -- ; // mark as visited } } } return res == INT_MAX ? -1 : res ; } };","title":"Shortest Distance from All Buildings"},{"location":"leetcode/breadth-first-search/notes/#shortest-path-visiting-all-nodes","text":"","title":"Shortest Path Visiting All Nodes"},{"location":"leetcode/breadth-first-search/notes/#shortest-path-to-get-all-keys","text":"","title":"Shortest Path to Get All Keys"},{"location":"leetcode/breadth-first-search/notes/#compute-connected-components-for-undirected-graphs","text":"","title":"Compute connected components for undirected graphs"},{"location":"leetcode/breadth-first-search/notes/#longest-increasing-path-in-a-matrix_1","text":"","title":"Longest Increasing Path in a Matrix"},{"location":"leetcode/breadth-first-search/notes/#number-of-connected-components-in-an-undirected-graph","text":"Build the undirected graph using adjacency list from the edge list representation. Call BFS in a for loop and use a visited vector to mark the status of the vertex explorations. Direct transform the above BFS solution to a DFS solution. C++ BFS solution class Solution { public : int countComponents ( int n , vector < pair < int , int >>& edges ) { vector < vector < int >> graph ( n , vector < int > ( 0 )); vector < bool > visited ( n , false ); for ( auto edge : edges ) { graph [ edge . first ]. push_back ( edge . second ); graph [ edge . second ]. push_back ( edge . first ); } queue < int > q ; int count = 0 ; for ( int i = 0 ; i < n ; ++ i ) { if ( ! visited [ i ]) { q . push ( i ); visited [ i ] = true ; while ( ! q . empty ()) { int t = q . front (); q . pop (); for ( auto a : graph [ t ]) { if ( ! visited [ a ]) { visited [ a ] = true ; q . push ( a ); } } } count ++ ; } } return count ; } }; C++ DFS solution class Solution { public : int countComponents ( int n , vector < pair < int , int >>& edges ) { vector < vector < int >> graph ( n , vector < int > ( 0 )); vector < bool > visited ( n , false ); int count = 0 ; for ( auto edge : edges ) { graph [ edge . first ]. push_back ( edge . second ); graph [ edge . second ]. push_back ( edge . first ); } for ( int i = 0 ; i < n ; ++ i ) { if ( ! visited [ i ]) { dfs_helper ( graph , visited , i ); count ++ ; } } return count ; } void dfs_helper ( vector < vector < int >>& graph , vector < bool >& visited , int i ) { visited [ i ] = true ; for ( auto a : graph [ i ]) { if ( ! visited [ a ]) { dfs_helper ( graph , visited , a ); } } } }; Union-Find solution class Solution { vector < int > parent ; public : int countComponents ( int n , vector < pair < int , int >>& edges ) { parent = vector < int > ( n , -1 ); for ( int i = 0 ; i < n ; ++ i ) parent [ i ] = i ; for ( auto edge : edges ) { int p = root ( edge . first ); int q = root ( edge . second ); if ( p != q ) parent [ q ] = p ; } int count = 0 ; for ( int i = 0 ; i < n ; ++ i ) { if ( parent [ i ] == i ) { count ++ ; } } return count ; } int root ( int i ) { while ( i != parent [ i ]) i = parent [ i ]; return i ; } };","title":"Number of Connected Components in an Undirected Graph"},{"location":"leetcode/depth-first-search/notes/","text":"Depth-First Search \u00b6 Algorithm \u00b6 DFS explore aggressively along a single path and only backtrack when necessary. It use a stack and usually implemented using recursion. The psudo code DFS(graph G, start vertex s) -- mark s as explored -- for every edge (s,v) : -- if v unexplored -- DFS(G,v) DFS properties \u00b6 Complexity \\Theta(|V| + |E|) \\Theta(|V| + |E|) The \"finish time\" compute the topological order of a DAG. It build a depth-first tree or forest, CLRS called predecessor subgraph. The discovery and finishing times have parenthesis structure. (parenthesis theorem) Nesting of descendants intervals: u.d < v.d < v.f < u.f u.d < v.d < v.f < u.f . d d : discovery time f f : finish time. If vertex v v is a descendant of vertex u u , at the time of discovery of u u , there is a path from u u to v v isn't been visited (all white path) (White-path theorem) Strongly connected components \u00b6 Problem that cannot be solve by BFS and DFS at the same time \u00b6 Single source shortest path problems \u00b6 Problem definition \u00b6 \\delta(s, v) = \\begin{cases} \\min\\{w(p): p \\text{ of } u \\rightarrow v \\}) & \\text{if } \\exists \\text{ any such path}\\\\ \\infty & \\text{otherwise (unreachable)} \\\\ \\end{cases} \\delta(s, v) = \\begin{cases} \\min\\{w(p): p \\text{ of } u \\rightarrow v \\}) & \\text{if } \\exists \\text{ any such path}\\\\ \\infty & \\text{otherwise (unreachable)} \\\\ \\end{cases} Negative edge and Negative edge circle \u00b6 Problems \u00b6 339. Nested List Weight Sum \u00b6 C++ DFS solution /** * // This is the interface that allows for creating nested lists. * // You should not implement it, or speculate about its implementation * class NestedInteger { * public: * // Constructor initializes an empty nested list. * NestedInteger(); * * // Constructor initializes a single integer. * NestedInteger(int value); * * // Return true if this NestedInteger holds a single integer, rather than a nested list. * bool isInteger() const; * * // Return the single integer that this NestedInteger holds, if it holds a single integer * // The result is undefined if this NestedInteger holds a nested list * int getInteger() const; * * // Set this NestedInteger to hold a single integer. * void setInteger(int value); * * // Set this NestedInteger to hold a nested list and adds a nested integer to it. * void add(const NestedInteger &ni); * * // Return the nested list that this NestedInteger holds, if it holds a nested list * // The result is undefined if this NestedInteger holds a single integer * const vector<NestedInteger> &getList() const; * }; */ class Solution { public : int depthSum ( vector < NestedInteger >& nestedList ) { return depthSumHelper ( nestedList , 1 ); } int depthSumHelper ( vector < NestedInteger >& nestedList , int depth ) { int result = 0 ; for ( auto curr : nestedList ) { if ( curr . isInteger ()) { result += ( curr . getInteger () * depth ); } else { result += depthSumHelper ( curr . getList (), depth + 1 ); } } return result ; } }; 364. Nested List Weight Sum II \u00b6 c++ DFS class Solution { public : int depthSumInverse ( vector < NestedInteger >& nestedList ) { int sum = 0 ; vector < int > results ; /* 1st place we iterate through a list */ for ( auto nestedInt : nestedList ) { dfs_helper ( nestedInt , 0 , results ); } /* calculate the results */ for ( int i = results . size () - 1 , level = 1 ; i >= 0 ; i -- , level ++ ) { sum += results [ i ] * level ; } return sum ; } void dfs_helper ( NestedInteger & nestedList , int level , vector < int >& results ) { if ( results . size () < level + 1 ) results . resize ( level + 1 ); if ( nestedList . isInteger ()) { results [ level ] += nestedList . getInteger (); } else { for ( auto ni : nestedList . getList ()) { /* 2nd place we iterate through a list */ dfs_helper ( ni , level + 1 , results ); } } } }; Reference \u00b6 Graph Data Structure And Algorithms GeeksforGeeks","title":"Depth-First Search (DFS)"},{"location":"leetcode/depth-first-search/notes/#depth-first-search","text":"","title":"Depth-First Search"},{"location":"leetcode/depth-first-search/notes/#algorithm","text":"DFS explore aggressively along a single path and only backtrack when necessary. It use a stack and usually implemented using recursion. The psudo code DFS(graph G, start vertex s) -- mark s as explored -- for every edge (s,v) : -- if v unexplored -- DFS(G,v)","title":"Algorithm"},{"location":"leetcode/depth-first-search/notes/#dfs-properties","text":"Complexity \\Theta(|V| + |E|) \\Theta(|V| + |E|) The \"finish time\" compute the topological order of a DAG. It build a depth-first tree or forest, CLRS called predecessor subgraph. The discovery and finishing times have parenthesis structure. (parenthesis theorem) Nesting of descendants intervals: u.d < v.d < v.f < u.f u.d < v.d < v.f < u.f . d d : discovery time f f : finish time. If vertex v v is a descendant of vertex u u , at the time of discovery of u u , there is a path from u u to v v isn't been visited (all white path) (White-path theorem)","title":"DFS properties"},{"location":"leetcode/depth-first-search/notes/#strongly-connected-components","text":"","title":"Strongly connected components"},{"location":"leetcode/depth-first-search/notes/#problem-that-cannot-be-solve-by-bfs-and-dfs-at-the-same-time","text":"","title":"Problem that cannot be solve by BFS and DFS at the same time"},{"location":"leetcode/depth-first-search/notes/#single-source-shortest-path-problems","text":"","title":"Single source shortest path problems"},{"location":"leetcode/depth-first-search/notes/#problem-definition","text":"\\delta(s, v) = \\begin{cases} \\min\\{w(p): p \\text{ of } u \\rightarrow v \\}) & \\text{if } \\exists \\text{ any such path}\\\\ \\infty & \\text{otherwise (unreachable)} \\\\ \\end{cases} \\delta(s, v) = \\begin{cases} \\min\\{w(p): p \\text{ of } u \\rightarrow v \\}) & \\text{if } \\exists \\text{ any such path}\\\\ \\infty & \\text{otherwise (unreachable)} \\\\ \\end{cases}","title":"Problem definition"},{"location":"leetcode/depth-first-search/notes/#negative-edge-and-negative-edge-circle","text":"","title":"Negative edge and Negative edge circle"},{"location":"leetcode/depth-first-search/notes/#problems","text":"","title":"Problems"},{"location":"leetcode/depth-first-search/notes/#339-nested-list-weight-sum","text":"C++ DFS solution /** * // This is the interface that allows for creating nested lists. * // You should not implement it, or speculate about its implementation * class NestedInteger { * public: * // Constructor initializes an empty nested list. * NestedInteger(); * * // Constructor initializes a single integer. * NestedInteger(int value); * * // Return true if this NestedInteger holds a single integer, rather than a nested list. * bool isInteger() const; * * // Return the single integer that this NestedInteger holds, if it holds a single integer * // The result is undefined if this NestedInteger holds a nested list * int getInteger() const; * * // Set this NestedInteger to hold a single integer. * void setInteger(int value); * * // Set this NestedInteger to hold a nested list and adds a nested integer to it. * void add(const NestedInteger &ni); * * // Return the nested list that this NestedInteger holds, if it holds a nested list * // The result is undefined if this NestedInteger holds a single integer * const vector<NestedInteger> &getList() const; * }; */ class Solution { public : int depthSum ( vector < NestedInteger >& nestedList ) { return depthSumHelper ( nestedList , 1 ); } int depthSumHelper ( vector < NestedInteger >& nestedList , int depth ) { int result = 0 ; for ( auto curr : nestedList ) { if ( curr . isInteger ()) { result += ( curr . getInteger () * depth ); } else { result += depthSumHelper ( curr . getList (), depth + 1 ); } } return result ; } };","title":"339. Nested List Weight Sum"},{"location":"leetcode/depth-first-search/notes/#364-nested-list-weight-sum-ii","text":"c++ DFS class Solution { public : int depthSumInverse ( vector < NestedInteger >& nestedList ) { int sum = 0 ; vector < int > results ; /* 1st place we iterate through a list */ for ( auto nestedInt : nestedList ) { dfs_helper ( nestedInt , 0 , results ); } /* calculate the results */ for ( int i = results . size () - 1 , level = 1 ; i >= 0 ; i -- , level ++ ) { sum += results [ i ] * level ; } return sum ; } void dfs_helper ( NestedInteger & nestedList , int level , vector < int >& results ) { if ( results . size () < level + 1 ) results . resize ( level + 1 ); if ( nestedList . isInteger ()) { results [ level ] += nestedList . getInteger (); } else { for ( auto ni : nestedList . getList ()) { /* 2nd place we iterate through a list */ dfs_helper ( ni , level + 1 , results ); } } } };","title":"364. Nested List Weight Sum II"},{"location":"leetcode/depth-first-search/notes/#reference","text":"Graph Data Structure And Algorithms GeeksforGeeks","title":"Reference"},{"location":"leetcode/dynamic-programming/notes/","text":"Dynamic Programming \u00b6 \u96be\u70b9 \u00b6 \u5982\u4f55\u5728\u5206\u6790\u6700\u540e\u4e00\u6b65\u65f6\u9009\u62e9\u6b63\u786e\u89d2\u5ea6\u5e76\u8003\u8651\u6240\u6709\u53ef\u80fd\u60c5\u51b5\uff0c\u8fdb\u800c\u5199\u51fa\u72b6\u6001\u8f6c\u5316\u65b9\u7a0b. i.e. Buy and Shell stock, Edit distance, Longest Increasing Subsequence. Knapsack problems \u00b6 \u80cc\u5305\u95ee\u9898\u4e5d\u8bb2 The knapsack problems often given the follow conditions N , the number of items M , the size of the knapsack s[i] , the size of the item i . v[i] , the value of the item i . c[i] , the maximum time item i can be used. c[i] could be an all 1 array, hence 0-1 knapsack problem. c[i] could also be inf , meaning the items can be used as many time as you want (bounded by the size of the knapsack). Knapsack problems ask Whether the given items can fit into the size of the knapsack. Find the maximum capacity the items can occupy the knapsack. Find the maximum value of items that can fit into the knapsack. Find the total number of ways the given item can fit into the size of the knapsack. (reuse is allowed) In all the above questions, the items can be reused or not reused, they are very different and the throught process and the solution are also different. Knapsack I \u00b6 Given N , M , s[i] , find the maximum size you can put into the Knapsack. DP O(nm) class Solution { public : int backPackI ( int m , vector < int > A ) { int n = A . size (); if ( n == 0 ) return 0 ; bool f [ n + 1 ][ m + 1 ]; f [ 0 ][ 0 ] = true ; for ( int j = 1 ; j <= m ; ++ j ) { f [ 0 ][ j ] = false ; } for ( int i = 1 ; i <= n ; ++ i ) { for ( int j = 0 ; j <= m ; ++ j ) { f [ i ][ j ] = f [ i - 1 ][ j ]; if ( j >= A [ i - 1 ]) { f [ i ][ j ] = f [ i ][ j ] || f [ i - 1 ][ j - A [ i - 1 ]]; } } } for ( int j = m ; j >= 0 ; -- j ) { if ( f [ n ][ j ] == true ) return j ; } return 0 ; } } DP O(m) space class Solution { public : int backPackI ( int m , vector < int > A ) { int n = A . size (); if ( n == 0 ) return 0 ; bool f [ m + 1 ]; f [ 0 ] = true ; for ( int i = 1 ; i <= n ; ++ i ) { for ( int j = m ; j >= 0 ; -- j ) { f [ j ] = f [ j - 1 ] if ( j >= A [ i - 1 ]) { f [ j ] = f [ j ] || f [ j - A [ i - 1 ]]; } } } for ( int j = m ; j >= 0 ; -- j ) { if ( f [ j ] == true ) return j ; } return 0 ; } } Knapsack II \u00b6 Given N , M , s[i] , v[i] , find the maximum value you can put into the Knapsack. \u601d\u8003\u65b9\u5f0f\u4efb\u7136\u662f\u4ece\u6700\u540e\u4e00\u4e2a\u7269\u54c1\u9009\u8fd8\u662f\u4e0d\u9009\uff0c\u53ea\u662f\u6211\u4eec\u73b0\u5728\u8003\u8651\u7684\u662f\u4ef7\u503c\u3002\u6b64\u65f6\u72b6\u6001\u5c31\u4e0d\u80fd\u662f\u53ef\u884c\u6027(Backpack)\u6216\u8005\u591a\u5c11\u79cd\u4e86(Backpack V), \u6211\u4eec\u8981\u7eaa\u5f55\u603b\u4ef7\u503c\u3002 state f[i][w] : \u8868\u793a\u524di\u4e2a\u7269\u54c1\u80fd\u62fc\u6210\u91cd\u91cfw\u7684\u603b\u4ef7\u503c(V)\u3002 state transition equaltion: f[i][w] = max(f[i\u22121][w], f[i\u22121][w\u2212A[i\u22121]]+V[i\u22121]|w\u2265A[i\u22121]\u4e14 f[i\u22121][w\u2212A[i\u22121]]\u2260\u22121) Initialization: f[0][0] = 0, f[0][1] = -1, ... f[0][w] = -1. -1 \u4ee3\u8868\u4e0d\u80fd\u88ab\u62fc\u51fa\u3002 DP O(mn) class Solution { public : /** * f[i][w]: \u524di\u4e2a\u7269\u54c1\u80fd\u591f\u62fc\u6210w\u7684\u603b\u4ef7\u503c * f[i][w] = max(f[i - 1][w], f[i - 1][w - A[i - 1]] + V[i - 1]) */ int backPackII ( int m , vector < int > A , vector < int > V ) { int n = A . size (); if ( n == 0 ) { return 0 ; } int f [ n + 1 ][ m + 1 ]; /* init */ f [ 0 ][ 0 ] = 0 ; for ( int j = 1 ; j <= m ; j ++ ) { f [ 0 ][ j ] = -1 ; } for ( int i = 1 ; i <= n ; i ++ ) { for ( int j = 0 ; j <= m ; j ++ ) { f [ i ][ j ] = f [ i - 1 ][ j ]; /* \u6b64\u5904\u5fc5\u987b\u521d\u59cb\u5316 */ if ( f [ i - 1 ][ j - A [ i - 1 ]] != -1 && j >= A [ i - 1 ]) { f [ i ][ j ] = max ( f [ i ][ j ], f [ i - 1 ][ j - A [ i - 1 ]] + V [ i - 1 ]); } } } int res = 0 ; for ( int j = 0 ; j <= m ; j ++ ) { if ( f [ n ][ j ] != -1 && f [ n ][ j ] > res ) { res = f [ n ][ j ]; } } return res ; } }; DP O(M) class Solution { public : /** * f[i][w]: \u524di\u4e2a\u7269\u54c1\u80fd\u591f\u62fc\u6210w\u7684\u603b\u4ef7\u503c * f[i][w] = max(f[i - 1][w], f[i - 1][w - A[i - 1]] + V[i - 1]) */ int backPackII ( int m , vector < int > A , vector < int > V ) { // write your code here int n = A . size (); if ( n == 0 ) { return 0 ; } int f [ m + 1 ]; /* init */ f [ 0 ] = 0 ; for ( int j = 1 ; j <= m ; j ++ ) { f [ j ] = -1 ; } for ( int i = 1 ; i <= n ; i ++ ) { for ( int j = m ; j >= 0 ; j -- ) { if ( f [ j - A [ i - 1 ]] != -1 && j >= A [ i - 1 ]) { f [ j ] = max ( f [ j ], f [ j - A [ i - 1 ]] + V [ i - 1 ]); } } } int res = 0 ; for ( int j = 0 ; j <= m ; j ++ ) { if ( f [ j ] != -1 && f [ j ] > res ) { res = f [ j ]; } } return res ; } }; Knapsack III \u00b6 Given N , M , s[i] , v[i] , find the maximum value you can put into the Knapsack while reuse is allowed Even though you can reuse, it is not limited, you can use items i at most m / s[i] times. In this sense, this problem is equivalent to the problem Knapsack IV . From O(MNK) O(MNK) to O(MN) O(MN) : notice the third loop can be optimized by closely looking for the redundant computation. For each i and j , we check all k s for the value of f[i - 1][j - k * A[i - 1]] to update f[i][j] . We can remove some of the redundant computation. Because the innermost loop will always look back an integer multiple of s[i] of the previous row f[i - 1] . For each i , we only looking for multiple times of s[i - 1] index before. We can use previous results directly for the current calculation then add v[i] instead of restart from k = 0 . This way we can get rid of the inner most loop. See the below diagram and sketch for the derivation of the state equation change. As a result, the solution code is very similar to the problem Knapsack II , except the single difference in indexing ( i instead of i - 1 ). but they are completely different, the similarity of the code is pure a coincidence. j 0 1 2 3 4 5 6 7 8 9 f[i-1] x x x x x x x x x x s[i] v v v 2 f[i][4] = max(f[i - 1][j - 0*2], f[i - 1][j - 1*2] + 1*v[i - 1], ...) f[i][6] = max(f[i - 1][j - 0*2], f[i][4] + v[i - 1]) = max(f[i - 1][j], f[i][j - s[i - 1]] + v[i - 1]) From O(MN) O(MN) to O(M) O(M) : This is from the following observation explained using the figure. It is a very clever idea in noticing that the new value only calculated from the the front index j - k*s[i-1] and the the old value from the same index (hence the j is iterating from 0 in the O(M) O(M) solution, in contrast, the O(M) O(M) solution in Knapsack II iterate j backward because need the old value not the new ones). DP O(MNK) class Solution { public : int backPackIII ( vector < int >& A , vector < int >& V , int m ) { int n = A . size (); int f [ n + 1 ][ m + 1 ]; f [ 0 ][ 0 ] = 0 ; for ( int j = 1 ; j < m ; ++ j ) { f [ 0 ][ j ] = -1 ; } for ( int i = 1 ; i <= n ; ++ i ) { for ( int j = 1 ; j <= m ; ++ j ) { f [ i ][ j ] = f [ i - 1 ][ j ]; for ( int k = 0 ; k <= m / A [ i - 1 ]; ++ k ) { if ( f [ i ][ j - k * A [ i - 1 ]] != -1 && j >= k * A [ i - 1 ]) { f [ i ][ j ] = max ( f [ i ][ j ], f [ i - 1 ][ j - k * A [ i - 1 ]] + k * V [ i - 1 ]); } } } } int res = 0 ; for ( int j = 0 ; j <= m ; ++ j ) { if ( f [ n ][ j ] != -1 && f [ n ][ j ] > res ) res = f [ n ][ j ]; } return res ; } } DP O(MN) class Solution { public : int backPackIII ( vector < int >& A , vector < int >& V , int m ) { int n = A . size (); int f [ n + 1 ][ m + 1 ]; f [ 0 ][ 0 ] = 0 ; for ( int j = 1 ; j < m ; ++ j ) { f [ 0 ][ j ] = -1 ; } for ( int i = 1 ; i <= n ; ++ i ) { for ( int j = 1 ; j <= m ; ++ j ) { f [ i ][ j ] = f [ i - 1 ][ j ]; if ( f [ i ][ j - A [ i - 1 ]] != -1 && j >= A [ i - 1 ]) { f [ i ][ j ] = max ( f [ i ][ j ], f [ i ][ j - A [ i - 1 ]] + V [ i - 1 ]); } } } int res = 0 ; for ( int j = 0 ; j <= m ; ++ j ) { if ( f [ n ][ j ] != -1 && f [ n ][ j ] > res ) res = f [ n ][ j ]; } return res ; } } DP O(M) class Solution { public : int backPackIII ( vector < int >& A , vector < int >& V , int m ) { int n = A . size (); if ( n == 0 ) { return 0 ; } int f [ m + 1 ]; /* init */ f [ 0 ] = 0 ; for ( int j = 1 ; j <= m ; j ++ ) { f [ j ] = -1 ; } for ( int i = 1 ; i <= n ; i ++ ) { for ( int j = 0 ; j <= m ; j ++ ) { if ( f [ j - A [ i - 1 ]] != -1 && j >= A [ i - 1 ]) { f [ j ] = max ( f [ j ], f [ j - A [ i - 1 ]] + V [ i - 1 ]); } } } int res = 0 ; for ( int j = 0 ; j <= m ; j ++ ) { if ( f [ j ] != -1 && f [ j ] > res ) { res = f [ j ]; } } return res ; } }; Knapsack IV \u00b6 Given N , M , s[i] , v[i] , c[i] , find the maximum value you can put into the Knapsack while the max time of usage of each item is given in c[i] . Knapsack V \u00b6 Given N , M , s[i] , find the number of possible Knapsack fills if each item can be used once . DP O(nm) class Solution { public : int backPackV ( vector < int >& nums , int T ) { int n = nums . size (); if ( n == 0 ) { return 0 ; } int f [ n + 1 ][ T + 1 ]; f [ 0 ][ 0 ] = 1 ; for ( int j = 1 ; j <= T ; j ++ ) { f [ 0 ][ j ] = 0 ; } for ( int i = 1 ; i <= n ; i ++ ) { for ( int j = 0 ; j <= T ; j ++ ) { f [ i ][ j ] = f [ i - 1 ][ j ]; if ( j >= nums [ i - 1 ]) { f [ i ][ j ] += f [ i - 1 ][ j - nums [ i - 1 ]]; } } } return f [ n ][ T ]; } }; DP O(m) class Solution { public : int backPackV ( vector < int >& nums , int T ) { int n = nums . size (); if ( n == 0 ) { return 0 ; } int f [ T + 1 ]; f [ 0 ] = 1 ; for ( int j = 1 ; j <= T ; j ++ ) { f [ j ] = 0 ; } for ( int i = 1 ; i <= n ; i ++ ) { for ( int j = T ; j >= 0 ; j -- ) { //f[j] = f[j - A[i - 1]] ==> f'[j] if ( j >= nums [ i - 1 ]) { // f'[j] // cover old f[j] f [ j ] += f [ j - nums [ i - 1 ]]; } } } return f [ T ]; } }; Knapsack VI \u00b6 Given N , M , s[i] , find the number of possible Knapsack fills if each item can be used unlimited times . \u8fd9\u9053\u9898\u7b49\u540c\u4e8eLeetcode\u91cc Combinations Sum IV \u8fd9\u91cc\u53ef\u4ee5\u968f\u4fbf\u53d6\uff0c\u4f3c\u4e4e\u9898\u76ee\u53d8\u5f97\u65e0\u6cd5\u4e0b\u624b\uff0c\u8003\u8651\u201c\u6700\u540e\u4e00\u6b65\u201d\u8fd9\u4e2a\u6280\u5de7\u4e0d\u80fd\u7528\u4e86\uff0c\u56e0\u4e3a\u6700\u540e\u4e00\u6b65\u53ef\u4ee5\u662f\u4efb\u610f\u4e00\u4e2a\u4e86\u3002 \u4f46\u4ecd\u7136\u53ef\u4ee5\u7528\u5b50\u95ee\u9898\u6765\u8003\u8651\u3002\u5148\u4e0d\u7ba1\u6700\u540e\u4e00\u6b65\u662f\u54ea\u4e00\u4e2a\uff0c\u6700\u540e\u4e00\u6b65\u4e4b\u524d\u7684\u76f8\u52a0\u7684\u603b\u548c\u4e00\u5b9a\u662f Target - x. \u8fd9\u6837\u5c31\u8f6c\u5316\u6210\u4e00\u4e2a\u5b50\u95ee\u9898\u53ef\u4ee5\u7528DP\u6765\u505a\u3002 \u5177\u4f53\u505a\u6cd5\u6211\u4eec\u53ef\u4ee5\u5bf9\u4e8e\u6bcf\u4e00\u4e2a\u5c0f\u4e8e\u201c\u603b\u627f\u91cd\u201d\u7684\u91cd\u91cf\u8fdb\u884c\u679a\u4e3e\u6700\u540e\u4e00\u6b65x\u3002\u53ef\u80fd\u7684x\u662f A[0], ..., A[i - 1] \u4e2d\u4efb\u610f\u4e00\u4e2a. Compare With knapsack V, how to deal with the unlimited usage DP O(m) class Solution { public : int backPackVI ( vector < int >& nums , int T ) { int n = nums . size (); if ( n == 0 ) { return 0 ; } int f [ T + 1 ]; f [ 0 ] = 1 ; /* for each sub problem */ for ( int i = 1 ; i <= T ; i ++ ) { f [ i ] = 0 ; /* enumerate the last step */ for ( int j = 0 ; j < n ; j ++ ) { if ( i >= nums [ j ]) { f [ i ] += f [ i - nums [ j ]]; } } } return f [ T ]; } }; How to print such solutions // Suppose we also interested in print one of the possible solution. How could we change the code? // f[i]: \u5b58\u591a\u5c11\u79cd\u65b9\u5f0f // pi[i]: \u5982\u679c f[i] >= 1, \u6700\u540e\u4e00\u4e2a\u6570\u5b57\u53ef\u4ee5\u662fpi[i] class Solution { public : int backPackVI ( vector < int >& nums , int T ) { int n = nums . size (); if ( n == 0 ) { return 0 ; } int f [ T + 1 ]; /* pi[i]: \u5982\u679ci\u53ef\u62fc\u51fa(f[i] >= 1), \u6700\u540e\u4e00\u4e2a\u662fpi[i] */ int pi [ T + 1 ]; f [ 0 ] = 1 ; for ( int i = 1 ; i <= T ; i ++ ) { f [ i ] = 0 ; for ( int j = 0 ; j < n ; j ++ ) { if ( i >= nums [ j ]) { f [ i ] += f [ i - nums [ j ]]; /* \u6700\u540e\u4e00\u4e2a\u662fnums[j]\u7684\u53ef\u62fc\u51fai */ if ( f [ i - nums [ j ]] > 0 ) { /* \u7eaa\u5f55\u4e0b\u6765 */ pi [ i ] = nums [ j ]; } } } } if ( f [ T ] > 0 ) { int i = T ; cout << i << \"=\" << endl ; while ( i != 0 ) { // sum is i now; // last number is pi[i] // previuos sum is i - pi[i] cout << pi [ i ] << endl ; i -= pi [ i ]; } } return f [ T ]; } }; Race Car \u00b6 waymo \u5750\u6807\u578b \u00b6 Triangle \u00b6 Unique Paths \u00b6 Unique Paths II \u00b6 Minimum Path Sum \u00b6 Bomb Enemy \u00b6 Dungeon Game \u00b6 221. Maximal Square \u00b6 Solution 1 monotonic stack Solution 2 Dynamic Programming C++ DP memoization class Solution { public : int maximalSquare ( vector < vector < char >>& matrix ) { if ( matrix . empty ()) return 0 ; int m = matrix . size (); int n = matrix [ 0 ]. size (); vector < vector < int >> sum ( m + 1 , vector < int > ( n + 1 , 0 )); int res = 0 ; for ( int i = 1 ; i <= m ; ++ i ) { for ( int j = 1 ; j <= n ; ++ j ) { sum [ i ][ j ] = matrix [ i - 1 ][ j - 1 ] - '0' + sum [ i - 1 ][ j ] + sum [ i ][ j - 1 ] - sum [ i - 1 ][ j - 1 ]; } } int area = 0 ; for ( int i = 1 ; i <= m ; ++ i ) { for ( int j = 1 ; j <= n ; ++ j ) { for ( int k = min ( m - i + 1 , n - j + 1 ); k > 0 ; -- k ) { area = sum [ i + k - 1 ][ j + k - 1 ] - sum [ i + k - 1 ][ j - 1 ] - sum [ i - 1 ][ j + k - 1 ] + sum [ i - 1 ][ j - 1 ]; if ( area == k * k ) { res = max ( res , area ); break ; } } } } return res ; } }; C++ DP subproblem class Solution { public : int maximalSquare ( vector < vector < char >>& matrix ) { if ( matrix . empty ()) return 0 ; int m = matrix . size (); int n = matrix [ 0 ]. size (); vector < vector < int >> sizes ( m , vector < int > ( n , 0 )); int res = 0 ; for ( int i = 0 ; i < m ; ++ i ) { for ( int j = 0 ; j < n ; ++ j ) { sizes [ i ][ j ] = matrix [ i ][ j ] - '0' ; if ( sizes [ i ][ j ] == 0 ) continue ; if ( i > 0 && j > 0 ) sizes [ i ][ j ] = min ( min ( sizes [ i - 1 ][ j - 1 ], sizes [ i - 1 ][ j ]), sizes [ i ][ j - 1 ]) + 1 ; res = max ( res , sizes [ i ][ j ] * sizes [ i ][ j ]); } } return res ; } }; \u5e8f\u5217\u578b \u00b6 Perfect Squares \u00b6 Longest Increasing Subsequence \u00b6 Number of Longest Increasing Subsequence \u00b6 Longest Increasing Continuous Subsequence \u00b6 \u5e8f\u5217\u578b + \u72b6\u6001 \u00b6 Paint House \u00b6 Paint House II \u00b6 House Robber \u00b6 House Robber II \u00b6 Best Time to Buy and Sell Stock \u00b6 Best Time to Buy and Sell Stock II \u00b6 Best Time to Buy and Sell Stock III \u00b6 Best Time to Buy and Sell Stock IV \u00b6 Portfolio Value Optimization \u00b6 There are two questions regarding this problem: leetcode discussion Can buy fractions of a stock Cannot buy fractions of a stock DP solution #include <iostream> #include <vector> #include <climits> #include <numeric> #include <algorithm> using namespace std ; class Solution { public : int maxProfit ( vector < int > curr_price , vector < int > new_price , vector < int > amount , int A ) { int n = curr_price . size (); int f [ n + 1 ][ A + 1 ]; // max profit of the first i stocks with payment j // init f [ 0 ][ 0 ] = 0 ; for ( int j = 1 ; j <= A ; ++ j ) { f [ 0 ][ j ] = -1 ; } // f[i][j] = max(f[i - 1][j], f[i - 1][j - k * curr_price[i]] + k * new_price[i]) for ( int i = 1 ; i <= n ; ++ i ) { for ( int j = 0 ; j <= A ; ++ j ) { f [ i ][ j ] = f [ i - 1 ][ j ]; for ( int k = 0 ; k <= amount [ i ]; ++ k ) { if ( f [ i - 1 ][ j - k * curr_price [ i - 1 ]] != -1 && j >= k * curr_price [ i - 1 ]) { f [ i ][ j ] = max ( f [ i ][ j ], f [ i - 1 ][ j - k * curr_price [ i - 1 ]] + k * new_price [ i - 1 ]); } } } } int res = 0 ; for ( int j = A ; j >= 0 ; -- j ) { if ( f [ n ][ j ] != -1 && f [ n ][ j ] > res ) { res = f [ n ][ j ]; } } return res ; } }; int main () { // test1 res = 255 // vector<int> v{15, 40, 25, 30}; // vector<int> w{45, 50, 35, 25}; // vector<int> s{3, 3, 3, 4}; // int m = 140; // test2 res = 60 vector < int > v { 15 , 20 }; vector < int > w { 30 , 45 }; vector < int > s { 3 , 3 }; int m = 30 ; int res = 0 ; Solution sol = Solution (); res = sol . maxProfit ( v , w , s , m ); cout << res ; } DP space O(M) #include <iostream> #include <vector> #include <climits> #include <numeric> #include <algorithm> using namespace std ; class Solution { public : int maxProfit ( vector < int > curr_price , vector < int > new_price , vector < int > amount , int A ) { int n = curr_price . size (); int f [ A + 1 ]; // max profit of the first i stocks with payment j // init f [ 0 ] = 0 ; for ( int j = 1 ; j <= A ; ++ j ) { f [ j ] = -1 ; } // f[i][j] = max(f[i - 1][j], f[i - 1][j - k * curr_price[i]] + k * new_price[i]) for ( int i = 1 ; i <= n ; ++ i ) { for ( int j = A ; j >= 0 ; -- j ) { for ( int k = 0 ; k <= amount [ i ]; ++ k ) { if ( f [ j - k * curr_price [ i - 1 ]] != -1 && j >= k * curr_price [ i - 1 ]) { f [ j ] = max ( f [ j ], f [ j - k * curr_price [ i - 1 ]] + k * new_price [ i - 1 ]); } } } } int res = 0 ; for ( int j = A ; j >= 0 ; -- j ) { if ( f [ j ] != -1 && f [ j ] > res ) { res = f [ j ]; } } return res ; } }; int main () { // test1 res = 255 // vector<int> v{15, 40, 25, 30}; // vector<int> w{45, 50, 35, 25}; // vector<int> s{3, 3, 3, 4}; // int m = 140; // test2 res = 60 vector < int > v { 15 , 20 }; vector < int > w { 30 , 45 }; vector < int > s { 3 , 3 }; int m = 30 ; int res = 0 ; Solution sol = Solution (); res = sol . maxProfit ( v , w , s , m ); cout << res ; } \u5212\u5206\u578b \u00b6 Word Break \u00b6 Solution 1 use set dictionary O(n^2), iterate forwards class Solution { public : bool wordBreak ( string s , vector < string >& wordDict ) { int m = s . length (); unordered_set < string > wordSet ( wordDict . begin (), wordDict . end ()); bool f [ m + 1 ] = { 0 }; f [ 0 ] = true ; for ( int i = 1 ; i <= m ; i ++ ) { for ( int j = 0 ; j <= i ; j ++ ) { if ( f [ j ] && wordSet . count ( s . substr ( j , i - j )) != 0 ) { f [ i ] = true ; break ; } } } return f [ m ]; } }; Solution 2 use set dictionary O(n^2), iterate backwards class Solution { public : bool wordBreak ( string s , vector < string >& wordDict ) { int n = s . length (); unordered_set < string > dict ( wordDict . begin (), wordDict . end ()); int f [ n + 1 ] = { 0 }; f [ 0 ] = 1 ; for ( int i = 0 ; i <= n ; ++ i ) { for ( int j = n - 1 ; j >= 0 ; -- j ) { if ( f [ j ] && dict . count ( s . substr ( j , i - j ))) { f [ i ] = 1 ; break ; } } } return f [ n ]; } }; Solution 3 (best solution) use vector, iterate through string Do not use word set to check exist or not, use each word as the last step. class Solution { public : bool wordBreak ( string s , vector < string >& wordDict ) { int n = s . length (); int f [ n + 1 ] = { 0 }; f [ 0 ] = 1 ; for ( int i = 0 ; i <= n ; ++ i ) { for ( string word : wordDict ) { if ( word . length () <= i && f [ i - word . length ()]) { if ( s . substr ( i - word . length (), word . length ()) == word ) { f [ i ] = 1 ; break ; } } } } return f [ n ]; } }; Maximum Vacation Days \u00b6 Solution 1 class Solution { public : int maxVacationDays ( vector < vector < int >>& flights , vector < vector < int >>& days ) { int n = flights . size (); int k = days [ 0 ]. size (); vector < vector < int >> f ( n , vector < int > ( k , -1 )); f [ 0 ][ 0 ] = days [ 0 ][ 0 ]; for ( int i = 1 ; i < n ; ++ i ) { if ( flights [ 0 ][ i ]) { f [ i ][ 0 ] = days [ i ][ 0 ]; } } for ( int d = 1 ; d < k ; ++ d ) { for ( int i = 0 ; i < n ; ++ i ) { for ( int j = 0 ; j < n ; ++ j ) { if (( flights [ j ][ i ] || i == j ) && f [ j ][ d - 1 ] != 1 ) { f [ i ][ d ] = max ( f [ i ][ d ], f [ j ][ d - 1 ] + days [ i ][ d ]); } } } } int result = 0 ; for ( int i = 0 ; i < n ; ++ i ) { result = max ( result , f [ i ][ k - 1 ]); } return result ; } }; Solution 2 DFS with Cache class Solution { vector < vector < int >> memo ; public : int maxVacationDays ( vector < vector < int >>& flights , vector < vector < int >>& days ) { vector < vector < int >> memo ( flights . size (), vector < int > ( days [ 0 ]. size (), INT_MIN )); return dfs ( flights , days , 0 , 0 , memo ); } int dfs ( vector < vector < int >>& flights , vector < vector < int >>& days , int start , int day , vector < vector < int >>& memo ) { if ( day == days [ 0 ]. size ()) { return 0 ; } if ( memo [ start ][ day ] != INT_MIN ) { return memo [ start ][ day ]; } int maxVal = 0 ; for ( int i = 0 ; i < flights . size (); ++ i ) { if ( flights [ start ][ i ] || start == i ) { maxVal = max ( maxVal , days [ i ][ day ] + dfs ( flights , days , i , day + 1 , memo )); } } memo [ start ][ day ] = maxVal ; return maxVal ; } }; Solution 3 (greedy, wrong) class Solution { public : int maxVacationDays ( vector < vector < int >>& flights , vector < vector < int >>& days ) { int n = flights . size (); int k = days [ 0 ]. size (); int start = 0 ; int result = 0 ; set < pair < int , int > , std :: greater < pair < int , int >>> s ; for ( int col = 0 ; col < k ; ++ col ) { for ( int row = 0 ; row < n ; ++ row ) { s . insert ({ days [ row ][ col ], row }); } int stay = 1 ; for ( auto p : s ) { if ( flights [ start ][ p . second ]) { result += p . first ; cout << p . first << \" \" << p . second << endl ; start = p . second ; stay = 0 ; break ; } } if ( stay ) { result += days [ start ][ col ]; } s . clear (); } return result ; } }; Decode Ways \u00b6 Decode Ways II \u00b6 Solution 1 class Solution { public : int numDecodings ( string s ) { const int MOD = 1000000007 ; int n = s . length (); long long f [ n + 1 ] = { 0 }; if ( n == 0 ) return 0 ; f [ 0 ] = 1 ; for ( int i = 1 ; i <= n ; ++ i ) { if ( s [ i - 1 ] == '*' ) { // not include '0' according to the problem statement f [ i ] = ( f [ i ] + f [ i - 1 ] * 9 ) % MOD ; // number of ways only decode one digit if ( i > 1 ) { // number of ways decode two digits if ( s [ i - 2 ] == '*' ) { f [ i ] = ( f [ i ] + f [ i - 2 ] * 15 ) % MOD ; // 11 - 26 } else if ( s [ i - 2 ] == '1' ) { f [ i ] = ( f [ i ] + f [ i - 2 ] * 9 ) % MOD ; // 11 - 19 } else if ( s [ i - 2 ] == '2' ) { f [ i ] = ( f [ i ] + f [ i - 2 ] * 6 ) % MOD ; // 21 - 26 } } } else { // s[i - 1] != '*', have to consider the case '0' if ( s [ i - 1 ] != '0' ) { // number of ways only decode one digit f [ i ] = ( f [ i ] + f [ i - 1 ]) % MOD ; } if ( i > 1 ) { // number of ways decode two digits if ( s [ i - 2 ] == '*' ) { if ( s [ i - 1 ] <= '6' ) { // '*' can represent 1, 2 f [ i ] = ( f [ i ] + f [ i - 2 ] * 2 ) % MOD ; } else { // '*' can only represent 1 f [ i ] = ( f [ i ] + f [ i - 2 ]) % MOD ; } } else { // no '*' case int t = ( s [ i - 2 ] - '0' ) * 10 + s [ i - 1 ] - '0' ; if ( t >= 10 && t <= 26 ) { f [ i ] = ( f [ i ] + f [ i - 2 ]) % MOD ; } } } } } return f [ n ] % MOD ; } }; \u53cc\u5e8f\u5217\u578b \u00b6 Longest Common Subsequence \u00b6 Solution 1 consider the last step, the case is NOT A[n-1] == B[l-1] and A[n-1] != B[m-1] but, three cases: A[n-1] is included, B[m-1] is included, both A[n-1] and B[m-1] are included, (A[n-1] == B[m-1]) . Interleaving String \u00b6 Solution 1 class Solution { public : bool isInterleave ( string s1 , string s2 , string s3 ) { // f[i][j] = OR(f[i][j - 1]|B[j - 1] in C, f[i - 1][j]|A[i - 1] in C); int m = s1 . length (); int n = s2 . length (); int l = s3 . length (); if ( m + n != l ) return false ; int f [ m + 1 ][ n + 1 ] = { 0 }; f [ 0 ][ 0 ] = 1 ; for ( int i = 0 ; i <= m ; ++ i ) { for ( int j = 0 ; j <= n ; ++ j ) { if ( i > 0 && s1 [ i - 1 ] == s3 [ i + j - 1 ]) { f [ i ][ j ] |= f [ i - 1 ][ j ]; } if ( j > 0 && s2 [ j - 1 ] == s3 [ i + j - 1 ]) { f [ i ][ j ] |= f [ i ][ j - 1 ]; } } } return f [ m ][ n ]; } }; Edit Distance \u00b6 Solution 1 state: f[i][j] is min edit distance to make the first i chars of A the same as first j chars of B, as we consider the last operation (insert, delete, replace). One key idea of this problem is after the operation, the length of the A and B are not necessarily i and j . The focus here is the \"min edit distance\", not the length of A or B. For example, if the last step is insert, A's length will become i + 1 , B's length will maintain j . You should also note that the A, B concept is imaginary, B is actually changed from A by editing once. As a result, we know i + 1 == j after the insertion operation. Don't iterpret the state f[i][j] as min edit distance of first i chars in A and first j chars in B and after the editing, A's length is i and B's length is j . You should seperate the two intepretations. class Solution { public : int minDistance ( string word1 , string word2 ) { int m = word1 . length (); int n = word2 . length (); int f [ m + 1 ][ n + 1 ]; int i , j ; for ( i = 0 ; i <= m ; i ++ ) { for ( j = 0 ; j <= n ; j ++ ) { if ( i == 0 ) { f [ i ][ j ] = j ; continue ; } if ( j == 0 ) { f [ i ][ j ] = i ; continue ; } // delete insert f [ i ][ j ] = min ( f [ i - 1 ][ j ] + 1 , f [ i ][ j - 1 ] + 1 ); // replace f [ i ][ j ] = min ( f [ i ][ j ], f [ i - 1 ][ j - 1 ] + 1 ); if ( word1 [ i - 1 ] == word2 [ j - 1 ]) { f [ i ][ j ] = min ( f [ i ][ j ], f [ i - 1 ][ j - 1 ]); } } } return f [ m ][ n ]; } }; Solution 2 Space optimized class Solution { public : int minDistance ( string word1 , string word2 ) { int m = word1 . length (); int n = word2 . length (); int f [ 2 ][ n + 1 ]; int prev = 0 , curr = 0 ; for ( int i = 0 ; i <= m ; i ++ ) { prev = curr ; curr = 1 - curr ; for ( int j = 0 ; j <= n ; j ++ ) { if ( i == 0 ) { f [ curr ][ j ] = j ; continue ; } if ( j == 0 ) { f [ curr ][ j ] = i ; continue ; } // delete insert f [ curr ][ j ] = min ( f [ prev ][ j ] + 1 , f [ curr ][ j - 1 ] + 1 ); // replace f [ curr ][ j ] = min ( f [ curr ][ j ], f [ prev ][ j - 1 ] + 1 ); if ( word1 [ i - 1 ] == word2 [ j - 1 ]) { f [ curr ][ j ] = min ( f [ curr ][ j ], f [ prev ][ j - 1 ]); } } } return f [ curr ][ n ]; } }; Memoization \u00b6 1049. Last Stone Weight II \u00b6 Solution 1 Recursion + memoization Solution 2 Use two set to simulate Solution 3 Transform into knapsack Thinking it as adding \"+\" and \"-\" in front of each number. The sum of all the numbers with \"+\" S1, the sum of all number with \"-\" S2, we want to find the minimum of |S1 - S2| because we have S1 + S2 = total . suppose S1 <= S2 , we want to minimize S2 - S1 = total - S1 - S1 = total - 2 * S1 ; We wil achieve the goal by maximizing S1 , so that the stete would be. f[i][j] represent the optimal value of use first i stones to achieve maximum j ( S1 ) value. C++ Memoization class Solution { int memo [ 30 ][ 3001 ]; public : int lastStoneWeightII ( vector < int >& stones ) { return helper ( stones , 0 , 0 ); } int helper ( vector < int >& stones , int i , int sum ) { if ( i == stones . size ()) return abs ( sum ); if ( memo [ i ][ sum ] != 0 ) return memo [ i ][ sum ]; memo [ i ][ sum ] = min ( helper ( stones , i + 1 , abs ( sum - stones [ i ])), helper ( stones , i + 1 , sum + stones [ i ])); return memo [ i ][ sum ]; } }; C++ Set simulation class Solution { public : int lastStoneWeightII ( vector < int >& stones ) { if ( stones . size () == 0 ) return 0 ; unordered_set < int > set_left ({ stones [ 0 ]}); for ( int i = 1 ; i < stones . size (); ++ i ) { unordered_set < int > set_right = set_left ; set_left . clear (); for ( int y : set_right ) { set_left . insert ( y + stones [ i ]); set_left . insert ( y - stones [ i ]); } } int ans = INT_MAX ; for ( int z : set_left ) { ans = min ( ans , abs ( z )); } return ans ; } }; C++ knapsack class Solution { public : int lastStoneWeightII ( vector < int >& stones ) { int n = stones . size (); int sum = accumulate ( begin ( stones ), end ( stones ), 0 ); vector < vector < int >> f ( n + 1 , vector < int > ( sum / 2 + 1 , 0 )); for ( int i = 1 ; i <= n ; ++ i ) { for ( int j = 1 ; j < sum / 2 + 1 ; ++ j ) { if ( j < stones [ i - 1 ]) { f [ i ][ j ] = f [ i - 1 ][ j ]; } else { f [ i ][ j ] = max ( f [ i - 1 ][ j ], f [ i - 1 ][ j - stones [ i - 1 ]] + stones [ i - 1 ]); } } } return sum - 2 * f [ n ][ sum / 2 ]; } }; C++ knapsack O(1) space class Solution { public : int lastStoneWeightII ( vector < int >& stones ) { int n = stones . size (); int sum = accumulate ( begin ( stones ), end ( stones ), 0 ); vector < int > f ( sum / 2 + 1 , 0 ); for ( int i = 1 ; i <= n ; ++ i ) { for ( int j = sum / 2 ; j >= 0 ; -- j ) { if ( j >= stones [ i - 1 ]) { f [ j ] = max ( f [ j ], f [ j - stones [ i - 1 ]] + stones [ i - 1 ]); } } } return sum - 2 * f [ sum / 2 ]; } }; Sentence Screen Fitting \u00b6 Solution 1 You can view the sentence as a space seperated English sentence and then use each row to \"frame\" the sentence. If the end of the frame overlap a space, we continue move to the next frame if available. If the end of the frame overlap a character, we should move the right the frame to the end of the prev word and move on to the next frame. class Solution { public : int wordsTyping ( vector < string >& sentence , int rows , int cols ) { int n = sentence . size (); string s ; for ( string w : sentence ) { s += string ( w + \" \" ); } int start = 0 , len = s . length (); //s with an extra trailing space. for ( int i = 0 ; i < rows ; i ++ ) { start += cols ; if ( s [ start % len ] == ' ' ) { ++ start ; } else { while ( start > 0 && s [( start - 1 ) % len ] != ' ' ) { -- start ; } } } return start / len ; } }; Solution 2 One insight of this solution is that every word in the sentence is possible to start a new row, but not necessary. For example, a long row can contain multiple of the setences, but the last word cannot fit into this sigle row, it start a new row. In this case only the first word in the sentence and the last word can start a new row. We use this insight in the following way, for each word, we assume it could start a new row, then use the following words in the sentence to fix the row, we record the total number of words that can fit this row in dp[i] , i is the index of the starting word in the sentence. After the calculation, we could count sum(dp[i]) , i is starting word's index. class Solution { public : int wordsTyping ( vector < string >& sentence , int rows , int cols ) { int n = sentence . size (); int f [ n ] = { 0 }; for ( int i = 0 ; i < n ; ++ i ) { int len = 0 , count = 0 , idx = i ; while ( len + sentence [ idx % n ]. length () <= cols ) { len += ( sentence [ idx % n ]. length () + 1 ); idx ++ ; count ++ ; } f [ i ] = count ; } int total = 0 ; for ( int i = 0 , idx = 0 ; i < rows ; ++ i ) { total += f [ idx ]; idx = ( f [ idx ] + idx ) % n ; } return total / n ; } }; TODO: This solution may be further improved because for some of the count, we never used. Can we calculate the final result by only one loop, or in a nested loop?","title":"Dynamic Programming"},{"location":"leetcode/dynamic-programming/notes/#dynamic-programming","text":"","title":"Dynamic Programming"},{"location":"leetcode/dynamic-programming/notes/#_1","text":"\u5982\u4f55\u5728\u5206\u6790\u6700\u540e\u4e00\u6b65\u65f6\u9009\u62e9\u6b63\u786e\u89d2\u5ea6\u5e76\u8003\u8651\u6240\u6709\u53ef\u80fd\u60c5\u51b5\uff0c\u8fdb\u800c\u5199\u51fa\u72b6\u6001\u8f6c\u5316\u65b9\u7a0b. i.e. Buy and Shell stock, Edit distance, Longest Increasing Subsequence.","title":"\u96be\u70b9"},{"location":"leetcode/dynamic-programming/notes/#knapsack-problems","text":"\u80cc\u5305\u95ee\u9898\u4e5d\u8bb2 The knapsack problems often given the follow conditions N , the number of items M , the size of the knapsack s[i] , the size of the item i . v[i] , the value of the item i . c[i] , the maximum time item i can be used. c[i] could be an all 1 array, hence 0-1 knapsack problem. c[i] could also be inf , meaning the items can be used as many time as you want (bounded by the size of the knapsack). Knapsack problems ask Whether the given items can fit into the size of the knapsack. Find the maximum capacity the items can occupy the knapsack. Find the maximum value of items that can fit into the knapsack. Find the total number of ways the given item can fit into the size of the knapsack. (reuse is allowed) In all the above questions, the items can be reused or not reused, they are very different and the throught process and the solution are also different.","title":"Knapsack problems"},{"location":"leetcode/dynamic-programming/notes/#knapsack-i","text":"Given N , M , s[i] , find the maximum size you can put into the Knapsack. DP O(nm) class Solution { public : int backPackI ( int m , vector < int > A ) { int n = A . size (); if ( n == 0 ) return 0 ; bool f [ n + 1 ][ m + 1 ]; f [ 0 ][ 0 ] = true ; for ( int j = 1 ; j <= m ; ++ j ) { f [ 0 ][ j ] = false ; } for ( int i = 1 ; i <= n ; ++ i ) { for ( int j = 0 ; j <= m ; ++ j ) { f [ i ][ j ] = f [ i - 1 ][ j ]; if ( j >= A [ i - 1 ]) { f [ i ][ j ] = f [ i ][ j ] || f [ i - 1 ][ j - A [ i - 1 ]]; } } } for ( int j = m ; j >= 0 ; -- j ) { if ( f [ n ][ j ] == true ) return j ; } return 0 ; } } DP O(m) space class Solution { public : int backPackI ( int m , vector < int > A ) { int n = A . size (); if ( n == 0 ) return 0 ; bool f [ m + 1 ]; f [ 0 ] = true ; for ( int i = 1 ; i <= n ; ++ i ) { for ( int j = m ; j >= 0 ; -- j ) { f [ j ] = f [ j - 1 ] if ( j >= A [ i - 1 ]) { f [ j ] = f [ j ] || f [ j - A [ i - 1 ]]; } } } for ( int j = m ; j >= 0 ; -- j ) { if ( f [ j ] == true ) return j ; } return 0 ; } }","title":"Knapsack I"},{"location":"leetcode/dynamic-programming/notes/#knapsack-ii","text":"Given N , M , s[i] , v[i] , find the maximum value you can put into the Knapsack. \u601d\u8003\u65b9\u5f0f\u4efb\u7136\u662f\u4ece\u6700\u540e\u4e00\u4e2a\u7269\u54c1\u9009\u8fd8\u662f\u4e0d\u9009\uff0c\u53ea\u662f\u6211\u4eec\u73b0\u5728\u8003\u8651\u7684\u662f\u4ef7\u503c\u3002\u6b64\u65f6\u72b6\u6001\u5c31\u4e0d\u80fd\u662f\u53ef\u884c\u6027(Backpack)\u6216\u8005\u591a\u5c11\u79cd\u4e86(Backpack V), \u6211\u4eec\u8981\u7eaa\u5f55\u603b\u4ef7\u503c\u3002 state f[i][w] : \u8868\u793a\u524di\u4e2a\u7269\u54c1\u80fd\u62fc\u6210\u91cd\u91cfw\u7684\u603b\u4ef7\u503c(V)\u3002 state transition equaltion: f[i][w] = max(f[i\u22121][w], f[i\u22121][w\u2212A[i\u22121]]+V[i\u22121]|w\u2265A[i\u22121]\u4e14 f[i\u22121][w\u2212A[i\u22121]]\u2260\u22121) Initialization: f[0][0] = 0, f[0][1] = -1, ... f[0][w] = -1. -1 \u4ee3\u8868\u4e0d\u80fd\u88ab\u62fc\u51fa\u3002 DP O(mn) class Solution { public : /** * f[i][w]: \u524di\u4e2a\u7269\u54c1\u80fd\u591f\u62fc\u6210w\u7684\u603b\u4ef7\u503c * f[i][w] = max(f[i - 1][w], f[i - 1][w - A[i - 1]] + V[i - 1]) */ int backPackII ( int m , vector < int > A , vector < int > V ) { int n = A . size (); if ( n == 0 ) { return 0 ; } int f [ n + 1 ][ m + 1 ]; /* init */ f [ 0 ][ 0 ] = 0 ; for ( int j = 1 ; j <= m ; j ++ ) { f [ 0 ][ j ] = -1 ; } for ( int i = 1 ; i <= n ; i ++ ) { for ( int j = 0 ; j <= m ; j ++ ) { f [ i ][ j ] = f [ i - 1 ][ j ]; /* \u6b64\u5904\u5fc5\u987b\u521d\u59cb\u5316 */ if ( f [ i - 1 ][ j - A [ i - 1 ]] != -1 && j >= A [ i - 1 ]) { f [ i ][ j ] = max ( f [ i ][ j ], f [ i - 1 ][ j - A [ i - 1 ]] + V [ i - 1 ]); } } } int res = 0 ; for ( int j = 0 ; j <= m ; j ++ ) { if ( f [ n ][ j ] != -1 && f [ n ][ j ] > res ) { res = f [ n ][ j ]; } } return res ; } }; DP O(M) class Solution { public : /** * f[i][w]: \u524di\u4e2a\u7269\u54c1\u80fd\u591f\u62fc\u6210w\u7684\u603b\u4ef7\u503c * f[i][w] = max(f[i - 1][w], f[i - 1][w - A[i - 1]] + V[i - 1]) */ int backPackII ( int m , vector < int > A , vector < int > V ) { // write your code here int n = A . size (); if ( n == 0 ) { return 0 ; } int f [ m + 1 ]; /* init */ f [ 0 ] = 0 ; for ( int j = 1 ; j <= m ; j ++ ) { f [ j ] = -1 ; } for ( int i = 1 ; i <= n ; i ++ ) { for ( int j = m ; j >= 0 ; j -- ) { if ( f [ j - A [ i - 1 ]] != -1 && j >= A [ i - 1 ]) { f [ j ] = max ( f [ j ], f [ j - A [ i - 1 ]] + V [ i - 1 ]); } } } int res = 0 ; for ( int j = 0 ; j <= m ; j ++ ) { if ( f [ j ] != -1 && f [ j ] > res ) { res = f [ j ]; } } return res ; } };","title":"Knapsack II"},{"location":"leetcode/dynamic-programming/notes/#knapsack-iii","text":"Given N , M , s[i] , v[i] , find the maximum value you can put into the Knapsack while reuse is allowed Even though you can reuse, it is not limited, you can use items i at most m / s[i] times. In this sense, this problem is equivalent to the problem Knapsack IV . From O(MNK) O(MNK) to O(MN) O(MN) : notice the third loop can be optimized by closely looking for the redundant computation. For each i and j , we check all k s for the value of f[i - 1][j - k * A[i - 1]] to update f[i][j] . We can remove some of the redundant computation. Because the innermost loop will always look back an integer multiple of s[i] of the previous row f[i - 1] . For each i , we only looking for multiple times of s[i - 1] index before. We can use previous results directly for the current calculation then add v[i] instead of restart from k = 0 . This way we can get rid of the inner most loop. See the below diagram and sketch for the derivation of the state equation change. As a result, the solution code is very similar to the problem Knapsack II , except the single difference in indexing ( i instead of i - 1 ). but they are completely different, the similarity of the code is pure a coincidence. j 0 1 2 3 4 5 6 7 8 9 f[i-1] x x x x x x x x x x s[i] v v v 2 f[i][4] = max(f[i - 1][j - 0*2], f[i - 1][j - 1*2] + 1*v[i - 1], ...) f[i][6] = max(f[i - 1][j - 0*2], f[i][4] + v[i - 1]) = max(f[i - 1][j], f[i][j - s[i - 1]] + v[i - 1]) From O(MN) O(MN) to O(M) O(M) : This is from the following observation explained using the figure. It is a very clever idea in noticing that the new value only calculated from the the front index j - k*s[i-1] and the the old value from the same index (hence the j is iterating from 0 in the O(M) O(M) solution, in contrast, the O(M) O(M) solution in Knapsack II iterate j backward because need the old value not the new ones). DP O(MNK) class Solution { public : int backPackIII ( vector < int >& A , vector < int >& V , int m ) { int n = A . size (); int f [ n + 1 ][ m + 1 ]; f [ 0 ][ 0 ] = 0 ; for ( int j = 1 ; j < m ; ++ j ) { f [ 0 ][ j ] = -1 ; } for ( int i = 1 ; i <= n ; ++ i ) { for ( int j = 1 ; j <= m ; ++ j ) { f [ i ][ j ] = f [ i - 1 ][ j ]; for ( int k = 0 ; k <= m / A [ i - 1 ]; ++ k ) { if ( f [ i ][ j - k * A [ i - 1 ]] != -1 && j >= k * A [ i - 1 ]) { f [ i ][ j ] = max ( f [ i ][ j ], f [ i - 1 ][ j - k * A [ i - 1 ]] + k * V [ i - 1 ]); } } } } int res = 0 ; for ( int j = 0 ; j <= m ; ++ j ) { if ( f [ n ][ j ] != -1 && f [ n ][ j ] > res ) res = f [ n ][ j ]; } return res ; } } DP O(MN) class Solution { public : int backPackIII ( vector < int >& A , vector < int >& V , int m ) { int n = A . size (); int f [ n + 1 ][ m + 1 ]; f [ 0 ][ 0 ] = 0 ; for ( int j = 1 ; j < m ; ++ j ) { f [ 0 ][ j ] = -1 ; } for ( int i = 1 ; i <= n ; ++ i ) { for ( int j = 1 ; j <= m ; ++ j ) { f [ i ][ j ] = f [ i - 1 ][ j ]; if ( f [ i ][ j - A [ i - 1 ]] != -1 && j >= A [ i - 1 ]) { f [ i ][ j ] = max ( f [ i ][ j ], f [ i ][ j - A [ i - 1 ]] + V [ i - 1 ]); } } } int res = 0 ; for ( int j = 0 ; j <= m ; ++ j ) { if ( f [ n ][ j ] != -1 && f [ n ][ j ] > res ) res = f [ n ][ j ]; } return res ; } } DP O(M) class Solution { public : int backPackIII ( vector < int >& A , vector < int >& V , int m ) { int n = A . size (); if ( n == 0 ) { return 0 ; } int f [ m + 1 ]; /* init */ f [ 0 ] = 0 ; for ( int j = 1 ; j <= m ; j ++ ) { f [ j ] = -1 ; } for ( int i = 1 ; i <= n ; i ++ ) { for ( int j = 0 ; j <= m ; j ++ ) { if ( f [ j - A [ i - 1 ]] != -1 && j >= A [ i - 1 ]) { f [ j ] = max ( f [ j ], f [ j - A [ i - 1 ]] + V [ i - 1 ]); } } } int res = 0 ; for ( int j = 0 ; j <= m ; j ++ ) { if ( f [ j ] != -1 && f [ j ] > res ) { res = f [ j ]; } } return res ; } };","title":"Knapsack III"},{"location":"leetcode/dynamic-programming/notes/#knapsack-iv","text":"Given N , M , s[i] , v[i] , c[i] , find the maximum value you can put into the Knapsack while the max time of usage of each item is given in c[i] .","title":"Knapsack IV"},{"location":"leetcode/dynamic-programming/notes/#knapsack-v","text":"Given N , M , s[i] , find the number of possible Knapsack fills if each item can be used once . DP O(nm) class Solution { public : int backPackV ( vector < int >& nums , int T ) { int n = nums . size (); if ( n == 0 ) { return 0 ; } int f [ n + 1 ][ T + 1 ]; f [ 0 ][ 0 ] = 1 ; for ( int j = 1 ; j <= T ; j ++ ) { f [ 0 ][ j ] = 0 ; } for ( int i = 1 ; i <= n ; i ++ ) { for ( int j = 0 ; j <= T ; j ++ ) { f [ i ][ j ] = f [ i - 1 ][ j ]; if ( j >= nums [ i - 1 ]) { f [ i ][ j ] += f [ i - 1 ][ j - nums [ i - 1 ]]; } } } return f [ n ][ T ]; } }; DP O(m) class Solution { public : int backPackV ( vector < int >& nums , int T ) { int n = nums . size (); if ( n == 0 ) { return 0 ; } int f [ T + 1 ]; f [ 0 ] = 1 ; for ( int j = 1 ; j <= T ; j ++ ) { f [ j ] = 0 ; } for ( int i = 1 ; i <= n ; i ++ ) { for ( int j = T ; j >= 0 ; j -- ) { //f[j] = f[j - A[i - 1]] ==> f'[j] if ( j >= nums [ i - 1 ]) { // f'[j] // cover old f[j] f [ j ] += f [ j - nums [ i - 1 ]]; } } } return f [ T ]; } };","title":"Knapsack V"},{"location":"leetcode/dynamic-programming/notes/#knapsack-vi","text":"Given N , M , s[i] , find the number of possible Knapsack fills if each item can be used unlimited times . \u8fd9\u9053\u9898\u7b49\u540c\u4e8eLeetcode\u91cc Combinations Sum IV \u8fd9\u91cc\u53ef\u4ee5\u968f\u4fbf\u53d6\uff0c\u4f3c\u4e4e\u9898\u76ee\u53d8\u5f97\u65e0\u6cd5\u4e0b\u624b\uff0c\u8003\u8651\u201c\u6700\u540e\u4e00\u6b65\u201d\u8fd9\u4e2a\u6280\u5de7\u4e0d\u80fd\u7528\u4e86\uff0c\u56e0\u4e3a\u6700\u540e\u4e00\u6b65\u53ef\u4ee5\u662f\u4efb\u610f\u4e00\u4e2a\u4e86\u3002 \u4f46\u4ecd\u7136\u53ef\u4ee5\u7528\u5b50\u95ee\u9898\u6765\u8003\u8651\u3002\u5148\u4e0d\u7ba1\u6700\u540e\u4e00\u6b65\u662f\u54ea\u4e00\u4e2a\uff0c\u6700\u540e\u4e00\u6b65\u4e4b\u524d\u7684\u76f8\u52a0\u7684\u603b\u548c\u4e00\u5b9a\u662f Target - x. \u8fd9\u6837\u5c31\u8f6c\u5316\u6210\u4e00\u4e2a\u5b50\u95ee\u9898\u53ef\u4ee5\u7528DP\u6765\u505a\u3002 \u5177\u4f53\u505a\u6cd5\u6211\u4eec\u53ef\u4ee5\u5bf9\u4e8e\u6bcf\u4e00\u4e2a\u5c0f\u4e8e\u201c\u603b\u627f\u91cd\u201d\u7684\u91cd\u91cf\u8fdb\u884c\u679a\u4e3e\u6700\u540e\u4e00\u6b65x\u3002\u53ef\u80fd\u7684x\u662f A[0], ..., A[i - 1] \u4e2d\u4efb\u610f\u4e00\u4e2a. Compare With knapsack V, how to deal with the unlimited usage DP O(m) class Solution { public : int backPackVI ( vector < int >& nums , int T ) { int n = nums . size (); if ( n == 0 ) { return 0 ; } int f [ T + 1 ]; f [ 0 ] = 1 ; /* for each sub problem */ for ( int i = 1 ; i <= T ; i ++ ) { f [ i ] = 0 ; /* enumerate the last step */ for ( int j = 0 ; j < n ; j ++ ) { if ( i >= nums [ j ]) { f [ i ] += f [ i - nums [ j ]]; } } } return f [ T ]; } }; How to print such solutions // Suppose we also interested in print one of the possible solution. How could we change the code? // f[i]: \u5b58\u591a\u5c11\u79cd\u65b9\u5f0f // pi[i]: \u5982\u679c f[i] >= 1, \u6700\u540e\u4e00\u4e2a\u6570\u5b57\u53ef\u4ee5\u662fpi[i] class Solution { public : int backPackVI ( vector < int >& nums , int T ) { int n = nums . size (); if ( n == 0 ) { return 0 ; } int f [ T + 1 ]; /* pi[i]: \u5982\u679ci\u53ef\u62fc\u51fa(f[i] >= 1), \u6700\u540e\u4e00\u4e2a\u662fpi[i] */ int pi [ T + 1 ]; f [ 0 ] = 1 ; for ( int i = 1 ; i <= T ; i ++ ) { f [ i ] = 0 ; for ( int j = 0 ; j < n ; j ++ ) { if ( i >= nums [ j ]) { f [ i ] += f [ i - nums [ j ]]; /* \u6700\u540e\u4e00\u4e2a\u662fnums[j]\u7684\u53ef\u62fc\u51fai */ if ( f [ i - nums [ j ]] > 0 ) { /* \u7eaa\u5f55\u4e0b\u6765 */ pi [ i ] = nums [ j ]; } } } } if ( f [ T ] > 0 ) { int i = T ; cout << i << \"=\" << endl ; while ( i != 0 ) { // sum is i now; // last number is pi[i] // previuos sum is i - pi[i] cout << pi [ i ] << endl ; i -= pi [ i ]; } } return f [ T ]; } };","title":"Knapsack VI"},{"location":"leetcode/dynamic-programming/notes/#race-car","text":"waymo","title":"Race Car"},{"location":"leetcode/dynamic-programming/notes/#_2","text":"","title":"\u5750\u6807\u578b"},{"location":"leetcode/dynamic-programming/notes/#triangle","text":"","title":"Triangle"},{"location":"leetcode/dynamic-programming/notes/#unique-paths","text":"","title":"Unique Paths"},{"location":"leetcode/dynamic-programming/notes/#unique-paths-ii","text":"","title":"Unique Paths II"},{"location":"leetcode/dynamic-programming/notes/#minimum-path-sum","text":"","title":"Minimum Path Sum"},{"location":"leetcode/dynamic-programming/notes/#bomb-enemy","text":"","title":"Bomb Enemy"},{"location":"leetcode/dynamic-programming/notes/#dungeon-game","text":"","title":"Dungeon Game"},{"location":"leetcode/dynamic-programming/notes/#221-maximal-square","text":"Solution 1 monotonic stack Solution 2 Dynamic Programming C++ DP memoization class Solution { public : int maximalSquare ( vector < vector < char >>& matrix ) { if ( matrix . empty ()) return 0 ; int m = matrix . size (); int n = matrix [ 0 ]. size (); vector < vector < int >> sum ( m + 1 , vector < int > ( n + 1 , 0 )); int res = 0 ; for ( int i = 1 ; i <= m ; ++ i ) { for ( int j = 1 ; j <= n ; ++ j ) { sum [ i ][ j ] = matrix [ i - 1 ][ j - 1 ] - '0' + sum [ i - 1 ][ j ] + sum [ i ][ j - 1 ] - sum [ i - 1 ][ j - 1 ]; } } int area = 0 ; for ( int i = 1 ; i <= m ; ++ i ) { for ( int j = 1 ; j <= n ; ++ j ) { for ( int k = min ( m - i + 1 , n - j + 1 ); k > 0 ; -- k ) { area = sum [ i + k - 1 ][ j + k - 1 ] - sum [ i + k - 1 ][ j - 1 ] - sum [ i - 1 ][ j + k - 1 ] + sum [ i - 1 ][ j - 1 ]; if ( area == k * k ) { res = max ( res , area ); break ; } } } } return res ; } }; C++ DP subproblem class Solution { public : int maximalSquare ( vector < vector < char >>& matrix ) { if ( matrix . empty ()) return 0 ; int m = matrix . size (); int n = matrix [ 0 ]. size (); vector < vector < int >> sizes ( m , vector < int > ( n , 0 )); int res = 0 ; for ( int i = 0 ; i < m ; ++ i ) { for ( int j = 0 ; j < n ; ++ j ) { sizes [ i ][ j ] = matrix [ i ][ j ] - '0' ; if ( sizes [ i ][ j ] == 0 ) continue ; if ( i > 0 && j > 0 ) sizes [ i ][ j ] = min ( min ( sizes [ i - 1 ][ j - 1 ], sizes [ i - 1 ][ j ]), sizes [ i ][ j - 1 ]) + 1 ; res = max ( res , sizes [ i ][ j ] * sizes [ i ][ j ]); } } return res ; } };","title":"221. Maximal Square"},{"location":"leetcode/dynamic-programming/notes/#_3","text":"","title":"\u5e8f\u5217\u578b"},{"location":"leetcode/dynamic-programming/notes/#perfect-squares","text":"","title":"Perfect Squares"},{"location":"leetcode/dynamic-programming/notes/#longest-increasing-subsequence","text":"","title":"Longest Increasing Subsequence"},{"location":"leetcode/dynamic-programming/notes/#number-of-longest-increasing-subsequence","text":"","title":"Number of Longest Increasing Subsequence"},{"location":"leetcode/dynamic-programming/notes/#longest-increasing-continuous-subsequence","text":"","title":"Longest Increasing Continuous Subsequence"},{"location":"leetcode/dynamic-programming/notes/#_4","text":"","title":"\u5e8f\u5217\u578b + \u72b6\u6001"},{"location":"leetcode/dynamic-programming/notes/#paint-house","text":"","title":"Paint House"},{"location":"leetcode/dynamic-programming/notes/#paint-house-ii","text":"","title":"Paint House II"},{"location":"leetcode/dynamic-programming/notes/#house-robber","text":"","title":"House Robber"},{"location":"leetcode/dynamic-programming/notes/#house-robber-ii","text":"","title":"House Robber II"},{"location":"leetcode/dynamic-programming/notes/#best-time-to-buy-and-sell-stock","text":"","title":"Best Time to Buy and Sell Stock"},{"location":"leetcode/dynamic-programming/notes/#best-time-to-buy-and-sell-stock-ii","text":"","title":"Best Time to Buy and Sell Stock II"},{"location":"leetcode/dynamic-programming/notes/#best-time-to-buy-and-sell-stock-iii","text":"","title":"Best Time to Buy and Sell Stock III"},{"location":"leetcode/dynamic-programming/notes/#best-time-to-buy-and-sell-stock-iv","text":"","title":"Best Time to Buy and Sell Stock IV"},{"location":"leetcode/dynamic-programming/notes/#portfolio-value-optimization","text":"There are two questions regarding this problem: leetcode discussion Can buy fractions of a stock Cannot buy fractions of a stock DP solution #include <iostream> #include <vector> #include <climits> #include <numeric> #include <algorithm> using namespace std ; class Solution { public : int maxProfit ( vector < int > curr_price , vector < int > new_price , vector < int > amount , int A ) { int n = curr_price . size (); int f [ n + 1 ][ A + 1 ]; // max profit of the first i stocks with payment j // init f [ 0 ][ 0 ] = 0 ; for ( int j = 1 ; j <= A ; ++ j ) { f [ 0 ][ j ] = -1 ; } // f[i][j] = max(f[i - 1][j], f[i - 1][j - k * curr_price[i]] + k * new_price[i]) for ( int i = 1 ; i <= n ; ++ i ) { for ( int j = 0 ; j <= A ; ++ j ) { f [ i ][ j ] = f [ i - 1 ][ j ]; for ( int k = 0 ; k <= amount [ i ]; ++ k ) { if ( f [ i - 1 ][ j - k * curr_price [ i - 1 ]] != -1 && j >= k * curr_price [ i - 1 ]) { f [ i ][ j ] = max ( f [ i ][ j ], f [ i - 1 ][ j - k * curr_price [ i - 1 ]] + k * new_price [ i - 1 ]); } } } } int res = 0 ; for ( int j = A ; j >= 0 ; -- j ) { if ( f [ n ][ j ] != -1 && f [ n ][ j ] > res ) { res = f [ n ][ j ]; } } return res ; } }; int main () { // test1 res = 255 // vector<int> v{15, 40, 25, 30}; // vector<int> w{45, 50, 35, 25}; // vector<int> s{3, 3, 3, 4}; // int m = 140; // test2 res = 60 vector < int > v { 15 , 20 }; vector < int > w { 30 , 45 }; vector < int > s { 3 , 3 }; int m = 30 ; int res = 0 ; Solution sol = Solution (); res = sol . maxProfit ( v , w , s , m ); cout << res ; } DP space O(M) #include <iostream> #include <vector> #include <climits> #include <numeric> #include <algorithm> using namespace std ; class Solution { public : int maxProfit ( vector < int > curr_price , vector < int > new_price , vector < int > amount , int A ) { int n = curr_price . size (); int f [ A + 1 ]; // max profit of the first i stocks with payment j // init f [ 0 ] = 0 ; for ( int j = 1 ; j <= A ; ++ j ) { f [ j ] = -1 ; } // f[i][j] = max(f[i - 1][j], f[i - 1][j - k * curr_price[i]] + k * new_price[i]) for ( int i = 1 ; i <= n ; ++ i ) { for ( int j = A ; j >= 0 ; -- j ) { for ( int k = 0 ; k <= amount [ i ]; ++ k ) { if ( f [ j - k * curr_price [ i - 1 ]] != -1 && j >= k * curr_price [ i - 1 ]) { f [ j ] = max ( f [ j ], f [ j - k * curr_price [ i - 1 ]] + k * new_price [ i - 1 ]); } } } } int res = 0 ; for ( int j = A ; j >= 0 ; -- j ) { if ( f [ j ] != -1 && f [ j ] > res ) { res = f [ j ]; } } return res ; } }; int main () { // test1 res = 255 // vector<int> v{15, 40, 25, 30}; // vector<int> w{45, 50, 35, 25}; // vector<int> s{3, 3, 3, 4}; // int m = 140; // test2 res = 60 vector < int > v { 15 , 20 }; vector < int > w { 30 , 45 }; vector < int > s { 3 , 3 }; int m = 30 ; int res = 0 ; Solution sol = Solution (); res = sol . maxProfit ( v , w , s , m ); cout << res ; }","title":"Portfolio Value Optimization"},{"location":"leetcode/dynamic-programming/notes/#_5","text":"","title":"\u5212\u5206\u578b"},{"location":"leetcode/dynamic-programming/notes/#word-break","text":"Solution 1 use set dictionary O(n^2), iterate forwards class Solution { public : bool wordBreak ( string s , vector < string >& wordDict ) { int m = s . length (); unordered_set < string > wordSet ( wordDict . begin (), wordDict . end ()); bool f [ m + 1 ] = { 0 }; f [ 0 ] = true ; for ( int i = 1 ; i <= m ; i ++ ) { for ( int j = 0 ; j <= i ; j ++ ) { if ( f [ j ] && wordSet . count ( s . substr ( j , i - j )) != 0 ) { f [ i ] = true ; break ; } } } return f [ m ]; } }; Solution 2 use set dictionary O(n^2), iterate backwards class Solution { public : bool wordBreak ( string s , vector < string >& wordDict ) { int n = s . length (); unordered_set < string > dict ( wordDict . begin (), wordDict . end ()); int f [ n + 1 ] = { 0 }; f [ 0 ] = 1 ; for ( int i = 0 ; i <= n ; ++ i ) { for ( int j = n - 1 ; j >= 0 ; -- j ) { if ( f [ j ] && dict . count ( s . substr ( j , i - j ))) { f [ i ] = 1 ; break ; } } } return f [ n ]; } }; Solution 3 (best solution) use vector, iterate through string Do not use word set to check exist or not, use each word as the last step. class Solution { public : bool wordBreak ( string s , vector < string >& wordDict ) { int n = s . length (); int f [ n + 1 ] = { 0 }; f [ 0 ] = 1 ; for ( int i = 0 ; i <= n ; ++ i ) { for ( string word : wordDict ) { if ( word . length () <= i && f [ i - word . length ()]) { if ( s . substr ( i - word . length (), word . length ()) == word ) { f [ i ] = 1 ; break ; } } } } return f [ n ]; } };","title":"Word Break"},{"location":"leetcode/dynamic-programming/notes/#maximum-vacation-days","text":"Solution 1 class Solution { public : int maxVacationDays ( vector < vector < int >>& flights , vector < vector < int >>& days ) { int n = flights . size (); int k = days [ 0 ]. size (); vector < vector < int >> f ( n , vector < int > ( k , -1 )); f [ 0 ][ 0 ] = days [ 0 ][ 0 ]; for ( int i = 1 ; i < n ; ++ i ) { if ( flights [ 0 ][ i ]) { f [ i ][ 0 ] = days [ i ][ 0 ]; } } for ( int d = 1 ; d < k ; ++ d ) { for ( int i = 0 ; i < n ; ++ i ) { for ( int j = 0 ; j < n ; ++ j ) { if (( flights [ j ][ i ] || i == j ) && f [ j ][ d - 1 ] != 1 ) { f [ i ][ d ] = max ( f [ i ][ d ], f [ j ][ d - 1 ] + days [ i ][ d ]); } } } } int result = 0 ; for ( int i = 0 ; i < n ; ++ i ) { result = max ( result , f [ i ][ k - 1 ]); } return result ; } }; Solution 2 DFS with Cache class Solution { vector < vector < int >> memo ; public : int maxVacationDays ( vector < vector < int >>& flights , vector < vector < int >>& days ) { vector < vector < int >> memo ( flights . size (), vector < int > ( days [ 0 ]. size (), INT_MIN )); return dfs ( flights , days , 0 , 0 , memo ); } int dfs ( vector < vector < int >>& flights , vector < vector < int >>& days , int start , int day , vector < vector < int >>& memo ) { if ( day == days [ 0 ]. size ()) { return 0 ; } if ( memo [ start ][ day ] != INT_MIN ) { return memo [ start ][ day ]; } int maxVal = 0 ; for ( int i = 0 ; i < flights . size (); ++ i ) { if ( flights [ start ][ i ] || start == i ) { maxVal = max ( maxVal , days [ i ][ day ] + dfs ( flights , days , i , day + 1 , memo )); } } memo [ start ][ day ] = maxVal ; return maxVal ; } }; Solution 3 (greedy, wrong) class Solution { public : int maxVacationDays ( vector < vector < int >>& flights , vector < vector < int >>& days ) { int n = flights . size (); int k = days [ 0 ]. size (); int start = 0 ; int result = 0 ; set < pair < int , int > , std :: greater < pair < int , int >>> s ; for ( int col = 0 ; col < k ; ++ col ) { for ( int row = 0 ; row < n ; ++ row ) { s . insert ({ days [ row ][ col ], row }); } int stay = 1 ; for ( auto p : s ) { if ( flights [ start ][ p . second ]) { result += p . first ; cout << p . first << \" \" << p . second << endl ; start = p . second ; stay = 0 ; break ; } } if ( stay ) { result += days [ start ][ col ]; } s . clear (); } return result ; } };","title":"Maximum Vacation Days"},{"location":"leetcode/dynamic-programming/notes/#decode-ways","text":"","title":"Decode Ways"},{"location":"leetcode/dynamic-programming/notes/#decode-ways-ii","text":"Solution 1 class Solution { public : int numDecodings ( string s ) { const int MOD = 1000000007 ; int n = s . length (); long long f [ n + 1 ] = { 0 }; if ( n == 0 ) return 0 ; f [ 0 ] = 1 ; for ( int i = 1 ; i <= n ; ++ i ) { if ( s [ i - 1 ] == '*' ) { // not include '0' according to the problem statement f [ i ] = ( f [ i ] + f [ i - 1 ] * 9 ) % MOD ; // number of ways only decode one digit if ( i > 1 ) { // number of ways decode two digits if ( s [ i - 2 ] == '*' ) { f [ i ] = ( f [ i ] + f [ i - 2 ] * 15 ) % MOD ; // 11 - 26 } else if ( s [ i - 2 ] == '1' ) { f [ i ] = ( f [ i ] + f [ i - 2 ] * 9 ) % MOD ; // 11 - 19 } else if ( s [ i - 2 ] == '2' ) { f [ i ] = ( f [ i ] + f [ i - 2 ] * 6 ) % MOD ; // 21 - 26 } } } else { // s[i - 1] != '*', have to consider the case '0' if ( s [ i - 1 ] != '0' ) { // number of ways only decode one digit f [ i ] = ( f [ i ] + f [ i - 1 ]) % MOD ; } if ( i > 1 ) { // number of ways decode two digits if ( s [ i - 2 ] == '*' ) { if ( s [ i - 1 ] <= '6' ) { // '*' can represent 1, 2 f [ i ] = ( f [ i ] + f [ i - 2 ] * 2 ) % MOD ; } else { // '*' can only represent 1 f [ i ] = ( f [ i ] + f [ i - 2 ]) % MOD ; } } else { // no '*' case int t = ( s [ i - 2 ] - '0' ) * 10 + s [ i - 1 ] - '0' ; if ( t >= 10 && t <= 26 ) { f [ i ] = ( f [ i ] + f [ i - 2 ]) % MOD ; } } } } } return f [ n ] % MOD ; } };","title":"Decode Ways II"},{"location":"leetcode/dynamic-programming/notes/#_6","text":"","title":"\u53cc\u5e8f\u5217\u578b"},{"location":"leetcode/dynamic-programming/notes/#longest-common-subsequence","text":"Solution 1 consider the last step, the case is NOT A[n-1] == B[l-1] and A[n-1] != B[m-1] but, three cases: A[n-1] is included, B[m-1] is included, both A[n-1] and B[m-1] are included, (A[n-1] == B[m-1]) .","title":"Longest Common Subsequence"},{"location":"leetcode/dynamic-programming/notes/#interleaving-string","text":"Solution 1 class Solution { public : bool isInterleave ( string s1 , string s2 , string s3 ) { // f[i][j] = OR(f[i][j - 1]|B[j - 1] in C, f[i - 1][j]|A[i - 1] in C); int m = s1 . length (); int n = s2 . length (); int l = s3 . length (); if ( m + n != l ) return false ; int f [ m + 1 ][ n + 1 ] = { 0 }; f [ 0 ][ 0 ] = 1 ; for ( int i = 0 ; i <= m ; ++ i ) { for ( int j = 0 ; j <= n ; ++ j ) { if ( i > 0 && s1 [ i - 1 ] == s3 [ i + j - 1 ]) { f [ i ][ j ] |= f [ i - 1 ][ j ]; } if ( j > 0 && s2 [ j - 1 ] == s3 [ i + j - 1 ]) { f [ i ][ j ] |= f [ i ][ j - 1 ]; } } } return f [ m ][ n ]; } };","title":"Interleaving String"},{"location":"leetcode/dynamic-programming/notes/#edit-distance","text":"Solution 1 state: f[i][j] is min edit distance to make the first i chars of A the same as first j chars of B, as we consider the last operation (insert, delete, replace). One key idea of this problem is after the operation, the length of the A and B are not necessarily i and j . The focus here is the \"min edit distance\", not the length of A or B. For example, if the last step is insert, A's length will become i + 1 , B's length will maintain j . You should also note that the A, B concept is imaginary, B is actually changed from A by editing once. As a result, we know i + 1 == j after the insertion operation. Don't iterpret the state f[i][j] as min edit distance of first i chars in A and first j chars in B and after the editing, A's length is i and B's length is j . You should seperate the two intepretations. class Solution { public : int minDistance ( string word1 , string word2 ) { int m = word1 . length (); int n = word2 . length (); int f [ m + 1 ][ n + 1 ]; int i , j ; for ( i = 0 ; i <= m ; i ++ ) { for ( j = 0 ; j <= n ; j ++ ) { if ( i == 0 ) { f [ i ][ j ] = j ; continue ; } if ( j == 0 ) { f [ i ][ j ] = i ; continue ; } // delete insert f [ i ][ j ] = min ( f [ i - 1 ][ j ] + 1 , f [ i ][ j - 1 ] + 1 ); // replace f [ i ][ j ] = min ( f [ i ][ j ], f [ i - 1 ][ j - 1 ] + 1 ); if ( word1 [ i - 1 ] == word2 [ j - 1 ]) { f [ i ][ j ] = min ( f [ i ][ j ], f [ i - 1 ][ j - 1 ]); } } } return f [ m ][ n ]; } }; Solution 2 Space optimized class Solution { public : int minDistance ( string word1 , string word2 ) { int m = word1 . length (); int n = word2 . length (); int f [ 2 ][ n + 1 ]; int prev = 0 , curr = 0 ; for ( int i = 0 ; i <= m ; i ++ ) { prev = curr ; curr = 1 - curr ; for ( int j = 0 ; j <= n ; j ++ ) { if ( i == 0 ) { f [ curr ][ j ] = j ; continue ; } if ( j == 0 ) { f [ curr ][ j ] = i ; continue ; } // delete insert f [ curr ][ j ] = min ( f [ prev ][ j ] + 1 , f [ curr ][ j - 1 ] + 1 ); // replace f [ curr ][ j ] = min ( f [ curr ][ j ], f [ prev ][ j - 1 ] + 1 ); if ( word1 [ i - 1 ] == word2 [ j - 1 ]) { f [ curr ][ j ] = min ( f [ curr ][ j ], f [ prev ][ j - 1 ]); } } } return f [ curr ][ n ]; } };","title":"Edit Distance"},{"location":"leetcode/dynamic-programming/notes/#memoization","text":"","title":"Memoization"},{"location":"leetcode/dynamic-programming/notes/#1049-last-stone-weight-ii","text":"Solution 1 Recursion + memoization Solution 2 Use two set to simulate Solution 3 Transform into knapsack Thinking it as adding \"+\" and \"-\" in front of each number. The sum of all the numbers with \"+\" S1, the sum of all number with \"-\" S2, we want to find the minimum of |S1 - S2| because we have S1 + S2 = total . suppose S1 <= S2 , we want to minimize S2 - S1 = total - S1 - S1 = total - 2 * S1 ; We wil achieve the goal by maximizing S1 , so that the stete would be. f[i][j] represent the optimal value of use first i stones to achieve maximum j ( S1 ) value. C++ Memoization class Solution { int memo [ 30 ][ 3001 ]; public : int lastStoneWeightII ( vector < int >& stones ) { return helper ( stones , 0 , 0 ); } int helper ( vector < int >& stones , int i , int sum ) { if ( i == stones . size ()) return abs ( sum ); if ( memo [ i ][ sum ] != 0 ) return memo [ i ][ sum ]; memo [ i ][ sum ] = min ( helper ( stones , i + 1 , abs ( sum - stones [ i ])), helper ( stones , i + 1 , sum + stones [ i ])); return memo [ i ][ sum ]; } }; C++ Set simulation class Solution { public : int lastStoneWeightII ( vector < int >& stones ) { if ( stones . size () == 0 ) return 0 ; unordered_set < int > set_left ({ stones [ 0 ]}); for ( int i = 1 ; i < stones . size (); ++ i ) { unordered_set < int > set_right = set_left ; set_left . clear (); for ( int y : set_right ) { set_left . insert ( y + stones [ i ]); set_left . insert ( y - stones [ i ]); } } int ans = INT_MAX ; for ( int z : set_left ) { ans = min ( ans , abs ( z )); } return ans ; } }; C++ knapsack class Solution { public : int lastStoneWeightII ( vector < int >& stones ) { int n = stones . size (); int sum = accumulate ( begin ( stones ), end ( stones ), 0 ); vector < vector < int >> f ( n + 1 , vector < int > ( sum / 2 + 1 , 0 )); for ( int i = 1 ; i <= n ; ++ i ) { for ( int j = 1 ; j < sum / 2 + 1 ; ++ j ) { if ( j < stones [ i - 1 ]) { f [ i ][ j ] = f [ i - 1 ][ j ]; } else { f [ i ][ j ] = max ( f [ i - 1 ][ j ], f [ i - 1 ][ j - stones [ i - 1 ]] + stones [ i - 1 ]); } } } return sum - 2 * f [ n ][ sum / 2 ]; } }; C++ knapsack O(1) space class Solution { public : int lastStoneWeightII ( vector < int >& stones ) { int n = stones . size (); int sum = accumulate ( begin ( stones ), end ( stones ), 0 ); vector < int > f ( sum / 2 + 1 , 0 ); for ( int i = 1 ; i <= n ; ++ i ) { for ( int j = sum / 2 ; j >= 0 ; -- j ) { if ( j >= stones [ i - 1 ]) { f [ j ] = max ( f [ j ], f [ j - stones [ i - 1 ]] + stones [ i - 1 ]); } } } return sum - 2 * f [ sum / 2 ]; } };","title":"1049. Last Stone Weight II"},{"location":"leetcode/dynamic-programming/notes/#sentence-screen-fitting","text":"Solution 1 You can view the sentence as a space seperated English sentence and then use each row to \"frame\" the sentence. If the end of the frame overlap a space, we continue move to the next frame if available. If the end of the frame overlap a character, we should move the right the frame to the end of the prev word and move on to the next frame. class Solution { public : int wordsTyping ( vector < string >& sentence , int rows , int cols ) { int n = sentence . size (); string s ; for ( string w : sentence ) { s += string ( w + \" \" ); } int start = 0 , len = s . length (); //s with an extra trailing space. for ( int i = 0 ; i < rows ; i ++ ) { start += cols ; if ( s [ start % len ] == ' ' ) { ++ start ; } else { while ( start > 0 && s [( start - 1 ) % len ] != ' ' ) { -- start ; } } } return start / len ; } }; Solution 2 One insight of this solution is that every word in the sentence is possible to start a new row, but not necessary. For example, a long row can contain multiple of the setences, but the last word cannot fit into this sigle row, it start a new row. In this case only the first word in the sentence and the last word can start a new row. We use this insight in the following way, for each word, we assume it could start a new row, then use the following words in the sentence to fix the row, we record the total number of words that can fit this row in dp[i] , i is the index of the starting word in the sentence. After the calculation, we could count sum(dp[i]) , i is starting word's index. class Solution { public : int wordsTyping ( vector < string >& sentence , int rows , int cols ) { int n = sentence . size (); int f [ n ] = { 0 }; for ( int i = 0 ; i < n ; ++ i ) { int len = 0 , count = 0 , idx = i ; while ( len + sentence [ idx % n ]. length () <= cols ) { len += ( sentence [ idx % n ]. length () + 1 ); idx ++ ; count ++ ; } f [ i ] = count ; } int total = 0 ; for ( int i = 0 , idx = 0 ; i < rows ; ++ i ) { total += f [ idx ]; idx = ( f [ idx ] + idx ) % n ; } return total / n ; } }; TODO: This solution may be further improved because for some of the count, we never used. Can we calculate the final result by only one loop, or in a nested loop?","title":"Sentence Screen Fitting"},{"location":"leetcode/favorite/notes/","text":"Favorite \u00b6 Array \u00b6 53. Maximum Subarray 801. Minimum Swaps To Make Sequences Increasing Stack \u00b6 84. Largest Rectangle in Histogram 907. Sum of Subarray Minimums Google Interview Problems Graph \u00b6 1066. Campus Bikes II","title":"Favorite"},{"location":"leetcode/favorite/notes/#favorite","text":"","title":"Favorite"},{"location":"leetcode/favorite/notes/#array","text":"53. Maximum Subarray 801. Minimum Swaps To Make Sequences Increasing","title":"Array"},{"location":"leetcode/favorite/notes/#stack","text":"84. Largest Rectangle in Histogram 907. Sum of Subarray Minimums Google Interview Problems","title":"Stack"},{"location":"leetcode/favorite/notes/#graph","text":"1066. Campus Bikes II","title":"Graph"},{"location":"leetcode/graph/notes/","text":"Graph \u00b6 Representations \u00b6 Edge lists \u00b6 An edge is represented using an array of two vertex numbers or objects contain the vertex numbers. Weights can be added as the third number or another member of the objects. Example: [[0,1], [0,6], [0,8], [1,4], [1,6], [1,9], [2,4], [2,6], [3,4], [3,5]] Space: \\Theta(E) \\Theta(E) Search edge: O(E) O(E) Adjacency matrix \u00b6 Use a matrix[i][j] to represent the edges. 1 is connected, 0 is not connected. Example: [ [0, 1, 0, 0, 0, 0, 1, 0, 1, 0], [1, 0, 0, 0, 1, 0, 1, 0, 0, 1], [0, 0, 0, 0, 1, 0, 1, 0, 0, 0], [0, 0, 0, 0, 1, 1, 0, 0, 1, 0], [0, 1, 1, 1, 0, 1, 0, 0, 0, 1], [0, 0, 0, 1, 1, 0, 0, 0, 0, 0], [1, 1, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 1], [1, 0, 0, 1, 0, 0, 0, 1, 0, 0], [0, 1, 0, 0, 1, 0, 0, 1, 0, 0] ] Check edge (i, j) exists: O(1) O(1) Space: \\Theta(V^2) \\Theta(V^2) Search djacent vertex: O(V) O(V) For undirected graph, adjacency matrix is symmetric. For a directed graph, the adjacency matrix need not be symmetric. Adjacency list \u00b6 Representing a graph with adjacency lists combines adjacency matrices with edge lists. For each vertex i , store an array of the vertices adjacent to it. Each vertex has a list. Space: \\Theta(2E) \\Theta(2E) for undirected graph. \\Theta(E) \\Theta(E) for directed graph. Check edge (i, j) exists: O(d) O(d) , d is the degree of the vertex i . 0 \\leq d \\leq V - 1 0 \\leq d \\leq V - 1 . If the graph is weighted, then each item in each adjacency list is either a two-item array or an object, giving the vertex number and the edge weight. Representing graphs Algorithms \u00b6 Weighted Graph Unweighted Graph Directed Graph Undirected Graph Graph Traversal Single Source Shortest Path Connected components Minimum spanning tree Kosaraju's Two-pass algorithm \u00b6 Reverse each arc of the original graph. First run of the DFS-Loop to computer the \"finish\" time of each node in the reversed graph. Book keeping: The \"source node of a node\". A source node is the first node in the DFS tree. The \"finish time of a node\". Second run of the DFS-Loop to computer the strongly connected components (SCC). The SCC is represented by the source node. Dijkstra algorithm (No negative edge) \u00b6 Dijkstra algorithm implementation need one data structure (think of it as an array) to record the path distance, another data structure (priority queue) to maintain the min-heap which is ordered by the key of the edge weights. The algorithm greedily select the smallest edge from the queue and update the optimal solution so far (called relaxation in graph algorithm jargon). Dijkstra algorithm Implemention requires to maintain two invariant, 1. All the vertices w_i w_i that not in S S will be kept in the heap. 2. The key should be the minimum weight of all the edges who's tail in S S , head point to w_i w_i . In every iteration, each w_i w_i need to run a local turnament to decide its new key and then update the heap. To compare Dijkastra to BFS, when you visit a new node, Dijkastra will do a relaxation (update keys by local turnament and reinsert heap) while BFS simply add the node to the queue. Dijkastra algorithm psudo code INITIALIZE-SINGLE-SOURCE(G, s) for each vertex v in G.V: v.d = inf v.parent = null s.d = 0 RELAX(u, v, w) if v.d > u.d + w(u, v) v.d = u.d + w(u, v) v.parent = u DIJKSTRA(G,w,s) INITIALIZE-SINGLE-SOURCE(G, s) S = None Q = G.V while Q != None u = EXTRACT-MIN(Q) S = S + {u} for each vertex v in G.adj[u]: RELAX(u, v, w) Bellman-Ford \u00b6 Bellman-Ford algorithm take care of negative edges in finding the single source shortest path in a graph. In addition, it also can be used to detect negative cycles reachable from the source. Notice Bellman-Ford algorithm can be optimized by exit when there is no new update happening. However, the worst case runtime is still O(V \\cdot, E) O(V \\cdot, E) . Bellman-Ford algorithm psudo code function BellmanFord(list vertices, list edges, vertex source) is // This implementation takes in a graph, represented as // lists of vertices (represented as integers [0..n-1]) and edges, // and fills two arrays (distance and predecessor) holding // the shortest path from the source to each vertex distance := list of size n predecessor := list of size n // Step 1: initialize graph for each vertex v in vertices do distance[v] := inf // Initialize the distance to all vertices to infinity predecessor[v] := null // And having a null predecessor distance[source] := 0 // The distance from the source to itself is, of course, zero // Step 2: relax edges repeatedly repeat |V|\u22121 times: for each edge (u, v) with weight w in edges do if distance[u] + w < distance[v] then distance[v] := distance[u] + w predecessor[v] := u // Step 3: check for negative-weight cycles for each edge (u, v) with weight w in edges do if distance[u] + w < distance[v] then error \"Graph contains a negative-weight cycle\" return distance, predecessor Shortest Path Faster Algorithm \u00b6 The algorithm is an improvement on Bellman-Ford algorithm. The idea is to limit the relaxation operation on only those nodes that have been relaxed before. We can use a queue to keep such nodes. However, the worst case runtime complexity is the same as Bellman-Ford, it is still O(V \\cdot, E) O(V \\cdot, E) . procedure Shortest-Path-Faster-Algorithm(G, s) 1 for each vertex v \u2260 s in V(G) 2 d(v) := \u221e 3 d(s) := 0 4 push s into Q 5 while Q is not empty do 6 u := poll Q 7 for each edge (u, v) in E(G) do 8 if d(u) + w(u, v) < d(v) then 9 d(v) := d(u) + w(u, v) 10 if v is not in Q then 11 push v into Q Bipartise \u00b6 One algorithm to use in graph bipartition is using coloring, namely apply DFS and color each node with different colors when visited them differently. Directed graph strongly connected components \u00b6 Interivew Strategy \u00b6 Clarification questions \u00b6 What's the input scale? What's the limit of depth? Does the graph have self-edge or duplications? what's the corresponding return value should be if yes? Solving graph problems \u00b6 DFS/BFS/UF -> topological -> Bellman Ford -> Dijkastra -> To traverse a cyclic graph, we could not use visited, think of remove a edge after visited it. One example problem is 332. Reconstruct Itinerary Problems \u00b6 332. Reconstruct Itinerary \u00b6 407. Trapping Rain Water II \u00b6 class Solution { public : int trapRainWater ( vector < vector < int >>& heightMap ) { if ( heightMap . empty ()) return 0 ; int m = heightMap . size (); int n = m ? heightMap [ 0 ]. size () : 0 ; if ( m <= 2 || n <= 2 ) { return 0 ; } int res = 0 , mx = 0 ; priority_queue < pair < int , int > , vector < pair < int , int >> , greater < pair < int , int >>> pq ; vector < vector < bool >> visited ( m , vector < bool > ( n , false )); int x [ 4 ] = { -1 , 0 , 1 , 0 }; int y [ 4 ] = { 0 , 1 , 0 , -1 }; // initialize the pq. for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { if ( i == 0 || i == m - 1 || j == 0 || j == n - 1 ) { pq . push ({ heightMap [ i ][ j ], i * n + j }); visited [ i ][ j ] = true ; } } } while ( ! pq . empty ()) { auto t = pq . top (); pq . pop (); int h = t . first ; int i = t . second / n ; int j = t . second % n ; mx = max ( h , mx ); for ( int k = 0 ; k < 4 ; k ++ ) { int p = i + x [ k ]; int q = j + y [ k ]; if ( p >= 0 && p < m && q >= 0 && q < n && ! visited [ p ][ q ]) { visited [ p ][ q ] = true ; if ( heightMap [ p ][ q ] < mx ) { res += mx - heightMap [ p ][ q ]; } pq . push ({ heightMap [ p ][ q ], p * n + q }); } } } return res ; } }; Critical Connections in a Network \u00b6 Critical Routers \u00b6 743. Network Delay Time \u00b6 Solution 1 Dijkstra algorithm What's the key for heap node? What's the data structure is needed? ( dist array, and a heap) Solution 2 Bellman Ford algorithm Solution 3 Shortest Path Best algorithm === C++ Dijkstra ```c++ class Solution { public: int networkDelayTime(vector<vector<int>>& times, int n, int k) { vector<vector<pair<int, int>>> graph(n + 1, vector<pair<int, int>>()); for (auto& t: times) { graph[t[0]].push_back({t[1], t[2]}); } priority_queue<pair<int, int>, vector<pair<int, int>>, greater<pair<int, int>>> pq; pq.push({0, k}); vector<int> dist(n + 1, INT_MAX); dist[k] = 0; while (!pq.empty()) { pair<int, int> t = pq.top(); pq.pop(); int u = t.second; for (auto x: graph[u]) { int v = x.first; int w = x.second; if (dist[v] > dist[u] + w) { dist[v] = dist[u] + w; pq.push({dist[v], v}); } } } int res = *max_element(dist.begin() + 1, dist.end()); return res == INT_MAX ? -1 : res; } }; ``` === C++ Bellman Ford ```c++ class Solution { public: int networkDelayTime(vector<vector<int>>& times, int n, int k) { vector<int> dist(n + 1, INT_MAX); dist[k] = 0; for (int i = 0; i < n; ++i) { for (auto& t : times) { int u = t[0], v = t[1], w = t[2]; if (dist[u] != INT_MAX && dist[v] > dist[u] + w) { dist[v] = dist[u] + w; } } } int res = *max_element(dist.begin() + 1, dist.end()); return res == INT_MAX ? -1 : res; } }; ``` 778. Swim in Rising Water \u00b6 Solution 1 Binary search + DFS Solution 2 Dijkstra C++ Binary Search + DFS class Solution { int x [ 4 ] = { 0 , -1 , 0 , 1 }; int y [ 4 ] = { -1 , 0 , 1 , 0 }; public : int swimInWater ( vector < vector < int >>& grid ) { int n = grid . size (); int begin = grid [ 0 ][ 0 ], end = n * n - 1 ; // binary search find a path with mini elevation while ( begin < end ) { int mid = begin + ( end - begin ) / 2 ; if ( pathExist ( grid , mid )) { end = mid ; } else { begin = mid + 1 ; } } return begin ; } bool pathExist ( vector < vector < int >> & grid , int mid ) { int n = grid . size (); vector < vector < int >> visited ( n , vector < int > ( n , 0 )); return dfs_helper ( grid , visited , n , mid , 0 , 0 ); } bool dfs_helper ( vector < vector < int >> & grid , vector < vector < int >>& visited , int n , int mid , int i , int j ) { visited [ i ][ j ] = 1 ; for ( int k = 0 ; k < 4 ; ++ k ) { int a = i + x [ k ]; int b = j + y [ k ]; if ( a < 0 || a >= n || b < 0 || b >= n || visited [ a ][ b ] == 1 || grid [ a ][ b ] > mid ) continue ; if ( a == n - 1 && b == n - 1 ) return true ; if ( dfs_helper ( grid , visited , n , mid , a , b )) return true ; } return false ; } }; C++ Dijkstra class Solution { int d [ 5 ] = { 0 , -1 , 0 , 1 , 0 }; public : int swimInWater ( vector < vector < int >>& grid ) { int n = grid . size (); priority_queue < pair < int , int > , vector < pair < int , int >> , greater < pair < int , int >>> pq ; pq . push ({ grid [ 0 ][ 0 ], 0 }); vector < vector < int >> visited ( n , vector < int > ( n , 0 )); visited [ 0 ][ 0 ] = 1 ; int res = 0 ; while ( ! pq . empty ()) { pair < int , int > t = pq . top (); pq . pop (); int x = t . second / n ; int y = t . second % n ; res = max ( res , t . first ); if ( x == n - 1 && y == n - 1 ) break ; for ( int i = 0 ; i < 4 ; ++ i ) { int a = x + d [ i ]; int b = y + d [ i + 1 ]; if ( a >= 0 && a < n && b >= 0 && b < n && visited [ a ][ b ] == 0 ) { visited [ a ][ b ] = 1 ; pq . push ({ grid [ a ][ b ], a * n + b }); } } } return res ; } }; 785. Is Graph Bipartite? \u00b6 Solution 1 Coloring neighboring nodes with alternative color class Solution { public : bool isBipartite ( vector < vector < int >>& graph ) { int n = graph . size (); vector < int > visited ( n , -1 ); for ( int i = 0 ; i < n ; ++ i ) { if ( visited [ i ] == -1 && ! dfs_helper ( graph , visited , i , 1 )) { return false ; } } return true ; } bool dfs_helper ( vector < vector < int >>& graph , vector < int >& visited , int i , int color ) { if ( visited [ i ] != -1 ) { return visited [ i ] == color ; // the color is ok } visited [ i ] = color ; // not colored before, now color it for ( auto v : graph [ i ]) { if ( ! dfs_helper ( graph , visited , v , 1 - color )) { return false ; } } return true ; } }; 1102. Path With Maximum Minimum Value \u00b6 Solution 1 Dijkstra Solution 2 Union-Find Python Union Find class Solution : def maximumMinimumPath ( self , A : List [ List [ int ]]) -> int : R , C = len ( A ), len ( A [ 0 ]) parent = [ i for i in range ( R * C )] dire = [( 0 , 1 ), ( 0 , - 1 ), ( 1 , 0 ), ( - 1 , 0 )] seen = [[ 0 for _ in range ( C )] for _ in range ( R )] def find ( x ): if parent [ x ] != x : parent [ x ] = find ( parent [ x ]) return parent [ x ] def union ( x , y ): rx , ry = find ( x ), find ( y ) if rx != ry : parent [ ry ] = rx points = [( x , y ) for x in range ( R ) for y in range ( C )] points . sort ( key = lambda x : A [ x [ 0 ]][ x [ 1 ]], reverse = True ) for x , y in points : seen [ x ][ y ] = 1 for dx , dy in dire : nx , ny = x + dx , y + dy if 0 <= nx < R and 0 <= ny < C and seen [ nx ][ ny ]: union ( x * C + y , nx * C + ny ) if find ( 0 ) == find ( R * C - 1 ): return A [ x ][ y ] return - 1 C++ Dijkstra class Solution { public : int maximumMinimumPath ( vector < vector < int >>& A ) { int m = A . size (); int n = A [ 0 ]. size (); int res = INT_MAX ; priority_queue < pair < int , int > , vector < pair < int , int >>> pq ; // max heap. pq . emplace ( A [ 0 ][ 0 ], 0 ); vector < vector < int >> visited ( m , vector < int > ( n , 0 )); visited [ 0 ][ 0 ] = -1 ; int d [ 5 ] = { 0 , 1 , 0 , -1 , 0 }; while ( ! pq . empty ()) { pair < int , int > t = pq . top (); pq . pop (); int cost = t . first ; int x = t . second / n ; int y = t . second % n ; res = min ( res , cost ); if ( x == m - 1 && y == n - 1 ) break ; for ( int k = 0 ; k < 4 ; k ++ ) { int r = x + d [ k ]; int c = y + d [ k + 1 ]; if ( r < 0 || r >= m || c < 0 || c >= n || visited [ r ][ c ] < 0 ) continue ; pq . emplace ( A [ r ][ c ], r * n + c ); visited [ r ][ c ] = -1 ; } } return res ; } }; 1514. Path with Maximum Probability \u00b6 class Solution { public : double maxProbability ( int n , vector < vector < int >>& edges , vector < double >& succProb , int start , int end ) { unordered_map < int , vector < pair < int , double >>> graph ; for ( int i = 0 ; i < edges . size (); ++ i ) { graph [ edges [ i ][ 0 ]]. push_back ({ edges [ i ][ 1 ], succProb [ i ]}); graph [ edges [ i ][ 1 ]]. push_back ({ edges [ i ][ 0 ], succProb [ i ]}); } vector < double > prob ( n , 0.0 ); prob [ start ] = 1.0 ; priority_queue < pair < double , int >> pq ; pq . push ({ 1.0 , start }); while ( ! pq . empty ()) { pair < double , int > u = pq . top (); pq . pop (); for ( auto vp : graph [ u . second ]) { int v = vp . first ; double p = vp . second ; if ( prob [ v ] < p * prob [ u . second ]) { prob [ v ] = p * prob [ u . second ]; pq . push ({ prob [ v ], v }); } } } return prob [ end ]; } }; 1631. Path With Minimum Effort \u00b6 C++ Dijkstra class Solution { public : int minimumEffortPath ( vector < vector < int >>& heights ) { int m = heights . size (); int n = heights [ 0 ]. size (); vector < vector < int >> dist ( m , vector < int > ( n , INT_MAX )); // min distance found so far. priority_queue < pair < int , int > , vector < pair < int , int >> , greater < pair < int , int >>> pq ; int d [ 5 ] = { 0 , 1 , 0 , -1 , 0 }; pq . emplace ( 0 , 0 ); // first: min effort, second: encoded (x, y) (=x * n + y); while ( ! pq . empty ()) { pair < int , int > t = pq . top (); int effort = t . first ; int x = t . second / n ; int y = t . second % n ; pq . pop (); if ( x == m - 1 && y == n - 1 ) return effort ; for ( int k = 0 ; k < 4 ; ++ k ) { int a = x + d [ k ]; int b = y + d [ k + 1 ]; if ( a < 0 || a >= m || b < 0 || b >= n ) continue ; int currEffort = max ( effort , abs ( heights [ a ][ b ] - heights [ x ][ y ])); if ( currEffort < dist [ a ][ b ]) { dist [ a ][ b ] = currEffort ; pq . push ({ currEffort , a * n + b }); } } } return -1 ; } }; Python Dijkstra class Solution : def minimumEffortPath ( self , heights : List [ List [ int ]]) -> int : m , n = map ( len , [ heights , heights [ 0 ]]) efforts = [[ math . inf ] * n for _ in range ( m )] efforts [ 0 ][ 0 ] = 0 heap = [( 0 , 0 , 0 )] while heap : effort , x , y = heapq . heappop ( heap ); if x == m - 1 and y == n - 1 : return effort for i , j in ( x , y - 1 ), ( x , y + 1 ), ( x - 1 , y ), ( x + 1 , y ): if i < 0 or i >= m or j < 0 or j >= n : continue currEffort = max ( effort , abs ( heights [ x ][ y ] - heights [ i ][ j ])) if efforts [ i ][ j ] > currEffort : efforts [ i ][ j ] = currEffort heapq . heappush ( heap , ( currEffort , i , j )) Java Dijkstra class Solution { private int [] d = { 0 , 1 , 0 , - 1 , 0 }; public int minimumEffortPath ( int [][] heights ) { int lo = 0 , hi = 1_000_000 ; while ( lo < hi ) { int effort = lo + ( hi - lo ) / 2 ; if ( isPath ( heights , effort )) { hi = effort ; } else { lo = effort + 1 ; } } return lo ; } private boolean isPath ( int [][] h , int effort ) { int m = h . length , n = h [ 0 ] . length ; Queue < int []> q = new LinkedList <> (); q . offer ( new int [ 2 ] ); Set < Integer > seen = new HashSet <> (); seen . add ( 0 ); while ( ! q . isEmpty ()) { int [] cur = q . poll (); int x = cur [ 0 ] , y = cur [ 1 ] ; if ( x == m - 1 && y == n - 1 ) { return true ; } for ( int k = 0 ; k < 4 ; ++ k ) { int r = x + d [ k ] , c = y + d [ k + 1 ] ; if ( 0 <= r && r < m && 0 <= c && c < n && effort >= Math . abs ( h [ r ][ c ] - h [ x ][ y ] ) && seen . add ( r * n + c )) { q . offer ( new int [] { r , c }); } } } return false ; } } Java Binary search + BFS class Solution { private int [] d = { 0 , 1 , 0 , - 1 , 0 }; public int minimumEffortPath ( int [][] heights ) { int lo = 0 , hi = 1_000_000 ; while ( lo < hi ) { int mid = lo + ( hi - lo ) / 2 ; if ( isPath ( heights , mid )) { hi = mid ; } else { lo = mid + 1 ; } } return lo ; } private boolean isPath ( int [][] h , int mid ) { int m = h . length , n = h [ 0 ] . length ; Queue < int []> q = new LinkedList <> (); q . offer ( new int [ 2 ] ); Set < Integer > seen = new HashSet <> (); seen . add ( 0 ); while ( ! q . isEmpty ()) { int [] cur = q . poll (); int x = cur [ 0 ] , y = cur [ 1 ] ; if ( x == m - 1 && y == n - 1 ) return true ; for ( int k = 0 ; k < 4 ; k ++ ) { int r = x + d [ k ] , c = y + d [ k + 1 ] ; if ( 0 <= r && r < m && 0 <= c && c < n && mid >= Math . abs ( h [ r ][ c ] - h [ x ][ y ] ) && seen . add ( r * n + c )) { q . offer ( new int [] { r , c }); } } } return false ; } }","title":"Graph"},{"location":"leetcode/graph/notes/#graph","text":"","title":"Graph"},{"location":"leetcode/graph/notes/#representations","text":"","title":"Representations"},{"location":"leetcode/graph/notes/#edge-lists","text":"An edge is represented using an array of two vertex numbers or objects contain the vertex numbers. Weights can be added as the third number or another member of the objects. Example: [[0,1], [0,6], [0,8], [1,4], [1,6], [1,9], [2,4], [2,6], [3,4], [3,5]] Space: \\Theta(E) \\Theta(E) Search edge: O(E) O(E)","title":"Edge lists"},{"location":"leetcode/graph/notes/#adjacency-matrix","text":"Use a matrix[i][j] to represent the edges. 1 is connected, 0 is not connected. Example: [ [0, 1, 0, 0, 0, 0, 1, 0, 1, 0], [1, 0, 0, 0, 1, 0, 1, 0, 0, 1], [0, 0, 0, 0, 1, 0, 1, 0, 0, 0], [0, 0, 0, 0, 1, 1, 0, 0, 1, 0], [0, 1, 1, 1, 0, 1, 0, 0, 0, 1], [0, 0, 0, 1, 1, 0, 0, 0, 0, 0], [1, 1, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 1], [1, 0, 0, 1, 0, 0, 0, 1, 0, 0], [0, 1, 0, 0, 1, 0, 0, 1, 0, 0] ] Check edge (i, j) exists: O(1) O(1) Space: \\Theta(V^2) \\Theta(V^2) Search djacent vertex: O(V) O(V) For undirected graph, adjacency matrix is symmetric. For a directed graph, the adjacency matrix need not be symmetric.","title":"Adjacency matrix"},{"location":"leetcode/graph/notes/#adjacency-list","text":"Representing a graph with adjacency lists combines adjacency matrices with edge lists. For each vertex i , store an array of the vertices adjacent to it. Each vertex has a list. Space: \\Theta(2E) \\Theta(2E) for undirected graph. \\Theta(E) \\Theta(E) for directed graph. Check edge (i, j) exists: O(d) O(d) , d is the degree of the vertex i . 0 \\leq d \\leq V - 1 0 \\leq d \\leq V - 1 . If the graph is weighted, then each item in each adjacency list is either a two-item array or an object, giving the vertex number and the edge weight. Representing graphs","title":"Adjacency list"},{"location":"leetcode/graph/notes/#algorithms","text":"Weighted Graph Unweighted Graph Directed Graph Undirected Graph Graph Traversal Single Source Shortest Path Connected components Minimum spanning tree","title":"Algorithms"},{"location":"leetcode/graph/notes/#kosarajus-two-pass-algorithm","text":"Reverse each arc of the original graph. First run of the DFS-Loop to computer the \"finish\" time of each node in the reversed graph. Book keeping: The \"source node of a node\". A source node is the first node in the DFS tree. The \"finish time of a node\". Second run of the DFS-Loop to computer the strongly connected components (SCC). The SCC is represented by the source node.","title":"Kosaraju's Two-pass algorithm"},{"location":"leetcode/graph/notes/#dijkstra-algorithm-no-negative-edge","text":"Dijkstra algorithm implementation need one data structure (think of it as an array) to record the path distance, another data structure (priority queue) to maintain the min-heap which is ordered by the key of the edge weights. The algorithm greedily select the smallest edge from the queue and update the optimal solution so far (called relaxation in graph algorithm jargon). Dijkstra algorithm Implemention requires to maintain two invariant, 1. All the vertices w_i w_i that not in S S will be kept in the heap. 2. The key should be the minimum weight of all the edges who's tail in S S , head point to w_i w_i . In every iteration, each w_i w_i need to run a local turnament to decide its new key and then update the heap. To compare Dijkastra to BFS, when you visit a new node, Dijkastra will do a relaxation (update keys by local turnament and reinsert heap) while BFS simply add the node to the queue. Dijkastra algorithm psudo code INITIALIZE-SINGLE-SOURCE(G, s) for each vertex v in G.V: v.d = inf v.parent = null s.d = 0 RELAX(u, v, w) if v.d > u.d + w(u, v) v.d = u.d + w(u, v) v.parent = u DIJKSTRA(G,w,s) INITIALIZE-SINGLE-SOURCE(G, s) S = None Q = G.V while Q != None u = EXTRACT-MIN(Q) S = S + {u} for each vertex v in G.adj[u]: RELAX(u, v, w)","title":"Dijkstra algorithm (No negative edge)"},{"location":"leetcode/graph/notes/#bellman-ford","text":"Bellman-Ford algorithm take care of negative edges in finding the single source shortest path in a graph. In addition, it also can be used to detect negative cycles reachable from the source. Notice Bellman-Ford algorithm can be optimized by exit when there is no new update happening. However, the worst case runtime is still O(V \\cdot, E) O(V \\cdot, E) . Bellman-Ford algorithm psudo code function BellmanFord(list vertices, list edges, vertex source) is // This implementation takes in a graph, represented as // lists of vertices (represented as integers [0..n-1]) and edges, // and fills two arrays (distance and predecessor) holding // the shortest path from the source to each vertex distance := list of size n predecessor := list of size n // Step 1: initialize graph for each vertex v in vertices do distance[v] := inf // Initialize the distance to all vertices to infinity predecessor[v] := null // And having a null predecessor distance[source] := 0 // The distance from the source to itself is, of course, zero // Step 2: relax edges repeatedly repeat |V|\u22121 times: for each edge (u, v) with weight w in edges do if distance[u] + w < distance[v] then distance[v] := distance[u] + w predecessor[v] := u // Step 3: check for negative-weight cycles for each edge (u, v) with weight w in edges do if distance[u] + w < distance[v] then error \"Graph contains a negative-weight cycle\" return distance, predecessor","title":"Bellman-Ford"},{"location":"leetcode/graph/notes/#shortest-path-faster-algorithm","text":"The algorithm is an improvement on Bellman-Ford algorithm. The idea is to limit the relaxation operation on only those nodes that have been relaxed before. We can use a queue to keep such nodes. However, the worst case runtime complexity is the same as Bellman-Ford, it is still O(V \\cdot, E) O(V \\cdot, E) . procedure Shortest-Path-Faster-Algorithm(G, s) 1 for each vertex v \u2260 s in V(G) 2 d(v) := \u221e 3 d(s) := 0 4 push s into Q 5 while Q is not empty do 6 u := poll Q 7 for each edge (u, v) in E(G) do 8 if d(u) + w(u, v) < d(v) then 9 d(v) := d(u) + w(u, v) 10 if v is not in Q then 11 push v into Q","title":"Shortest Path Faster Algorithm"},{"location":"leetcode/graph/notes/#bipartise","text":"One algorithm to use in graph bipartition is using coloring, namely apply DFS and color each node with different colors when visited them differently.","title":"Bipartise"},{"location":"leetcode/graph/notes/#directed-graph-strongly-connected-components","text":"","title":"Directed graph strongly connected components"},{"location":"leetcode/graph/notes/#interivew-strategy","text":"","title":"Interivew Strategy"},{"location":"leetcode/graph/notes/#clarification-questions","text":"What's the input scale? What's the limit of depth? Does the graph have self-edge or duplications? what's the corresponding return value should be if yes?","title":"Clarification questions"},{"location":"leetcode/graph/notes/#solving-graph-problems","text":"DFS/BFS/UF -> topological -> Bellman Ford -> Dijkastra -> To traverse a cyclic graph, we could not use visited, think of remove a edge after visited it. One example problem is 332. Reconstruct Itinerary","title":"Solving graph problems"},{"location":"leetcode/graph/notes/#problems","text":"","title":"Problems"},{"location":"leetcode/graph/notes/#332-reconstruct-itinerary","text":"","title":"332. Reconstruct Itinerary"},{"location":"leetcode/graph/notes/#407-trapping-rain-water-ii","text":"class Solution { public : int trapRainWater ( vector < vector < int >>& heightMap ) { if ( heightMap . empty ()) return 0 ; int m = heightMap . size (); int n = m ? heightMap [ 0 ]. size () : 0 ; if ( m <= 2 || n <= 2 ) { return 0 ; } int res = 0 , mx = 0 ; priority_queue < pair < int , int > , vector < pair < int , int >> , greater < pair < int , int >>> pq ; vector < vector < bool >> visited ( m , vector < bool > ( n , false )); int x [ 4 ] = { -1 , 0 , 1 , 0 }; int y [ 4 ] = { 0 , 1 , 0 , -1 }; // initialize the pq. for ( int i = 0 ; i < m ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { if ( i == 0 || i == m - 1 || j == 0 || j == n - 1 ) { pq . push ({ heightMap [ i ][ j ], i * n + j }); visited [ i ][ j ] = true ; } } } while ( ! pq . empty ()) { auto t = pq . top (); pq . pop (); int h = t . first ; int i = t . second / n ; int j = t . second % n ; mx = max ( h , mx ); for ( int k = 0 ; k < 4 ; k ++ ) { int p = i + x [ k ]; int q = j + y [ k ]; if ( p >= 0 && p < m && q >= 0 && q < n && ! visited [ p ][ q ]) { visited [ p ][ q ] = true ; if ( heightMap [ p ][ q ] < mx ) { res += mx - heightMap [ p ][ q ]; } pq . push ({ heightMap [ p ][ q ], p * n + q }); } } } return res ; } };","title":"407. Trapping Rain Water II"},{"location":"leetcode/graph/notes/#critical-connections-in-a-network","text":"","title":"Critical Connections in a Network"},{"location":"leetcode/graph/notes/#critical-routers","text":"","title":"Critical Routers"},{"location":"leetcode/graph/notes/#743-network-delay-time","text":"Solution 1 Dijkstra algorithm What's the key for heap node? What's the data structure is needed? ( dist array, and a heap) Solution 2 Bellman Ford algorithm Solution 3 Shortest Path Best algorithm === C++ Dijkstra ```c++ class Solution { public: int networkDelayTime(vector<vector<int>>& times, int n, int k) { vector<vector<pair<int, int>>> graph(n + 1, vector<pair<int, int>>()); for (auto& t: times) { graph[t[0]].push_back({t[1], t[2]}); } priority_queue<pair<int, int>, vector<pair<int, int>>, greater<pair<int, int>>> pq; pq.push({0, k}); vector<int> dist(n + 1, INT_MAX); dist[k] = 0; while (!pq.empty()) { pair<int, int> t = pq.top(); pq.pop(); int u = t.second; for (auto x: graph[u]) { int v = x.first; int w = x.second; if (dist[v] > dist[u] + w) { dist[v] = dist[u] + w; pq.push({dist[v], v}); } } } int res = *max_element(dist.begin() + 1, dist.end()); return res == INT_MAX ? -1 : res; } }; ``` === C++ Bellman Ford ```c++ class Solution { public: int networkDelayTime(vector<vector<int>>& times, int n, int k) { vector<int> dist(n + 1, INT_MAX); dist[k] = 0; for (int i = 0; i < n; ++i) { for (auto& t : times) { int u = t[0], v = t[1], w = t[2]; if (dist[u] != INT_MAX && dist[v] > dist[u] + w) { dist[v] = dist[u] + w; } } } int res = *max_element(dist.begin() + 1, dist.end()); return res == INT_MAX ? -1 : res; } }; ```","title":"743. Network Delay Time"},{"location":"leetcode/graph/notes/#778-swim-in-rising-water","text":"Solution 1 Binary search + DFS Solution 2 Dijkstra C++ Binary Search + DFS class Solution { int x [ 4 ] = { 0 , -1 , 0 , 1 }; int y [ 4 ] = { -1 , 0 , 1 , 0 }; public : int swimInWater ( vector < vector < int >>& grid ) { int n = grid . size (); int begin = grid [ 0 ][ 0 ], end = n * n - 1 ; // binary search find a path with mini elevation while ( begin < end ) { int mid = begin + ( end - begin ) / 2 ; if ( pathExist ( grid , mid )) { end = mid ; } else { begin = mid + 1 ; } } return begin ; } bool pathExist ( vector < vector < int >> & grid , int mid ) { int n = grid . size (); vector < vector < int >> visited ( n , vector < int > ( n , 0 )); return dfs_helper ( grid , visited , n , mid , 0 , 0 ); } bool dfs_helper ( vector < vector < int >> & grid , vector < vector < int >>& visited , int n , int mid , int i , int j ) { visited [ i ][ j ] = 1 ; for ( int k = 0 ; k < 4 ; ++ k ) { int a = i + x [ k ]; int b = j + y [ k ]; if ( a < 0 || a >= n || b < 0 || b >= n || visited [ a ][ b ] == 1 || grid [ a ][ b ] > mid ) continue ; if ( a == n - 1 && b == n - 1 ) return true ; if ( dfs_helper ( grid , visited , n , mid , a , b )) return true ; } return false ; } }; C++ Dijkstra class Solution { int d [ 5 ] = { 0 , -1 , 0 , 1 , 0 }; public : int swimInWater ( vector < vector < int >>& grid ) { int n = grid . size (); priority_queue < pair < int , int > , vector < pair < int , int >> , greater < pair < int , int >>> pq ; pq . push ({ grid [ 0 ][ 0 ], 0 }); vector < vector < int >> visited ( n , vector < int > ( n , 0 )); visited [ 0 ][ 0 ] = 1 ; int res = 0 ; while ( ! pq . empty ()) { pair < int , int > t = pq . top (); pq . pop (); int x = t . second / n ; int y = t . second % n ; res = max ( res , t . first ); if ( x == n - 1 && y == n - 1 ) break ; for ( int i = 0 ; i < 4 ; ++ i ) { int a = x + d [ i ]; int b = y + d [ i + 1 ]; if ( a >= 0 && a < n && b >= 0 && b < n && visited [ a ][ b ] == 0 ) { visited [ a ][ b ] = 1 ; pq . push ({ grid [ a ][ b ], a * n + b }); } } } return res ; } };","title":"778. Swim in Rising Water"},{"location":"leetcode/graph/notes/#785-is-graph-bipartite","text":"Solution 1 Coloring neighboring nodes with alternative color class Solution { public : bool isBipartite ( vector < vector < int >>& graph ) { int n = graph . size (); vector < int > visited ( n , -1 ); for ( int i = 0 ; i < n ; ++ i ) { if ( visited [ i ] == -1 && ! dfs_helper ( graph , visited , i , 1 )) { return false ; } } return true ; } bool dfs_helper ( vector < vector < int >>& graph , vector < int >& visited , int i , int color ) { if ( visited [ i ] != -1 ) { return visited [ i ] == color ; // the color is ok } visited [ i ] = color ; // not colored before, now color it for ( auto v : graph [ i ]) { if ( ! dfs_helper ( graph , visited , v , 1 - color )) { return false ; } } return true ; } };","title":"785. Is Graph Bipartite?"},{"location":"leetcode/graph/notes/#1102-path-with-maximum-minimum-value","text":"Solution 1 Dijkstra Solution 2 Union-Find Python Union Find class Solution : def maximumMinimumPath ( self , A : List [ List [ int ]]) -> int : R , C = len ( A ), len ( A [ 0 ]) parent = [ i for i in range ( R * C )] dire = [( 0 , 1 ), ( 0 , - 1 ), ( 1 , 0 ), ( - 1 , 0 )] seen = [[ 0 for _ in range ( C )] for _ in range ( R )] def find ( x ): if parent [ x ] != x : parent [ x ] = find ( parent [ x ]) return parent [ x ] def union ( x , y ): rx , ry = find ( x ), find ( y ) if rx != ry : parent [ ry ] = rx points = [( x , y ) for x in range ( R ) for y in range ( C )] points . sort ( key = lambda x : A [ x [ 0 ]][ x [ 1 ]], reverse = True ) for x , y in points : seen [ x ][ y ] = 1 for dx , dy in dire : nx , ny = x + dx , y + dy if 0 <= nx < R and 0 <= ny < C and seen [ nx ][ ny ]: union ( x * C + y , nx * C + ny ) if find ( 0 ) == find ( R * C - 1 ): return A [ x ][ y ] return - 1 C++ Dijkstra class Solution { public : int maximumMinimumPath ( vector < vector < int >>& A ) { int m = A . size (); int n = A [ 0 ]. size (); int res = INT_MAX ; priority_queue < pair < int , int > , vector < pair < int , int >>> pq ; // max heap. pq . emplace ( A [ 0 ][ 0 ], 0 ); vector < vector < int >> visited ( m , vector < int > ( n , 0 )); visited [ 0 ][ 0 ] = -1 ; int d [ 5 ] = { 0 , 1 , 0 , -1 , 0 }; while ( ! pq . empty ()) { pair < int , int > t = pq . top (); pq . pop (); int cost = t . first ; int x = t . second / n ; int y = t . second % n ; res = min ( res , cost ); if ( x == m - 1 && y == n - 1 ) break ; for ( int k = 0 ; k < 4 ; k ++ ) { int r = x + d [ k ]; int c = y + d [ k + 1 ]; if ( r < 0 || r >= m || c < 0 || c >= n || visited [ r ][ c ] < 0 ) continue ; pq . emplace ( A [ r ][ c ], r * n + c ); visited [ r ][ c ] = -1 ; } } return res ; } };","title":"1102. Path With Maximum Minimum Value"},{"location":"leetcode/graph/notes/#1514-path-with-maximum-probability","text":"class Solution { public : double maxProbability ( int n , vector < vector < int >>& edges , vector < double >& succProb , int start , int end ) { unordered_map < int , vector < pair < int , double >>> graph ; for ( int i = 0 ; i < edges . size (); ++ i ) { graph [ edges [ i ][ 0 ]]. push_back ({ edges [ i ][ 1 ], succProb [ i ]}); graph [ edges [ i ][ 1 ]]. push_back ({ edges [ i ][ 0 ], succProb [ i ]}); } vector < double > prob ( n , 0.0 ); prob [ start ] = 1.0 ; priority_queue < pair < double , int >> pq ; pq . push ({ 1.0 , start }); while ( ! pq . empty ()) { pair < double , int > u = pq . top (); pq . pop (); for ( auto vp : graph [ u . second ]) { int v = vp . first ; double p = vp . second ; if ( prob [ v ] < p * prob [ u . second ]) { prob [ v ] = p * prob [ u . second ]; pq . push ({ prob [ v ], v }); } } } return prob [ end ]; } };","title":"1514. Path with Maximum Probability"},{"location":"leetcode/graph/notes/#1631-path-with-minimum-effort","text":"C++ Dijkstra class Solution { public : int minimumEffortPath ( vector < vector < int >>& heights ) { int m = heights . size (); int n = heights [ 0 ]. size (); vector < vector < int >> dist ( m , vector < int > ( n , INT_MAX )); // min distance found so far. priority_queue < pair < int , int > , vector < pair < int , int >> , greater < pair < int , int >>> pq ; int d [ 5 ] = { 0 , 1 , 0 , -1 , 0 }; pq . emplace ( 0 , 0 ); // first: min effort, second: encoded (x, y) (=x * n + y); while ( ! pq . empty ()) { pair < int , int > t = pq . top (); int effort = t . first ; int x = t . second / n ; int y = t . second % n ; pq . pop (); if ( x == m - 1 && y == n - 1 ) return effort ; for ( int k = 0 ; k < 4 ; ++ k ) { int a = x + d [ k ]; int b = y + d [ k + 1 ]; if ( a < 0 || a >= m || b < 0 || b >= n ) continue ; int currEffort = max ( effort , abs ( heights [ a ][ b ] - heights [ x ][ y ])); if ( currEffort < dist [ a ][ b ]) { dist [ a ][ b ] = currEffort ; pq . push ({ currEffort , a * n + b }); } } } return -1 ; } }; Python Dijkstra class Solution : def minimumEffortPath ( self , heights : List [ List [ int ]]) -> int : m , n = map ( len , [ heights , heights [ 0 ]]) efforts = [[ math . inf ] * n for _ in range ( m )] efforts [ 0 ][ 0 ] = 0 heap = [( 0 , 0 , 0 )] while heap : effort , x , y = heapq . heappop ( heap ); if x == m - 1 and y == n - 1 : return effort for i , j in ( x , y - 1 ), ( x , y + 1 ), ( x - 1 , y ), ( x + 1 , y ): if i < 0 or i >= m or j < 0 or j >= n : continue currEffort = max ( effort , abs ( heights [ x ][ y ] - heights [ i ][ j ])) if efforts [ i ][ j ] > currEffort : efforts [ i ][ j ] = currEffort heapq . heappush ( heap , ( currEffort , i , j )) Java Dijkstra class Solution { private int [] d = { 0 , 1 , 0 , - 1 , 0 }; public int minimumEffortPath ( int [][] heights ) { int lo = 0 , hi = 1_000_000 ; while ( lo < hi ) { int effort = lo + ( hi - lo ) / 2 ; if ( isPath ( heights , effort )) { hi = effort ; } else { lo = effort + 1 ; } } return lo ; } private boolean isPath ( int [][] h , int effort ) { int m = h . length , n = h [ 0 ] . length ; Queue < int []> q = new LinkedList <> (); q . offer ( new int [ 2 ] ); Set < Integer > seen = new HashSet <> (); seen . add ( 0 ); while ( ! q . isEmpty ()) { int [] cur = q . poll (); int x = cur [ 0 ] , y = cur [ 1 ] ; if ( x == m - 1 && y == n - 1 ) { return true ; } for ( int k = 0 ; k < 4 ; ++ k ) { int r = x + d [ k ] , c = y + d [ k + 1 ] ; if ( 0 <= r && r < m && 0 <= c && c < n && effort >= Math . abs ( h [ r ][ c ] - h [ x ][ y ] ) && seen . add ( r * n + c )) { q . offer ( new int [] { r , c }); } } } return false ; } } Java Binary search + BFS class Solution { private int [] d = { 0 , 1 , 0 , - 1 , 0 }; public int minimumEffortPath ( int [][] heights ) { int lo = 0 , hi = 1_000_000 ; while ( lo < hi ) { int mid = lo + ( hi - lo ) / 2 ; if ( isPath ( heights , mid )) { hi = mid ; } else { lo = mid + 1 ; } } return lo ; } private boolean isPath ( int [][] h , int mid ) { int m = h . length , n = h [ 0 ] . length ; Queue < int []> q = new LinkedList <> (); q . offer ( new int [ 2 ] ); Set < Integer > seen = new HashSet <> (); seen . add ( 0 ); while ( ! q . isEmpty ()) { int [] cur = q . poll (); int x = cur [ 0 ] , y = cur [ 1 ] ; if ( x == m - 1 && y == n - 1 ) return true ; for ( int k = 0 ; k < 4 ; k ++ ) { int r = x + d [ k ] , c = y + d [ k + 1 ] ; if ( 0 <= r && r < m && 0 <= c && c < n && mid >= Math . abs ( h [ r ][ c ] - h [ x ][ y ] ) && seen . add ( r * n + c )) { q . offer ( new int [] { r , c }); } } } return false ; } }","title":"1631. Path With Minimum Effort"},{"location":"leetcode/hash/notes/","text":"Hash Table \u00b6 Isomorphic Strings \u00b6 Python Two Hash Solution class Solution : def isIsomorphic ( self , s : str , t : str ) -> bool : if len ( s ) != len ( t ): return False ; sdict = {} tdict = {} for a , b in zip ( s , t ): if a not in sdict : if b in tdict : return False else : sdict [ a ] = b tdict [ b ] = a elif sdict [ a ] != b : return False return True Python Two Hash Solution class Solution { public : bool isIsomorphic ( string s , string t ) { if ( s . length () != t . length ()) return false ; vector < int > m1 ( 256 , -1 ); vector < int > m2 ( 256 , -1 ); for ( int i = 0 ; i < s . length (); i ++ ) { if ( m1 [ s [ i ]] != m2 [ t [ i ]]) return false ; m1 [ s [ i ]] = i ; m2 [ t [ i ]] = i ; } return true ; } };","title":"Hash"},{"location":"leetcode/hash/notes/#hash-table","text":"","title":"Hash Table"},{"location":"leetcode/hash/notes/#isomorphic-strings","text":"Python Two Hash Solution class Solution : def isIsomorphic ( self , s : str , t : str ) -> bool : if len ( s ) != len ( t ): return False ; sdict = {} tdict = {} for a , b in zip ( s , t ): if a not in sdict : if b in tdict : return False else : sdict [ a ] = b tdict [ b ] = a elif sdict [ a ] != b : return False return True Python Two Hash Solution class Solution { public : bool isIsomorphic ( string s , string t ) { if ( s . length () != t . length ()) return false ; vector < int > m1 ( 256 , -1 ); vector < int > m2 ( 256 , -1 ); for ( int i = 0 ; i < s . length (); i ++ ) { if ( m1 [ s [ i ]] != m2 [ t [ i ]]) return false ; m1 [ s [ i ]] = i ; m2 [ t [ i ]] = i ; } return true ; } };","title":"Isomorphic Strings"},{"location":"leetcode/heap/notes/","text":"Heap \u00b6 Binary heap properties \u00b6 Usually implemented using an array of elements but visualized as a complete binary tree (not a full binary tree). From the array index, we can compute the parent index from child's index and vice versa. The element values are partially ordered, briefly, they are ordered level by level. There is no ordering information of element values in the same level. The minimum number of elements in a binary heap with height h h is 2^h 2^h , the maximum elements in it is 2^{h + 1} - 1 2^{h + 1} - 1 The height of a binary heap is the floor of \\log n \\log n . n n is the total number of elements. The minimum element of a maximum heap is in the leaf nodes. With the array representation for storing an n-element heap, the leaves are the nodes indexed by \\lfloor n/2 \\rfloor + 1, \\lfloor n/2 \\rfloor + 2, \\dots, n \\lfloor n/2 \\rfloor + 1, \\lfloor n/2 \\rfloor + 2, \\dots, n . Heap sort \u00b6 Heap implementation using an array \u00b6 Heap implementation using HeapNode \u00b6 Priority queue operations \u00b6 Supported operations \u00b6 # Operation complexity comment 1 buildMaxHeap(A) O(n) O(n) Build a priority queue routine iteratively call maxHeapify() n/2 n/2 times. The complexity is O(n) O(n) instead of O(n \\log n) O(n \\log n) 2 maxHeapify(A, i) O(\\log n) O(\\log n) Maintains the heap properties (by bubble up/down) for subtree rooted at i . Assumes the subtrees of i are max heap before calling this routine. 3 insert() O(\\log n) O(\\log n) Insert a element to the heap. 4 peekMaximum() O(1) O(1) Return the max element without remove it from the max-heap. 5 popMaximum() O(\\log n) O(\\log n) Remove the max element from the max-heap. 6 increaseKey() O(\\log n) O(\\log n) Update the element's key. This essentially changed the priority of an element. 7 delete() Not supported by the binary heap implementation, but can be achieved by introduce more complex data structures. C++ STL priority queue \u00b6 Notice if the elements in the queue is some complex container or object, you have to declare priority_queue with its underline container type, and the binary predicate std::greater if necessary. Here is an example in the solution of the problem Minimum Unique Word Abbreviation priority_queue < pair < int , string > , vector < pair < int , string >> , greater < pair < int , string >>> pq ; This line of code declared a priority queue pq , whose element is piar<int, string> , with the minimum element on the top. But minimum what? How the priority queue sort a pair<int, string> element? In this case it implicitly sort the pair<int, string> element base on the first int value. How can we sort the priority queue element based on the second value of the pair<int, string> . To achieve that, we cannot use the building function object greater we have write a customized function object and pass to the declaration of the pq like this, class mycomparison { public : mycomparison () {} bool operator () ( const pair < int , string > & a , const pair < int , string > & b ) const { return a . second . compare ( b . second ); } }; ... priority_queue < pair < int , string > , vector < pair < int , string >> , mycomparison > pq ; ... We may need to write a customized binary predicate function in order to implement the min-heap or max-heap . The binary predicate function depends on the underline container and also the type of the container element. Java Priority Queue \u00b6 Primitive types Queue < Integer > pq = new PriorityQueue <> (); // default is min heap pq . add ( 1 ); pq . add ( 2 ); pq . poll (); // return the min value; Use with objects public class CustomerOrder implements Comparable < CustomerOrder > { private int orderId ; private double orderAmount ; private String customerName ; public CustomerOrder ( int orderId , double orderAmount , String customerName ) { this . orderId = orderId ; this . orderAmount = orderAmount ; this . customerName = customerName ; } @Override public int compareTo ( CustomerOrder o ) { return o . orderId > this . orderId ? 1 : - 1 ; } @Override public String toString () { return \"orderId:\" + this . orderId + \", orderAmount:\" + this . orderAmount + \", customerName:\" + customerName ; } public double getOrderAmount () { return orderAmount ; } } Queue < CustomerOrder > pq = new PriorityQueue <> ( new CustomIntegerComparator ()); // to overwrite the compareTo comparator static class CustomerOrderComparator implements Comparator < CustomerOrder > { @Override public int compare ( CustomerOrder o1 , CustomerOrder o2 ) { return o1 . getOrderAmount () < o2 . getOrderAmount () ? 1 : - 1 ; } } Queue < CustomerOrder > pq = new PriorityQueue <> ( new CustomOrderComparator ()); Custom Ordering static class CustomIntegerComparator implements Comparator < Integer > { @Override public int compare ( Integer o1 , Integer o2 ) { return o1 < o2 ? 1 : - 1 ; // reverse ordering } } Queue < Integer > pq = new PriorityQueue <> ( new CustomIntegerComparator ()); Lambda comparator Java 8 Queue < int []> pq = new PriorityQueue <> ( Comparator . comparingInt ( a -> a [ 0 ] )); Python heapq package \u00b6 Python heapq package provides a library of build heap data structure and heap related operations. It supports the following methods, heapq.heappush(heap, item) heapq.heappop(heap) heapq.heappushpop(heap, item) heapq.heapify(list) heapreplace(heap, item) equivalent to heappoppush (not exists). heapq.merge(*iterable, key=None, reverse=False) heapq.nlargest(n, iterable, key=None) heapq.nsmallest(n, iterable, key=None) Priority Queue \u00b6 Merge k Sorted Lists \u00b6 Solution 1 Priority queue Use a min priority queue to store all the list, iteratively pop the \"min list\", take the head node to the final list and push back to the priority queue if the \"min list\" still have node. Priority queue solution is O(N \\log k) O(N \\log k) /** * Definition for singly-linked list. * struct ListNode { * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) {} * }; */ class listComparison { public : listComparison () {} bool operator ( const ListNode * a , const ListNode * b ) const { return a -> val > b -> val ; } } class Solution { public : ListNode * mergeKLists ( vector < ListNode *>& lists ) { priority_queue < ListNode * , vector < ListNode *> , listComparison > q ; for ( auto list : lists ) { if ( list ) { q . push ( list ); } } ListNode dummy ( INT_MIN ); ListNode * curr = & dummy ; while ( ! q . empty ()) { curr -> next = q . top (); q . pop (); curr = curr -> next ; if ( curr -> next ) { q . push ( curr -> next ); } } return dummy . next ; } } Solution 2 Divide and conquer merge Use the divide and conquer idea to reduce the problem to a smaller problem and then solve the smallest problem, combined together with the solution of small problems, we will arrived at the final solution. Compare the following solution to merge sort algorithm. The complexity is O(N \\log k) O(N \\log k) . Because merge two list will take O(N) O(N) , and we will do O(\\log k) O(\\log k) time of merging. class Solution { public : ListNode * mergeKLists ( vector < ListNode *>& lists ) { return mergeKListsHelper ( lists , 0 , lists . size () - 1 ); } ListNode * mergeKListsHelper ( vector < ListNode *>& lists , int s , int e ) { if ( s > e ) return nullptr ; if ( s == e ) return lists [ s ]; if ( s < e ) { int m = s + ( e - s ) / 2 ; ListNode * l1 = mergeKListsHelper ( lists , s , m ); ListNode * l2 = mergeKListsHelper ( lists , m + 1 , e ); return merge ( l1 , l2 ); } } ListNode * merge ( ListNode * l1 , ListNode * l2 ) { ListNode dummy ( INT_MIN ); ListNode * curr = & dummy ; while ( l1 && l2 ) { if ( l1 -> val < l2 -> val ) { curr -> next = l1 ; l1 = l1 -> next ; } else { curr -> next = l2 ; l2 = l2 -> next ; } curr = curr -> next ; } curr -> next = l1 ? l1 : l2 ; return dummy . next ; } }; Minimum Unique Word Abbreviation \u00b6 Smallest Range Covering Elements from K Lists \u00b6 Solution 1 Priority queue Notice this solution pushed some meta data to the priority queue. class Solution { public : struct mycompare { bool operator () ( pair < int , int >& a , pair < int , int >& b ) { return a . first > b . first ; } }; vector < int > smallestRange ( vector < vector < int >>& nums ) { int n = nums . size (); priority_queue < pair < int , int > , vector < pair < int , int >> , mycompare > pq ; int end = INT_MIN ; for ( int i = 0 ; i < n ; i ++ ) { end = max ( end , nums [ i ][ 0 ]); // push the first element of kth vector and k to the pq pq . push ({ nums [ i ][ 0 ], i }); } vector < int > idx ( n , 0 ); // keep tracking the index to the k arrays. vector < int > res = { -100000 , 100000 }; // the range is given. while ( pq . size () == n ) { int start = pq . top (). first ; int k = pq . top (). second ; pq . pop (); if ( end - start < res [ 1 ] - res [ 0 ]) { res [ 0 ] = start ; res [ 1 ] = end ; } // if nums[k] have more element, we push next to the queue. if ( ++ idx [ k ] < nums [ k ]. size ()) { pq . push ({ nums [ k ][ idx [ k ]], k }); end = max ( end , nums [ k ][ idx [ k ]]); } } return res ; } }; Kth Largest Element in an Array \u00b6 Solution 1 priority queue Build a heap takes O(n) . max-heap O(n) + O(klogk) . class Solution { public : int findKthLargest ( vector < int >& nums , int k ) { priority_queue < int > pq ( nums . begin (), nums . end ()); k -- ; while ( k -- > 0 ) { pq . pop (); } return pq . top (); } }; Solution 2 priority queue min-heap O(n*logk) . class Solution { public : int findKthLargest ( vector < int >& nums , int k ) { priority_queue < int , vector < int > , greater < int >> pq ; for ( int n : nums ) { pq . push ( n ); if ( pq . size () > k ) { pq . pop (); } } return pq . top (); } }; Solution 3 quickselect algorithm class Solution { public : int findKthLargest ( vector < int >& nums , int k ) { int n = nums . size (); if ( n < k ) return 0 ; return orderStats ( nums , 0 , n - 1 , k ); } int orderStats ( vector < int >& nums , int start , int end , int k ) { if ( start == end ) return nums [ start ]; int p = partition ( nums , start , end ); int order = p - start + 1 ; if ( order == k ) { return nums [ p ]; } else if ( order > k ) { return orderStats ( nums , start , p - 1 , k ); } else { return orderStats ( nums , p + 1 , end , k - order ); } } int partition ( vector < int >& nums , int start , int end ) { if ( start == end ) return 0 ; int p = start + floor ( rand () % ( end - start + 1 )); swap ( nums [ p ], nums [ end ]); int pivot = nums [ end ]; int i = start - 1 , j ; for ( j = start ; j < end ; ++ j ) { if ( nums [ j ] >= pivot ) { swap ( nums [ ++ i ], nums [ j ]); } } swap ( nums [ end ], nums [ i + 1 ]); return i + 1 ; } }; The Skyline Problem \u00b6 Solution 1 priority queue maximum heap class Solution { public : vector < pair < int , int >> getSkyline ( vector < vector < int >>& buildings ) { vector < pair < int , int >> res ; int i = 0 , x = 0 , h = 0 , len = buildings . size (); priority_queue < pair < int , int >> q ; // max heap store <height, end> while ( i < len || ! q . empty ()) { // building i start before the end of current tallest building if ( q . empty () || i < len && buildings [ i ][ 0 ] <= q . top (). second ) { x = buildings [ i ][ 0 ]; // current start // overlapped start of multiple buildings (compare to if) while ( i < len && buildings [ i ][ 0 ] == x ) { q . push ({ buildings [ i ][ 2 ], buildings [ i ][ 1 ]}); i ++ ; } } else { // building i start after the end of current tallest building x = q . top (). second ; // current end // pop all buildings that end <= currently tallest buildings while ( ! q . empty () && q . top (). second <= x ) q . pop (); } h = q . empty () ? 0 : q . top (). first ; if ( res . empty () || res . back (). second != h ) { res . push_back ({ x , h }); } } return res ; } }; Solution 2 multiset + scanline solution class Solution { private : static bool cmp ( pair < int , int > p1 , pair < int , int > p2 ){ if ( p1 . first != p2 . first ) return p1 . first < p2 . first ; return p1 . second > p2 . second ; // if x duplicate, tallest building first } public : vector < pair < int , int >> getSkyline ( vector < vector < int >>& buildings ) { vector < pair < int , int >> h ; // store the start and end. vector < pair < int , int >> res ; multiset < int , greater < int >> s ; int prev = 0 , curr = 0 ; // scanline technique to obtain the starting point and end point for ( auto & a : buildings ) { h . push_back ({ a [ 0 ], a [ 2 ]}); h . push_back ({ a [ 1 ], - a [ 2 ]}); // use \"-\" mark the end point } sort ( h . begin (), h . end (), cmp ); s . insert ( 0 ); // edge case, consider a.second == 0 for ( auto & a : h ) { if ( a . second > 0 ) s . insert ( a . second ); // if it is a start, insert to the set else s . erase ( s . find ( - a . second )); // if it is a end, erase the inserted start curr = * s . begin (); // take the maximum from the set if ( curr != prev ) { // remove duplicate res . push_back ({ a . first , curr }); prev = curr ; } } return res ; } }; Sliding Window Median \u00b6 Solution 1 heap + hash (simulated) class Solution { public : vector < double > medianSlidingWindow ( vector < int >& nums , int k ) { int n = nums . size (); vector < double > res ; priority_queue < int > lo ; priority_queue < int , vector < int > , greater < int >> hi ; unordered_map < int , int > hash_heap ; int i = 0 ; while ( i < k ) lo . push ( nums [ i ++ ]); for ( int j = 0 ; j < k / 2 ; j ++ ) { hi . push ( lo . top ()); lo . pop (); } while ( true ) { res . push_back ( k & 1 ? lo . top () : (( double ) lo . top () + ( double ) hi . top ()) * 0.5 ); if ( i >= nums . size ()) break ; int out_num = nums [ i - k ]; // first element in the window int in_num = nums [ i ++ ]; // new element entering window int balance = 0 ; // -1, when out_num in lo, +1 when out_num in hi. // balance is the count of element diff in lo and hi. // lo.size() == hi.size() -> balance = 0 // lo.size() > hi.size() --> balance > 0 // lo.size() < hi.size() --> balance < 0 balance += ( out_num <= lo . top () ? -1 : 1 ); hash_heap [ out_num ] ++ ; // count of the invalid element, will remove when it on the top. if ( ! lo . empty () && in_num <= lo . top ()) { balance ++ ; lo . push ( in_num ); } else { balance -- ; hi . push ( in_num ); } if ( balance < 0 ) { // lo lack of element. lo . push ( hi . top ()); hi . pop (); balance ++ ; } else if ( balance > 0 ) { // hi lack of element hi . push ( lo . top ()); lo . pop (); balance -- ; } // lazy remove of elements previous removed, mimic hashheap of java. while ( ! lo . empty () && hash_heap [ lo . top ()]) { hash_heap [ lo . top ()] -- ; lo . pop (); } // lazy remove of elements previous removed, mimic hashheap of java. while ( ! hi . empty () && hash_heap [ hi . top ()]) { hash_heap [ hi . top ()] -- ; hi . pop (); } } return res ; } }; 373. Find K Pairs with Smallest Sums \u00b6 use the following visual add when writing your code. The best solution choose the smallest K pairs layer by layer from top left to bottom right. From the visualization, you can see that this problem is equivalent to the problem Kth Smallest Element in a Sorted Matrix 2 4 6 +------------ 1 | 3 5 7 7 | 9 11 13 11 | 13 15 17 Python heapq and generator def kSmallestPairs ( self , nums1 , nums2 , k ): streams = map ( lambda u : ([ u + v , u , v ] for v in nums2 ), nums1 ) stream = heapq . merge ( * streams ) # streams is iterables return [ suv [ 1 :] for suv in itertools . islice ( stream , k )] Java O(KlogK) https://leetcode.com/problems/find-k-pairs-with-smallest-sums/discuss/84551/simple-Java-O(KlogK)-solution-with-explanation C++ Priority Queue class cmp { public : bool operator () ( const pair < int , int >& a , const pair < int , int >& b ) { return a . first + a . second < b . first + b . second ; } }; class Solution { public : vector < pair < int , int >> kSmallestPairs ( vector < int >& nums1 , vector < int >& nums2 , int k ) { vector < pair < int , int >> res ; priority_queue < pair < int , int > , vector < pair < int , int >> , cmp > pq ; for ( int i = 0 ; i < min (( int ) nums1 . size (), k ); ++ i ) { for ( int j = 0 ; j < min (( int ) nums2 . size (), k ); ++ j ) { pq . push ({ nums1 [ i ], nums2 [ j ]}); if ( pq . size () > k ) { pq . pop (); } } } while ( ! pq . empty ()) { res . push_back ( pq . top ()); pq . pop (); } return res ; } }; C++ priority queue optimized (scheduler model) class Solution { public : vector < pair < int , int >> kSmallestPairs ( vector < int >& nums1 , vector < int >& nums2 , int k ) { // Base cases if ( nums1 . size () == 0 || nums2 . size () == 0 || k == 0 ) { return { }; } // Result pairs vector < pair < int , int >> result ; // Prioritized scheduling based on sum (consider it as edge weight in graph) // Similar to Prim's algorithm // Min sum pq storing pair indices of nums1 index and nums2 index auto pqCmp = [ & nums1 , & nums2 ]( const pair < int , int >& p1 , const pair < int , int >& p2 ) { return nums1 [ p1 . first ] + nums2 [ p1 . second ] > nums1 [ p2 . first ] + nums2 [ p2 . second ]; }; priority_queue < pair < int , int > , vector < pair < int , int >> , decltype ( pqCmp ) > pq ( pqCmp ); // Visited is required to avoid duplicate scheduling vector < vector < bool >> visited ( nums1 . size (), vector < bool > ( nums2 . size (), false )); // Push 0, 0 as the seed for scheduling pq . push ( make_pair ( 0 , 0 )); // Find k smallest sum pairs while ( k && ! pq . empty ()) { // Current min sum index pair is at pq top auto top = pq . top (); pq . pop (); result . push_back ( make_pair ( nums1 [ top . first ], nums2 [ top . second ])); // Advance num1 index and schedule ++ top . first ; if ( top . first < nums1 . size () && ! visited [ top . first ][ top . second ]) { pq . push ( top ); visited [ top . first ][ top . second ] = true ; } -- top . first ; // Advance num2 index and schedule ++ top . second ; if ( top . second < nums2 . size () && ! visited [ top . first ][ top . second ]) { pq . push ( top ); visited [ top . first ][ top . second ] = true ; } -- top . second ; -- k ; } return result ; } }; Use priority queue to rearrange tasks (characters, string, etc.) \u00b6 Rearrange String k Distance Apart \u00b6 Solution 1 Greedy + Priority Queue class Solution { public : string rearrangeString ( string s , int k ) { if ( k <= 0 ) return s ; unordered_map < char , int > mp ; for ( auto & c : s ) mp [ c ] ++ ; priority_queue < pair < int , char > , vector < pair < int , char >> , std :: less < pair < int , char >>> pq ; // max heap; for ( auto & p : mp ) { pq . push ({ p . second , p . first }); } string res ; while ( ! pq . empty ()) { vector < pair < int , char >> used ; int i = 0 ; while ( i < k && ! pq . empty ()) { auto t = pq . top (); pq . pop (); res . push_back ( t . second ); i ++ ; if ( -- t . first > 0 ) { used . push_back ( t ); } } if ( i != k && used . size ()) return \"\" ; for ( auto & p : used ) { pq . push ( p ); } } return res ; } }; Task Scheduler \u00b6 Solution 1 Greedy + Priority Queue class Solution { public : class cmp { public : cmp () {} bool operator ()( const pair < int , char >& a , const pair < int , char >& b ){ return a . first < b . first ; } }; int leastInterval ( vector < char >& tasks , int n ) { int chmap [ 26 ] = { 0 }; priority_queue < pair < int , char > , vector < pair < int , char >> , cmp > pq ; for ( char c : tasks ) { chmap [ c - 'A' ] ++ ; } for ( int i = 0 ; i < 26 ; ++ i ) { if ( chmap [ i ]) { pq . push ({ chmap [ i ], i + 'A' }); } } int cycle = n + 1 ; int res = 0 ; while ( ! pq . empty ()) { vector < pair < int , char >> used ; for ( int i = 0 ; i < cycle ; ++ i ) { // pick total of n + 1 in freq order if ( ! pq . empty ()) { used . push_back ( pq . top ()); pq . pop (); } } for ( auto p : used ) { if ( -- p . first ) { pq . push ( p ); // put back used chars if extra char exist after the useage. } } // handled partial cycle and full cycle res += ! pq . empty () ? cycle : used . size (); } return res ; } }; Reorganize String \u00b6 Solution 1 Greedy + Priority Queue class Solution { public : class cmp { public : cmp () {} bool operator ()( const pair < int , char >& a , const pair < int , char >& b ){ return a . first < b . first ; } }; string reorganizeString ( string S ) { int n = S . length (); int chmap [ 26 ] = { 0 }; // max heap priority_queue < pair < int , char > , vector < pair < int , char >> , cmp > pq ; for ( auto & c : S ) { //store to map chmap [ c - 'a' ] ++ ; } // push to priority queue for ( int i = 0 ; i < 26 ; ++ i ) { if ( chmap [ i ]) { pq . push ({ chmap [ i ], i + 'a' }); } } if ( pq . top (). first > ( n + 1 ) / 2 ) return \"\" ; string res = \"\" ; while ( ! pq . empty ()) { pair < int , char > p = pq . top (); pq . pop (); if ( res . empty () || res . back () != p . second ) { res . push_back ( p . second ); if ( -- p . first > 0 ) { // use one char pq . push ( p ); } } else { // cannot use duplicate char, take another pair < int , char > q = pq . top (); pq . pop (); res . push_back ( q . second ); if ( -- q . first > 0 ) { // use another char pq . push ( q ); } // remember put p back to the pq pq . push ( p ); } } return res ; } };","title":"Heap"},{"location":"leetcode/heap/notes/#heap","text":"","title":"Heap"},{"location":"leetcode/heap/notes/#binary-heap-properties","text":"Usually implemented using an array of elements but visualized as a complete binary tree (not a full binary tree). From the array index, we can compute the parent index from child's index and vice versa. The element values are partially ordered, briefly, they are ordered level by level. There is no ordering information of element values in the same level. The minimum number of elements in a binary heap with height h h is 2^h 2^h , the maximum elements in it is 2^{h + 1} - 1 2^{h + 1} - 1 The height of a binary heap is the floor of \\log n \\log n . n n is the total number of elements. The minimum element of a maximum heap is in the leaf nodes. With the array representation for storing an n-element heap, the leaves are the nodes indexed by \\lfloor n/2 \\rfloor + 1, \\lfloor n/2 \\rfloor + 2, \\dots, n \\lfloor n/2 \\rfloor + 1, \\lfloor n/2 \\rfloor + 2, \\dots, n .","title":"Binary heap properties"},{"location":"leetcode/heap/notes/#heap-sort","text":"","title":"Heap sort"},{"location":"leetcode/heap/notes/#heap-implementation-using-an-array","text":"","title":"Heap implementation using an array"},{"location":"leetcode/heap/notes/#heap-implementation-using-heapnode","text":"","title":"Heap implementation using HeapNode"},{"location":"leetcode/heap/notes/#priority-queue-operations","text":"","title":"Priority queue operations"},{"location":"leetcode/heap/notes/#supported-operations","text":"# Operation complexity comment 1 buildMaxHeap(A) O(n) O(n) Build a priority queue routine iteratively call maxHeapify() n/2 n/2 times. The complexity is O(n) O(n) instead of O(n \\log n) O(n \\log n) 2 maxHeapify(A, i) O(\\log n) O(\\log n) Maintains the heap properties (by bubble up/down) for subtree rooted at i . Assumes the subtrees of i are max heap before calling this routine. 3 insert() O(\\log n) O(\\log n) Insert a element to the heap. 4 peekMaximum() O(1) O(1) Return the max element without remove it from the max-heap. 5 popMaximum() O(\\log n) O(\\log n) Remove the max element from the max-heap. 6 increaseKey() O(\\log n) O(\\log n) Update the element's key. This essentially changed the priority of an element. 7 delete() Not supported by the binary heap implementation, but can be achieved by introduce more complex data structures.","title":"Supported operations"},{"location":"leetcode/heap/notes/#c-stl-priority-queue","text":"Notice if the elements in the queue is some complex container or object, you have to declare priority_queue with its underline container type, and the binary predicate std::greater if necessary. Here is an example in the solution of the problem Minimum Unique Word Abbreviation priority_queue < pair < int , string > , vector < pair < int , string >> , greater < pair < int , string >>> pq ; This line of code declared a priority queue pq , whose element is piar<int, string> , with the minimum element on the top. But minimum what? How the priority queue sort a pair<int, string> element? In this case it implicitly sort the pair<int, string> element base on the first int value. How can we sort the priority queue element based on the second value of the pair<int, string> . To achieve that, we cannot use the building function object greater we have write a customized function object and pass to the declaration of the pq like this, class mycomparison { public : mycomparison () {} bool operator () ( const pair < int , string > & a , const pair < int , string > & b ) const { return a . second . compare ( b . second ); } }; ... priority_queue < pair < int , string > , vector < pair < int , string >> , mycomparison > pq ; ... We may need to write a customized binary predicate function in order to implement the min-heap or max-heap . The binary predicate function depends on the underline container and also the type of the container element.","title":"C++ STL priority queue"},{"location":"leetcode/heap/notes/#java-priority-queue","text":"Primitive types Queue < Integer > pq = new PriorityQueue <> (); // default is min heap pq . add ( 1 ); pq . add ( 2 ); pq . poll (); // return the min value; Use with objects public class CustomerOrder implements Comparable < CustomerOrder > { private int orderId ; private double orderAmount ; private String customerName ; public CustomerOrder ( int orderId , double orderAmount , String customerName ) { this . orderId = orderId ; this . orderAmount = orderAmount ; this . customerName = customerName ; } @Override public int compareTo ( CustomerOrder o ) { return o . orderId > this . orderId ? 1 : - 1 ; } @Override public String toString () { return \"orderId:\" + this . orderId + \", orderAmount:\" + this . orderAmount + \", customerName:\" + customerName ; } public double getOrderAmount () { return orderAmount ; } } Queue < CustomerOrder > pq = new PriorityQueue <> ( new CustomIntegerComparator ()); // to overwrite the compareTo comparator static class CustomerOrderComparator implements Comparator < CustomerOrder > { @Override public int compare ( CustomerOrder o1 , CustomerOrder o2 ) { return o1 . getOrderAmount () < o2 . getOrderAmount () ? 1 : - 1 ; } } Queue < CustomerOrder > pq = new PriorityQueue <> ( new CustomOrderComparator ()); Custom Ordering static class CustomIntegerComparator implements Comparator < Integer > { @Override public int compare ( Integer o1 , Integer o2 ) { return o1 < o2 ? 1 : - 1 ; // reverse ordering } } Queue < Integer > pq = new PriorityQueue <> ( new CustomIntegerComparator ()); Lambda comparator Java 8 Queue < int []> pq = new PriorityQueue <> ( Comparator . comparingInt ( a -> a [ 0 ] ));","title":"Java Priority Queue"},{"location":"leetcode/heap/notes/#python-heapq-package","text":"Python heapq package provides a library of build heap data structure and heap related operations. It supports the following methods, heapq.heappush(heap, item) heapq.heappop(heap) heapq.heappushpop(heap, item) heapq.heapify(list) heapreplace(heap, item) equivalent to heappoppush (not exists). heapq.merge(*iterable, key=None, reverse=False) heapq.nlargest(n, iterable, key=None) heapq.nsmallest(n, iterable, key=None)","title":"Python heapq package"},{"location":"leetcode/heap/notes/#priority-queue","text":"","title":"Priority Queue"},{"location":"leetcode/heap/notes/#merge-k-sorted-lists","text":"Solution 1 Priority queue Use a min priority queue to store all the list, iteratively pop the \"min list\", take the head node to the final list and push back to the priority queue if the \"min list\" still have node. Priority queue solution is O(N \\log k) O(N \\log k) /** * Definition for singly-linked list. * struct ListNode { * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) {} * }; */ class listComparison { public : listComparison () {} bool operator ( const ListNode * a , const ListNode * b ) const { return a -> val > b -> val ; } } class Solution { public : ListNode * mergeKLists ( vector < ListNode *>& lists ) { priority_queue < ListNode * , vector < ListNode *> , listComparison > q ; for ( auto list : lists ) { if ( list ) { q . push ( list ); } } ListNode dummy ( INT_MIN ); ListNode * curr = & dummy ; while ( ! q . empty ()) { curr -> next = q . top (); q . pop (); curr = curr -> next ; if ( curr -> next ) { q . push ( curr -> next ); } } return dummy . next ; } } Solution 2 Divide and conquer merge Use the divide and conquer idea to reduce the problem to a smaller problem and then solve the smallest problem, combined together with the solution of small problems, we will arrived at the final solution. Compare the following solution to merge sort algorithm. The complexity is O(N \\log k) O(N \\log k) . Because merge two list will take O(N) O(N) , and we will do O(\\log k) O(\\log k) time of merging. class Solution { public : ListNode * mergeKLists ( vector < ListNode *>& lists ) { return mergeKListsHelper ( lists , 0 , lists . size () - 1 ); } ListNode * mergeKListsHelper ( vector < ListNode *>& lists , int s , int e ) { if ( s > e ) return nullptr ; if ( s == e ) return lists [ s ]; if ( s < e ) { int m = s + ( e - s ) / 2 ; ListNode * l1 = mergeKListsHelper ( lists , s , m ); ListNode * l2 = mergeKListsHelper ( lists , m + 1 , e ); return merge ( l1 , l2 ); } } ListNode * merge ( ListNode * l1 , ListNode * l2 ) { ListNode dummy ( INT_MIN ); ListNode * curr = & dummy ; while ( l1 && l2 ) { if ( l1 -> val < l2 -> val ) { curr -> next = l1 ; l1 = l1 -> next ; } else { curr -> next = l2 ; l2 = l2 -> next ; } curr = curr -> next ; } curr -> next = l1 ? l1 : l2 ; return dummy . next ; } };","title":"Merge k Sorted Lists"},{"location":"leetcode/heap/notes/#minimum-unique-word-abbreviation","text":"","title":"Minimum Unique Word Abbreviation"},{"location":"leetcode/heap/notes/#smallest-range-covering-elements-from-k-lists","text":"Solution 1 Priority queue Notice this solution pushed some meta data to the priority queue. class Solution { public : struct mycompare { bool operator () ( pair < int , int >& a , pair < int , int >& b ) { return a . first > b . first ; } }; vector < int > smallestRange ( vector < vector < int >>& nums ) { int n = nums . size (); priority_queue < pair < int , int > , vector < pair < int , int >> , mycompare > pq ; int end = INT_MIN ; for ( int i = 0 ; i < n ; i ++ ) { end = max ( end , nums [ i ][ 0 ]); // push the first element of kth vector and k to the pq pq . push ({ nums [ i ][ 0 ], i }); } vector < int > idx ( n , 0 ); // keep tracking the index to the k arrays. vector < int > res = { -100000 , 100000 }; // the range is given. while ( pq . size () == n ) { int start = pq . top (). first ; int k = pq . top (). second ; pq . pop (); if ( end - start < res [ 1 ] - res [ 0 ]) { res [ 0 ] = start ; res [ 1 ] = end ; } // if nums[k] have more element, we push next to the queue. if ( ++ idx [ k ] < nums [ k ]. size ()) { pq . push ({ nums [ k ][ idx [ k ]], k }); end = max ( end , nums [ k ][ idx [ k ]]); } } return res ; } };","title":"Smallest Range Covering Elements from K Lists"},{"location":"leetcode/heap/notes/#kth-largest-element-in-an-array","text":"Solution 1 priority queue Build a heap takes O(n) . max-heap O(n) + O(klogk) . class Solution { public : int findKthLargest ( vector < int >& nums , int k ) { priority_queue < int > pq ( nums . begin (), nums . end ()); k -- ; while ( k -- > 0 ) { pq . pop (); } return pq . top (); } }; Solution 2 priority queue min-heap O(n*logk) . class Solution { public : int findKthLargest ( vector < int >& nums , int k ) { priority_queue < int , vector < int > , greater < int >> pq ; for ( int n : nums ) { pq . push ( n ); if ( pq . size () > k ) { pq . pop (); } } return pq . top (); } }; Solution 3 quickselect algorithm class Solution { public : int findKthLargest ( vector < int >& nums , int k ) { int n = nums . size (); if ( n < k ) return 0 ; return orderStats ( nums , 0 , n - 1 , k ); } int orderStats ( vector < int >& nums , int start , int end , int k ) { if ( start == end ) return nums [ start ]; int p = partition ( nums , start , end ); int order = p - start + 1 ; if ( order == k ) { return nums [ p ]; } else if ( order > k ) { return orderStats ( nums , start , p - 1 , k ); } else { return orderStats ( nums , p + 1 , end , k - order ); } } int partition ( vector < int >& nums , int start , int end ) { if ( start == end ) return 0 ; int p = start + floor ( rand () % ( end - start + 1 )); swap ( nums [ p ], nums [ end ]); int pivot = nums [ end ]; int i = start - 1 , j ; for ( j = start ; j < end ; ++ j ) { if ( nums [ j ] >= pivot ) { swap ( nums [ ++ i ], nums [ j ]); } } swap ( nums [ end ], nums [ i + 1 ]); return i + 1 ; } };","title":"Kth Largest Element in an Array"},{"location":"leetcode/heap/notes/#the-skyline-problem","text":"Solution 1 priority queue maximum heap class Solution { public : vector < pair < int , int >> getSkyline ( vector < vector < int >>& buildings ) { vector < pair < int , int >> res ; int i = 0 , x = 0 , h = 0 , len = buildings . size (); priority_queue < pair < int , int >> q ; // max heap store <height, end> while ( i < len || ! q . empty ()) { // building i start before the end of current tallest building if ( q . empty () || i < len && buildings [ i ][ 0 ] <= q . top (). second ) { x = buildings [ i ][ 0 ]; // current start // overlapped start of multiple buildings (compare to if) while ( i < len && buildings [ i ][ 0 ] == x ) { q . push ({ buildings [ i ][ 2 ], buildings [ i ][ 1 ]}); i ++ ; } } else { // building i start after the end of current tallest building x = q . top (). second ; // current end // pop all buildings that end <= currently tallest buildings while ( ! q . empty () && q . top (). second <= x ) q . pop (); } h = q . empty () ? 0 : q . top (). first ; if ( res . empty () || res . back (). second != h ) { res . push_back ({ x , h }); } } return res ; } }; Solution 2 multiset + scanline solution class Solution { private : static bool cmp ( pair < int , int > p1 , pair < int , int > p2 ){ if ( p1 . first != p2 . first ) return p1 . first < p2 . first ; return p1 . second > p2 . second ; // if x duplicate, tallest building first } public : vector < pair < int , int >> getSkyline ( vector < vector < int >>& buildings ) { vector < pair < int , int >> h ; // store the start and end. vector < pair < int , int >> res ; multiset < int , greater < int >> s ; int prev = 0 , curr = 0 ; // scanline technique to obtain the starting point and end point for ( auto & a : buildings ) { h . push_back ({ a [ 0 ], a [ 2 ]}); h . push_back ({ a [ 1 ], - a [ 2 ]}); // use \"-\" mark the end point } sort ( h . begin (), h . end (), cmp ); s . insert ( 0 ); // edge case, consider a.second == 0 for ( auto & a : h ) { if ( a . second > 0 ) s . insert ( a . second ); // if it is a start, insert to the set else s . erase ( s . find ( - a . second )); // if it is a end, erase the inserted start curr = * s . begin (); // take the maximum from the set if ( curr != prev ) { // remove duplicate res . push_back ({ a . first , curr }); prev = curr ; } } return res ; } };","title":"The Skyline Problem"},{"location":"leetcode/heap/notes/#sliding-window-median","text":"Solution 1 heap + hash (simulated) class Solution { public : vector < double > medianSlidingWindow ( vector < int >& nums , int k ) { int n = nums . size (); vector < double > res ; priority_queue < int > lo ; priority_queue < int , vector < int > , greater < int >> hi ; unordered_map < int , int > hash_heap ; int i = 0 ; while ( i < k ) lo . push ( nums [ i ++ ]); for ( int j = 0 ; j < k / 2 ; j ++ ) { hi . push ( lo . top ()); lo . pop (); } while ( true ) { res . push_back ( k & 1 ? lo . top () : (( double ) lo . top () + ( double ) hi . top ()) * 0.5 ); if ( i >= nums . size ()) break ; int out_num = nums [ i - k ]; // first element in the window int in_num = nums [ i ++ ]; // new element entering window int balance = 0 ; // -1, when out_num in lo, +1 when out_num in hi. // balance is the count of element diff in lo and hi. // lo.size() == hi.size() -> balance = 0 // lo.size() > hi.size() --> balance > 0 // lo.size() < hi.size() --> balance < 0 balance += ( out_num <= lo . top () ? -1 : 1 ); hash_heap [ out_num ] ++ ; // count of the invalid element, will remove when it on the top. if ( ! lo . empty () && in_num <= lo . top ()) { balance ++ ; lo . push ( in_num ); } else { balance -- ; hi . push ( in_num ); } if ( balance < 0 ) { // lo lack of element. lo . push ( hi . top ()); hi . pop (); balance ++ ; } else if ( balance > 0 ) { // hi lack of element hi . push ( lo . top ()); lo . pop (); balance -- ; } // lazy remove of elements previous removed, mimic hashheap of java. while ( ! lo . empty () && hash_heap [ lo . top ()]) { hash_heap [ lo . top ()] -- ; lo . pop (); } // lazy remove of elements previous removed, mimic hashheap of java. while ( ! hi . empty () && hash_heap [ hi . top ()]) { hash_heap [ hi . top ()] -- ; hi . pop (); } } return res ; } };","title":"Sliding Window Median"},{"location":"leetcode/heap/notes/#373-find-k-pairs-with-smallest-sums","text":"use the following visual add when writing your code. The best solution choose the smallest K pairs layer by layer from top left to bottom right. From the visualization, you can see that this problem is equivalent to the problem Kth Smallest Element in a Sorted Matrix 2 4 6 +------------ 1 | 3 5 7 7 | 9 11 13 11 | 13 15 17 Python heapq and generator def kSmallestPairs ( self , nums1 , nums2 , k ): streams = map ( lambda u : ([ u + v , u , v ] for v in nums2 ), nums1 ) stream = heapq . merge ( * streams ) # streams is iterables return [ suv [ 1 :] for suv in itertools . islice ( stream , k )] Java O(KlogK) https://leetcode.com/problems/find-k-pairs-with-smallest-sums/discuss/84551/simple-Java-O(KlogK)-solution-with-explanation C++ Priority Queue class cmp { public : bool operator () ( const pair < int , int >& a , const pair < int , int >& b ) { return a . first + a . second < b . first + b . second ; } }; class Solution { public : vector < pair < int , int >> kSmallestPairs ( vector < int >& nums1 , vector < int >& nums2 , int k ) { vector < pair < int , int >> res ; priority_queue < pair < int , int > , vector < pair < int , int >> , cmp > pq ; for ( int i = 0 ; i < min (( int ) nums1 . size (), k ); ++ i ) { for ( int j = 0 ; j < min (( int ) nums2 . size (), k ); ++ j ) { pq . push ({ nums1 [ i ], nums2 [ j ]}); if ( pq . size () > k ) { pq . pop (); } } } while ( ! pq . empty ()) { res . push_back ( pq . top ()); pq . pop (); } return res ; } }; C++ priority queue optimized (scheduler model) class Solution { public : vector < pair < int , int >> kSmallestPairs ( vector < int >& nums1 , vector < int >& nums2 , int k ) { // Base cases if ( nums1 . size () == 0 || nums2 . size () == 0 || k == 0 ) { return { }; } // Result pairs vector < pair < int , int >> result ; // Prioritized scheduling based on sum (consider it as edge weight in graph) // Similar to Prim's algorithm // Min sum pq storing pair indices of nums1 index and nums2 index auto pqCmp = [ & nums1 , & nums2 ]( const pair < int , int >& p1 , const pair < int , int >& p2 ) { return nums1 [ p1 . first ] + nums2 [ p1 . second ] > nums1 [ p2 . first ] + nums2 [ p2 . second ]; }; priority_queue < pair < int , int > , vector < pair < int , int >> , decltype ( pqCmp ) > pq ( pqCmp ); // Visited is required to avoid duplicate scheduling vector < vector < bool >> visited ( nums1 . size (), vector < bool > ( nums2 . size (), false )); // Push 0, 0 as the seed for scheduling pq . push ( make_pair ( 0 , 0 )); // Find k smallest sum pairs while ( k && ! pq . empty ()) { // Current min sum index pair is at pq top auto top = pq . top (); pq . pop (); result . push_back ( make_pair ( nums1 [ top . first ], nums2 [ top . second ])); // Advance num1 index and schedule ++ top . first ; if ( top . first < nums1 . size () && ! visited [ top . first ][ top . second ]) { pq . push ( top ); visited [ top . first ][ top . second ] = true ; } -- top . first ; // Advance num2 index and schedule ++ top . second ; if ( top . second < nums2 . size () && ! visited [ top . first ][ top . second ]) { pq . push ( top ); visited [ top . first ][ top . second ] = true ; } -- top . second ; -- k ; } return result ; } };","title":"373. Find K Pairs with Smallest Sums"},{"location":"leetcode/heap/notes/#use-priority-queue-to-rearrange-tasks-characters-string-etc","text":"","title":"Use priority queue to rearrange tasks (characters, string, etc.)"},{"location":"leetcode/heap/notes/#rearrange-string-k-distance-apart","text":"Solution 1 Greedy + Priority Queue class Solution { public : string rearrangeString ( string s , int k ) { if ( k <= 0 ) return s ; unordered_map < char , int > mp ; for ( auto & c : s ) mp [ c ] ++ ; priority_queue < pair < int , char > , vector < pair < int , char >> , std :: less < pair < int , char >>> pq ; // max heap; for ( auto & p : mp ) { pq . push ({ p . second , p . first }); } string res ; while ( ! pq . empty ()) { vector < pair < int , char >> used ; int i = 0 ; while ( i < k && ! pq . empty ()) { auto t = pq . top (); pq . pop (); res . push_back ( t . second ); i ++ ; if ( -- t . first > 0 ) { used . push_back ( t ); } } if ( i != k && used . size ()) return \"\" ; for ( auto & p : used ) { pq . push ( p ); } } return res ; } };","title":"Rearrange String k Distance Apart"},{"location":"leetcode/heap/notes/#task-scheduler","text":"Solution 1 Greedy + Priority Queue class Solution { public : class cmp { public : cmp () {} bool operator ()( const pair < int , char >& a , const pair < int , char >& b ){ return a . first < b . first ; } }; int leastInterval ( vector < char >& tasks , int n ) { int chmap [ 26 ] = { 0 }; priority_queue < pair < int , char > , vector < pair < int , char >> , cmp > pq ; for ( char c : tasks ) { chmap [ c - 'A' ] ++ ; } for ( int i = 0 ; i < 26 ; ++ i ) { if ( chmap [ i ]) { pq . push ({ chmap [ i ], i + 'A' }); } } int cycle = n + 1 ; int res = 0 ; while ( ! pq . empty ()) { vector < pair < int , char >> used ; for ( int i = 0 ; i < cycle ; ++ i ) { // pick total of n + 1 in freq order if ( ! pq . empty ()) { used . push_back ( pq . top ()); pq . pop (); } } for ( auto p : used ) { if ( -- p . first ) { pq . push ( p ); // put back used chars if extra char exist after the useage. } } // handled partial cycle and full cycle res += ! pq . empty () ? cycle : used . size (); } return res ; } };","title":"Task Scheduler"},{"location":"leetcode/heap/notes/#reorganize-string","text":"Solution 1 Greedy + Priority Queue class Solution { public : class cmp { public : cmp () {} bool operator ()( const pair < int , char >& a , const pair < int , char >& b ){ return a . first < b . first ; } }; string reorganizeString ( string S ) { int n = S . length (); int chmap [ 26 ] = { 0 }; // max heap priority_queue < pair < int , char > , vector < pair < int , char >> , cmp > pq ; for ( auto & c : S ) { //store to map chmap [ c - 'a' ] ++ ; } // push to priority queue for ( int i = 0 ; i < 26 ; ++ i ) { if ( chmap [ i ]) { pq . push ({ chmap [ i ], i + 'a' }); } } if ( pq . top (). first > ( n + 1 ) / 2 ) return \"\" ; string res = \"\" ; while ( ! pq . empty ()) { pair < int , char > p = pq . top (); pq . pop (); if ( res . empty () || res . back () != p . second ) { res . push_back ( p . second ); if ( -- p . first > 0 ) { // use one char pq . push ( p ); } } else { // cannot use duplicate char, take another pair < int , char > q = pq . top (); pq . pop (); res . push_back ( q . second ); if ( -- q . first > 0 ) { // use another char pq . push ( q ); } // remember put p back to the pq pq . push ( p ); } } return res ; } };","title":"Reorganize String"},{"location":"leetcode/interval/notes/","text":"Interval problems \u00b6 Interval problem solving techniques \u00b6 sweep line technique iterval tree using a STL map binary search to locate an interval in the list Special cases in disjoint interval set problems \u00b6 Special case that the interval may have duplicates. i.e. Problem Number of Airplanes in the Sky Notice the search result in ::begin() and ::end() in interval query. Data structure of disjoint interval set \u00b6 Two containers vector and map can store disjoint intervals. Disjoint interval operations \u00b6 Pay attention to the interval representation, whether it is open end or close end. The code is slitly different. Range Module is a design problem asking you to implement the following all three operations. insertInterval (addInterval, mergeInterval) \u00b6 ```C++ tab=\"C++ vector\" void addRange(int left, int right) { vector > tmp; int pos = 0; for (int i = 0; i < intervals.size(); i++) { if (left > intervals[i].second) { tmp.push_back(intervals[i]); pos++; } else if (right < intervals[i].first) { tmp.push_back(intervals[i]); } else { left = min(intervals[i].first, left); right = max(intervals[i].second, right); } } tmp.insert(tmp.begin() + pos, {left, right}); swap(intervals, tmp); } C++ tab=\"C++ map\" void addRange(int left, int right) { auto l = m.upper_bound(left); auto r = m.upper_bound(right); // rule out the case that l overlap with previous if (l != m.begin()) { if((--l)->second < left) { ++l; } } // sure left is non-overlap, check the right if (l != r) { int lmin = min(l->first, left); int rmax = max(right, (--r)->second); m.erase(l, ++r); // remove iterator m[lmin] = rmax; } else { // both no overlap m[left] = right; } } ``` queryInterval \u00b6 Since we can use a sorted vector list or a map to store the interval. We could have two different implementation. The key to do the query is how to search the position, we use upper_bound . ```C++ tab=\"C++ vector\" bool queryRange(int left, int right) { int n = invals.size(), l = 0, r = n; while (l < r) { int m = l + (r - l) / 2; if (invals[m].first <= left) { l = m + 1; } else { r = m; } } if (l == 0 || invals[--l].second < right) { return false; } return true; } C++ tab=\"C++ map\" bool queryRange(int left, int right) { auto l = m.upper_bound(left); if (l == m.begin() || (--l)->second < right) { return false; } return true; } ``` deleteInterval \u00b6 ```C++ tab=\"C++ vector\" void removeRange(int left, int right) { int n = invals.size(); vector > tmp; for (int i = 0; i < n; i++) { if (invals[i].second <= left || invals[i].first >= right) tmp.push_back(invals[i]); else { if (invals[i].first < left) tmp.push_back({invals[i].first, left}); if (invals[i].second > right) tmp.push_back({right, invals[i].second}); } } swap(invals, tmp); } C++ tab=\"C++ map\" void removeRange(int left, int right) { auto l = m.upper_bound(left); auto r = m.upper_bound(right); if (l != m.begin()) { --l; if(l->second < left) { ++l; } } // nothing need to be removed if (l == r) { return; } int l1 = min(left, l->first); int r1 = max(right, (--r)->second); m.erase(l, ++r); if (l1 != left) { m[l1] = left; } if (r1 != right) { m[right] = r1; } } ``` Problems \u00b6 56.Merge intervals \u00b6 Merge interval need first sort the interval then put the first interval to the result vector, iteratively compare the end of the vector to interval[i] , either update the end of the res.back().end or directly push the interval[i] to the end of the vector. Note You cannot use index to refer to the neighboring elements. because the vector is being modified. ```C++ tab=\"\" class Solution { public: vector merge(vector & intervals) { if (intervals.empty()) return vector (); sort(intervals.begin(), intervals.end(), { return a.start < b.start; }); vector<Interval> res; res.push_back(intervals[0]); for (int i = 1; i < intervals.size(); ++i) { if (res.back().end >= intervals[i].start) { res.back().end = max(res.back().end, intervals[i].end); } else { res.push_back(intervals[i]); } } return res; } }; ``` 57. Insert Interval \u00b6 Given the new interval [a, b] , consider each interval in the input [x, y] . Completely disjoint to the left (b < x) . Completely disjoint to the right (y < a) . Overlap with each other. (uniformly handled by min , max ) The key to write code is Tracking the inserting position. Insert the new interval to the vector. Finally check the special cases: insert at the begin, insert at the end. C++ tab=\"One pass I\" class Solution { public: vector<Interval> insert(vector<Interval>& intervals, Interval newInterval) { int n = intervals.size(); vector<Interval> res; int insertPos = 0; for (int i = 0; i < n; ++i) { if (newInterval.start > intervals[i].end) { // start no overlap res.push_back(intervals[i]); insertPos++; } else if (newInterval.end < intervals[i].start) { // end no overlap. res.push_back(intervals[i]); } else { // overlap happens. newInterval.start = min(newInterval.start, intervals[i].start); newInterval.end = max(newInterval.end, intervals[i].end); } } res.insert(res.begin() + insertPos, newInterval); return res; } }; ```C++ tab=\"One pass II\" class Solution { public: vector insert(vector & intervals, Interval newInterval) { int n = intervals.size(); vector res; for (int i = 0; i <= n; ++i) { if (i == n || newInterval.end < intervals[i].start) { // end no overlap. res.push_back(newInterval); while (i < n) res.push_back(intervals[i++]); // i will be after n } else if (newInterval.start > intervals[i].end) { // start no overlap res.push_back(intervals[i]); } else { // overlap happens. newInterval.start = min(newInterval.start, intervals[i].start); newInterval.end = max(newInterval.end, intervals[i].end); } } return res; } }; ``` Number of Airplanes in the Sky \u00b6 We can use a map to record each coordinate, add all coordinates up for each interval C++ class Solution { public: int countOfAirplanes(vector<Interval> &airplanes) { unordered_map<int, int> mp; int res = 0; for (auto a : airplanes) { for (int i = a.start; i < a.end; ++i) { mp[i]++; res = max(res, mp[i]); } } return res; } }; Solution 2 Sweep line Sweep line technique usually seperate the intervals into two vectors. In this problem we seperate them and mark each point as start or end. Notice you cannot use container like set or map because it may have duplicate intervals. You need to use vectors and then sort them. class Solution { public : int countOfAirplanes ( vector < Interval > & airplanes ) { vector < pair < int , int >> points ; for ( auto & a : airplanes ) { points . push_back ({ a . start , 1 }); points . push_back ({ a . end , -1 }); } sort ( points . begin (), points . end ()); int res = 0 , count = 0 ; for ( auto & p : points ) { count += p . second ; res = max ( res , count ); } return res ; } }; 435. Non-overlapping Intervals \u00b6 Solution 1 Greedy algorithm ```C++ tab=\"C++, sort by start\" class Solution { public: int eraseOverlapIntervals(vector >& intervals) { int n = intervals.size(); int res = 0; sort(intervals.begin(), intervals.end(), [](const vector<int>& a, const vector<int>& b){ return a[0] < b[0]; }); int left = 0; for (int i = 1; i < n; i++) { if (intervals[left][1] > intervals[i][0]) { if (intervals[left][1] > intervals[i][1]) { left = i; } res++; } else { left = i; } } return res; } }; C++ tab=\"C++, sort by end\" class Solution { public: int eraseOverlapIntervals(vector >& intervals) { int n = intervals.size(); int res = 0; sort(intervals.begin(), intervals.end(), { return a[1] < b[1]; }); int left = 0; int end = intervals[0][1]; for (int i = 1; i < n; i++) { if (interval[i][0] < end) { res++; } else { end = intervals[i][1]; // no need to remove, update the new end } } return res } } ``` 452. Minimum Number of Arrows to Burst Balloons \u00b6 Solution 1 Sweep line, using map. 252. Meeting Rooms \u00b6 253. Meeting Rooms II \u00b6 Solution 1 Sweep line, split the start and end, put them into a vector. same as the solution to the Number of Airplanes in the Sky Notice the when a start and an end overlap, the sort will sort base on the pair::second. This make the dup end value goes before the start value so that we will not got over count the rooms. ```C++ tab= /* special case: 2 5 <---- this 5 come first than the start 5 5 9 / class Solution { public: int minMeetingRooms(vector & intervals) { vector > points; for (auto& i : intervals) { points.push_back(make_pair(i.start, 1)); points.push_back(make_pair(i.end, -1)); // this -1 < 1 is important } sort(points.begin(), points.end()); int rooms = 0, count = 0; for (auto& p : points) { count += p.second; rooms = max(rooms, count); } return rooms; } }; C++ tab=\"lamda sorting\" class Solution { public: int minMeetingRooms(vector >& intervals) { int n = intervals.size(); if (n == 0) return 0; vector > points; for (auto &interval: intervals) { points.push_back({interval[0], 1}); points.push_back({interval[1], -1}); } sort(points.begin(), points.end(), { if (a[0] == b[0]) { return a[1] < b[1]; } else { return a[0] < b[0]; } }); int res = 0; int rooms = 0; for (auto &p : points) { rooms += p[1]; res = max(res, rooms); } return res; } }; ``` Solution 2 ```C++ tab= class Solution { public: int minMeetingRooms(vector & intervals) { int n = intervals.size(); vector s(n, 0); vector e(n, 0); for (int i = 0; i < n; i++) { s[i] = intervals[i].start; e[i] = intervals[i].end; } sort(s.begin(), s.end()); sort(e.begin(), e.end()); int rooms = 0; int j = 0; for (int i = 0; i < n; i++) { if (s[i] < e[j]) { /* meeting i start before the lastest end meeting j */ rooms++; } else { /* s[i] >= e[j] indicate new meeting i will start after meeting j end, reuse the room */ j++; /* increament to the next end */ } } return rooms; } }; ``` 729. My Calendar I \u00b6 Solution 1 Check intervals (vector) overlapping 1. Consider the 3 different overlapping cases. Can be summarized using max(start1, start2) < min(end1, end2) . C++ tab= class MyCalendar { vector<pair<int, int>> calendar; public: MyCalendar() { } bool book(int start, int end) { for (auto book : calendar) { if (max(book.first, start) < min(book.second, end)) return false; calendar.push_back({start, end}); } return true; } }; Solution 2 Using map to optimize to O(logn) ```C++ tab= class MyCalendar { map calendar; public: MyCalendar() { } bool book(int start, int end) { auto it = calendar.lower_bound(start); if (it != calendar.end() && it->first < end) return false; if (it != calendar.begin() && prev(it)->second > start) return false; calendar[start] = end; return true; } }; ``` 731. My Calendar II \u00b6 Solution 1 reuse object and keep a copy of the intervals in it * Notice we can reuse the class MyCalendar from the problem My Calendar I. * Each interval booked also keeped in another MyCalendar objects. For each new booking, we first see whether the new booking overlaps with existing one, if so we get the overlap as a new interval and try to book the MyCalendar project which keep the same intervals. C++ tab= class MyCalendar { vector<pair<int, int>> calendar; public: MyCalendar() { } bool book(int start, int end) { for (auto book : calendar) { if (max(book.first, start) < min(book.second, end)) return false; } calendar.push_back({start, end}); return true; } }; class MyCalendarTwo { vector<pair<int, int>> calendar; public: MyCalendarTwo() { } bool book(int start, int end) { MyCalendar overlaps; for (auto book : calendar) { if (max(book.first, start) < min(book.second, end)) { pair<int, int> overlap = getOverlap(book.first, book.second, start, end); if (!overlaps.book(overlap.first, overlap.second)) return false; } } calendar.push_back({start, end}); return true; } pair<int, int> getOverlap(int a0, int a1, int b0, int b1) { return {max(a0, b0), min(a1, b1)}; } }; Solution 2 Sweep line like Number of Air Plans in the Sky Notice this in this problem we can use map or set because they will not affect the count. If there is duplicates intervals, the map::second will increase accordingly. Compare this to the Number of Air Plans in the Sky problem to see why that one cannot use map or set . Notice the schedule in this calendar object is lost, we can only use it to check whether there are triple booking. class MyCalendarTwo { map < int , int > calendar ; public : MyCalendarTwo () { } bool book ( int start , int end ) { calendar [ start ] ++ ; calendar [ end ] -- ; int count = 0 ; for ( auto & c : calendar ) { count += c . second ; if ( count == 3 ) { calendar [ start ] -- ; calendar [ end ] ++ ; return false ; } } return true ; } }; 732. My Calendar III \u00b6 Solution 1 Sweep line solution Remember to consider whether duplicates schedule exists or not and how it affect the solution. class MyCalendarThree { map < int , int > calendar ; public : MyCalendarThree () { } int book ( int start , int end ) { calendar [ start ] ++ ; calendar [ end ] -- ; int k = 0 , count = 0 ; for ( const auto & m : calendar ) { k = max ( k , count += m . second ); } return k ; } }; Solution 2 Fancy iterator We use the map::emplace return a pair of <iterator, bool> to indicate it inserted successfully or not class MyCalendarThree { map < int , int > calendar ; int k ; public : MyCalendarThree () : k ( 0 ) { } map < int , int >:: iterator insert ( int t ) { pair < map < int , int >:: iterator , bool > iter_bool = calendar . emplace ( t , 0 ); if ( iter_bool . second && iter_bool . first != calendar . begin ()) iter_bool . first -> second = prev ( iter_bool . first ) -> second ; return iter_bool . first ; } int book ( int start , int end ) { for ( auto i = insert ( start ), j = insert ( end ); i != j ; ++ i ) { k = max ( k , ++ ( i -> second )); } return k ; } }; 715. Range Module \u00b6 Solution 1 Using vector to stroe the disjoint intervals class RangeModule { public : RangeModule () { } void addRange ( int left , int right ) { vector < pair < int , int >> tmp ; int pos = 0 ; for ( int i = 0 ; i < intervals . size (); i ++ ) { if ( left > intervals [ i ]. second ) { tmp . push_back ( intervals [ i ]); pos ++ ; } else if ( right < intervals [ i ]. first ) { tmp . push_back ( intervals [ i ]); } else { left = min ( intervals [ i ]. first , left ); right = max ( intervals [ i ]. second , right ); } } tmp . insert ( tmp . begin () + pos , { left , right }); swap ( intervals , tmp ); } bool queryRange ( int left , int right ) { int n = intervals . size (); int l = 0 ; int r = n ; while ( l < r ) { int m = l + ( r - l ) / 2 ; if ( intervals [ m ]. first <= left ) { l = m + 1 ; } else { r = m ; } } if ( l == 0 || intervals [ -- l ]. second < right ) { return false ; } return true ; } void removeRange ( int left , int right ) { int n = intervals . size (); vector < pair < int , int >> tmp ; for ( int i = 0 ; i < n ; i ++ ) { if ( intervals [ i ]. second <= left || intervals [ i ]. first >= right ) { tmp . push_back ( intervals [ i ]); } else { if ( intervals [ i ]. first < left ) { tmp . push_back ({ intervals [ i ]. first , left }); } if ( intervals [ i ]. second > right ) { tmp . push_back ({ right , intervals [ i ]. second }); } } } swap ( intervals , tmp ); } private : vector < pair < int , int >> intervals ; }; /** * Your RangeModule object will be instantiated and called as such: * RangeModule* obj = new RangeModule(); * obj->addRange(left,right); * bool param_2 = obj->queryRange(left,right); * obj->removeRange(left,right); */ Solution 2 Using map to stroe the disjoint intervals class RangeModule { public : RangeModule () {} /* 1_3, 5___8, 12___15 */ void addRange ( int left , int right ) { auto l = m . upper_bound ( left ); auto r = m . upper_bound ( right ); // rule out the case that l overlap with previous if ( l != m . begin ()) { if (( -- l ) -> second < left ) { ++ l ; } } // sure left is non-overlap, check the right if ( l != r ) { int lmin = min ( l -> first , left ); int rmax = max ( right , ( -- r ) -> second ); m . erase ( l , ++ r ); // remove iterator m [ lmin ] = rmax ; } else { // both no overlap m [ left ] = right ; } } bool queryRange ( int left , int right ) { auto l = m . upper_bound ( left ); if ( l == m . begin () || ( -- l ) -> second < right ) { return false ; } return true ; } /* l r 1_3, 5__8, 12_______20, 22____25 4_6 4____9 4___________15 <- impossible 2__4 */ void removeRange ( int left , int right ) { auto l = m . upper_bound ( left ); auto r = m . upper_bound ( right ); if ( l != m . begin ()) { -- l ; if ( l -> second < left ) { ++ l ; } } // nothing need to be removed if ( l == r ) { return ; } int l1 = min ( left , l -> first ); int r1 = max ( right , ( -- r ) -> second ); m . erase ( l , ++ r ); if ( l1 != left ) { m [ l1 ] = left ; } if ( r1 != right ) { m [ right ] = r1 ; } } private : map < int , int > m ; }; 436. Find Right Interval \u00b6 Solution 1 using map ```C++ tab= class Solution { public: vector findRightInterval(vector & intervals) { int n = intervals.size(); map m; for (int i = 0; i < n; ++i) { m[intervals[i].start] = i; } vector<int> res; for (auto interval : intervals) { auto idx = m.lower_bound(interval.end); if (idx != m.end()) { res.push_back(idx->second); } else { res.push_back(-1); } } return res; } }; ``` 435. Non-overlapping Intervals \u00b6 Solution 1 Greedy 1. Iterate through the intervals, remove the one with larger end. C++ tab= class Solution { public: int eraseOverlapIntervals(vector<Interval>& intervals) { int n = intervals.size(); int res = 0; sort(intervals.begin(), intervals.end(), [](const Interval& a, const Interval& b){ return a.start < b.start; }); int prev = 0; for (int i = 1; i < n; ++i) { if (intervals[prev].end > intervals[i].start) { if (intervals[prev].end > intervals[i].end) { prev = i; } res++; } else { // end <= start no overlap prev = i; } } return res; } }; Solution 2 class Solution { public : int eraseOverlapIntervals ( vector < Interval >& intervals ) { int n = intervals . size (); if ( n == 0 ) return 0 ; sort ( intervals . begin (), intervals . end (), []( const Interval & a , const Interval & b ){ return a . end < b . end ; }); int res = 0 ; int end = INT_MIN ; for ( int i = 0 ; i < n ; ++ i ) { if ( intervals [ i ]. start < end ) { res ++ ; } else { end = intervals [ i ]. end ; } } return res ; } }; 352. Data Stream as Disjoint Intervals \u00b6 Solution 1 Use vector as the container We take the stream and store them in a vector in increasing order based on the start. Eeach new number will find its insertion position using lower_bound . With the iterator, we look back to see whether the number can merge with an interval whose start is smaller. If so, the iterator walk back on step. class SummaryRanges { vector < Interval > intervals ; public : /** Initialize your data structure here. */ SummaryRanges () { } void addNum ( int val ) { Interval curr ( val , val ); int start = val , end = val ; auto iter = lower_bound ( intervals . begin (), intervals . end (), curr , []( const Interval & a , const Interval & b ){ return a . start < b . start ; }); if ( iter != intervals . begin () && ( iter - 1 ) -> end + 1 >= val ) -- iter ; while ( iter != intervals . end () && iter -> end + 1 >= val && val + 1 >= iter -> start ) { start = min ( start , iter -> start ); end = max ( end , iter -> end ); iter = intervals . erase ( iter ); // invalided previous iterator, return next iter } intervals . insert ( iter , Interval ( start , end )); } vector < Interval > getIntervals () { return intervals ; } }; Solution 2 Use map or set as the container class SummaryRanges { struct cmp { bool operator ()( const Interval & a , const Interval & b ){ return a . start < b . start ; } }; set < Interval , cmp > intervals ; public : /** Initialize your data structure here. */ SummaryRanges () { } void addNum ( int val ) { Interval curr ( val , val ); int start = val , end = val ; auto iter = intervals . lower_bound ( curr ); if ( iter != intervals . begin () && ( -- iter ) -> end + 1 < val ) ++ iter ; while ( iter != intervals . end () && iter -> end + 1 >= val && val + 1 >= iter -> start ) { start = min ( start , iter -> start ); end = max ( end , iter -> end ); iter = intervals . erase ( iter ); // invalided previous iterator, return next iter } intervals . insert ( iter , Interval ( start , end )); } vector < Interval > getIntervals () { vector < Interval > res ; for ( auto & interval : intervals ) res . push_back ( interval ); return res ; } }; Solution 3 Use the insert interval subroutine. 915. Partition Array into Disjoint Intervals \u00b6 163. Missing Ranges \u00b6 Solution 1 This problem show us the importance of defining the loop invariant. If we use The one ahead of the first variable prev to keep the loop invariant, it saved many code to handle the coner cases. Another trick is factor out the logic to generate the final result in to a funciton. ```C++ \"C++ succient solution\" class Solution { public: vector findMissingRanges(vector & nums, int lower, int upper) { int n = nums.size(); vector res; long prev = 0; long curr = 0; prev = (long)lower - 1; for (int i = 0; i < n; i++) { curr = nums[i]; //curr = i == n ? (long)upper + 1 : nums[i]; if (curr - prev > 1) { res.push_back(getRange(prev + 1, curr - 1)); } prev = curr; } /* the case can be included in the above case by changing line 10 and 11 */ if (upper > prev) { res.push_back(getRange(prev + 1, upper)); } return res; } string getRange(int a, int b) { return a == b ? to_string(a) : to_string(a) + \"->\" + to_string(b); } }; C++ tab=\"C++ lengthy solution\" class Solution { public: vector findMissingRanges(vector & nums, int lower, int upper) { vector res; if (nums.size() == 0) { if (lower == upper) res.push_back(to_string(lower)); else res.push_back(to_string(lower) + \"->\" + to_string(upper)); return res; } long s = nums[0]; if (lower < s - 1) { res.push_back(to_string(lower) + \"->\" + to_string(s - 1)); } else if (lower < s) { res.push_back(to_string(lower)); } for (int i = 0; i < nums.size() - 1; i++) { long a = nums[i]; long b = nums[i + 1]; if (a == b - 1) { continue; } else if (a == b - 2) { res.push_back(to_string(a + 1)); } else if (a < b - 2) { res.push_back(to_string(a + 1) + \"->\" + to_string(b - 1)); } } long e = nums[nums.size() - 1]; if (e < upper - 1) { res.push_back(to_string(e + 1) + \"->\" + to_string(upper)); } else if (e < upper) { res.push_back(to_string(upper)); } return res; } }; ``` 228. Summary Ranges \u00b6 Pay attention to the integer overflow when you test whether one number is greater that another. [-2147483648,-2147483647,2147483647] Another thing is that to_string() can also change long and float number to string. ```C++ tab= class Solution { public: vector summaryRanges(vector & nums) { int n = nums.size(); vector res; if (n == 0) { return res; } long start = nums[0]; long prev = nums[0]; for (int i = 1; i < n; i++) { if ((long)nums[i] - prev > 1) { string s = start < prev ? to_string((int)start) + \"->\" + to_string((int)prev) : to_string((int)start); res.push_back(s); start = nums[i]; } prev = nums[i]; } res.push_back(start < prev ? to_string((int)start) + \"->\" + to_string((int)prev) : to_string((int)start)); return res; } }; ``` 938. Range Sum of BST \u00b6 370. Range Addition \u00b6 597. Range Addition II \u00b6","title":"Interval"},{"location":"leetcode/interval/notes/#interval-problems","text":"","title":"Interval problems"},{"location":"leetcode/interval/notes/#interval-problem-solving-techniques","text":"sweep line technique iterval tree using a STL map binary search to locate an interval in the list","title":"Interval problem solving techniques"},{"location":"leetcode/interval/notes/#special-cases-in-disjoint-interval-set-problems","text":"Special case that the interval may have duplicates. i.e. Problem Number of Airplanes in the Sky Notice the search result in ::begin() and ::end() in interval query.","title":"Special cases in disjoint interval set problems"},{"location":"leetcode/interval/notes/#data-structure-of-disjoint-interval-set","text":"Two containers vector and map can store disjoint intervals.","title":"Data structure of disjoint interval set"},{"location":"leetcode/interval/notes/#disjoint-interval-operations","text":"Pay attention to the interval representation, whether it is open end or close end. The code is slitly different. Range Module is a design problem asking you to implement the following all three operations.","title":"Disjoint interval operations"},{"location":"leetcode/interval/notes/#insertinterval-addinterval-mergeinterval","text":"```C++ tab=\"C++ vector\" void addRange(int left, int right) { vector > tmp; int pos = 0; for (int i = 0; i < intervals.size(); i++) { if (left > intervals[i].second) { tmp.push_back(intervals[i]); pos++; } else if (right < intervals[i].first) { tmp.push_back(intervals[i]); } else { left = min(intervals[i].first, left); right = max(intervals[i].second, right); } } tmp.insert(tmp.begin() + pos, {left, right}); swap(intervals, tmp); } C++ tab=\"C++ map\" void addRange(int left, int right) { auto l = m.upper_bound(left); auto r = m.upper_bound(right); // rule out the case that l overlap with previous if (l != m.begin()) { if((--l)->second < left) { ++l; } } // sure left is non-overlap, check the right if (l != r) { int lmin = min(l->first, left); int rmax = max(right, (--r)->second); m.erase(l, ++r); // remove iterator m[lmin] = rmax; } else { // both no overlap m[left] = right; } } ```","title":"insertInterval (addInterval, mergeInterval)"},{"location":"leetcode/interval/notes/#queryinterval","text":"Since we can use a sorted vector list or a map to store the interval. We could have two different implementation. The key to do the query is how to search the position, we use upper_bound . ```C++ tab=\"C++ vector\" bool queryRange(int left, int right) { int n = invals.size(), l = 0, r = n; while (l < r) { int m = l + (r - l) / 2; if (invals[m].first <= left) { l = m + 1; } else { r = m; } } if (l == 0 || invals[--l].second < right) { return false; } return true; } C++ tab=\"C++ map\" bool queryRange(int left, int right) { auto l = m.upper_bound(left); if (l == m.begin() || (--l)->second < right) { return false; } return true; } ```","title":"queryInterval"},{"location":"leetcode/interval/notes/#deleteinterval","text":"```C++ tab=\"C++ vector\" void removeRange(int left, int right) { int n = invals.size(); vector > tmp; for (int i = 0; i < n; i++) { if (invals[i].second <= left || invals[i].first >= right) tmp.push_back(invals[i]); else { if (invals[i].first < left) tmp.push_back({invals[i].first, left}); if (invals[i].second > right) tmp.push_back({right, invals[i].second}); } } swap(invals, tmp); } C++ tab=\"C++ map\" void removeRange(int left, int right) { auto l = m.upper_bound(left); auto r = m.upper_bound(right); if (l != m.begin()) { --l; if(l->second < left) { ++l; } } // nothing need to be removed if (l == r) { return; } int l1 = min(left, l->first); int r1 = max(right, (--r)->second); m.erase(l, ++r); if (l1 != left) { m[l1] = left; } if (r1 != right) { m[right] = r1; } } ```","title":"deleteInterval"},{"location":"leetcode/interval/notes/#problems","text":"","title":"Problems"},{"location":"leetcode/interval/notes/#56merge-intervals","text":"Merge interval need first sort the interval then put the first interval to the result vector, iteratively compare the end of the vector to interval[i] , either update the end of the res.back().end or directly push the interval[i] to the end of the vector. Note You cannot use index to refer to the neighboring elements. because the vector is being modified. ```C++ tab=\"\" class Solution { public: vector merge(vector & intervals) { if (intervals.empty()) return vector (); sort(intervals.begin(), intervals.end(), { return a.start < b.start; }); vector<Interval> res; res.push_back(intervals[0]); for (int i = 1; i < intervals.size(); ++i) { if (res.back().end >= intervals[i].start) { res.back().end = max(res.back().end, intervals[i].end); } else { res.push_back(intervals[i]); } } return res; } }; ```","title":"56.Merge intervals"},{"location":"leetcode/interval/notes/#57-insert-interval","text":"Given the new interval [a, b] , consider each interval in the input [x, y] . Completely disjoint to the left (b < x) . Completely disjoint to the right (y < a) . Overlap with each other. (uniformly handled by min , max ) The key to write code is Tracking the inserting position. Insert the new interval to the vector. Finally check the special cases: insert at the begin, insert at the end. C++ tab=\"One pass I\" class Solution { public: vector<Interval> insert(vector<Interval>& intervals, Interval newInterval) { int n = intervals.size(); vector<Interval> res; int insertPos = 0; for (int i = 0; i < n; ++i) { if (newInterval.start > intervals[i].end) { // start no overlap res.push_back(intervals[i]); insertPos++; } else if (newInterval.end < intervals[i].start) { // end no overlap. res.push_back(intervals[i]); } else { // overlap happens. newInterval.start = min(newInterval.start, intervals[i].start); newInterval.end = max(newInterval.end, intervals[i].end); } } res.insert(res.begin() + insertPos, newInterval); return res; } }; ```C++ tab=\"One pass II\" class Solution { public: vector insert(vector & intervals, Interval newInterval) { int n = intervals.size(); vector res; for (int i = 0; i <= n; ++i) { if (i == n || newInterval.end < intervals[i].start) { // end no overlap. res.push_back(newInterval); while (i < n) res.push_back(intervals[i++]); // i will be after n } else if (newInterval.start > intervals[i].end) { // start no overlap res.push_back(intervals[i]); } else { // overlap happens. newInterval.start = min(newInterval.start, intervals[i].start); newInterval.end = max(newInterval.end, intervals[i].end); } } return res; } }; ```","title":"57. Insert Interval"},{"location":"leetcode/interval/notes/#number-of-airplanes-in-the-sky","text":"We can use a map to record each coordinate, add all coordinates up for each interval C++ class Solution { public: int countOfAirplanes(vector<Interval> &airplanes) { unordered_map<int, int> mp; int res = 0; for (auto a : airplanes) { for (int i = a.start; i < a.end; ++i) { mp[i]++; res = max(res, mp[i]); } } return res; } }; Solution 2 Sweep line Sweep line technique usually seperate the intervals into two vectors. In this problem we seperate them and mark each point as start or end. Notice you cannot use container like set or map because it may have duplicate intervals. You need to use vectors and then sort them. class Solution { public : int countOfAirplanes ( vector < Interval > & airplanes ) { vector < pair < int , int >> points ; for ( auto & a : airplanes ) { points . push_back ({ a . start , 1 }); points . push_back ({ a . end , -1 }); } sort ( points . begin (), points . end ()); int res = 0 , count = 0 ; for ( auto & p : points ) { count += p . second ; res = max ( res , count ); } return res ; } };","title":"Number of Airplanes in the Sky"},{"location":"leetcode/interval/notes/#435-non-overlapping-intervals","text":"Solution 1 Greedy algorithm ```C++ tab=\"C++, sort by start\" class Solution { public: int eraseOverlapIntervals(vector >& intervals) { int n = intervals.size(); int res = 0; sort(intervals.begin(), intervals.end(), [](const vector<int>& a, const vector<int>& b){ return a[0] < b[0]; }); int left = 0; for (int i = 1; i < n; i++) { if (intervals[left][1] > intervals[i][0]) { if (intervals[left][1] > intervals[i][1]) { left = i; } res++; } else { left = i; } } return res; } }; C++ tab=\"C++, sort by end\" class Solution { public: int eraseOverlapIntervals(vector >& intervals) { int n = intervals.size(); int res = 0; sort(intervals.begin(), intervals.end(), { return a[1] < b[1]; }); int left = 0; int end = intervals[0][1]; for (int i = 1; i < n; i++) { if (interval[i][0] < end) { res++; } else { end = intervals[i][1]; // no need to remove, update the new end } } return res } } ```","title":"435. Non-overlapping Intervals"},{"location":"leetcode/interval/notes/#452-minimum-number-of-arrows-to-burst-balloons","text":"Solution 1 Sweep line, using map.","title":"452. Minimum Number of Arrows to Burst Balloons"},{"location":"leetcode/interval/notes/#252-meeting-rooms","text":"","title":"252. Meeting Rooms"},{"location":"leetcode/interval/notes/#253-meeting-rooms-ii","text":"Solution 1 Sweep line, split the start and end, put them into a vector. same as the solution to the Number of Airplanes in the Sky Notice the when a start and an end overlap, the sort will sort base on the pair::second. This make the dup end value goes before the start value so that we will not got over count the rooms. ```C++ tab= /* special case: 2 5 <---- this 5 come first than the start 5 5 9 / class Solution { public: int minMeetingRooms(vector & intervals) { vector > points; for (auto& i : intervals) { points.push_back(make_pair(i.start, 1)); points.push_back(make_pair(i.end, -1)); // this -1 < 1 is important } sort(points.begin(), points.end()); int rooms = 0, count = 0; for (auto& p : points) { count += p.second; rooms = max(rooms, count); } return rooms; } }; C++ tab=\"lamda sorting\" class Solution { public: int minMeetingRooms(vector >& intervals) { int n = intervals.size(); if (n == 0) return 0; vector > points; for (auto &interval: intervals) { points.push_back({interval[0], 1}); points.push_back({interval[1], -1}); } sort(points.begin(), points.end(), { if (a[0] == b[0]) { return a[1] < b[1]; } else { return a[0] < b[0]; } }); int res = 0; int rooms = 0; for (auto &p : points) { rooms += p[1]; res = max(res, rooms); } return res; } }; ``` Solution 2 ```C++ tab= class Solution { public: int minMeetingRooms(vector & intervals) { int n = intervals.size(); vector s(n, 0); vector e(n, 0); for (int i = 0; i < n; i++) { s[i] = intervals[i].start; e[i] = intervals[i].end; } sort(s.begin(), s.end()); sort(e.begin(), e.end()); int rooms = 0; int j = 0; for (int i = 0; i < n; i++) { if (s[i] < e[j]) { /* meeting i start before the lastest end meeting j */ rooms++; } else { /* s[i] >= e[j] indicate new meeting i will start after meeting j end, reuse the room */ j++; /* increament to the next end */ } } return rooms; } }; ```","title":"253. Meeting Rooms II"},{"location":"leetcode/interval/notes/#729-my-calendar-i","text":"Solution 1 Check intervals (vector) overlapping 1. Consider the 3 different overlapping cases. Can be summarized using max(start1, start2) < min(end1, end2) . C++ tab= class MyCalendar { vector<pair<int, int>> calendar; public: MyCalendar() { } bool book(int start, int end) { for (auto book : calendar) { if (max(book.first, start) < min(book.second, end)) return false; calendar.push_back({start, end}); } return true; } }; Solution 2 Using map to optimize to O(logn) ```C++ tab= class MyCalendar { map calendar; public: MyCalendar() { } bool book(int start, int end) { auto it = calendar.lower_bound(start); if (it != calendar.end() && it->first < end) return false; if (it != calendar.begin() && prev(it)->second > start) return false; calendar[start] = end; return true; } }; ```","title":"729. My Calendar I"},{"location":"leetcode/interval/notes/#731-my-calendar-ii","text":"Solution 1 reuse object and keep a copy of the intervals in it * Notice we can reuse the class MyCalendar from the problem My Calendar I. * Each interval booked also keeped in another MyCalendar objects. For each new booking, we first see whether the new booking overlaps with existing one, if so we get the overlap as a new interval and try to book the MyCalendar project which keep the same intervals. C++ tab= class MyCalendar { vector<pair<int, int>> calendar; public: MyCalendar() { } bool book(int start, int end) { for (auto book : calendar) { if (max(book.first, start) < min(book.second, end)) return false; } calendar.push_back({start, end}); return true; } }; class MyCalendarTwo { vector<pair<int, int>> calendar; public: MyCalendarTwo() { } bool book(int start, int end) { MyCalendar overlaps; for (auto book : calendar) { if (max(book.first, start) < min(book.second, end)) { pair<int, int> overlap = getOverlap(book.first, book.second, start, end); if (!overlaps.book(overlap.first, overlap.second)) return false; } } calendar.push_back({start, end}); return true; } pair<int, int> getOverlap(int a0, int a1, int b0, int b1) { return {max(a0, b0), min(a1, b1)}; } }; Solution 2 Sweep line like Number of Air Plans in the Sky Notice this in this problem we can use map or set because they will not affect the count. If there is duplicates intervals, the map::second will increase accordingly. Compare this to the Number of Air Plans in the Sky problem to see why that one cannot use map or set . Notice the schedule in this calendar object is lost, we can only use it to check whether there are triple booking. class MyCalendarTwo { map < int , int > calendar ; public : MyCalendarTwo () { } bool book ( int start , int end ) { calendar [ start ] ++ ; calendar [ end ] -- ; int count = 0 ; for ( auto & c : calendar ) { count += c . second ; if ( count == 3 ) { calendar [ start ] -- ; calendar [ end ] ++ ; return false ; } } return true ; } };","title":"731. My Calendar II"},{"location":"leetcode/interval/notes/#732-my-calendar-iii","text":"Solution 1 Sweep line solution Remember to consider whether duplicates schedule exists or not and how it affect the solution. class MyCalendarThree { map < int , int > calendar ; public : MyCalendarThree () { } int book ( int start , int end ) { calendar [ start ] ++ ; calendar [ end ] -- ; int k = 0 , count = 0 ; for ( const auto & m : calendar ) { k = max ( k , count += m . second ); } return k ; } }; Solution 2 Fancy iterator We use the map::emplace return a pair of <iterator, bool> to indicate it inserted successfully or not class MyCalendarThree { map < int , int > calendar ; int k ; public : MyCalendarThree () : k ( 0 ) { } map < int , int >:: iterator insert ( int t ) { pair < map < int , int >:: iterator , bool > iter_bool = calendar . emplace ( t , 0 ); if ( iter_bool . second && iter_bool . first != calendar . begin ()) iter_bool . first -> second = prev ( iter_bool . first ) -> second ; return iter_bool . first ; } int book ( int start , int end ) { for ( auto i = insert ( start ), j = insert ( end ); i != j ; ++ i ) { k = max ( k , ++ ( i -> second )); } return k ; } };","title":"732. My Calendar III"},{"location":"leetcode/interval/notes/#715-range-module","text":"Solution 1 Using vector to stroe the disjoint intervals class RangeModule { public : RangeModule () { } void addRange ( int left , int right ) { vector < pair < int , int >> tmp ; int pos = 0 ; for ( int i = 0 ; i < intervals . size (); i ++ ) { if ( left > intervals [ i ]. second ) { tmp . push_back ( intervals [ i ]); pos ++ ; } else if ( right < intervals [ i ]. first ) { tmp . push_back ( intervals [ i ]); } else { left = min ( intervals [ i ]. first , left ); right = max ( intervals [ i ]. second , right ); } } tmp . insert ( tmp . begin () + pos , { left , right }); swap ( intervals , tmp ); } bool queryRange ( int left , int right ) { int n = intervals . size (); int l = 0 ; int r = n ; while ( l < r ) { int m = l + ( r - l ) / 2 ; if ( intervals [ m ]. first <= left ) { l = m + 1 ; } else { r = m ; } } if ( l == 0 || intervals [ -- l ]. second < right ) { return false ; } return true ; } void removeRange ( int left , int right ) { int n = intervals . size (); vector < pair < int , int >> tmp ; for ( int i = 0 ; i < n ; i ++ ) { if ( intervals [ i ]. second <= left || intervals [ i ]. first >= right ) { tmp . push_back ( intervals [ i ]); } else { if ( intervals [ i ]. first < left ) { tmp . push_back ({ intervals [ i ]. first , left }); } if ( intervals [ i ]. second > right ) { tmp . push_back ({ right , intervals [ i ]. second }); } } } swap ( intervals , tmp ); } private : vector < pair < int , int >> intervals ; }; /** * Your RangeModule object will be instantiated and called as such: * RangeModule* obj = new RangeModule(); * obj->addRange(left,right); * bool param_2 = obj->queryRange(left,right); * obj->removeRange(left,right); */ Solution 2 Using map to stroe the disjoint intervals class RangeModule { public : RangeModule () {} /* 1_3, 5___8, 12___15 */ void addRange ( int left , int right ) { auto l = m . upper_bound ( left ); auto r = m . upper_bound ( right ); // rule out the case that l overlap with previous if ( l != m . begin ()) { if (( -- l ) -> second < left ) { ++ l ; } } // sure left is non-overlap, check the right if ( l != r ) { int lmin = min ( l -> first , left ); int rmax = max ( right , ( -- r ) -> second ); m . erase ( l , ++ r ); // remove iterator m [ lmin ] = rmax ; } else { // both no overlap m [ left ] = right ; } } bool queryRange ( int left , int right ) { auto l = m . upper_bound ( left ); if ( l == m . begin () || ( -- l ) -> second < right ) { return false ; } return true ; } /* l r 1_3, 5__8, 12_______20, 22____25 4_6 4____9 4___________15 <- impossible 2__4 */ void removeRange ( int left , int right ) { auto l = m . upper_bound ( left ); auto r = m . upper_bound ( right ); if ( l != m . begin ()) { -- l ; if ( l -> second < left ) { ++ l ; } } // nothing need to be removed if ( l == r ) { return ; } int l1 = min ( left , l -> first ); int r1 = max ( right , ( -- r ) -> second ); m . erase ( l , ++ r ); if ( l1 != left ) { m [ l1 ] = left ; } if ( r1 != right ) { m [ right ] = r1 ; } } private : map < int , int > m ; };","title":"715. Range Module"},{"location":"leetcode/interval/notes/#436-find-right-interval","text":"Solution 1 using map ```C++ tab= class Solution { public: vector findRightInterval(vector & intervals) { int n = intervals.size(); map m; for (int i = 0; i < n; ++i) { m[intervals[i].start] = i; } vector<int> res; for (auto interval : intervals) { auto idx = m.lower_bound(interval.end); if (idx != m.end()) { res.push_back(idx->second); } else { res.push_back(-1); } } return res; } }; ```","title":"436. Find Right Interval"},{"location":"leetcode/interval/notes/#435-non-overlapping-intervals_1","text":"Solution 1 Greedy 1. Iterate through the intervals, remove the one with larger end. C++ tab= class Solution { public: int eraseOverlapIntervals(vector<Interval>& intervals) { int n = intervals.size(); int res = 0; sort(intervals.begin(), intervals.end(), [](const Interval& a, const Interval& b){ return a.start < b.start; }); int prev = 0; for (int i = 1; i < n; ++i) { if (intervals[prev].end > intervals[i].start) { if (intervals[prev].end > intervals[i].end) { prev = i; } res++; } else { // end <= start no overlap prev = i; } } return res; } }; Solution 2 class Solution { public : int eraseOverlapIntervals ( vector < Interval >& intervals ) { int n = intervals . size (); if ( n == 0 ) return 0 ; sort ( intervals . begin (), intervals . end (), []( const Interval & a , const Interval & b ){ return a . end < b . end ; }); int res = 0 ; int end = INT_MIN ; for ( int i = 0 ; i < n ; ++ i ) { if ( intervals [ i ]. start < end ) { res ++ ; } else { end = intervals [ i ]. end ; } } return res ; } };","title":"435. Non-overlapping Intervals"},{"location":"leetcode/interval/notes/#352-data-stream-as-disjoint-intervals","text":"Solution 1 Use vector as the container We take the stream and store them in a vector in increasing order based on the start. Eeach new number will find its insertion position using lower_bound . With the iterator, we look back to see whether the number can merge with an interval whose start is smaller. If so, the iterator walk back on step. class SummaryRanges { vector < Interval > intervals ; public : /** Initialize your data structure here. */ SummaryRanges () { } void addNum ( int val ) { Interval curr ( val , val ); int start = val , end = val ; auto iter = lower_bound ( intervals . begin (), intervals . end (), curr , []( const Interval & a , const Interval & b ){ return a . start < b . start ; }); if ( iter != intervals . begin () && ( iter - 1 ) -> end + 1 >= val ) -- iter ; while ( iter != intervals . end () && iter -> end + 1 >= val && val + 1 >= iter -> start ) { start = min ( start , iter -> start ); end = max ( end , iter -> end ); iter = intervals . erase ( iter ); // invalided previous iterator, return next iter } intervals . insert ( iter , Interval ( start , end )); } vector < Interval > getIntervals () { return intervals ; } }; Solution 2 Use map or set as the container class SummaryRanges { struct cmp { bool operator ()( const Interval & a , const Interval & b ){ return a . start < b . start ; } }; set < Interval , cmp > intervals ; public : /** Initialize your data structure here. */ SummaryRanges () { } void addNum ( int val ) { Interval curr ( val , val ); int start = val , end = val ; auto iter = intervals . lower_bound ( curr ); if ( iter != intervals . begin () && ( -- iter ) -> end + 1 < val ) ++ iter ; while ( iter != intervals . end () && iter -> end + 1 >= val && val + 1 >= iter -> start ) { start = min ( start , iter -> start ); end = max ( end , iter -> end ); iter = intervals . erase ( iter ); // invalided previous iterator, return next iter } intervals . insert ( iter , Interval ( start , end )); } vector < Interval > getIntervals () { vector < Interval > res ; for ( auto & interval : intervals ) res . push_back ( interval ); return res ; } }; Solution 3 Use the insert interval subroutine.","title":"352. Data Stream as Disjoint Intervals"},{"location":"leetcode/interval/notes/#915-partition-array-into-disjoint-intervals","text":"","title":"915. Partition Array into Disjoint Intervals"},{"location":"leetcode/interval/notes/#163-missing-ranges","text":"Solution 1 This problem show us the importance of defining the loop invariant. If we use The one ahead of the first variable prev to keep the loop invariant, it saved many code to handle the coner cases. Another trick is factor out the logic to generate the final result in to a funciton. ```C++ \"C++ succient solution\" class Solution { public: vector findMissingRanges(vector & nums, int lower, int upper) { int n = nums.size(); vector res; long prev = 0; long curr = 0; prev = (long)lower - 1; for (int i = 0; i < n; i++) { curr = nums[i]; //curr = i == n ? (long)upper + 1 : nums[i]; if (curr - prev > 1) { res.push_back(getRange(prev + 1, curr - 1)); } prev = curr; } /* the case can be included in the above case by changing line 10 and 11 */ if (upper > prev) { res.push_back(getRange(prev + 1, upper)); } return res; } string getRange(int a, int b) { return a == b ? to_string(a) : to_string(a) + \"->\" + to_string(b); } }; C++ tab=\"C++ lengthy solution\" class Solution { public: vector findMissingRanges(vector & nums, int lower, int upper) { vector res; if (nums.size() == 0) { if (lower == upper) res.push_back(to_string(lower)); else res.push_back(to_string(lower) + \"->\" + to_string(upper)); return res; } long s = nums[0]; if (lower < s - 1) { res.push_back(to_string(lower) + \"->\" + to_string(s - 1)); } else if (lower < s) { res.push_back(to_string(lower)); } for (int i = 0; i < nums.size() - 1; i++) { long a = nums[i]; long b = nums[i + 1]; if (a == b - 1) { continue; } else if (a == b - 2) { res.push_back(to_string(a + 1)); } else if (a < b - 2) { res.push_back(to_string(a + 1) + \"->\" + to_string(b - 1)); } } long e = nums[nums.size() - 1]; if (e < upper - 1) { res.push_back(to_string(e + 1) + \"->\" + to_string(upper)); } else if (e < upper) { res.push_back(to_string(upper)); } return res; } }; ```","title":"163. Missing Ranges"},{"location":"leetcode/interval/notes/#228-summary-ranges","text":"Pay attention to the integer overflow when you test whether one number is greater that another. [-2147483648,-2147483647,2147483647] Another thing is that to_string() can also change long and float number to string. ```C++ tab= class Solution { public: vector summaryRanges(vector & nums) { int n = nums.size(); vector res; if (n == 0) { return res; } long start = nums[0]; long prev = nums[0]; for (int i = 1; i < n; i++) { if ((long)nums[i] - prev > 1) { string s = start < prev ? to_string((int)start) + \"->\" + to_string((int)prev) : to_string((int)start); res.push_back(s); start = nums[i]; } prev = nums[i]; } res.push_back(start < prev ? to_string((int)start) + \"->\" + to_string((int)prev) : to_string((int)start)); return res; } }; ```","title":"228. Summary Ranges"},{"location":"leetcode/interval/notes/#938-range-sum-of-bst","text":"","title":"938. Range Sum of BST"},{"location":"leetcode/interval/notes/#370-range-addition","text":"","title":"370. Range Addition"},{"location":"leetcode/interval/notes/#597-range-addition-ii","text":"","title":"597. Range Addition II"},{"location":"leetcode/linked-list/notes/","text":"Linked List \u00b6 Tricks to solve linked list \u00b6 Edge case need to watch in linked list \u00b6 When to use Dummy node \u00b6 Iterate tricks \u00b6 trick 1: If the iteration is for two linked lists with different length. You can use while (l1 || l2) , inside the while loop, you can maintain the loop invariance by checking the nullptr and considering all edge cases. Recursive in linked list \u00b6 Catergory 1 Iterate linked lists \u00b6 Add Two Numbers \u00b6 Think about how to make the iteration elegant without dealing many if else conditions /** * Definition for singly-linked list. * struct ListNode { * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) {} * }; */ class Solution { public : ListNode * addTwoNumbers ( ListNode * l1 , ListNode * l2 ) { ListNode dummy ( INT_MIN ); ListNode * prev = & dummy ; int sum = 0 ; while ( l1 || l2 || sum ) { sum += ( l1 ? l1 -> val : 0 ) + ( l2 ? l2 -> val : 0 ); ListNode * node = new ListNode ( sum % 10 ); prev -> next = node ; prev = node ; l1 = l1 ? l1 -> next : l1 ; l2 = l2 ? l2 -> next : l2 ; sum /= 10 ; } return dummy . next ; } }; Add Two Numbers II \u00b6 It seems we need a stack. We can use a stack variable or use recursion. C++ stack /** * Definition for singly-linked list. * struct ListNode { * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) {} * }; */ class Solution { public : ListNode * addTwoNumbers ( ListNode * l1 , ListNode * l2 ) { stack < int > s1 , s2 ; while ( l1 ) { s1 . push ( l1 -> val ); l1 = l1 -> next ; } while ( l2 ) { s2 . push ( l2 -> val ); l2 = l2 -> next ; } int sum = 0 ; ListNode * p = nullptr , * head = new ListNode ( 0 ); while ( ! s1 . empty () || ! s2 . empty ()) { if ( ! s1 . empty ()) { sum += s1 . top (); s1 . pop (); } if ( ! s2 . empty ()) { sum += s2 . top (); s2 . pop (); } head -> val = sum % 10 ; p = new ListNode ( sum / 10 ); p -> next = head ; head = p ; sum /= 10 ; } return head -> val == 0 ? head -> next : head ; } }; Plus One Linked List \u00b6 We need a stack. Using recursion can help us handle the case well /** * Definition for singly-linked list. * struct ListNode { * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) {} * }; */ class Solution { public : ListNode * plusOne ( ListNode * head ) { int carry = 0 ; helper ( head , carry ); if ( carry > 0 ) { ListNode * node = new ListNode ( carry ); node -> next = head ; head = node ; } return head ; } // carry can also be passed by return value void helper ( ListNode * head , int & carry ) { // base case if ( head -> next == nullptr ) { int tmp = head -> val ; head -> val = ( head -> val + 1 + carry ) % 10 ; carry = ( tmp + 1 + carry ) / 10 ; return ; } helper ( head -> next , carry ); if ( carry > 0 ) { int tmp = head -> val ; head -> val = ( head -> val + carry ) % 10 ; carry = ( tmp + carry ) / 10 ; } } }; Catergory 2 Remove nodes from linked lists \u00b6 Remove Nth Node From End of List \u00b6 use slow and fast node to \"measure\" the nth node and remove it. /** * Definition for singly-linked list. * struct ListNode { * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) {} * }; */ class Solution { public : ListNode * removeNthFromEnd ( ListNode * head , int n ) { ListNode dummy ( INT_MIN ); dummy . next = head ; ListNode * slow = & dummy ; ListNode * fast = & dummy ; while ( n > 0 ) { fast = fast -> next ; n -- ; } while ( fast -> next ) { slow = slow -> next ; fast = fast -> next ; } slow -> next = slow -> next -> next ; return dummy . next ; } }; Remove Linked List Elements \u00b6 This is an easy problem, but can you use two * programing to solve it? C++ solution /** * Definition for singly-linked list. * struct ListNode { * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) {} * }; */ class Solution { public : ListNode * removeElements ( ListNode * head , int val ) { ListNode dummy ( 0 ); dummy . next = head ; ListNode * curr = & dummy ; while ( curr -> next ) { if ( curr -> next -> val == val ) { ListNode * next = curr -> next ; curr -> next = next -> next ; } else { curr = curr -> next ; } } return dummy . next ; } }; Two * solution /** * Definition for singly-linked list. * struct ListNode { * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) {} * }; */ class Solution { public : ListNode * removeElements ( ListNode * head , int val ) { // if (head == nullptr) return nullptr; ListNode ** headptr = & head ; ListNode ** curr = headptr ; while ( * curr != NULL ) { ListNode * entry = * curr ; if ( entry -> val == val ) { * curr = entry -> next ; } else { curr = & ( entry -> next ); } } return * headptr ; } }; Remove Duplicates from Sorted List \u00b6 /** * Definition for singly-linked list. * struct ListNode { * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) {} * }; */ class Solution { public : ListNode * deleteDuplicates ( ListNode * head ) { if ( head == nullptr || head -> next == nullptr ) return head ; ListNode * prev = head ; ListNode * curr = head -> next ; while ( curr ) { if ( curr -> val == prev -> val ) { curr = curr -> next ; prev -> next = curr ; } else { curr = curr -> next ; prev = prev -> next ; } } return head ; } }; Remove Duplicates from Sorted List II \u00b6 /** * Definition for singly-linked list. * struct ListNode { * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) {} * }; */ class Solution { public : ListNode * deleteDuplicates ( ListNode * head ) { if ( head == nullptr ) return nullptr ; ListNode dummy ( INT_MIN ); dummy . next = head ; ListNode * prev = & dummy ; ListNode * curr = head ; while ( curr ) { if ( curr -> next && curr -> val == curr -> next -> val ) { int tmp = curr -> val ; while ( curr && curr -> val == tmp ) { curr = curr -> next ; } prev -> next = curr ; } else { prev = curr ; curr = curr -> next ; } } return dummy . next ; } }; Remove Zero Sum Consecutive Nodes from Linked List \u00b6 Catergory 3 Reverse linked lists \u00b6 Reverse Linked List \u00b6 C++ iterative with dummy /** * Definition for singly-linked list. * struct ListNode { * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) {} * }; */ class Solution { public : ListNode * reverse ( ListNode * head ) { ListNode * new_head = nullptr ; while ( head != nullptr ) { ListNode * tmp = head -> next ; head -> next = new_head ; new_head = head ; head = tmp ; } return new_head ; } }; C++ iterative without dummy /** * Definition for singly-linked list. * struct ListNode { * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) {} * }; */ class Solution { public : ListNode * reverseList ( ListNode * head ) { if ( head == nullptr ) return nullptr ; ListNode * curr = head ; while ( curr -> next ) { ListNode * tmp = curr -> next ; curr -> next = tmp -> next ; tmp -> next = head ; head = tmp ; } return head ; } }; C++ recursive /** * Definition for singly-linked list. * struct ListNode { * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) {} * }; */ class Solution { public : ListNode * reverseList ( ListNode * head ) { if ( head == nullptr || head -> next == nullptr ) return head ; ListNode * new_head = reverseList ( head -> next ); head -> next -> next = head ; head -> next = nullptr ; return new_head ; } }; Reverse Linked List II \u00b6 Palindrome Linked List \u00b6 Reverse Nodes in k-Group \u00b6 Catergory 4 Merge linked lists \u00b6 Merge Two Sorted Lists \u00b6 Merge k Sorted Lists \u00b6 Reorder List \u00b6 Sort List \u00b6 Catergory 5 Cycle in linked lists \u00b6 708. Insert into a Sorted Circular Linked List \u00b6 What the loop invariant? It is easy to handle the common case prev <= insertVal <= curr . For the special case when \"wrap around\" the circle, it could be insertVal is the max or min, but in both cases, the insertion operation is the same. Python loop invariant \"\"\" # Definition for a Node. class Node: def __init__(self, val=None, next=None): self.val = val self.next = next \"\"\" class Solution : def insert ( self , head : 'Node' , insertVal : int ) -> 'Node' : node = Node ( insertVal ) node . next = node if not head : return node prev , curr = head , head . next while not prev . val <= insertVal <= curr . val and not prev . val > curr . val > insertVal and not insertVal > prev . val > curr . val : prev , curr = prev . next , curr . next if prev == head : break prev . next = node node . next = curr return head C++ loop invariant /* // Definition for a Node. class Node { public: int val = NULL; Node* next = NULL; Node() {} Node(int _val, Node* _next) { val = _val; next = _next; } }; */ class Solution { public : Node * insert ( Node * head , int insertVal ) { if ( head == nullptr ) { head = new Node ( insertVal , nullptr ); head -> next = head ; return head ; } Node * prev = head ; Node * next = head -> next ; while ( ! ( prev -> val <= insertVal && insertVal <= next -> val ) && ! ( prev -> val > next -> val && insertVal < next -> val ) && ! ( prev -> val > next -> val && insertVal > prev -> val )) { prev = prev -> next ; next = next -> next ; if ( prev == head ) break ; } prev -> next = new Node ( insertVal , next ); return head ; } }; Linked List Cycle \u00b6 Linked List Cycle II \u00b6 Intersection of Two Linked Lists \u00b6 Rotate List \u00b6 Catergory 6 Linked List property \u00b6","title":"Linked List"},{"location":"leetcode/linked-list/notes/#linked-list","text":"","title":"Linked List"},{"location":"leetcode/linked-list/notes/#tricks-to-solve-linked-list","text":"","title":"Tricks to solve linked list"},{"location":"leetcode/linked-list/notes/#edge-case-need-to-watch-in-linked-list","text":"","title":"Edge case need to watch in linked list"},{"location":"leetcode/linked-list/notes/#when-to-use-dummy-node","text":"","title":"When to use Dummy node"},{"location":"leetcode/linked-list/notes/#iterate-tricks","text":"trick 1: If the iteration is for two linked lists with different length. You can use while (l1 || l2) , inside the while loop, you can maintain the loop invariance by checking the nullptr and considering all edge cases.","title":"Iterate tricks"},{"location":"leetcode/linked-list/notes/#recursive-in-linked-list","text":"","title":"Recursive in linked list"},{"location":"leetcode/linked-list/notes/#catergory-1-iterate-linked-lists","text":"","title":"Catergory 1 Iterate linked lists"},{"location":"leetcode/linked-list/notes/#add-two-numbers","text":"Think about how to make the iteration elegant without dealing many if else conditions /** * Definition for singly-linked list. * struct ListNode { * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) {} * }; */ class Solution { public : ListNode * addTwoNumbers ( ListNode * l1 , ListNode * l2 ) { ListNode dummy ( INT_MIN ); ListNode * prev = & dummy ; int sum = 0 ; while ( l1 || l2 || sum ) { sum += ( l1 ? l1 -> val : 0 ) + ( l2 ? l2 -> val : 0 ); ListNode * node = new ListNode ( sum % 10 ); prev -> next = node ; prev = node ; l1 = l1 ? l1 -> next : l1 ; l2 = l2 ? l2 -> next : l2 ; sum /= 10 ; } return dummy . next ; } };","title":"Add Two Numbers"},{"location":"leetcode/linked-list/notes/#add-two-numbers-ii","text":"It seems we need a stack. We can use a stack variable or use recursion. C++ stack /** * Definition for singly-linked list. * struct ListNode { * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) {} * }; */ class Solution { public : ListNode * addTwoNumbers ( ListNode * l1 , ListNode * l2 ) { stack < int > s1 , s2 ; while ( l1 ) { s1 . push ( l1 -> val ); l1 = l1 -> next ; } while ( l2 ) { s2 . push ( l2 -> val ); l2 = l2 -> next ; } int sum = 0 ; ListNode * p = nullptr , * head = new ListNode ( 0 ); while ( ! s1 . empty () || ! s2 . empty ()) { if ( ! s1 . empty ()) { sum += s1 . top (); s1 . pop (); } if ( ! s2 . empty ()) { sum += s2 . top (); s2 . pop (); } head -> val = sum % 10 ; p = new ListNode ( sum / 10 ); p -> next = head ; head = p ; sum /= 10 ; } return head -> val == 0 ? head -> next : head ; } };","title":"Add Two Numbers II"},{"location":"leetcode/linked-list/notes/#plus-one-linked-list","text":"We need a stack. Using recursion can help us handle the case well /** * Definition for singly-linked list. * struct ListNode { * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) {} * }; */ class Solution { public : ListNode * plusOne ( ListNode * head ) { int carry = 0 ; helper ( head , carry ); if ( carry > 0 ) { ListNode * node = new ListNode ( carry ); node -> next = head ; head = node ; } return head ; } // carry can also be passed by return value void helper ( ListNode * head , int & carry ) { // base case if ( head -> next == nullptr ) { int tmp = head -> val ; head -> val = ( head -> val + 1 + carry ) % 10 ; carry = ( tmp + 1 + carry ) / 10 ; return ; } helper ( head -> next , carry ); if ( carry > 0 ) { int tmp = head -> val ; head -> val = ( head -> val + carry ) % 10 ; carry = ( tmp + carry ) / 10 ; } } };","title":"Plus One Linked List"},{"location":"leetcode/linked-list/notes/#catergory-2-remove-nodes-from-linked-lists","text":"","title":"Catergory 2 Remove nodes from linked lists"},{"location":"leetcode/linked-list/notes/#remove-nth-node-from-end-of-list","text":"use slow and fast node to \"measure\" the nth node and remove it. /** * Definition for singly-linked list. * struct ListNode { * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) {} * }; */ class Solution { public : ListNode * removeNthFromEnd ( ListNode * head , int n ) { ListNode dummy ( INT_MIN ); dummy . next = head ; ListNode * slow = & dummy ; ListNode * fast = & dummy ; while ( n > 0 ) { fast = fast -> next ; n -- ; } while ( fast -> next ) { slow = slow -> next ; fast = fast -> next ; } slow -> next = slow -> next -> next ; return dummy . next ; } };","title":"Remove Nth Node From End of List"},{"location":"leetcode/linked-list/notes/#remove-linked-list-elements","text":"This is an easy problem, but can you use two * programing to solve it? C++ solution /** * Definition for singly-linked list. * struct ListNode { * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) {} * }; */ class Solution { public : ListNode * removeElements ( ListNode * head , int val ) { ListNode dummy ( 0 ); dummy . next = head ; ListNode * curr = & dummy ; while ( curr -> next ) { if ( curr -> next -> val == val ) { ListNode * next = curr -> next ; curr -> next = next -> next ; } else { curr = curr -> next ; } } return dummy . next ; } }; Two * solution /** * Definition for singly-linked list. * struct ListNode { * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) {} * }; */ class Solution { public : ListNode * removeElements ( ListNode * head , int val ) { // if (head == nullptr) return nullptr; ListNode ** headptr = & head ; ListNode ** curr = headptr ; while ( * curr != NULL ) { ListNode * entry = * curr ; if ( entry -> val == val ) { * curr = entry -> next ; } else { curr = & ( entry -> next ); } } return * headptr ; } };","title":"Remove Linked List Elements"},{"location":"leetcode/linked-list/notes/#remove-duplicates-from-sorted-list","text":"/** * Definition for singly-linked list. * struct ListNode { * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) {} * }; */ class Solution { public : ListNode * deleteDuplicates ( ListNode * head ) { if ( head == nullptr || head -> next == nullptr ) return head ; ListNode * prev = head ; ListNode * curr = head -> next ; while ( curr ) { if ( curr -> val == prev -> val ) { curr = curr -> next ; prev -> next = curr ; } else { curr = curr -> next ; prev = prev -> next ; } } return head ; } };","title":"Remove Duplicates from Sorted List"},{"location":"leetcode/linked-list/notes/#remove-duplicates-from-sorted-list-ii","text":"/** * Definition for singly-linked list. * struct ListNode { * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) {} * }; */ class Solution { public : ListNode * deleteDuplicates ( ListNode * head ) { if ( head == nullptr ) return nullptr ; ListNode dummy ( INT_MIN ); dummy . next = head ; ListNode * prev = & dummy ; ListNode * curr = head ; while ( curr ) { if ( curr -> next && curr -> val == curr -> next -> val ) { int tmp = curr -> val ; while ( curr && curr -> val == tmp ) { curr = curr -> next ; } prev -> next = curr ; } else { prev = curr ; curr = curr -> next ; } } return dummy . next ; } };","title":"Remove Duplicates from Sorted List II"},{"location":"leetcode/linked-list/notes/#remove-zero-sum-consecutive-nodes-from-linked-list","text":"","title":"Remove Zero Sum Consecutive Nodes from Linked List"},{"location":"leetcode/linked-list/notes/#catergory-3-reverse-linked-lists","text":"","title":"Catergory 3 Reverse linked lists"},{"location":"leetcode/linked-list/notes/#reverse-linked-list","text":"C++ iterative with dummy /** * Definition for singly-linked list. * struct ListNode { * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) {} * }; */ class Solution { public : ListNode * reverse ( ListNode * head ) { ListNode * new_head = nullptr ; while ( head != nullptr ) { ListNode * tmp = head -> next ; head -> next = new_head ; new_head = head ; head = tmp ; } return new_head ; } }; C++ iterative without dummy /** * Definition for singly-linked list. * struct ListNode { * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) {} * }; */ class Solution { public : ListNode * reverseList ( ListNode * head ) { if ( head == nullptr ) return nullptr ; ListNode * curr = head ; while ( curr -> next ) { ListNode * tmp = curr -> next ; curr -> next = tmp -> next ; tmp -> next = head ; head = tmp ; } return head ; } }; C++ recursive /** * Definition for singly-linked list. * struct ListNode { * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) {} * }; */ class Solution { public : ListNode * reverseList ( ListNode * head ) { if ( head == nullptr || head -> next == nullptr ) return head ; ListNode * new_head = reverseList ( head -> next ); head -> next -> next = head ; head -> next = nullptr ; return new_head ; } };","title":"Reverse Linked List"},{"location":"leetcode/linked-list/notes/#reverse-linked-list-ii","text":"","title":"Reverse Linked List II"},{"location":"leetcode/linked-list/notes/#palindrome-linked-list","text":"","title":"Palindrome Linked List"},{"location":"leetcode/linked-list/notes/#reverse-nodes-in-k-group","text":"","title":"Reverse Nodes in k-Group"},{"location":"leetcode/linked-list/notes/#catergory-4-merge-linked-lists","text":"","title":"Catergory 4 Merge linked lists"},{"location":"leetcode/linked-list/notes/#merge-two-sorted-lists","text":"","title":"Merge Two Sorted Lists"},{"location":"leetcode/linked-list/notes/#merge-k-sorted-lists","text":"","title":"Merge k Sorted Lists"},{"location":"leetcode/linked-list/notes/#reorder-list","text":"","title":"Reorder List"},{"location":"leetcode/linked-list/notes/#sort-list","text":"","title":"Sort List"},{"location":"leetcode/linked-list/notes/#catergory-5-cycle-in-linked-lists","text":"","title":"Catergory 5 Cycle in linked lists"},{"location":"leetcode/linked-list/notes/#708-insert-into-a-sorted-circular-linked-list","text":"What the loop invariant? It is easy to handle the common case prev <= insertVal <= curr . For the special case when \"wrap around\" the circle, it could be insertVal is the max or min, but in both cases, the insertion operation is the same. Python loop invariant \"\"\" # Definition for a Node. class Node: def __init__(self, val=None, next=None): self.val = val self.next = next \"\"\" class Solution : def insert ( self , head : 'Node' , insertVal : int ) -> 'Node' : node = Node ( insertVal ) node . next = node if not head : return node prev , curr = head , head . next while not prev . val <= insertVal <= curr . val and not prev . val > curr . val > insertVal and not insertVal > prev . val > curr . val : prev , curr = prev . next , curr . next if prev == head : break prev . next = node node . next = curr return head C++ loop invariant /* // Definition for a Node. class Node { public: int val = NULL; Node* next = NULL; Node() {} Node(int _val, Node* _next) { val = _val; next = _next; } }; */ class Solution { public : Node * insert ( Node * head , int insertVal ) { if ( head == nullptr ) { head = new Node ( insertVal , nullptr ); head -> next = head ; return head ; } Node * prev = head ; Node * next = head -> next ; while ( ! ( prev -> val <= insertVal && insertVal <= next -> val ) && ! ( prev -> val > next -> val && insertVal < next -> val ) && ! ( prev -> val > next -> val && insertVal > prev -> val )) { prev = prev -> next ; next = next -> next ; if ( prev == head ) break ; } prev -> next = new Node ( insertVal , next ); return head ; } };","title":"708. Insert into a Sorted Circular Linked List"},{"location":"leetcode/linked-list/notes/#linked-list-cycle","text":"","title":"Linked List Cycle"},{"location":"leetcode/linked-list/notes/#linked-list-cycle-ii","text":"","title":"Linked List Cycle II"},{"location":"leetcode/linked-list/notes/#intersection-of-two-linked-lists","text":"","title":"Intersection of Two Linked Lists"},{"location":"leetcode/linked-list/notes/#rotate-list","text":"","title":"Rotate List"},{"location":"leetcode/linked-list/notes/#catergory-6-linked-list-property","text":"","title":"Catergory 6 Linked List property"},{"location":"leetcode/math/notes/","text":"Math and Geometry \u00b6 Cartesian plane \u00b6 Problems \u00b6 478. Generate Random Point in a Circle \u00b6 497. Random Point in Non-overlapping Rectangles \u00b6 Solution 1 Trick to locate the coordinate in a matrix ```C++ tab=\"C++\" class Solution { int n = 0; vector > rectangles; vector psum; public: Solution(vector >& rects) { rectangles = rects; for (auto& r : rectangles) { n += (r[2] - r[0] + 1) * (r[3] - r[1] + 1); psum.push_back(n); } } vector<int> pick() { vector<int> res(2); // which rect to select? int target = rand() % n; int left = 0; int right = psum.size(); // binary search: find the first greater element while (left != right) { int mid = left + (right - left) / 2; if (psum[mid] <= target) { left = mid + 1; } else { right = mid; } } vector<int> r = rectangles[left]; // which point to select inside the selected rectangle? int width = r[2] - r[0] + 1; int height = r[3] - r[1] + 1; int base = psum[left] - width * height; return {r[0] + (target - base) % width, r[1] + (target - base) / width}; } }; ```","title":"Math"},{"location":"leetcode/math/notes/#math-and-geometry","text":"","title":"Math and Geometry"},{"location":"leetcode/math/notes/#cartesian-plane","text":"","title":"Cartesian plane"},{"location":"leetcode/math/notes/#problems","text":"","title":"Problems"},{"location":"leetcode/math/notes/#478-generate-random-point-in-a-circle","text":"","title":"478. Generate Random Point in a Circle"},{"location":"leetcode/math/notes/#497-random-point-in-non-overlapping-rectangles","text":"Solution 1 Trick to locate the coordinate in a matrix ```C++ tab=\"C++\" class Solution { int n = 0; vector > rectangles; vector psum; public: Solution(vector >& rects) { rectangles = rects; for (auto& r : rectangles) { n += (r[2] - r[0] + 1) * (r[3] - r[1] + 1); psum.push_back(n); } } vector<int> pick() { vector<int> res(2); // which rect to select? int target = rand() % n; int left = 0; int right = psum.size(); // binary search: find the first greater element while (left != right) { int mid = left + (right - left) / 2; if (psum[mid] <= target) { left = mid + 1; } else { right = mid; } } vector<int> r = rectangles[left]; // which point to select inside the selected rectangle? int width = r[2] - r[0] + 1; int height = r[3] - r[1] + 1; int base = psum[left] - width * height; return {r[0] + (target - base) % width, r[1] + (target - base) / width}; } }; ```","title":"497. Random Point in Non-overlapping Rectangles"},{"location":"leetcode/reservoir-sampling/notes/","text":"Reservoir Sampling \u00b6 Problem statement \u00b6 Randomly select 1 item (or k items) from a stream of items of unknown length. Each item should be selected with equal probability. Proof \u00b6 Starting with the case that only 1 element need to be chosen randomly with equal probability, that is an element a_i a_i is chosen with probability 1/n 1/n . How should we implement the \"choose\" action? In other words, in obtaining a element a_i a_i , how could we programmatically decide whether to keep it or not keep it? If we know the probability to keep a_i a_i in timestamp i i , and the probability to not replace it after a_i a_i , we can calcualte the probability of a_i a_i beening selected ultimately. But how? Define P(a_i) P(a_i) the probability that a_i a_i is the final element selected. We know from the requirement that each element should be selected with equal probability P(a_1) = \\dots = P(a_i) = P(a_{i+1}) = \\dots = P(a_n) = 1/n P(a_1) = \\dots = P(a_i) = P(a_{i+1}) = \\dots = P(a_n) = 1/n . For a_i a_i , we have the following: P(a_i) = P(a_i\\text{ is selected at timestamp } i) \\cdot P(a_i \\text{ have not been replaced at timestamp: } i + 1, i + 2, \\dots, n) P(a_i) = P(a_i\\text{ is selected at timestamp } i) \\cdot P(a_i \\text{ have not been replaced at timestamp: } i + 1, i + 2, \\dots, n) If at timestamp i i , the data stream terminates. If we select the element a_i a_i with probability \\frac{1}{i} \\frac{1}{i} , it fulfilled the requirement of the problem statement. If there are more element followed, we have to ensure those will not be selected (not to replace a_i a_i ), otherwise, it will contradict to the assumption that a_i a_i is the final element being selected. For element a_{i+1} a_{i+1} , it will be selected with probability \\frac{1}{i+1} \\frac{1}{i+1} , and not being seleted with probability 1-\\frac{1}{i+1} 1-\\frac{1}{i+1} . thus the above probability becomes: \\begin{eqnarray*} P(a_i) & = & P(a_i\\text{ is selected at timestamp } i) \\cdot P(a_i \\text{ have not been replaced at timestamp: } i + 1, i + 2, \\dots, n)\\\\ & = & \\frac{1}{i} \\cdot (1-\\frac{1}{i+1}) \\cdot (1-\\frac{1}{i+2}) \\dots \\cdot (1-\\frac{1}{n}) \\\\ & = & \\frac{1}{i} \\cdot (\\frac{i}{i+1}) \\cdot (\\frac{i+1}{i+2}) \\dots (\\frac{n-2}{n-1}) \\cdot (\\frac{n-1}{n}) \\\\ & = & \\frac{1}{n} \\end{eqnarray*} \\begin{eqnarray*} P(a_i) & = & P(a_i\\text{ is selected at timestamp } i) \\cdot P(a_i \\text{ have not been replaced at timestamp: } i + 1, i + 2, \\dots, n)\\\\ & = & \\frac{1}{i} \\cdot (1-\\frac{1}{i+1}) \\cdot (1-\\frac{1}{i+2}) \\dots \\cdot (1-\\frac{1}{n}) \\\\ & = & \\frac{1}{i} \\cdot (\\frac{i}{i+1}) \\cdot (\\frac{i+1}{i+2}) \\dots (\\frac{n-2}{n-1}) \\cdot (\\frac{n-1}{n}) \\\\ & = & \\frac{1}{n} \\end{eqnarray*} Next, we will take a look at the case that select k k element, with each outcome with an equal probability. Reference \u00b6 https://www.youtube.com/watch?v=Ybra0uGEkpM https://gregable.com/2007/10/reservoir-sampling.html Problem \u00b6 382. Linked List Random Node \u00b6 /** * Definition for singly-linked list. * struct ListNode { * int val; * ListNode *next; * ListNode() : val(0), next(nullptr) {} * ListNode(int x) : val(x), next(nullptr) {} * ListNode(int x, ListNode *next) : val(x), next(next) {} * }; */ class Solution { ListNode * head ; public : /** @param head The linked list's head. Note that the head is guaranteed to be not null, so it contains at least one node. */ Solution ( ListNode * head ) { this -> head = head ; } /** Returns a random node's value. */ int getRandom () { int res = this -> head -> val ; int c = 2 ; ListNode * curr = this -> head -> next ; while ( curr ) { int j = rand () % c ; if ( j == 0 ) { res = curr -> val ; } c ++ ; curr = curr -> next ; } return res ; } }; /** * Your Solution object will be instantiated and called as such: * Solution* obj = new Solution(head); * int param_1 = obj->getRandom(); */ 398. Random Pick Index \u00b6 class Solution { vector < int > vec ; public : Solution ( vector < int > nums ) { vec = nums ; } int pick ( int target ) { int cnt = 0 , res = -1 ; for ( int i = 0 ; i < vec . size (); ++ i ) { if ( vec [ i ] != target ) continue ; ++ cnt ; if ( rand () % cnt == 0 ) res = i ; } return res ; } }; /** * Your Solution object will be instantiated and called as such: * Solution obj = new Solution(nums); * int param_1 = obj.pick(target); */","title":"Reservoir Sampling"},{"location":"leetcode/reservoir-sampling/notes/#reservoir-sampling","text":"","title":"Reservoir Sampling"},{"location":"leetcode/reservoir-sampling/notes/#problem-statement","text":"Randomly select 1 item (or k items) from a stream of items of unknown length. Each item should be selected with equal probability.","title":"Problem statement"},{"location":"leetcode/reservoir-sampling/notes/#proof","text":"Starting with the case that only 1 element need to be chosen randomly with equal probability, that is an element a_i a_i is chosen with probability 1/n 1/n . How should we implement the \"choose\" action? In other words, in obtaining a element a_i a_i , how could we programmatically decide whether to keep it or not keep it? If we know the probability to keep a_i a_i in timestamp i i , and the probability to not replace it after a_i a_i , we can calcualte the probability of a_i a_i beening selected ultimately. But how? Define P(a_i) P(a_i) the probability that a_i a_i is the final element selected. We know from the requirement that each element should be selected with equal probability P(a_1) = \\dots = P(a_i) = P(a_{i+1}) = \\dots = P(a_n) = 1/n P(a_1) = \\dots = P(a_i) = P(a_{i+1}) = \\dots = P(a_n) = 1/n . For a_i a_i , we have the following: P(a_i) = P(a_i\\text{ is selected at timestamp } i) \\cdot P(a_i \\text{ have not been replaced at timestamp: } i + 1, i + 2, \\dots, n) P(a_i) = P(a_i\\text{ is selected at timestamp } i) \\cdot P(a_i \\text{ have not been replaced at timestamp: } i + 1, i + 2, \\dots, n) If at timestamp i i , the data stream terminates. If we select the element a_i a_i with probability \\frac{1}{i} \\frac{1}{i} , it fulfilled the requirement of the problem statement. If there are more element followed, we have to ensure those will not be selected (not to replace a_i a_i ), otherwise, it will contradict to the assumption that a_i a_i is the final element being selected. For element a_{i+1} a_{i+1} , it will be selected with probability \\frac{1}{i+1} \\frac{1}{i+1} , and not being seleted with probability 1-\\frac{1}{i+1} 1-\\frac{1}{i+1} . thus the above probability becomes: \\begin{eqnarray*} P(a_i) & = & P(a_i\\text{ is selected at timestamp } i) \\cdot P(a_i \\text{ have not been replaced at timestamp: } i + 1, i + 2, \\dots, n)\\\\ & = & \\frac{1}{i} \\cdot (1-\\frac{1}{i+1}) \\cdot (1-\\frac{1}{i+2}) \\dots \\cdot (1-\\frac{1}{n}) \\\\ & = & \\frac{1}{i} \\cdot (\\frac{i}{i+1}) \\cdot (\\frac{i+1}{i+2}) \\dots (\\frac{n-2}{n-1}) \\cdot (\\frac{n-1}{n}) \\\\ & = & \\frac{1}{n} \\end{eqnarray*} \\begin{eqnarray*} P(a_i) & = & P(a_i\\text{ is selected at timestamp } i) \\cdot P(a_i \\text{ have not been replaced at timestamp: } i + 1, i + 2, \\dots, n)\\\\ & = & \\frac{1}{i} \\cdot (1-\\frac{1}{i+1}) \\cdot (1-\\frac{1}{i+2}) \\dots \\cdot (1-\\frac{1}{n}) \\\\ & = & \\frac{1}{i} \\cdot (\\frac{i}{i+1}) \\cdot (\\frac{i+1}{i+2}) \\dots (\\frac{n-2}{n-1}) \\cdot (\\frac{n-1}{n}) \\\\ & = & \\frac{1}{n} \\end{eqnarray*} Next, we will take a look at the case that select k k element, with each outcome with an equal probability.","title":"Proof"},{"location":"leetcode/reservoir-sampling/notes/#reference","text":"https://www.youtube.com/watch?v=Ybra0uGEkpM https://gregable.com/2007/10/reservoir-sampling.html","title":"Reference"},{"location":"leetcode/reservoir-sampling/notes/#problem","text":"","title":"Problem"},{"location":"leetcode/reservoir-sampling/notes/#382-linked-list-random-node","text":"/** * Definition for singly-linked list. * struct ListNode { * int val; * ListNode *next; * ListNode() : val(0), next(nullptr) {} * ListNode(int x) : val(x), next(nullptr) {} * ListNode(int x, ListNode *next) : val(x), next(next) {} * }; */ class Solution { ListNode * head ; public : /** @param head The linked list's head. Note that the head is guaranteed to be not null, so it contains at least one node. */ Solution ( ListNode * head ) { this -> head = head ; } /** Returns a random node's value. */ int getRandom () { int res = this -> head -> val ; int c = 2 ; ListNode * curr = this -> head -> next ; while ( curr ) { int j = rand () % c ; if ( j == 0 ) { res = curr -> val ; } c ++ ; curr = curr -> next ; } return res ; } }; /** * Your Solution object will be instantiated and called as such: * Solution* obj = new Solution(head); * int param_1 = obj->getRandom(); */","title":"382. Linked List Random Node"},{"location":"leetcode/reservoir-sampling/notes/#398-random-pick-index","text":"class Solution { vector < int > vec ; public : Solution ( vector < int > nums ) { vec = nums ; } int pick ( int target ) { int cnt = 0 , res = -1 ; for ( int i = 0 ; i < vec . size (); ++ i ) { if ( vec [ i ] != target ) continue ; ++ cnt ; if ( rand () % cnt == 0 ) res = i ; } return res ; } }; /** * Your Solution object will be instantiated and called as such: * Solution obj = new Solution(nums); * int param_1 = obj.pick(target); */","title":"398. Random Pick Index"},{"location":"leetcode/sliding-window/notes/","text":"Sliding Window \u00b6 Problem types \u00b6 Monotonic deque to acheive O(n) \u00b6 If we need to find the max/min from a sliding window. The first idea would be use ordered container to store them and then access the max/min. But it usually requires O(nlogn) O(nlogn) or even worse. While we use deque in sliding window problem we can achieve runtime complexity of O(n) O(n) e.g. 1425. Constrained Subsequence Sum , 1438. Longest Continuous Subarray With Absolute Diff Less Than or Equal to Limit The general idea is to use deque to keep only the necessary information. During each iteration, we use the window size and the order info to keep the invariant: All elements that are not possible to become a solution are removed from queue; All elements in the queue form a valid window size, so that we can calculate the solution. Another caveat is when we maintain the invariant, should we use while or use if ? This is realative easy to determine in practice, you just need to verify whether the invariant is maintained correctly. Use if for one operation and while for multiple operations. Problems \u00b6 209. Minimum Size Subarray Sum \u00b6 Solution 1: Using two pointers class Solution { public : int minSubArrayLen ( int target , vector < int >& nums ) { int n = nums . size (); int res = n + 1 ; int left = 0 ; int sum = 0 ; for ( int i = 0 ; i < n ; ++ i ) { sum += nums [ i ]; while ( sum >= target ) { res = min ( res , i - left + 1 ); sum -= nums [ left ]; left ++ ; } } return res == n + 1 ? 0 : res ; } }; 239. Sliding Window Maximum \u00b6 Solution 1 Use Deque class Solution { public : vector < int > maxSlidingWindow ( vector < int >& nums , int k ) { int n = nums . size (); vector < int > res ; deque < int > deq ; for ( int i = 0 ; i < n ; i ++ ) { // keep the non increasing element in the deque while ( ! deq . empty () && nums [ i ] >= nums [ deq . back ()]) { deq . pop_back (); } deq . push_back ( i ); // i reach to the size of the window, add result if ( i >= k - 1 ) res . push_back ( nums [ deq . front ()]); // using if (not while) because add one element to the deque if ( deq . front () <= i - k + 1 ) deq . pop_front (); // to ensure the window size is k } return res ; } }; 862. Shortest Subarray with Sum at Least K \u00b6 904. Fruit Into Baskets \u00b6 992. Subarrays with K Different Integers \u00b6 930. Binary Subarrays With Sum \u00b6 1004. Max Consecutive Ones III \u00b6 1234. Replace the Substring for Balanced String \u00b6 1248. Count Number of Nice Subarrays \u00b6 1358. Number of Substrings Containing All Three Characters \u00b6 1425. Constrained Subsequence Sum \u00b6 Solution 1 Use ordered data structure to keep the min and max. Solution 2 Use deque to optimize into linear complexity. C++ multiset O(nlogn) class Solution { public : int constrainedSubsetSum ( vector < int >& nums , int k ) { int n = nums . size (); multiset < int > s { INT_MIN }; int res = INT_MIN ; vector < int > dp ( n ); for ( int i = 0 ; i < n ; ++ i ) { if ( i > k ) // note here j - i <= k, len <= k + 1, i - len is // the first legitimate element before ith element s . erase ( s . equal_range ( dp [ i - k - 1 ]). first ); dp [ i ] = max ( * s . rbegin (), 0 ) + nums [ i ]; s . insert ( dp [ i ]); res = max ( res , dp [ i ]); } return res ; } }; C++ monotonic deque O(n) class Solution { public : int constrainedSubsetSum ( vector < int >& nums , int k ) { int n = nums . size (); vector < int > dp ( n ); deque < int > q ; int res = INT_MIN ; for ( int i = 0 ; i < n ; ++ i ) { if ( i > k && q . front () == i - k - 1 ) { q . pop_front (); } dp [ i ] = ( q . empty () ? 0 : max ( dp [ q . front ()], 0 )) + nums [ i ]; // maintain invariant in the sliding window while ( ! q . empty () && dp [ i ] >= dp [ q . back ()]) q . pop_back (); q . push_back ( i ); res = max ( res , dp [ i ]); } return res ; } }; 1438. Longest Continuous Subarray With Absolute Diff Less Than or Equal to Limit \u00b6 Solution 1 Use ordered data structure to keep the min and max. Solution 2 Use deque to optimize into linear complexity. C++ multiset O(nlogn) class Solution { public : int longestSubarray ( vector < int >& nums , int limit ) { multiset < int > mset ; int left = 0 ; int res = 0 ; for ( int i = 0 ; i < nums . size (); ++ i ) { mset . insert ( nums [ i ]); // maintain the invariant while ( * mset . rbegin () - * mset . begin () > limit ) { mset . erase ( mset . equal_range ( nums [ left ++ ]). first ); } res = max ( res , i - left + 1 ); } return res ; } }; C++ deque O(n) class Solution { public : int longestSubarray ( vector < int >& nums , int limit ) { deque < int > min_deq ; deque < int > max_deq ; int left = 0 ; int res = 0 ; // O(n), enque once, deque once. for ( int i = 0 ; i < nums . size (); ++ i ) { // invariant: only keep the min in the deque while ( ! min_deq . empty () && nums [ i ] < min_deq . back ()) min_deq . pop_back (); min_deq . push_back ( nums [ i ]); // invariant: only keep the max in the deque while ( ! max_deq . empty () && nums [ i ] > max_deq . back ()) max_deq . pop_back (); max_deq . push_back ( nums [ i ]); // invariant: move the left point to meet the constrain while ( max_deq . front () - min_deq . front () > limit ) { if ( max_deq . front () == nums [ left ]) max_deq . pop_front (); if ( min_deq . front () == nums [ left ]) min_deq . pop_front (); left ++ ; } // up date the result res = max ( res , i - left + 1 ); } return res ; } };","title":"Sliding Window"},{"location":"leetcode/sliding-window/notes/#sliding-window","text":"","title":"Sliding Window"},{"location":"leetcode/sliding-window/notes/#problem-types","text":"","title":"Problem types"},{"location":"leetcode/sliding-window/notes/#monotonic-deque-to-acheive-on","text":"If we need to find the max/min from a sliding window. The first idea would be use ordered container to store them and then access the max/min. But it usually requires O(nlogn) O(nlogn) or even worse. While we use deque in sliding window problem we can achieve runtime complexity of O(n) O(n) e.g. 1425. Constrained Subsequence Sum , 1438. Longest Continuous Subarray With Absolute Diff Less Than or Equal to Limit The general idea is to use deque to keep only the necessary information. During each iteration, we use the window size and the order info to keep the invariant: All elements that are not possible to become a solution are removed from queue; All elements in the queue form a valid window size, so that we can calculate the solution. Another caveat is when we maintain the invariant, should we use while or use if ? This is realative easy to determine in practice, you just need to verify whether the invariant is maintained correctly. Use if for one operation and while for multiple operations.","title":"Monotonic deque to acheive O(n)"},{"location":"leetcode/sliding-window/notes/#problems","text":"","title":"Problems"},{"location":"leetcode/sliding-window/notes/#209-minimum-size-subarray-sum","text":"Solution 1: Using two pointers class Solution { public : int minSubArrayLen ( int target , vector < int >& nums ) { int n = nums . size (); int res = n + 1 ; int left = 0 ; int sum = 0 ; for ( int i = 0 ; i < n ; ++ i ) { sum += nums [ i ]; while ( sum >= target ) { res = min ( res , i - left + 1 ); sum -= nums [ left ]; left ++ ; } } return res == n + 1 ? 0 : res ; } };","title":"209. Minimum Size Subarray Sum"},{"location":"leetcode/sliding-window/notes/#239-sliding-window-maximum","text":"Solution 1 Use Deque class Solution { public : vector < int > maxSlidingWindow ( vector < int >& nums , int k ) { int n = nums . size (); vector < int > res ; deque < int > deq ; for ( int i = 0 ; i < n ; i ++ ) { // keep the non increasing element in the deque while ( ! deq . empty () && nums [ i ] >= nums [ deq . back ()]) { deq . pop_back (); } deq . push_back ( i ); // i reach to the size of the window, add result if ( i >= k - 1 ) res . push_back ( nums [ deq . front ()]); // using if (not while) because add one element to the deque if ( deq . front () <= i - k + 1 ) deq . pop_front (); // to ensure the window size is k } return res ; } };","title":"239. Sliding Window Maximum"},{"location":"leetcode/sliding-window/notes/#862-shortest-subarray-with-sum-at-least-k","text":"","title":"862. Shortest Subarray with Sum at Least K"},{"location":"leetcode/sliding-window/notes/#904-fruit-into-baskets","text":"","title":"904. Fruit Into Baskets"},{"location":"leetcode/sliding-window/notes/#992-subarrays-with-k-different-integers","text":"","title":"992. Subarrays with K Different Integers"},{"location":"leetcode/sliding-window/notes/#930-binary-subarrays-with-sum","text":"","title":"930. Binary Subarrays With Sum"},{"location":"leetcode/sliding-window/notes/#1004-max-consecutive-ones-iii","text":"","title":"1004. Max Consecutive Ones III"},{"location":"leetcode/sliding-window/notes/#1234-replace-the-substring-for-balanced-string","text":"","title":"1234. Replace the Substring for Balanced String"},{"location":"leetcode/sliding-window/notes/#1248-count-number-of-nice-subarrays","text":"","title":"1248. Count Number of Nice Subarrays"},{"location":"leetcode/sliding-window/notes/#1358-number-of-substrings-containing-all-three-characters","text":"","title":"1358. Number of Substrings Containing All Three Characters"},{"location":"leetcode/sliding-window/notes/#1425-constrained-subsequence-sum","text":"Solution 1 Use ordered data structure to keep the min and max. Solution 2 Use deque to optimize into linear complexity. C++ multiset O(nlogn) class Solution { public : int constrainedSubsetSum ( vector < int >& nums , int k ) { int n = nums . size (); multiset < int > s { INT_MIN }; int res = INT_MIN ; vector < int > dp ( n ); for ( int i = 0 ; i < n ; ++ i ) { if ( i > k ) // note here j - i <= k, len <= k + 1, i - len is // the first legitimate element before ith element s . erase ( s . equal_range ( dp [ i - k - 1 ]). first ); dp [ i ] = max ( * s . rbegin (), 0 ) + nums [ i ]; s . insert ( dp [ i ]); res = max ( res , dp [ i ]); } return res ; } }; C++ monotonic deque O(n) class Solution { public : int constrainedSubsetSum ( vector < int >& nums , int k ) { int n = nums . size (); vector < int > dp ( n ); deque < int > q ; int res = INT_MIN ; for ( int i = 0 ; i < n ; ++ i ) { if ( i > k && q . front () == i - k - 1 ) { q . pop_front (); } dp [ i ] = ( q . empty () ? 0 : max ( dp [ q . front ()], 0 )) + nums [ i ]; // maintain invariant in the sliding window while ( ! q . empty () && dp [ i ] >= dp [ q . back ()]) q . pop_back (); q . push_back ( i ); res = max ( res , dp [ i ]); } return res ; } };","title":"1425. Constrained Subsequence Sum"},{"location":"leetcode/sliding-window/notes/#1438-longest-continuous-subarray-with-absolute-diff-less-than-or-equal-to-limit","text":"Solution 1 Use ordered data structure to keep the min and max. Solution 2 Use deque to optimize into linear complexity. C++ multiset O(nlogn) class Solution { public : int longestSubarray ( vector < int >& nums , int limit ) { multiset < int > mset ; int left = 0 ; int res = 0 ; for ( int i = 0 ; i < nums . size (); ++ i ) { mset . insert ( nums [ i ]); // maintain the invariant while ( * mset . rbegin () - * mset . begin () > limit ) { mset . erase ( mset . equal_range ( nums [ left ++ ]). first ); } res = max ( res , i - left + 1 ); } return res ; } }; C++ deque O(n) class Solution { public : int longestSubarray ( vector < int >& nums , int limit ) { deque < int > min_deq ; deque < int > max_deq ; int left = 0 ; int res = 0 ; // O(n), enque once, deque once. for ( int i = 0 ; i < nums . size (); ++ i ) { // invariant: only keep the min in the deque while ( ! min_deq . empty () && nums [ i ] < min_deq . back ()) min_deq . pop_back (); min_deq . push_back ( nums [ i ]); // invariant: only keep the max in the deque while ( ! max_deq . empty () && nums [ i ] > max_deq . back ()) max_deq . pop_back (); max_deq . push_back ( nums [ i ]); // invariant: move the left point to meet the constrain while ( max_deq . front () - min_deq . front () > limit ) { if ( max_deq . front () == nums [ left ]) max_deq . pop_front (); if ( min_deq . front () == nums [ left ]) min_deq . pop_front (); left ++ ; } // up date the result res = max ( res , i - left + 1 ); } return res ; } };","title":"1438. Longest Continuous Subarray With Absolute Diff Less Than or Equal to Limit"},{"location":"leetcode/stack/notes/","text":"Stack \u00b6 Key problem types \u00b6 monotonic stack \u00b6 It can be monotonic increase or decreasing based on application. The data structure is used to trade off space for time (to achieve O(N) complexity). The iteration can be forward and backward, there is no essential diference, but can change the way to think of the problem. The core is to maintain the invariant and do the work at the right moment . The invariant is to keep the element ordered in the stack in strict increasing/descreasing order. The work is done at either during the element is popped or after popped. From the [Next Greater Element I] and [Next Greater Element II], we can see that no matter the forward or backword iteration, the stack values are strictly decreasing. check whether you need to push the index or the value. You can put both if needed. To simplify the code, we can use sentinel values to eliminate redundent codes. monotonic stack cheat sheet \u00b6 You can iterate the array both forward or backword, forward iteration is simpler. You can push the index to the stack or simly push the array value. Usually, when we need to meet index range constrain, we have to push the index to the stack. Be careful about the comparasion between the current value and the stack top, Make sure choose the correct operators ( > > or >= >= , < < or <= <= ). Sometimes you need to look more than the stack top, for example Trapping Rain Water . Problems \u00b6 42. Trapping Rain Water \u00b6 Solution 1 monotonic Stack Solution 2 scan the tallest from left and tallest from right, then canculate. C++ monotonic stack class Solution { public : int trap ( vector < int >& height ) { int n = height . size (); int res = 0 ; stack < int > s ; for ( int i = 0 ; i < n ; ++ i ) { while ( ! s . empty () && height [ s . top ()] < height [ i ]) { int curr = height [ s . top ()]; s . pop (); int prev = s . empty () ? 0 : height [ s . top ()]; int h = min ( prev , height [ i ]) - curr ; int w = s . empty () ? 0 : i - s . top () - 1 ; res += h * w ; } s . push ( i ); } return res ; } }; C++ Naive class Solution { public : int trap ( vector < int >& height ) { int n = height . size (); vector < int > left ( n , 0 ); vector < int > right ( n , 0 ); if ( n < 1 ) return 0 ; left [ 0 ] = height [ 0 ]; for ( int i = 1 ; i < n ; ++ i ) { left [ i ] = max ( left [ i - 1 ], height [ i ]); } right [ n - 1 ] = height [ n - 1 ]; for ( int i = n - 2 ; i >= 0 ; -- i ) { right [ i ] = max ( right [ i + 1 ], height [ i ]); } int res = 0 ; for ( int i = 0 ; i < n ; ++ i ) { int h = min ( left [ i ], right [ i ]) - height [ i ]; res += h > 0 ? h : 0 ; } return res ; } }; 84. Largest Rectangle in Histogram \u00b6 Solution 1 Scan to left and right from each index Solution 2 Using monotonous increase stack C++ naive solution class Solution { public : int largestRectangleArea ( vector < int >& heights ) { int n = heights . size (); vector < int > left ( n ); vector < int > right ( n ); int res = 0 ; for ( int i = 0 ; i < n ; ++ i ) { int j = i ; while ( j > 0 && heights [ i ] <= heights [ j - 1 ]) { j = left [ j - 1 ]; } left [ i ] = j ; j = i ; while ( j < n - 1 && heights [ i ] <= heights [ j + 1 ]) { j = right [ j + 1 ]; } right [ i ] = j ; } for ( int i = 0 ; i < n ; ++ i ) { res = max ( res , heights [ i ] * ( right [ i ] - left [ i ] + 1 )); } return res ; } }; C++ monotonous stack class Solution { public : int largestRectangleArea ( vector < int >& heights ) { int n = heights . size (); stack < int > s ; int res = 0 ; for ( int i = 0 ; i < n ; ++ i ) { int height = 0 ; while ( ! s . empty () && heights [ i ] < heights [ s . top ()]) { // calculate the area determined by the stack top hist // when pop it off the stack int height = heights [ s . top ()]; s . pop (); int width = 0 ; if ( ! s . empty ()) { width = i - s . top () - 1 ; } else { width = i ; } res = max ( res , height * width ); } s . push ( i ); } // pop the rest of the elemnt off the stack while ( ! s . empty ()) { int height = heights [ s . top ()]; s . pop (); int width = 0 ; if ( ! s . empty ()) { width = n - s . top () - 1 ; } else { width = n ; } res = max ( res , height * width ); } return res ; } }; C++ Use monotonic stack with sentinel class Solution { public : int largestRectangleArea ( vector < int >& heights ) { int n = heights . size (); stack < int > s ; int res = 0 ; vector < int > hist ; // add sentinel element at begining and end heights . insert ( heights . begin (), 0 ); heights . push_back ( 0 ); s . push ( 0 ); for ( int i = 1 ; i < n + 2 ; ++ i ) { int height = 0 ; while ( heights [ s . top ()] > heights [ i ]) { // calculate the area determined by the stack top hist // when pop it off the stack int height = heights [ s . top ()]; s . pop (); int width = i - s . top () - 1 ; res = max ( res , height * width ); } s . push ( i ); } return res ; } }; 85. Maximal Rectangle \u00b6 Solution 1 use monotonic stack. We can treat it as a 2D version of 84. Largest Rectangle in Histogram C++ monotonic stack class Solution { public : int getMaxArea ( vector < int > v ) { int n = v . size (); int res = 0 ; stack < int > s ; for ( int i = 0 ; i < n ; ++ i ) { while ( ! s . empty () && v [ s . top ()] > v [ i ]) { int height = v [ s . top ()]; s . pop (); int width = s . empty () ? i : i - s . top () - 1 ; res = max ( res , height * width ); } s . push ( i ); } // pop the rest of the elements while ( ! s . empty ()) { int height = v [ s . top ()]; s . pop (); int width = s . empty () ? n : n - s . top () - 1 ; res = max ( res , height * width ); } return res ; } int maximalRectangle ( vector < vector < char >>& matrix ) { int m = matrix . size (); int n = m == 0 ? 0 : matrix [ 0 ]. size (); int res = 0 ; vector < int > v ( n , 0 ); // aggregate vertically then canculte area. for ( int r = 0 ; r < m ; ++ r ) { for ( int c = 0 ; c < n ; ++ c ) { if ( matrix [ r ][ c ] == '1' ) { v [ c ] ++ ; } else { v [ c ] = 0 ; } } res = max ( res , getMaxArea ( v )); } return res ; } }; C++ Monotonic stack use a sentinel at end class Solution { public : int getMaxArea ( vector < int > v ) { int n = v . size (); int res = 0 ; stack < int > s ; v . push_back ( -1 ); for ( int i = 0 ; i <= n ; ++ i ) { while ( ! s . empty () && v [ s . top ()] > v [ i ]) { int height = v [ s . top ()]; s . pop (); int width = s . empty () ? i : i - s . top () - 1 ; res = max ( res , height * width ); } s . push ( i ); } return res ; } int maximalRectangle ( vector < vector < char >>& matrix ) { int m = matrix . size (); int n = m == 0 ? 0 : matrix [ 0 ]. size (); int res = 0 ; vector < int > v ( n , 0 ); // aggregate vertically then canculte area. for ( int r = 0 ; r < m ; ++ r ) { for ( int c = 0 ; c < n ; ++ c ) { if ( matrix [ r ][ c ] == '1' ) { v [ c ] ++ ; } else { v [ c ] = 0 ; } } res = max ( res , getMaxArea ( v )); } return res ; } }; 221. Maximal Square \u00b6 class Solution { public : int maximalSquare ( vector < vector < char >>& matrix ) { int m = matrix . size (); int n = m == 0 ? 0 : matrix [ 0 ]. size (); int res = 0 ; vector < int > v ( n , 0 ); for ( int i = 0 ; i < m ; ++ i ) { for ( int j = 0 ; j < n ; ++ j ) { if ( matrix [ i ][ j ] == '1' ) { v [ j ] ++ ; } else { v [ j ] = 0 ; } } res = max ( res , maxArea ( v )); } return res ; } int maxArea ( vector < int > v ) { int n = v . size (); int res = 0 ; stack < int > s ; v . insert ( v . begin (), 0 ); v . push_back ( 0 ); for ( int i = 0 ; i < n + 2 ; ++ i ) { while ( ! s . empty () && v [ s . top ()] > v [ i ]) { int h = v [ s . top ()]; s . pop (); int w = s . empty () ? i : i - s . top () - 1 ; int d = min ( h , w ); res = max ( res , d * d ); } s . push ( i ); } return res ; } }; 496. Next Greater Element I \u00b6 A naive solution will be to use a map to record the index of an element, then iterate the nums1 , find each element in nums2 , then scan right to get the next greater element. To improve the time complexity, we can use binary search to find the next greater element The best solution will be using a stack to find all the greater element for all nums2 elements and store them in a map, then lookup the elements in num1 from the map. C++ monotonic stack forward class Solution { public : vector < int > nextGreaterElement ( vector < int >& nums1 , vector < int >& nums2 ) { unordered_map < int , int > nextG ; vector < int > res ; stack < int > s ; for ( int i = 0 ; i < nums2 . size (); ++ i ) { // decreasing stack while ( ! s . empty () && s . top () < nums2 [ i ]) { nextG [ s . top ()] = nums2 [ i ]; s . pop (); } s . push ( nums2 [ i ]); } // clear the stack while ( ! s . empty ()) { nextG [ s . top ()] = -1 ; s . pop (); } for ( int i = 0 ; i < nums1 . size (); ++ i ) { res . push_back ( nextG [ nums1 [ i ]]); } return res ; } }; C++ monotonic stack backward class Solution { public : vector < int > nextGreaterElement ( vector < int >& nums1 , vector < int >& nums2 ) { unordered_map < int , int > nextG ; stack < int > s ; for ( int i = nums2 . size () - 1 ; i >= 0 ; -- i ) { while ( ! s . empty () && s . top () <= nums2 [ i ]) { s . pop (); } // next greater element after ith element nextG [ nums2 [ i ]] = s . empty () ? -1 : s . top (); s . push ( nums2 [ i ]); } vector < int > res ; for ( int i = 0 ; i < nums1 . size (); ++ i ) { res . push_back ( nextG [ nums1 [ i ]]); } return res ; } }; 503. Next Greater Element II \u00b6 C++ monotonic stack backward class Solution { public : vector < int > nextGreaterElements ( vector < int >& nums ) { int n = nums . size (); stack < int > s ; vector < int > res ( n , -1 ); for ( int i = 0 ; i < 2 * n ; ++ i ) { while ( ! s . empty () && nums [ i % n ] > nums [ s . top ()]) { res [ s . top ()] = nums [ i % n ]; s . pop (); } // notice the index is pushed to the stack s . push ( i % n ); } return res ; } }; C++ monotonic decreasing stack class Solution { public : vector < int > nextGreaterElements ( vector < int >& nums ) { int n = nums . size (); stack < int > s ; vector < int > res ( n , -1 ); for ( int i = 2 * n - 1 ; i > 0 ; -- i ) { while ( ! s . empty () && nums [ i % n ] >= nums [ s . top ()]) { s . pop (); } res [ i % n ] = s . empty () ? -1 : nums [ s . top ()]; s . push ( i % n ); } return res ; } }; C++ Naive class Solution { public : vector < int > nextGreaterElements ( vector < int >& nums ) { int n = nums . size (); vector < int > res ; for ( int i = 0 ; i < n ; i ++ ) { int j = ( i + 1 ) % n ; int flag = 0 ; while ( j != i ) { if ( nums [ j ] > nums [ i ]) { res . push_back ( nums [ j ]); flag = 1 ; break ; } j ++ ; j %= n ; // circular interate } if ( ! flag ) { res . push_back ( -1 ); } } return res ; } }; 901. Online Stock Span \u00b6 Solution 1 Monotonic stack Iterate left to right, keep the current result together with the element in the stack (pop each element meet the constrain off the stack) The stack have strict decreasing values class StockSpanner { stack < pair < int , int >> s ; int count = 0 ; public : StockSpanner () { } int next ( int price ) { count = 1 ; while ( ! s . empty () && s . top (). first <= price ) { count += s . top (). second ; s . pop (); } s . push ({ price , count }); return count ; } }; 739. Daily Temperatures \u00b6 C++ monotonic stack class Solution { public : vector < int > dailyTemperatures ( vector < int >& T ) { int n = T . size (); vector < int > res ( n , 0 ); stack < int > s ; for ( int i = 0 ; i < n ; ++ i ) { while ( ! s . empty () && T [ i ] > T [ s . top ()]) { res [ s . top ()] = i - s . top (); s . pop (); } s . push ( i ); } return res ; } }; 907. Sum of Subarray Minimums \u00b6 Solution 1 monotonous increase stack C++ monotonic stack class Solution { public : int sumSubarrayMins ( vector < int >& A ) { stack < pair < int , int >> s1 , s2 ; vector < int > left ( A . size ()), right ( A . size ()); // distance between next smaller element on the left for ( int i = 0 ; i < A . size (); i ++ ) { int count = 1 ; // maintains a monotonic increase stack while ( ! s1 . empty () && s1 . top (). first > A [ i ]) { count += s1 . top (). second ; s1 . pop (); } s1 . push ({ A [ i ], count }); left [ i ] = count ; } for ( int i = A . size () - 1 ; i >= 0 ; i -- ) { int count = 1 ; while ( ! s2 . empty () && s2 . top (). first >= A [ i ]) { count += s2 . top (). second ; s2 . pop (); } s2 . push ({ A [ i ], count }); right [ i ] = count ; } int res = 0 ; int MOD = 1e9 + 7 ; for ( int i = 0 ; i < A . size (); i ++ ) { res = ( res + A [ i ] * left [ i ] * right [ i ]) % MOD ; } return res ; } }; Java monotonic stack class Solution { public : int sumSubarrayMins ( vector < int >& arr ) { int n = arr . size (); stack < int > s ; int res = 0 ; int MOD = 1e9 + 7 ; for ( int i = 0 ; i <= n ; ++ i ) { while ( ! s . empty () && ( i == n || arr [ s . top ()] > arr [ i ])) { int m = s . top (); s . pop (); int l = s . empty () ? m + 1 : m - s . top (); int r = i - m ; res += arr [ m ] * l * r ; } s . push ( i ); } return res % MOD ; } }; Java monotonic stack public int sumSubarrayMins ( int [] A ) { long res = 0 ; Deque < Integer > stack = new ArrayDeque <> (); for ( int i = 0 ; i <= A . length ; i ++ ) { while ( ! stack . isEmpty () && ( i == A . length || A [ stack . peek () ] > A [ i ] )) { int mid = stack . pop (); int L = mid - ( stack . isEmpty () ? - 1 : stack . peek ()); int R = i - mid ; res += ( long ) A [ mid ] * L * R ; } stack . push ( i ); } return ( int ) ( res % 1_000_000_007 ); } 1019. Next Greater Node In Linked List \u00b6 C++ monotonic stack /** * Definition for singly-linked list. * struct ListNode { * int val; * ListNode *next; * ListNode() : val(0), next(nullptr) {} * ListNode(int x) : val(x), next(nullptr) {} * ListNode(int x, ListNode *next) : val(x), next(next) {} * }; */ class Solution { public : vector < int > nextLargerNodes ( ListNode * head ) { stack < int > s , idx ; ListNode * curr = head ; int len = 0 ; while ( curr ) { len ++ ; curr = curr -> next ; } int i = 0 ; vector < int > res ( len , 0 ); curr = head ; while ( curr != nullptr ) { int val = curr -> val ; while ( ! s . empty () && val > s . top ()) { res [ idx . top ()] = val ; s . pop (), idx . pop (); } s . push ( val ), idx . push ( i ); curr = curr -> next ; i ++ ; } return res ; } }; 402. Remove K Digits \u00b6 Solution 1 using monotonic stack. Think of how to remove the lagest digit from left to right? Notice the speical case: 10200 . class Solution { public : string removeKdigits ( string num , int k ) { int n = num . length (); int m = n - k ; string res = \"\" ; for ( int i = 0 ; i < n ; ++ i ) { while ( k && ! res . empty () && res . back () > num [ i ]) { res . pop_back (); -- k ; } res . push_back ( num [ i ]); } res . resize ( m ); while ( ! res . empty () && res [ 0 ] == '0' ) { res . erase ( res . begin ()); } return res . empty () ? \"0\" : res ; } }; 1130. Minimum Cost Tree From Leaf Values \u00b6 1425. Constrained Subsequence Sum \u00b6 1776. Car Fleet II \u00b6 Solution I Monotonic stack Notice in this problem the invariant is much more complicated. Beside the order (so that the cars can collide), you also have to make sure the next car haven't collide ealier to some lower speed, you have to look at the global optimal solution. For example, car_1, car_2, car_3, when you look at only car_1 and car_2, you may find that they are going to collide at t1, but car_2 and car_3 could collide much erlier, the correct solution could be smaller than t1. C++ monotonic stack class Solution { public : vector < double > getCollisionTimes ( vector < vector < int >>& cars ) { auto collideT = [ & ]( int i , int j ) -> double { // cars[i][0] < cars[j][0], cars[i][1] > cars[j][0] return static_cast < double > ( cars [ j ][ 0 ] - cars [ i ][ 0 ]) / ( cars [ i ][ 1 ] - cars [ j ][ 1 ]); }; int n = cars . size (); stack < int > s ; vector < double > res ( n , -1 ); for ( int i = n - 1 ; i >= 0 ; -- i ) { // not only you have to be fast to catch next car, // but also the next car have not collided before, if it collided before // you have to look for the slower cars (which could be collided much ealier) while ( ! s . empty () && ( cars [ i ][ 1 ] <= cars [ s . top ()][ 1 ] || ( s . size () > 1 && collideT ( i , s . top ()) > res [ s . top ()]))) { s . pop (); } res [ i ] = s . empty () ? -1 : collideT ( i , s . top ()); s . push ( i ); } return res ; } }; C++ Brute force class Solution { public : vector < double > getCollisionTimes ( vector < vector < int >>& cars ) { int n = cars . size (); vector < double > ans ( n , -1.0 ); for ( int i = n - 1 ; i >= 0 ; -- i ) { int p = cars [ i ][ 0 ]; int v = cars [ i ][ 1 ]; for ( int j = i - 1 ; j >= 0 ; -- j ) { int p_j = cars [ j ][ 0 ]; int v_j = cars [ j ][ 1 ]; int dp = p - p_j ; int dv = v_j - v ; if ( dv <= 0 ) break ; double t = ( double ) dp / dv ; if ( ans [ j ] < 0 || t < ans [ j ]) ans [ j ] = t ; else break ; } } return ans ; } };","title":"Stack"},{"location":"leetcode/stack/notes/#stack","text":"","title":"Stack"},{"location":"leetcode/stack/notes/#key-problem-types","text":"","title":"Key problem types"},{"location":"leetcode/stack/notes/#monotonic-stack","text":"It can be monotonic increase or decreasing based on application. The data structure is used to trade off space for time (to achieve O(N) complexity). The iteration can be forward and backward, there is no essential diference, but can change the way to think of the problem. The core is to maintain the invariant and do the work at the right moment . The invariant is to keep the element ordered in the stack in strict increasing/descreasing order. The work is done at either during the element is popped or after popped. From the [Next Greater Element I] and [Next Greater Element II], we can see that no matter the forward or backword iteration, the stack values are strictly decreasing. check whether you need to push the index or the value. You can put both if needed. To simplify the code, we can use sentinel values to eliminate redundent codes.","title":"monotonic stack"},{"location":"leetcode/stack/notes/#monotonic-stack-cheat-sheet","text":"You can iterate the array both forward or backword, forward iteration is simpler. You can push the index to the stack or simly push the array value. Usually, when we need to meet index range constrain, we have to push the index to the stack. Be careful about the comparasion between the current value and the stack top, Make sure choose the correct operators ( > > or >= >= , < < or <= <= ). Sometimes you need to look more than the stack top, for example Trapping Rain Water .","title":"monotonic stack cheat sheet"},{"location":"leetcode/stack/notes/#problems","text":"","title":"Problems"},{"location":"leetcode/stack/notes/#42-trapping-rain-water","text":"Solution 1 monotonic Stack Solution 2 scan the tallest from left and tallest from right, then canculate. C++ monotonic stack class Solution { public : int trap ( vector < int >& height ) { int n = height . size (); int res = 0 ; stack < int > s ; for ( int i = 0 ; i < n ; ++ i ) { while ( ! s . empty () && height [ s . top ()] < height [ i ]) { int curr = height [ s . top ()]; s . pop (); int prev = s . empty () ? 0 : height [ s . top ()]; int h = min ( prev , height [ i ]) - curr ; int w = s . empty () ? 0 : i - s . top () - 1 ; res += h * w ; } s . push ( i ); } return res ; } }; C++ Naive class Solution { public : int trap ( vector < int >& height ) { int n = height . size (); vector < int > left ( n , 0 ); vector < int > right ( n , 0 ); if ( n < 1 ) return 0 ; left [ 0 ] = height [ 0 ]; for ( int i = 1 ; i < n ; ++ i ) { left [ i ] = max ( left [ i - 1 ], height [ i ]); } right [ n - 1 ] = height [ n - 1 ]; for ( int i = n - 2 ; i >= 0 ; -- i ) { right [ i ] = max ( right [ i + 1 ], height [ i ]); } int res = 0 ; for ( int i = 0 ; i < n ; ++ i ) { int h = min ( left [ i ], right [ i ]) - height [ i ]; res += h > 0 ? h : 0 ; } return res ; } };","title":"42. Trapping Rain Water"},{"location":"leetcode/stack/notes/#84-largest-rectangle-in-histogram","text":"Solution 1 Scan to left and right from each index Solution 2 Using monotonous increase stack C++ naive solution class Solution { public : int largestRectangleArea ( vector < int >& heights ) { int n = heights . size (); vector < int > left ( n ); vector < int > right ( n ); int res = 0 ; for ( int i = 0 ; i < n ; ++ i ) { int j = i ; while ( j > 0 && heights [ i ] <= heights [ j - 1 ]) { j = left [ j - 1 ]; } left [ i ] = j ; j = i ; while ( j < n - 1 && heights [ i ] <= heights [ j + 1 ]) { j = right [ j + 1 ]; } right [ i ] = j ; } for ( int i = 0 ; i < n ; ++ i ) { res = max ( res , heights [ i ] * ( right [ i ] - left [ i ] + 1 )); } return res ; } }; C++ monotonous stack class Solution { public : int largestRectangleArea ( vector < int >& heights ) { int n = heights . size (); stack < int > s ; int res = 0 ; for ( int i = 0 ; i < n ; ++ i ) { int height = 0 ; while ( ! s . empty () && heights [ i ] < heights [ s . top ()]) { // calculate the area determined by the stack top hist // when pop it off the stack int height = heights [ s . top ()]; s . pop (); int width = 0 ; if ( ! s . empty ()) { width = i - s . top () - 1 ; } else { width = i ; } res = max ( res , height * width ); } s . push ( i ); } // pop the rest of the elemnt off the stack while ( ! s . empty ()) { int height = heights [ s . top ()]; s . pop (); int width = 0 ; if ( ! s . empty ()) { width = n - s . top () - 1 ; } else { width = n ; } res = max ( res , height * width ); } return res ; } }; C++ Use monotonic stack with sentinel class Solution { public : int largestRectangleArea ( vector < int >& heights ) { int n = heights . size (); stack < int > s ; int res = 0 ; vector < int > hist ; // add sentinel element at begining and end heights . insert ( heights . begin (), 0 ); heights . push_back ( 0 ); s . push ( 0 ); for ( int i = 1 ; i < n + 2 ; ++ i ) { int height = 0 ; while ( heights [ s . top ()] > heights [ i ]) { // calculate the area determined by the stack top hist // when pop it off the stack int height = heights [ s . top ()]; s . pop (); int width = i - s . top () - 1 ; res = max ( res , height * width ); } s . push ( i ); } return res ; } };","title":"84. Largest Rectangle in Histogram"},{"location":"leetcode/stack/notes/#85-maximal-rectangle","text":"Solution 1 use monotonic stack. We can treat it as a 2D version of 84. Largest Rectangle in Histogram C++ monotonic stack class Solution { public : int getMaxArea ( vector < int > v ) { int n = v . size (); int res = 0 ; stack < int > s ; for ( int i = 0 ; i < n ; ++ i ) { while ( ! s . empty () && v [ s . top ()] > v [ i ]) { int height = v [ s . top ()]; s . pop (); int width = s . empty () ? i : i - s . top () - 1 ; res = max ( res , height * width ); } s . push ( i ); } // pop the rest of the elements while ( ! s . empty ()) { int height = v [ s . top ()]; s . pop (); int width = s . empty () ? n : n - s . top () - 1 ; res = max ( res , height * width ); } return res ; } int maximalRectangle ( vector < vector < char >>& matrix ) { int m = matrix . size (); int n = m == 0 ? 0 : matrix [ 0 ]. size (); int res = 0 ; vector < int > v ( n , 0 ); // aggregate vertically then canculte area. for ( int r = 0 ; r < m ; ++ r ) { for ( int c = 0 ; c < n ; ++ c ) { if ( matrix [ r ][ c ] == '1' ) { v [ c ] ++ ; } else { v [ c ] = 0 ; } } res = max ( res , getMaxArea ( v )); } return res ; } }; C++ Monotonic stack use a sentinel at end class Solution { public : int getMaxArea ( vector < int > v ) { int n = v . size (); int res = 0 ; stack < int > s ; v . push_back ( -1 ); for ( int i = 0 ; i <= n ; ++ i ) { while ( ! s . empty () && v [ s . top ()] > v [ i ]) { int height = v [ s . top ()]; s . pop (); int width = s . empty () ? i : i - s . top () - 1 ; res = max ( res , height * width ); } s . push ( i ); } return res ; } int maximalRectangle ( vector < vector < char >>& matrix ) { int m = matrix . size (); int n = m == 0 ? 0 : matrix [ 0 ]. size (); int res = 0 ; vector < int > v ( n , 0 ); // aggregate vertically then canculte area. for ( int r = 0 ; r < m ; ++ r ) { for ( int c = 0 ; c < n ; ++ c ) { if ( matrix [ r ][ c ] == '1' ) { v [ c ] ++ ; } else { v [ c ] = 0 ; } } res = max ( res , getMaxArea ( v )); } return res ; } };","title":"85. Maximal Rectangle"},{"location":"leetcode/stack/notes/#221-maximal-square","text":"class Solution { public : int maximalSquare ( vector < vector < char >>& matrix ) { int m = matrix . size (); int n = m == 0 ? 0 : matrix [ 0 ]. size (); int res = 0 ; vector < int > v ( n , 0 ); for ( int i = 0 ; i < m ; ++ i ) { for ( int j = 0 ; j < n ; ++ j ) { if ( matrix [ i ][ j ] == '1' ) { v [ j ] ++ ; } else { v [ j ] = 0 ; } } res = max ( res , maxArea ( v )); } return res ; } int maxArea ( vector < int > v ) { int n = v . size (); int res = 0 ; stack < int > s ; v . insert ( v . begin (), 0 ); v . push_back ( 0 ); for ( int i = 0 ; i < n + 2 ; ++ i ) { while ( ! s . empty () && v [ s . top ()] > v [ i ]) { int h = v [ s . top ()]; s . pop (); int w = s . empty () ? i : i - s . top () - 1 ; int d = min ( h , w ); res = max ( res , d * d ); } s . push ( i ); } return res ; } };","title":"221. Maximal Square"},{"location":"leetcode/stack/notes/#496-next-greater-element-i","text":"A naive solution will be to use a map to record the index of an element, then iterate the nums1 , find each element in nums2 , then scan right to get the next greater element. To improve the time complexity, we can use binary search to find the next greater element The best solution will be using a stack to find all the greater element for all nums2 elements and store them in a map, then lookup the elements in num1 from the map. C++ monotonic stack forward class Solution { public : vector < int > nextGreaterElement ( vector < int >& nums1 , vector < int >& nums2 ) { unordered_map < int , int > nextG ; vector < int > res ; stack < int > s ; for ( int i = 0 ; i < nums2 . size (); ++ i ) { // decreasing stack while ( ! s . empty () && s . top () < nums2 [ i ]) { nextG [ s . top ()] = nums2 [ i ]; s . pop (); } s . push ( nums2 [ i ]); } // clear the stack while ( ! s . empty ()) { nextG [ s . top ()] = -1 ; s . pop (); } for ( int i = 0 ; i < nums1 . size (); ++ i ) { res . push_back ( nextG [ nums1 [ i ]]); } return res ; } }; C++ monotonic stack backward class Solution { public : vector < int > nextGreaterElement ( vector < int >& nums1 , vector < int >& nums2 ) { unordered_map < int , int > nextG ; stack < int > s ; for ( int i = nums2 . size () - 1 ; i >= 0 ; -- i ) { while ( ! s . empty () && s . top () <= nums2 [ i ]) { s . pop (); } // next greater element after ith element nextG [ nums2 [ i ]] = s . empty () ? -1 : s . top (); s . push ( nums2 [ i ]); } vector < int > res ; for ( int i = 0 ; i < nums1 . size (); ++ i ) { res . push_back ( nextG [ nums1 [ i ]]); } return res ; } };","title":"496. Next Greater Element I"},{"location":"leetcode/stack/notes/#503-next-greater-element-ii","text":"C++ monotonic stack backward class Solution { public : vector < int > nextGreaterElements ( vector < int >& nums ) { int n = nums . size (); stack < int > s ; vector < int > res ( n , -1 ); for ( int i = 0 ; i < 2 * n ; ++ i ) { while ( ! s . empty () && nums [ i % n ] > nums [ s . top ()]) { res [ s . top ()] = nums [ i % n ]; s . pop (); } // notice the index is pushed to the stack s . push ( i % n ); } return res ; } }; C++ monotonic decreasing stack class Solution { public : vector < int > nextGreaterElements ( vector < int >& nums ) { int n = nums . size (); stack < int > s ; vector < int > res ( n , -1 ); for ( int i = 2 * n - 1 ; i > 0 ; -- i ) { while ( ! s . empty () && nums [ i % n ] >= nums [ s . top ()]) { s . pop (); } res [ i % n ] = s . empty () ? -1 : nums [ s . top ()]; s . push ( i % n ); } return res ; } }; C++ Naive class Solution { public : vector < int > nextGreaterElements ( vector < int >& nums ) { int n = nums . size (); vector < int > res ; for ( int i = 0 ; i < n ; i ++ ) { int j = ( i + 1 ) % n ; int flag = 0 ; while ( j != i ) { if ( nums [ j ] > nums [ i ]) { res . push_back ( nums [ j ]); flag = 1 ; break ; } j ++ ; j %= n ; // circular interate } if ( ! flag ) { res . push_back ( -1 ); } } return res ; } };","title":"503. Next Greater Element II"},{"location":"leetcode/stack/notes/#901-online-stock-span","text":"Solution 1 Monotonic stack Iterate left to right, keep the current result together with the element in the stack (pop each element meet the constrain off the stack) The stack have strict decreasing values class StockSpanner { stack < pair < int , int >> s ; int count = 0 ; public : StockSpanner () { } int next ( int price ) { count = 1 ; while ( ! s . empty () && s . top (). first <= price ) { count += s . top (). second ; s . pop (); } s . push ({ price , count }); return count ; } };","title":"901. Online Stock Span"},{"location":"leetcode/stack/notes/#739-daily-temperatures","text":"C++ monotonic stack class Solution { public : vector < int > dailyTemperatures ( vector < int >& T ) { int n = T . size (); vector < int > res ( n , 0 ); stack < int > s ; for ( int i = 0 ; i < n ; ++ i ) { while ( ! s . empty () && T [ i ] > T [ s . top ()]) { res [ s . top ()] = i - s . top (); s . pop (); } s . push ( i ); } return res ; } };","title":"739. Daily Temperatures"},{"location":"leetcode/stack/notes/#907-sum-of-subarray-minimums","text":"Solution 1 monotonous increase stack C++ monotonic stack class Solution { public : int sumSubarrayMins ( vector < int >& A ) { stack < pair < int , int >> s1 , s2 ; vector < int > left ( A . size ()), right ( A . size ()); // distance between next smaller element on the left for ( int i = 0 ; i < A . size (); i ++ ) { int count = 1 ; // maintains a monotonic increase stack while ( ! s1 . empty () && s1 . top (). first > A [ i ]) { count += s1 . top (). second ; s1 . pop (); } s1 . push ({ A [ i ], count }); left [ i ] = count ; } for ( int i = A . size () - 1 ; i >= 0 ; i -- ) { int count = 1 ; while ( ! s2 . empty () && s2 . top (). first >= A [ i ]) { count += s2 . top (). second ; s2 . pop (); } s2 . push ({ A [ i ], count }); right [ i ] = count ; } int res = 0 ; int MOD = 1e9 + 7 ; for ( int i = 0 ; i < A . size (); i ++ ) { res = ( res + A [ i ] * left [ i ] * right [ i ]) % MOD ; } return res ; } }; Java monotonic stack class Solution { public : int sumSubarrayMins ( vector < int >& arr ) { int n = arr . size (); stack < int > s ; int res = 0 ; int MOD = 1e9 + 7 ; for ( int i = 0 ; i <= n ; ++ i ) { while ( ! s . empty () && ( i == n || arr [ s . top ()] > arr [ i ])) { int m = s . top (); s . pop (); int l = s . empty () ? m + 1 : m - s . top (); int r = i - m ; res += arr [ m ] * l * r ; } s . push ( i ); } return res % MOD ; } }; Java monotonic stack public int sumSubarrayMins ( int [] A ) { long res = 0 ; Deque < Integer > stack = new ArrayDeque <> (); for ( int i = 0 ; i <= A . length ; i ++ ) { while ( ! stack . isEmpty () && ( i == A . length || A [ stack . peek () ] > A [ i ] )) { int mid = stack . pop (); int L = mid - ( stack . isEmpty () ? - 1 : stack . peek ()); int R = i - mid ; res += ( long ) A [ mid ] * L * R ; } stack . push ( i ); } return ( int ) ( res % 1_000_000_007 ); }","title":"907. Sum of Subarray Minimums"},{"location":"leetcode/stack/notes/#1019-next-greater-node-in-linked-list","text":"C++ monotonic stack /** * Definition for singly-linked list. * struct ListNode { * int val; * ListNode *next; * ListNode() : val(0), next(nullptr) {} * ListNode(int x) : val(x), next(nullptr) {} * ListNode(int x, ListNode *next) : val(x), next(next) {} * }; */ class Solution { public : vector < int > nextLargerNodes ( ListNode * head ) { stack < int > s , idx ; ListNode * curr = head ; int len = 0 ; while ( curr ) { len ++ ; curr = curr -> next ; } int i = 0 ; vector < int > res ( len , 0 ); curr = head ; while ( curr != nullptr ) { int val = curr -> val ; while ( ! s . empty () && val > s . top ()) { res [ idx . top ()] = val ; s . pop (), idx . pop (); } s . push ( val ), idx . push ( i ); curr = curr -> next ; i ++ ; } return res ; } };","title":"1019. Next Greater Node In Linked List"},{"location":"leetcode/stack/notes/#402-remove-k-digits","text":"Solution 1 using monotonic stack. Think of how to remove the lagest digit from left to right? Notice the speical case: 10200 . class Solution { public : string removeKdigits ( string num , int k ) { int n = num . length (); int m = n - k ; string res = \"\" ; for ( int i = 0 ; i < n ; ++ i ) { while ( k && ! res . empty () && res . back () > num [ i ]) { res . pop_back (); -- k ; } res . push_back ( num [ i ]); } res . resize ( m ); while ( ! res . empty () && res [ 0 ] == '0' ) { res . erase ( res . begin ()); } return res . empty () ? \"0\" : res ; } };","title":"402. Remove K Digits"},{"location":"leetcode/stack/notes/#1130-minimum-cost-tree-from-leaf-values","text":"","title":"1130. Minimum Cost Tree From Leaf Values"},{"location":"leetcode/stack/notes/#1425-constrained-subsequence-sum","text":"","title":"1425. Constrained Subsequence Sum"},{"location":"leetcode/stack/notes/#1776-car-fleet-ii","text":"Solution I Monotonic stack Notice in this problem the invariant is much more complicated. Beside the order (so that the cars can collide), you also have to make sure the next car haven't collide ealier to some lower speed, you have to look at the global optimal solution. For example, car_1, car_2, car_3, when you look at only car_1 and car_2, you may find that they are going to collide at t1, but car_2 and car_3 could collide much erlier, the correct solution could be smaller than t1. C++ monotonic stack class Solution { public : vector < double > getCollisionTimes ( vector < vector < int >>& cars ) { auto collideT = [ & ]( int i , int j ) -> double { // cars[i][0] < cars[j][0], cars[i][1] > cars[j][0] return static_cast < double > ( cars [ j ][ 0 ] - cars [ i ][ 0 ]) / ( cars [ i ][ 1 ] - cars [ j ][ 1 ]); }; int n = cars . size (); stack < int > s ; vector < double > res ( n , -1 ); for ( int i = n - 1 ; i >= 0 ; -- i ) { // not only you have to be fast to catch next car, // but also the next car have not collided before, if it collided before // you have to look for the slower cars (which could be collided much ealier) while ( ! s . empty () && ( cars [ i ][ 1 ] <= cars [ s . top ()][ 1 ] || ( s . size () > 1 && collideT ( i , s . top ()) > res [ s . top ()]))) { s . pop (); } res [ i ] = s . empty () ? -1 : collideT ( i , s . top ()); s . push ( i ); } return res ; } }; C++ Brute force class Solution { public : vector < double > getCollisionTimes ( vector < vector < int >>& cars ) { int n = cars . size (); vector < double > ans ( n , -1.0 ); for ( int i = n - 1 ; i >= 0 ; -- i ) { int p = cars [ i ][ 0 ]; int v = cars [ i ][ 1 ]; for ( int j = i - 1 ; j >= 0 ; -- j ) { int p_j = cars [ j ][ 0 ]; int v_j = cars [ j ][ 1 ]; int dp = p - p_j ; int dv = v_j - v ; if ( dv <= 0 ) break ; double t = ( double ) dp / dv ; if ( ans [ j ] < 0 || t < ans [ j ]) ans [ j ] = t ; else break ; } } return ans ; } };","title":"1776. Car Fleet II"},{"location":"leetcode/string/notes/","text":"String Problems \u00b6 Understand the problem by listing all the test cases first One of the most important principle to solve a string problem during an interview is to first make sure you have all the possible test cases laid out. Loop invariance \u00b6 To keep loop invariance, you have to be really clear about the different type of chars in the string that may affect the complexity of the loop. The following problems can be tricky to implement when you keep the wrong loop invariance or have identified the wrong if condition or while condition. 157. Read N Characters Given Read4 \u00b6 Solution 1 C++ loop invariance /** * The read4 API is defined in the parent class Reader4. * int read4(char *buf4); */ class Solution { public : /** * @param buf Destination buffer * @param n Number of characters to read * @return The number of actual characters read */ int read ( char * buf , int n ) { int total = 0 ; int len = 0 ; while ( total < n ) { len = read4 ( buf + total ); if ( len == 0 ) { break ; } total += len ; if ( total > n ) { total = n ; } } return total ; } }; 158. Read N Characters Given Read4 II - Call multiple times \u00b6 C++ one byte by one byte /** * The read4 API is defined in the parent class Reader4. * int read4(char *buf); */ class Solution { char buf4 [ 4 ]; int pos = 0 , len = 0 ; public : /** * @param buf Destination buffer * @param n Number of characters to read * @return The number of actual characters read */ int read ( char * buf , int n ) { for ( int i = 0 ; i < n ; i ++ ) { if ( pos == len ) { len = read4 ( buf4 ); pos = 0 ; } if ( len == 0 ) { return i ; } if ( pos < len ) { buf [ i ] = buf4 [ pos ++ ]; } } return n ; } }; 394. Decode String \u00b6 Iterative solution v.s. Recursive solution Make sure you have all the test cases first Consider the special cases. i.e. nested brackets 3[a]2[a3[bc]] , and unbracketed chars 3[a]abcd and 3[a]2[a3[bc]aa] This solution requires a lot of attention to details. You should draw the variable and exercise it using example. Use loop invariance and only process one char at a time. We can also use the divide and conquer idea and using recursive helper function to solve the problem. The key to implement this solution is to keep the loop invariance in mind. C++ Iterative solution class Solution { public : string decodeString ( string s ) { int n = s . size (); stack < int > stk_cnt ; stack < string > stk_str ; int cnt = 0 ; string tmp_str = \"\" ; for ( int i = 0 ; i < n ; i ++ ) { if ( isdigit ( s [ i ])) { cnt = cnt * 10 + s [ i ] - '0' ; } else if ( s [ i ] == '[' ) { stk_cnt . push ( cnt ); stk_str . push ( tmp_str ); cnt = 0 ; tmp_str . clear (); } else if ( s [ i ] == ']' ) { // output, or modify stacks int c = stk_cnt . top (); stk_cnt . pop (); for ( int i = 0 ; i < c ; i ++ ) { stk_str . top () += tmp_str ; } tmp_str = stk_str . top (); stk_str . pop (); } else { tmp_str += s [ i ]; } } return stk_str . empty () ? tmp_str : stk_str . top (); } }; C++ Recursive solution class Solution { public : string decodeString ( string s ) { int i = 0 ; return decode_helper ( s , i ); } string decode_helper ( string s , int & i ) { int n = s . size (); string res = \"\" ; while ( i < n && s [ i ] != ']' ) { if ( s [ i ] < '0' || s [ i ] > '9' ) { //if (isalpha(s[i])) { res += s [ i ++ ]; } else { int cnt = 0 ; //while (i < n && isdigit(s[i])) { while ( i < n && s [ i ] >= '0' && s [ i ] <= '9' ) { cnt = cnt * 10 + s [ i ++ ] - '0' ; } ++ i ; // '[' string t = decode_helper ( s , i ); ++ i ; // ']' while ( cnt -- > 0 ) { res += t ; } } } return res ; } }; 388. Longest Absolute File Path \u00b6 1233. Remove Sub-Folders from the Filesystem \u00b6 class Solution { public : vector < string > removeSubfolders ( vector < string >& folder ) { int n = folder . size (); vector < string > res ; if ( n == 0 ) { return res ; } // sort sort ( folder . begin (), folder . end (), less < string > ()); // remove string root = folder [ 0 ]; res . push_back ( folder [ 0 ]); for ( int i = 1 ; i < n ; i ++ ) { // need to consider both \"/a/b\", \"/a/bc\" if ( folder [ i ]. substr ( 0 , root . length ()) != root || folder [ i ][ root . length ()] != '/' ) { res . push_back ( folder [ i ]); root = folder [ i ]; } } return res ; } };; 1422. Maximum Score After Splitting a String \u00b6 The steps to solve this type of problem is enumerate all the possible test cases write a naive solution to cover all test cases refactor to improve the complexity C++ Two pass class Solution { public : int maxScore ( string s ) { int len = s . size (); if ( len == 0 ) { return 0 ; } int ones = 0 ; int zeros = 0 ; int max_score = INT_MIN ; for ( int i = 0 ; i < len ; i ++ ) { if ( s [ i ] - '0' == 1 ) ones ++ ; } for ( int i = 0 ; i < len - 1 ; i ++ ) { if ( s [ i ] - '0' == 0 ) zeros ++ ; else ones -- ; max_score = max ( max_score , zeros + ones ); } return max_score ; } }; C++ One Pass class Solution { public : int maxScore ( string s ) { int len = s . size (); if ( len == 0 ) { return 0 ; } int ones = 0 ; int zeros = 0 ; int max_score = INT_MIN ; for ( int i = 0 ; i < len ; i ++ ) { if ( s [ i ] - '0' == 0 ) zeros ++ ; else ones ++ ; if ( i < len - 1 ) max_score = max ( max_score , zeros - ones ); } return max_score + ones ; } }; Substring search and substring window problem \u00b6 This type of problems asking for a substring that fulfill certain properties. The idea of a general solution is to keep a substring window by moving two \"pointers\" and keep a loop invariance. This algorithm is linear. Remember the following principles while solving the problem. Using while loop is better. Identify when to move the left pointer. Be clear about how the map is modified meanwhile. Maintain the invariance: the map keeps the info about chars in [left, i) . 3. Longest Substring Without Repeating Characters \u00b6 Solution 1 Loop invariance This solution is very neat that it uniformly take care of two cases: 1) first time discovered a char. 2) discovered a preated char, by cleverly set the initial value of the dict value to -1 and start = -1 ; The idea can be described as follows: set the start points to current position if you found a duplicate(notice whe a char is first discovered, because we set start = -1 , start will also update). Otherwise just record the current position in the dictionary and update the length. The start = dict[s[i]] maintained the invariance so that i - start will never be wrong. C++ One pass iteration class Solution { public : int lengthOfLongestSubstring ( string s ) { vector < int > dict ( 256 , -1 ); int maxLen = 0 , start = -1 ; for ( int i = 0 ; i < s . length (); ++ i ) { if ( dict [ s [ i ]] > start ) start = dict [ s [ i ]]; dict [ s [ i ]] = i ; maxLen = max ( maxLen , i - start ); } return maxLen ; } }; C++ Two pointer loop invariant class Solution { public : int lengthOfLongestSubstring ( string s ) { unordered_map < char , int > mp ; int left = 0 ; int count = 0 ; int res = 0 ; for ( int i = 0 ; i < s . length (); i ++ ) { m [ s [ i ]] ++ ; if ( m [ s [ i ]] > 1 ) count ++ ; while ( count > 0 ) { m [ s [ left ]] -- ; if ( m [ s [ left ]] == 1 ) { count -- ; } left ++ ; } res = max ( res , i - left + 1 ); } return res ; } } C++ Two pointer loop invariant II class Solution { public : int lengthOfLongestSubstring ( string s ) { if ( s . length () == 0 ) return 0 ; int map [ 256 ] = { 0 }; int res = INT_MIN ; int j = 0 ; for ( int i = 0 ; i < s . length (); i ++ ) { while ( j < s . length () && map [ s [ j ]] == 0 ) { map [ s [ j ]] ++ ; res = max ( res , j - i + 1 ); j ++ ; } map [ s [ i ]] -- ; } return res ; } }; 159. Longest Substring with At Most Two Distinct Characters \u00b6 We can use a hash table to record the chars we have seen, the key is the char, the value is the count of the char seen so far. Once we have more than two hash entries, we should stop looking further and think about removing the third char. So that we can count the desired length. We use a pointer left to keep the left boundary of the interested chars. C++ loop invariant with map class Solution { public : int lengthOfLongestSubstringTwoDistinct ( string s ) { int max_len = 0 ; int left = 0 ; unordered_map < char , int > m ; for ( int i = 0 ; i < s . length (); i ++ ) }{ m [ s [ i ]] ++ ; while ( m . size () > 2 ) { if ( -- m [ s [ left ]] == 0 ) m . erase ( s [ left ]); left ++ ; } max_len = max ( max_len , i - left + 1 ); } return max_len ; } }; C++ using char array as map class Solution { public : int lengthOfLongestSubstringTwoDistinct ( string s ) { int max_len = 0 ; int left = 0 ; int cnt = 0 ; int m [ 128 ] = { 0 }; for ( int i = 0 ; i < s . length (); i ++ ) { if ( m [ s [ i ]] ++ == 0 ) cnt ++ ; while ( cnt > 2 ) { if ( -- m [ s [ left ++ ]] == 0 ) cnt -- ; } max_len = max ( max_len , i - left + 1 ); } return max_len ; } }; C++ Two pointers class Solution { public : int lengthOfLongestSubstringTwoDistinct ( string s ) { int max_len = 0 ; int cnt = 0 ; int m [ 128 ] = { 0 }; for ( int i = 0 , j = 0 ; i < s . length (); i ++ ) { while ( j < s . length ()) { if ( m [ s [ j ]] == 0 ) { cnt ++ ; } m [ s [ j ]] ++ ; /* loop invariance: maintain the hash only have 2 distinct chars */ while ( cnt > 2 ) { if ( -- m [ s [ i ++ ]] == 0 ) cnt -- ; } max_len = max ( max_len , j - i + 1 ); j ++ ; } } return max_len ; } }; 340. Longest Substring with At Most K Distinct Characters \u00b6 This problem is essentially equivalent to the Longest Substring with At Most Two Distinct Characters problem. class Solution { public : int lengthOfLongestSubstringKDistinct ( string s , int k ) { int max_len = 0 ; int left = 0 ; int cnt = 0 ; int map [ 128 ] = { 0 }; for ( int i = 0 ; i < s . length (); i ++ ) { if ( map [ s [ i ]] == 0 ) cnt ++ ; map [ s [ i ]] ++ ; while ( cnt > k ) { if ( -- map [ s [ left ++ ]] == 0 ) cnt -- ; } max_len = max ( max_len , i - left + 1 ); } return max_len ; } };","title":"String"},{"location":"leetcode/string/notes/#string-problems","text":"Understand the problem by listing all the test cases first One of the most important principle to solve a string problem during an interview is to first make sure you have all the possible test cases laid out.","title":"String Problems"},{"location":"leetcode/string/notes/#loop-invariance","text":"To keep loop invariance, you have to be really clear about the different type of chars in the string that may affect the complexity of the loop. The following problems can be tricky to implement when you keep the wrong loop invariance or have identified the wrong if condition or while condition.","title":"Loop invariance"},{"location":"leetcode/string/notes/#157-read-n-characters-given-read4","text":"Solution 1 C++ loop invariance /** * The read4 API is defined in the parent class Reader4. * int read4(char *buf4); */ class Solution { public : /** * @param buf Destination buffer * @param n Number of characters to read * @return The number of actual characters read */ int read ( char * buf , int n ) { int total = 0 ; int len = 0 ; while ( total < n ) { len = read4 ( buf + total ); if ( len == 0 ) { break ; } total += len ; if ( total > n ) { total = n ; } } return total ; } };","title":"157. Read N Characters Given Read4"},{"location":"leetcode/string/notes/#158-read-n-characters-given-read4-ii-call-multiple-times","text":"C++ one byte by one byte /** * The read4 API is defined in the parent class Reader4. * int read4(char *buf); */ class Solution { char buf4 [ 4 ]; int pos = 0 , len = 0 ; public : /** * @param buf Destination buffer * @param n Number of characters to read * @return The number of actual characters read */ int read ( char * buf , int n ) { for ( int i = 0 ; i < n ; i ++ ) { if ( pos == len ) { len = read4 ( buf4 ); pos = 0 ; } if ( len == 0 ) { return i ; } if ( pos < len ) { buf [ i ] = buf4 [ pos ++ ]; } } return n ; } };","title":"158. Read N Characters Given Read4 II - Call multiple times"},{"location":"leetcode/string/notes/#394-decode-string","text":"Iterative solution v.s. Recursive solution Make sure you have all the test cases first Consider the special cases. i.e. nested brackets 3[a]2[a3[bc]] , and unbracketed chars 3[a]abcd and 3[a]2[a3[bc]aa] This solution requires a lot of attention to details. You should draw the variable and exercise it using example. Use loop invariance and only process one char at a time. We can also use the divide and conquer idea and using recursive helper function to solve the problem. The key to implement this solution is to keep the loop invariance in mind. C++ Iterative solution class Solution { public : string decodeString ( string s ) { int n = s . size (); stack < int > stk_cnt ; stack < string > stk_str ; int cnt = 0 ; string tmp_str = \"\" ; for ( int i = 0 ; i < n ; i ++ ) { if ( isdigit ( s [ i ])) { cnt = cnt * 10 + s [ i ] - '0' ; } else if ( s [ i ] == '[' ) { stk_cnt . push ( cnt ); stk_str . push ( tmp_str ); cnt = 0 ; tmp_str . clear (); } else if ( s [ i ] == ']' ) { // output, or modify stacks int c = stk_cnt . top (); stk_cnt . pop (); for ( int i = 0 ; i < c ; i ++ ) { stk_str . top () += tmp_str ; } tmp_str = stk_str . top (); stk_str . pop (); } else { tmp_str += s [ i ]; } } return stk_str . empty () ? tmp_str : stk_str . top (); } }; C++ Recursive solution class Solution { public : string decodeString ( string s ) { int i = 0 ; return decode_helper ( s , i ); } string decode_helper ( string s , int & i ) { int n = s . size (); string res = \"\" ; while ( i < n && s [ i ] != ']' ) { if ( s [ i ] < '0' || s [ i ] > '9' ) { //if (isalpha(s[i])) { res += s [ i ++ ]; } else { int cnt = 0 ; //while (i < n && isdigit(s[i])) { while ( i < n && s [ i ] >= '0' && s [ i ] <= '9' ) { cnt = cnt * 10 + s [ i ++ ] - '0' ; } ++ i ; // '[' string t = decode_helper ( s , i ); ++ i ; // ']' while ( cnt -- > 0 ) { res += t ; } } } return res ; } };","title":"394. Decode String"},{"location":"leetcode/string/notes/#388-longest-absolute-file-path","text":"","title":"388. Longest Absolute File Path"},{"location":"leetcode/string/notes/#1233-remove-sub-folders-from-the-filesystem","text":"class Solution { public : vector < string > removeSubfolders ( vector < string >& folder ) { int n = folder . size (); vector < string > res ; if ( n == 0 ) { return res ; } // sort sort ( folder . begin (), folder . end (), less < string > ()); // remove string root = folder [ 0 ]; res . push_back ( folder [ 0 ]); for ( int i = 1 ; i < n ; i ++ ) { // need to consider both \"/a/b\", \"/a/bc\" if ( folder [ i ]. substr ( 0 , root . length ()) != root || folder [ i ][ root . length ()] != '/' ) { res . push_back ( folder [ i ]); root = folder [ i ]; } } return res ; } };;","title":"1233. Remove Sub-Folders from the Filesystem"},{"location":"leetcode/string/notes/#1422-maximum-score-after-splitting-a-string","text":"The steps to solve this type of problem is enumerate all the possible test cases write a naive solution to cover all test cases refactor to improve the complexity C++ Two pass class Solution { public : int maxScore ( string s ) { int len = s . size (); if ( len == 0 ) { return 0 ; } int ones = 0 ; int zeros = 0 ; int max_score = INT_MIN ; for ( int i = 0 ; i < len ; i ++ ) { if ( s [ i ] - '0' == 1 ) ones ++ ; } for ( int i = 0 ; i < len - 1 ; i ++ ) { if ( s [ i ] - '0' == 0 ) zeros ++ ; else ones -- ; max_score = max ( max_score , zeros + ones ); } return max_score ; } }; C++ One Pass class Solution { public : int maxScore ( string s ) { int len = s . size (); if ( len == 0 ) { return 0 ; } int ones = 0 ; int zeros = 0 ; int max_score = INT_MIN ; for ( int i = 0 ; i < len ; i ++ ) { if ( s [ i ] - '0' == 0 ) zeros ++ ; else ones ++ ; if ( i < len - 1 ) max_score = max ( max_score , zeros - ones ); } return max_score + ones ; } };","title":"1422. Maximum Score After Splitting a String"},{"location":"leetcode/string/notes/#substring-search-and-substring-window-problem","text":"This type of problems asking for a substring that fulfill certain properties. The idea of a general solution is to keep a substring window by moving two \"pointers\" and keep a loop invariance. This algorithm is linear. Remember the following principles while solving the problem. Using while loop is better. Identify when to move the left pointer. Be clear about how the map is modified meanwhile. Maintain the invariance: the map keeps the info about chars in [left, i) .","title":"Substring search and substring window problem"},{"location":"leetcode/string/notes/#3-longest-substring-without-repeating-characters","text":"Solution 1 Loop invariance This solution is very neat that it uniformly take care of two cases: 1) first time discovered a char. 2) discovered a preated char, by cleverly set the initial value of the dict value to -1 and start = -1 ; The idea can be described as follows: set the start points to current position if you found a duplicate(notice whe a char is first discovered, because we set start = -1 , start will also update). Otherwise just record the current position in the dictionary and update the length. The start = dict[s[i]] maintained the invariance so that i - start will never be wrong. C++ One pass iteration class Solution { public : int lengthOfLongestSubstring ( string s ) { vector < int > dict ( 256 , -1 ); int maxLen = 0 , start = -1 ; for ( int i = 0 ; i < s . length (); ++ i ) { if ( dict [ s [ i ]] > start ) start = dict [ s [ i ]]; dict [ s [ i ]] = i ; maxLen = max ( maxLen , i - start ); } return maxLen ; } }; C++ Two pointer loop invariant class Solution { public : int lengthOfLongestSubstring ( string s ) { unordered_map < char , int > mp ; int left = 0 ; int count = 0 ; int res = 0 ; for ( int i = 0 ; i < s . length (); i ++ ) { m [ s [ i ]] ++ ; if ( m [ s [ i ]] > 1 ) count ++ ; while ( count > 0 ) { m [ s [ left ]] -- ; if ( m [ s [ left ]] == 1 ) { count -- ; } left ++ ; } res = max ( res , i - left + 1 ); } return res ; } } C++ Two pointer loop invariant II class Solution { public : int lengthOfLongestSubstring ( string s ) { if ( s . length () == 0 ) return 0 ; int map [ 256 ] = { 0 }; int res = INT_MIN ; int j = 0 ; for ( int i = 0 ; i < s . length (); i ++ ) { while ( j < s . length () && map [ s [ j ]] == 0 ) { map [ s [ j ]] ++ ; res = max ( res , j - i + 1 ); j ++ ; } map [ s [ i ]] -- ; } return res ; } };","title":"3. Longest Substring Without Repeating Characters"},{"location":"leetcode/string/notes/#159-longest-substring-with-at-most-two-distinct-characters","text":"We can use a hash table to record the chars we have seen, the key is the char, the value is the count of the char seen so far. Once we have more than two hash entries, we should stop looking further and think about removing the third char. So that we can count the desired length. We use a pointer left to keep the left boundary of the interested chars. C++ loop invariant with map class Solution { public : int lengthOfLongestSubstringTwoDistinct ( string s ) { int max_len = 0 ; int left = 0 ; unordered_map < char , int > m ; for ( int i = 0 ; i < s . length (); i ++ ) }{ m [ s [ i ]] ++ ; while ( m . size () > 2 ) { if ( -- m [ s [ left ]] == 0 ) m . erase ( s [ left ]); left ++ ; } max_len = max ( max_len , i - left + 1 ); } return max_len ; } }; C++ using char array as map class Solution { public : int lengthOfLongestSubstringTwoDistinct ( string s ) { int max_len = 0 ; int left = 0 ; int cnt = 0 ; int m [ 128 ] = { 0 }; for ( int i = 0 ; i < s . length (); i ++ ) { if ( m [ s [ i ]] ++ == 0 ) cnt ++ ; while ( cnt > 2 ) { if ( -- m [ s [ left ++ ]] == 0 ) cnt -- ; } max_len = max ( max_len , i - left + 1 ); } return max_len ; } }; C++ Two pointers class Solution { public : int lengthOfLongestSubstringTwoDistinct ( string s ) { int max_len = 0 ; int cnt = 0 ; int m [ 128 ] = { 0 }; for ( int i = 0 , j = 0 ; i < s . length (); i ++ ) { while ( j < s . length ()) { if ( m [ s [ j ]] == 0 ) { cnt ++ ; } m [ s [ j ]] ++ ; /* loop invariance: maintain the hash only have 2 distinct chars */ while ( cnt > 2 ) { if ( -- m [ s [ i ++ ]] == 0 ) cnt -- ; } max_len = max ( max_len , j - i + 1 ); j ++ ; } } return max_len ; } };","title":"159. Longest Substring with At Most Two Distinct Characters"},{"location":"leetcode/string/notes/#340-longest-substring-with-at-most-k-distinct-characters","text":"This problem is essentially equivalent to the Longest Substring with At Most Two Distinct Characters problem. class Solution { public : int lengthOfLongestSubstringKDistinct ( string s , int k ) { int max_len = 0 ; int left = 0 ; int cnt = 0 ; int map [ 128 ] = { 0 }; for ( int i = 0 ; i < s . length (); i ++ ) { if ( map [ s [ i ]] == 0 ) cnt ++ ; map [ s [ i ]] ++ ; while ( cnt > k ) { if ( -- map [ s [ left ++ ]] == 0 ) cnt -- ; } max_len = max ( max_len , i - left + 1 ); } return max_len ; } };","title":"340. Longest Substring with At Most K Distinct Characters"},{"location":"leetcode/topological-sort/notes/","text":"Topological Sort \u00b6 Algorithm \u00b6 Motivation: sequence tasks while respecing all precedence constraints. If graph G exists a directed cycle, no topological ordering. Every directed acyclic graph must have a sink vertex. The psudo code DFS-Loop (graph G) -- mark all nodes unexplored -- current_label = n [to keep track of ordering] -- for each vertex -- if v not yet explored [in previous DFS call ] -- DFS(G,v) DFS(graph G, start vertex s) -- for every edge (s,v) -- if v not yet explored -- mark v explored -- DFS(G,v) -- set f(s) = current_label -- current_label = current_label - 1 One possible improvement is to use three flags to distinguish the non-visited , visiting , and visited nodes. It detects circle when run into a vertex marked as visiting as the current vertex is in visiting state. Because in DAG, the property of DFS states that a descendant have to be visited before its predecessor finished . For example, DFS start from s s , and just discovered vertex v v , recursive call on vertex v v discovred the s s in \"visiting\" state, this indicate there exists a cycle. BFS Topological sorting \u00b6 Apply BFS in DAG topological sorting requires to start with the vertices that have 0 indegree, namely the source vertices. We need a queue and a indegree vector for solving topological sorting problems. The BFS queue may start with more than one source vertices because topological sorted graph may start with two or more vertices (indegree == 0). When BFS visit an adjacent vertex of the queue-front vertex, the indegree of this visited vertex should be decreased by 1, as we \"visited\" this arc. If the indegree of the visited vertex is reduced to 0, we push it to the queue. The idea of using BFS in topological sorting is to explore frontier of the vertices that currently have 0 non-visited arcs leading to the vertices (indegree is 0). What if the given graph have a single vertex? How to check whether the topological sorting is unique or not? How to output all the unique topological sorted sequence of vertices? Course Schedule \u00b6 Solution 1 DFS Solution 2 BFS C++ DFS class Solution { public : bool canFinish ( int numCourses , vector < pair < int , int >>& prerequisites ) { vector < vector < int >> graph ( numCourses , vector < int > ( 0 )); vector < int > visit ( numCourses , 0 ); /* construct the graph in adjacency list */ for ( auto a : prerequisites ) { graph [ a . second ]. push_back ( a . first ); } for ( int i = 0 ; i < numCourses ; ++ i ) { if ( ! canFinishDFS ( graph , visit , i )) return false ; } return true ; } bool canFinishDFS ( vector < vector < int >>& graph , vector < int >& visit , int i ) { if ( visit [ i ] == -1 ) return false ; // visiting if ( visit [ i ] == 1 ) return true ; visit [ i ] = -1 ; for ( auto a : graph [ i ]) { if ( ! canFinishDFS ( graph , visit , a )) return false ; } visit [ i ] = 1 ; return true ; } }; C++ BFS class Solution { public : bool canFinish ( int numCourses , vector < pair < int , int >>& prerequisites ) { vector < vector < int >> graph ( numCourses , vector < int > ( 0 )); vector < int > in ( numCourses , 0 ); for ( auto a : prerequisites ) { graph [ a . second ]. push_back ( a . first ); ++ in [ a . first ]; /* indegree of nodes */ } queue < int > q ; /* locate the \"start\" of the directed acyclic graph */ for ( int i = 0 ; i < numCourses ; ++ i ) { if ( in [ i ] == 0 ) q . push ( i ); } while ( ! q . empty ()) { int t = q . front (); q . pop (); for ( auto a : graph [ t ]) { -- in [ a ]; /* visit the edge t->a */ if ( in [ a ] == 0 ) q . push ( a ); } } /* if there are cycle, (with node indegree > 0) */ for ( int i = 0 ; i < numCourses ; ++ i ) { if ( in [ i ] != 0 ) return false ; } return true ; } }; Course Schedule II \u00b6 Solution 1 DFS Solution 2 BFS C++ DFS class Solution { public : vector < int > findOrder ( int numCourses , vector < pair < int , int >>& prerequisites ) { int n = prerequisites . size (); vector < vector < int >> graph ( numCourses , vector < int > ( 0 )); vector < int > visit ( numCourses , 0 ); /* 0: not visited, 1: visiting, -1: visited */ vector < int > res ; for ( auto a : prerequisites ) { graph [ a . second ]. push_back ( a . first ); } for ( int i = 0 ; i < numCourses ; ++ i ) { if ( ! visit [ i ] && ! findOrderDFS ( graph , visit , i , res )) { return {}; } } reverse ( res . begin (), res . end ()); return res ; } bool findOrderDFS ( vector < vector < int >>& graph , vector < int >& visit , int i , vector < int >& res ) { if ( visit [ i ] == 1 ) return false ; /* visiting */ if ( visit [ i ] == -1 ) return true ; visit [ i ] = 1 ; for ( auto a : graph [ i ]) { if ( ! findOrderDFS ( graph , visit , a , res )) { return false ; } } visit [ i ] = -1 ; res . push_back ( i ); return true ; } }; C++ BFS class Solution { public : vector < int > findOrder ( int numCourses , vector < pair < int , int >>& prerequisites ) { vector < vector < int >> graph ( numCourses , vector < int > ( 0 )); vector < int > in ( numCourses , 0 ); vector < int > res ; // identify the sink edge by indegree for ( auto a : prerequisites ) { graph [ a . second ]. push_back ( a . first ); ++ in [ a . first ]; } queue < int > q ; for ( int i = 0 ; i < numCourses ; ++ i ) { //push the \"sink\" vetex to the queue if ( in [ i ] == 0 ) q . push ( i ); } while ( ! q . empty ()) { int t = q . front (); q . pop (); res . push_back ( t ); for ( auto a : graph [ t ]) { -- in [ a ]; if ( in [ a ] == 0 ) q . push ( a ); // add a new sink vertex } } // circle detection for ( int i = 0 ; i < numCourses ; ++ i ) { if ( in [ i ] != 0 ) return {}; } return res ; } }; Alien Dictionary \u00b6 Solution 1 Topological Sort use DFS to get the topological order use BFS to traverse to get the result C++ Topological sort class Solution { public : string alienOrder ( vector < string >& words ) { set < pair < char , char >> g ; // adj list graph, second -> first, unordered_set < char > set ; // all unique characters set vector < int > in ( 256 , 0 ); // indegree for each chars string res = \"\" ; for ( auto a : words ) // store all unique char of the dict words set . insert ( a . begin (), a . end ()); for ( int i = 0 ; i < words . size () - 1 ; i ++ ) { int min_len = min ( words [ i ]. size (), words [ i + 1 ]. size ()), j ; for ( j = 0 ; j < min_len ; j ++ ) { if ( words [ i ][ j ] != words [ i + 1 ][ j ]) { // build graph from the dictionary g . insert ({ words [ i ][ j ], words [ i + 1 ][ j ]}); // word[0][0] is the sink vertex break ; // take only one } } } // calculate indegree of nodes for ( auto c : g ) ++ in [ c . second ]; // push sink node to the queue queue < int > q ; for ( char c : set ) { if ( in [ c ] == 0 ) { q . push ( c ); res += c ; // sink vertex added to result } } while ( ! q . empty ()) { // BFS to output the sorted order char a = q . front (); q . pop (); for ( auto c : g ) { if ( c . first == a ) { // for each incomming edge of sink node a. -- in [ c . second ]; // here we treat the topological order c.second -> c.first if ( in [ c . second ] == 0 ) { q . push ( c . second ); res += c . second ; } } } } return res . size () == set . size () ? res : \"\" ; } }; Sequence Reconstruction \u00b6 Solution 1 Topological Sort How to check whether the topological sorting is unique or not? How to output all the unique topological sorted sequence of vertices? C++ Topological sort class Solution { public : bool sequenceReconstruction ( vector < int >& org , vector < vector < int >>& seqs ) { int n = org . size (); int m = seqs . size (); vector < vector < int >> graph ( n + 1 , vector < int > ( 0 )); vector < int > in ( n + 1 , 0 ); bool empty = true ; for ( auto seq : seqs ) { if ( seq . empty ()) continue ; if ( seq . size () < 2 && ( seq [ 0 ] < 1 || seq [ 0 ] > n )) return false ; empty = false ; for ( int i = 0 ; i < seq . size () - 1 ; ++ i ) { int u = seq [ i ]; int v = seq [ i + 1 ]; if ( u < 1 || u > n || v < 1 || v > n ) return false ; graph [ u ]. push_back ( v ); // build the graph in [ v ] ++ ; // compute the indegree } } if ( empty ) return false ; queue < int > q ; for ( int i = 1 ; i <= n ; ++ i ) { if ( in [ i ] == 0 ) { q . push ( i ); } } int k = 0 ; while ( ! q . empty ()) { // BFS to compute the topological order // inorder to get a unique sequence, the q.size() should always be 1, // this means that the topological order is unique, consider the first example if ( q . size () > 1 ) return false ; int t = q . front (); q . pop (); if ( t != org [ k ++ ]) return false ; for ( auto a : graph [ t ]) { in [ a ] -- ; if ( in [ a ] == 0 ) q . push ( a ); } } return k == n ; } };","title":"Topological Sort"},{"location":"leetcode/topological-sort/notes/#topological-sort","text":"","title":"Topological Sort"},{"location":"leetcode/topological-sort/notes/#algorithm","text":"Motivation: sequence tasks while respecing all precedence constraints. If graph G exists a directed cycle, no topological ordering. Every directed acyclic graph must have a sink vertex. The psudo code DFS-Loop (graph G) -- mark all nodes unexplored -- current_label = n [to keep track of ordering] -- for each vertex -- if v not yet explored [in previous DFS call ] -- DFS(G,v) DFS(graph G, start vertex s) -- for every edge (s,v) -- if v not yet explored -- mark v explored -- DFS(G,v) -- set f(s) = current_label -- current_label = current_label - 1 One possible improvement is to use three flags to distinguish the non-visited , visiting , and visited nodes. It detects circle when run into a vertex marked as visiting as the current vertex is in visiting state. Because in DAG, the property of DFS states that a descendant have to be visited before its predecessor finished . For example, DFS start from s s , and just discovered vertex v v , recursive call on vertex v v discovred the s s in \"visiting\" state, this indicate there exists a cycle.","title":"Algorithm"},{"location":"leetcode/topological-sort/notes/#bfs-topological-sorting","text":"Apply BFS in DAG topological sorting requires to start with the vertices that have 0 indegree, namely the source vertices. We need a queue and a indegree vector for solving topological sorting problems. The BFS queue may start with more than one source vertices because topological sorted graph may start with two or more vertices (indegree == 0). When BFS visit an adjacent vertex of the queue-front vertex, the indegree of this visited vertex should be decreased by 1, as we \"visited\" this arc. If the indegree of the visited vertex is reduced to 0, we push it to the queue. The idea of using BFS in topological sorting is to explore frontier of the vertices that currently have 0 non-visited arcs leading to the vertices (indegree is 0). What if the given graph have a single vertex? How to check whether the topological sorting is unique or not? How to output all the unique topological sorted sequence of vertices?","title":"BFS Topological sorting"},{"location":"leetcode/topological-sort/notes/#course-schedule","text":"Solution 1 DFS Solution 2 BFS C++ DFS class Solution { public : bool canFinish ( int numCourses , vector < pair < int , int >>& prerequisites ) { vector < vector < int >> graph ( numCourses , vector < int > ( 0 )); vector < int > visit ( numCourses , 0 ); /* construct the graph in adjacency list */ for ( auto a : prerequisites ) { graph [ a . second ]. push_back ( a . first ); } for ( int i = 0 ; i < numCourses ; ++ i ) { if ( ! canFinishDFS ( graph , visit , i )) return false ; } return true ; } bool canFinishDFS ( vector < vector < int >>& graph , vector < int >& visit , int i ) { if ( visit [ i ] == -1 ) return false ; // visiting if ( visit [ i ] == 1 ) return true ; visit [ i ] = -1 ; for ( auto a : graph [ i ]) { if ( ! canFinishDFS ( graph , visit , a )) return false ; } visit [ i ] = 1 ; return true ; } }; C++ BFS class Solution { public : bool canFinish ( int numCourses , vector < pair < int , int >>& prerequisites ) { vector < vector < int >> graph ( numCourses , vector < int > ( 0 )); vector < int > in ( numCourses , 0 ); for ( auto a : prerequisites ) { graph [ a . second ]. push_back ( a . first ); ++ in [ a . first ]; /* indegree of nodes */ } queue < int > q ; /* locate the \"start\" of the directed acyclic graph */ for ( int i = 0 ; i < numCourses ; ++ i ) { if ( in [ i ] == 0 ) q . push ( i ); } while ( ! q . empty ()) { int t = q . front (); q . pop (); for ( auto a : graph [ t ]) { -- in [ a ]; /* visit the edge t->a */ if ( in [ a ] == 0 ) q . push ( a ); } } /* if there are cycle, (with node indegree > 0) */ for ( int i = 0 ; i < numCourses ; ++ i ) { if ( in [ i ] != 0 ) return false ; } return true ; } };","title":"Course Schedule"},{"location":"leetcode/topological-sort/notes/#course-schedule-ii","text":"Solution 1 DFS Solution 2 BFS C++ DFS class Solution { public : vector < int > findOrder ( int numCourses , vector < pair < int , int >>& prerequisites ) { int n = prerequisites . size (); vector < vector < int >> graph ( numCourses , vector < int > ( 0 )); vector < int > visit ( numCourses , 0 ); /* 0: not visited, 1: visiting, -1: visited */ vector < int > res ; for ( auto a : prerequisites ) { graph [ a . second ]. push_back ( a . first ); } for ( int i = 0 ; i < numCourses ; ++ i ) { if ( ! visit [ i ] && ! findOrderDFS ( graph , visit , i , res )) { return {}; } } reverse ( res . begin (), res . end ()); return res ; } bool findOrderDFS ( vector < vector < int >>& graph , vector < int >& visit , int i , vector < int >& res ) { if ( visit [ i ] == 1 ) return false ; /* visiting */ if ( visit [ i ] == -1 ) return true ; visit [ i ] = 1 ; for ( auto a : graph [ i ]) { if ( ! findOrderDFS ( graph , visit , a , res )) { return false ; } } visit [ i ] = -1 ; res . push_back ( i ); return true ; } }; C++ BFS class Solution { public : vector < int > findOrder ( int numCourses , vector < pair < int , int >>& prerequisites ) { vector < vector < int >> graph ( numCourses , vector < int > ( 0 )); vector < int > in ( numCourses , 0 ); vector < int > res ; // identify the sink edge by indegree for ( auto a : prerequisites ) { graph [ a . second ]. push_back ( a . first ); ++ in [ a . first ]; } queue < int > q ; for ( int i = 0 ; i < numCourses ; ++ i ) { //push the \"sink\" vetex to the queue if ( in [ i ] == 0 ) q . push ( i ); } while ( ! q . empty ()) { int t = q . front (); q . pop (); res . push_back ( t ); for ( auto a : graph [ t ]) { -- in [ a ]; if ( in [ a ] == 0 ) q . push ( a ); // add a new sink vertex } } // circle detection for ( int i = 0 ; i < numCourses ; ++ i ) { if ( in [ i ] != 0 ) return {}; } return res ; } };","title":"Course Schedule II"},{"location":"leetcode/topological-sort/notes/#alien-dictionary","text":"Solution 1 Topological Sort use DFS to get the topological order use BFS to traverse to get the result C++ Topological sort class Solution { public : string alienOrder ( vector < string >& words ) { set < pair < char , char >> g ; // adj list graph, second -> first, unordered_set < char > set ; // all unique characters set vector < int > in ( 256 , 0 ); // indegree for each chars string res = \"\" ; for ( auto a : words ) // store all unique char of the dict words set . insert ( a . begin (), a . end ()); for ( int i = 0 ; i < words . size () - 1 ; i ++ ) { int min_len = min ( words [ i ]. size (), words [ i + 1 ]. size ()), j ; for ( j = 0 ; j < min_len ; j ++ ) { if ( words [ i ][ j ] != words [ i + 1 ][ j ]) { // build graph from the dictionary g . insert ({ words [ i ][ j ], words [ i + 1 ][ j ]}); // word[0][0] is the sink vertex break ; // take only one } } } // calculate indegree of nodes for ( auto c : g ) ++ in [ c . second ]; // push sink node to the queue queue < int > q ; for ( char c : set ) { if ( in [ c ] == 0 ) { q . push ( c ); res += c ; // sink vertex added to result } } while ( ! q . empty ()) { // BFS to output the sorted order char a = q . front (); q . pop (); for ( auto c : g ) { if ( c . first == a ) { // for each incomming edge of sink node a. -- in [ c . second ]; // here we treat the topological order c.second -> c.first if ( in [ c . second ] == 0 ) { q . push ( c . second ); res += c . second ; } } } } return res . size () == set . size () ? res : \"\" ; } };","title":"Alien Dictionary"},{"location":"leetcode/topological-sort/notes/#sequence-reconstruction","text":"Solution 1 Topological Sort How to check whether the topological sorting is unique or not? How to output all the unique topological sorted sequence of vertices? C++ Topological sort class Solution { public : bool sequenceReconstruction ( vector < int >& org , vector < vector < int >>& seqs ) { int n = org . size (); int m = seqs . size (); vector < vector < int >> graph ( n + 1 , vector < int > ( 0 )); vector < int > in ( n + 1 , 0 ); bool empty = true ; for ( auto seq : seqs ) { if ( seq . empty ()) continue ; if ( seq . size () < 2 && ( seq [ 0 ] < 1 || seq [ 0 ] > n )) return false ; empty = false ; for ( int i = 0 ; i < seq . size () - 1 ; ++ i ) { int u = seq [ i ]; int v = seq [ i + 1 ]; if ( u < 1 || u > n || v < 1 || v > n ) return false ; graph [ u ]. push_back ( v ); // build the graph in [ v ] ++ ; // compute the indegree } } if ( empty ) return false ; queue < int > q ; for ( int i = 1 ; i <= n ; ++ i ) { if ( in [ i ] == 0 ) { q . push ( i ); } } int k = 0 ; while ( ! q . empty ()) { // BFS to compute the topological order // inorder to get a unique sequence, the q.size() should always be 1, // this means that the topological order is unique, consider the first example if ( q . size () > 1 ) return false ; int t = q . front (); q . pop (); if ( t != org [ k ++ ]) return false ; for ( auto a : graph [ t ]) { in [ a ] -- ; if ( in [ a ] == 0 ) q . push ( a ); } } return k == n ; } };","title":"Sequence Reconstruction"},{"location":"leetcode/tree/notes/","text":"Tree \u00b6 Summary \u00b6 These binary tree problems could be divided into the following type of questions: Path sum problem Traversal problem Binary tree property problem Ancestor and successor problem Build binary tree problem Binary tree manipulation problem Each question type could be solved using the similar thought process, and problems in each individual type share the similar traps and common pitfalls. I will write some of my consolidated notes for them. Binary tree property problems \u00b6 Kth Smallest Element in a BST \u00b6 Find Mode in Binary Search Tree \u00b6 Count Univalue Subtrees \u00b6 Closest Binary Search Tree Value \u00b6 Closest Binary Search Tree Value II \u00b6 Binary Tree Tilt \u00b6 Path sum and longest path problem \u00b6 Path Sum \u00b6 Path Sum II \u00b6 Path Sum III* (how the linear solution using hash works?) \u00b6 Path Sum IV \u00b6 Binary Tree Maximum Path Sum* \u00b6 Sum Root to Leaf Numbers \u00b6 Binary Tree Paths \u00b6 Sum of Left Leaves \u00b6 Binary Tree Longest Consecutive Sequence \u00b6 Binary Tree Longest Consecutive Sequence II \u00b6 Longest Univalue Path \u00b6 Most Frequent Subtree Sum \u00b6 Diameter of Binary Tree \u00b6 Preorder traversal \u00b6 Binary Tree Preorder Traversal \u00b6 ```C++ tab=\"Morris traversal (preorder)\" hl_lines=\"30\" / * Definition for a binary tree node. * struct TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode() : val(0), left(nullptr), right(nullptr) {} * TreeNode(int x) : val(x), left(nullptr), right(nullptr) {} * TreeNode(int x, TreeNode left, TreeNode right) : val(x), left(left), right(right) {} * }; / class Solution { public: vector preorderTraversal(TreeNode root) { TreeNode curr = root; TreeNode prev = nullptr; vector res; while (curr != nullptr) { if (curr->left == nullptr) { res.push_back(curr->val); curr = curr->right; } else { prev = curr->left; while (prev->right != nullptr && prev->right != curr) prev = prev->right; if (prev->right == nullptr) { res.push_back(curr->val); prev->right = curr; curr = curr->left; } else { prev->right = nullptr; curr = curr->right; } } } return res; } }; C++ tab=\"Iterative using stack\" class Solution { public: vector preorderTraversal(TreeNode root) { vector res; if (root == nullptr) return res; stack s; s.push(root); while (!s.empty()) { TreeNode node = s.top(); s.pop(); res.push_back(node->val); if (node->right) // push right first s.push(node->right); if (node->left) s.push(node->left); } return res; } }; ``` ```C++ tab=\"Recursive\" class Solution { public: vector preorderTraversal(TreeNode *root) { vector res; if (root == NULL) return res; helper(root, res); return res; } void helper(TreeNode *root, vector<int> &res){ if (root == NULL) return; res.push_back(root->val); helper(root->left, res); helper(root->right, res); } }; ``` Verify Preorder Sequence in Binary Search Tree \u00b6 Verify Preorder Serialization of a Binary Tree \u00b6 Inorder traversal \u00b6 Binary Tree Inorder Traversal \u00b6 C++ tab=\"Morris traversal inorder\" /** * Definition for a binary tree node. * struct TreeNode { * int val; * TreeNode *left; * TreeNode *right; * TreeNode() : val(0), left(nullptr), right(nullptr) {} * TreeNode(int x) : val(x), left(nullptr), right(nullptr) {} * TreeNode(int x, TreeNode *left, TreeNode *right) : val(x), left(left), right(right) {} * }; */ class Solution { public: vector<int> inorderTraversal(TreeNode *root) { TreeNode* curr = root; TreeNode* prev = nullptr; vector<int> res; while (curr != nullptr) { if (curr->left == nullptr) { res.push_back(curr->val); curr = curr->right; } else { prev = curr->left; while (prev->right != nullptr && prev->right != curr) prev = prev->right; if (prev->right == nullptr) { prev->right = curr; curr = curr->left; } else { prev->right = nullptr; res.push_back(curr->val); curr = curr->right; } } } return res; } }; ```C++ tab=\"Iterative using a stack\" class Solution { public: vector inorderTraversal(TreeNode root) { vector res; if (root == nullptr) return res; stack s; TreeNode curr = root; while (!s.empty() || curr) { if (curr) { s.push_back(curr); curr = curr->left; } else { curr = s.top(); s.pop(); res.push_back(curr->val); curr = curr->right; } } return res; } } C++ tab=\"Recursive\" class Solution { public: vector inorderTraversal(TreeNode root) { // write your code here vector res; if (root == NULL) return res; helper(root, res); return res; } void helper(TreeNode root, vector & res) { if (root == NULL) { return; } helper(root->left, res); res.push_back(root->val); helper(root->right, res); } }; ``` Recover Binary Search Tree \u00b6 Kth Smallest Element in a BST \u00b6 Closest Binary Search Tree Value \u00b6 Closest Binary Search Tree Value II* \u00b6 Minimum Absolute Difference in BST \u00b6 Postorder traversal \u00b6 Binary Tree Postorder Traversal \u00b6 ```C++ tab=\"iterative using 2 stacks\" / * Definition of TreeNode: * class TreeNode { * public: * int val; * TreeNode left, right; * TreeNode(int val) { * this->val = val; * this->left = this->right = NULL; * } * } */ class Solution { / * @param root: The root of binary tree. * @return: Postorder in vector which contains node values. / public: vector postorderTraversal(TreeNode root) { vector res; if (root == NULL) return res; stack s; stack output; s.push(root); while (!s.empty()) { TreeNode *cur = s.top(); output.push(cur); s.pop(); /* push left first */ if (cur->left) { s.push(cur->left); } /* push right second */ if (cur->right) { s.push(cur->right); } } /* populate the result */ while (!output.empty()) { res.push_back(output.top()->val); output.pop(); } return res; } }; C++ tab=\"Recursive solution\" class Solution { public: vector inorderTraversal(TreeNode root) { // write your code here vector res; if (root == NULL) return res; helper(root, res); return res; } void helper(TreeNode root, vector & res) { if (root == NULL) { return; } helper(root->left, res); helper(root->right, res); res.push_back(root->val); } }; ``` ```C++ tab=\"Morris like traversal ussing a stack\" class Solution { public: vector postorderTraversal(TreeNode *root) { vector res; if (root == nullptr) return res; stack<TreeNode *> s; TreeNode* prev = nullptr; s.push(root); while (!s.empty()) { TreeNode* curr = s.top(); if (!prev || prev->right == curr || prev->left == curr) { if (curr->left) { s.push(curr->left); } else if (curr->right) { s.push(curr->right); } else { res.push_back(curr->val); s.pop(); } } else if (curr->left == prev) { if (curr->right) { s.push(curr->right); } else { res.push_back(curr->val); s.pop(); } } else if (curr->right == prev) { res.push_back(curr->val); s.pop(); } prev = curr; } } }; ``` Find Duplicate Subtrees \u00b6 Level order traversal \u00b6 Binary tree vertical order traversal \u00b6 Tree serialize and deserialize \u00b6 Tree node manipulate problems \u00b6","title":"Tree"},{"location":"leetcode/tree/notes/#tree","text":"","title":"Tree"},{"location":"leetcode/tree/notes/#summary","text":"These binary tree problems could be divided into the following type of questions: Path sum problem Traversal problem Binary tree property problem Ancestor and successor problem Build binary tree problem Binary tree manipulation problem Each question type could be solved using the similar thought process, and problems in each individual type share the similar traps and common pitfalls. I will write some of my consolidated notes for them.","title":"Summary"},{"location":"leetcode/tree/notes/#binary-tree-property-problems","text":"","title":"Binary tree property problems"},{"location":"leetcode/tree/notes/#kth-smallest-element-in-a-bst","text":"","title":"Kth Smallest Element in a BST"},{"location":"leetcode/tree/notes/#find-mode-in-binary-search-tree","text":"","title":"Find Mode in Binary Search Tree"},{"location":"leetcode/tree/notes/#count-univalue-subtrees","text":"","title":"Count Univalue Subtrees"},{"location":"leetcode/tree/notes/#closest-binary-search-tree-value","text":"","title":"Closest Binary Search Tree Value"},{"location":"leetcode/tree/notes/#closest-binary-search-tree-value-ii","text":"","title":"Closest Binary Search Tree Value II"},{"location":"leetcode/tree/notes/#binary-tree-tilt","text":"","title":"Binary Tree Tilt"},{"location":"leetcode/tree/notes/#path-sum-and-longest-path-problem","text":"","title":"Path sum and longest path problem"},{"location":"leetcode/tree/notes/#path-sum","text":"","title":"Path Sum"},{"location":"leetcode/tree/notes/#path-sum-ii","text":"","title":"Path Sum II"},{"location":"leetcode/tree/notes/#path-sum-iii-how-the-linear-solution-using-hash-works","text":"","title":"Path Sum III* (how the linear solution using hash works?)"},{"location":"leetcode/tree/notes/#path-sum-iv","text":"","title":"Path Sum IV"},{"location":"leetcode/tree/notes/#binary-tree-maximum-path-sum","text":"","title":"Binary Tree Maximum Path Sum*"},{"location":"leetcode/tree/notes/#sum-root-to-leaf-numbers","text":"","title":"Sum Root to Leaf Numbers"},{"location":"leetcode/tree/notes/#binary-tree-paths","text":"","title":"Binary Tree Paths"},{"location":"leetcode/tree/notes/#sum-of-left-leaves","text":"","title":"Sum of Left Leaves"},{"location":"leetcode/tree/notes/#binary-tree-longest-consecutive-sequence","text":"","title":"Binary Tree Longest Consecutive Sequence"},{"location":"leetcode/tree/notes/#binary-tree-longest-consecutive-sequence-ii","text":"","title":"Binary Tree Longest Consecutive Sequence II"},{"location":"leetcode/tree/notes/#longest-univalue-path","text":"","title":"Longest Univalue Path"},{"location":"leetcode/tree/notes/#most-frequent-subtree-sum","text":"","title":"Most Frequent Subtree Sum"},{"location":"leetcode/tree/notes/#diameter-of-binary-tree","text":"","title":"Diameter of Binary Tree"},{"location":"leetcode/tree/notes/#preorder-traversal","text":"","title":"Preorder traversal"},{"location":"leetcode/tree/notes/#binary-tree-preorder-traversal","text":"```C++ tab=\"Morris traversal (preorder)\" hl_lines=\"30\" / * Definition for a binary tree node. * struct TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode() : val(0), left(nullptr), right(nullptr) {} * TreeNode(int x) : val(x), left(nullptr), right(nullptr) {} * TreeNode(int x, TreeNode left, TreeNode right) : val(x), left(left), right(right) {} * }; / class Solution { public: vector preorderTraversal(TreeNode root) { TreeNode curr = root; TreeNode prev = nullptr; vector res; while (curr != nullptr) { if (curr->left == nullptr) { res.push_back(curr->val); curr = curr->right; } else { prev = curr->left; while (prev->right != nullptr && prev->right != curr) prev = prev->right; if (prev->right == nullptr) { res.push_back(curr->val); prev->right = curr; curr = curr->left; } else { prev->right = nullptr; curr = curr->right; } } } return res; } }; C++ tab=\"Iterative using stack\" class Solution { public: vector preorderTraversal(TreeNode root) { vector res; if (root == nullptr) return res; stack s; s.push(root); while (!s.empty()) { TreeNode node = s.top(); s.pop(); res.push_back(node->val); if (node->right) // push right first s.push(node->right); if (node->left) s.push(node->left); } return res; } }; ``` ```C++ tab=\"Recursive\" class Solution { public: vector preorderTraversal(TreeNode *root) { vector res; if (root == NULL) return res; helper(root, res); return res; } void helper(TreeNode *root, vector<int> &res){ if (root == NULL) return; res.push_back(root->val); helper(root->left, res); helper(root->right, res); } }; ```","title":"Binary Tree Preorder Traversal"},{"location":"leetcode/tree/notes/#verify-preorder-sequence-in-binary-search-tree","text":"","title":"Verify Preorder Sequence in Binary Search Tree"},{"location":"leetcode/tree/notes/#verify-preorder-serialization-of-a-binary-tree","text":"","title":"Verify Preorder Serialization of a Binary Tree"},{"location":"leetcode/tree/notes/#inorder-traversal","text":"","title":"Inorder traversal"},{"location":"leetcode/tree/notes/#binary-tree-inorder-traversal","text":"C++ tab=\"Morris traversal inorder\" /** * Definition for a binary tree node. * struct TreeNode { * int val; * TreeNode *left; * TreeNode *right; * TreeNode() : val(0), left(nullptr), right(nullptr) {} * TreeNode(int x) : val(x), left(nullptr), right(nullptr) {} * TreeNode(int x, TreeNode *left, TreeNode *right) : val(x), left(left), right(right) {} * }; */ class Solution { public: vector<int> inorderTraversal(TreeNode *root) { TreeNode* curr = root; TreeNode* prev = nullptr; vector<int> res; while (curr != nullptr) { if (curr->left == nullptr) { res.push_back(curr->val); curr = curr->right; } else { prev = curr->left; while (prev->right != nullptr && prev->right != curr) prev = prev->right; if (prev->right == nullptr) { prev->right = curr; curr = curr->left; } else { prev->right = nullptr; res.push_back(curr->val); curr = curr->right; } } } return res; } }; ```C++ tab=\"Iterative using a stack\" class Solution { public: vector inorderTraversal(TreeNode root) { vector res; if (root == nullptr) return res; stack s; TreeNode curr = root; while (!s.empty() || curr) { if (curr) { s.push_back(curr); curr = curr->left; } else { curr = s.top(); s.pop(); res.push_back(curr->val); curr = curr->right; } } return res; } } C++ tab=\"Recursive\" class Solution { public: vector inorderTraversal(TreeNode root) { // write your code here vector res; if (root == NULL) return res; helper(root, res); return res; } void helper(TreeNode root, vector & res) { if (root == NULL) { return; } helper(root->left, res); res.push_back(root->val); helper(root->right, res); } }; ```","title":"Binary Tree Inorder Traversal"},{"location":"leetcode/tree/notes/#recover-binary-search-tree","text":"","title":"Recover Binary Search Tree"},{"location":"leetcode/tree/notes/#kth-smallest-element-in-a-bst_1","text":"","title":"Kth Smallest Element in a BST"},{"location":"leetcode/tree/notes/#closest-binary-search-tree-value_1","text":"","title":"Closest Binary Search Tree Value"},{"location":"leetcode/tree/notes/#closest-binary-search-tree-value-ii_1","text":"","title":"Closest Binary Search Tree Value II*"},{"location":"leetcode/tree/notes/#minimum-absolute-difference-in-bst","text":"","title":"Minimum Absolute Difference in BST"},{"location":"leetcode/tree/notes/#postorder-traversal","text":"","title":"Postorder traversal"},{"location":"leetcode/tree/notes/#binary-tree-postorder-traversal","text":"```C++ tab=\"iterative using 2 stacks\" / * Definition of TreeNode: * class TreeNode { * public: * int val; * TreeNode left, right; * TreeNode(int val) { * this->val = val; * this->left = this->right = NULL; * } * } */ class Solution { / * @param root: The root of binary tree. * @return: Postorder in vector which contains node values. / public: vector postorderTraversal(TreeNode root) { vector res; if (root == NULL) return res; stack s; stack output; s.push(root); while (!s.empty()) { TreeNode *cur = s.top(); output.push(cur); s.pop(); /* push left first */ if (cur->left) { s.push(cur->left); } /* push right second */ if (cur->right) { s.push(cur->right); } } /* populate the result */ while (!output.empty()) { res.push_back(output.top()->val); output.pop(); } return res; } }; C++ tab=\"Recursive solution\" class Solution { public: vector inorderTraversal(TreeNode root) { // write your code here vector res; if (root == NULL) return res; helper(root, res); return res; } void helper(TreeNode root, vector & res) { if (root == NULL) { return; } helper(root->left, res); helper(root->right, res); res.push_back(root->val); } }; ``` ```C++ tab=\"Morris like traversal ussing a stack\" class Solution { public: vector postorderTraversal(TreeNode *root) { vector res; if (root == nullptr) return res; stack<TreeNode *> s; TreeNode* prev = nullptr; s.push(root); while (!s.empty()) { TreeNode* curr = s.top(); if (!prev || prev->right == curr || prev->left == curr) { if (curr->left) { s.push(curr->left); } else if (curr->right) { s.push(curr->right); } else { res.push_back(curr->val); s.pop(); } } else if (curr->left == prev) { if (curr->right) { s.push(curr->right); } else { res.push_back(curr->val); s.pop(); } } else if (curr->right == prev) { res.push_back(curr->val); s.pop(); } prev = curr; } } }; ```","title":"Binary Tree Postorder Traversal"},{"location":"leetcode/tree/notes/#find-duplicate-subtrees","text":"","title":"Find Duplicate Subtrees"},{"location":"leetcode/tree/notes/#level-order-traversal","text":"","title":"Level order traversal"},{"location":"leetcode/tree/notes/#binary-tree-vertical-order-traversal","text":"","title":"Binary tree vertical order traversal"},{"location":"leetcode/tree/notes/#tree-serialize-and-deserialize","text":"","title":"Tree serialize and deserialize"},{"location":"leetcode/tree/notes/#tree-node-manipulate-problems","text":"","title":"Tree node manipulate problems"},{"location":"leetcode/trie/notes/","text":"Trie \u00b6 To implement a trie, you should first have the mental model of a trie node. Especially how a char is stored in the node. It is not explicitly stored in a node, but use the index of the pointer array to represent char (by pointing the pointer to a node). For build and query the trie, you can either implement recursive or iterative solution. Notice the deleteKey might be the hardest API to implement and track the correctness. Trie APIs \u00b6 ```C++ tab=\"C++ Trie APIs\" class Node { public: bool isEnd; int val; vector next; Node () { isEnd = false; val = INT_MIN; next = vector<Node*>(256, nullptr); } }; class Trie { public: Node* root; const int NUM_CHARS = 256; // recursive helper function to get all the keys with a common prefix void collectByPrefix(Node* node, string curr, vector<string>& keys); // recursive helper function to get all the keys that match a pattern void collectByPattern(Node* node, string curr, string pattern, vector<string>& keys); // recursive helper function to search the longest word in the trie that match target int search(Node* node, string target, int d, int length); // recursive helper function to detele a key and return the trie root Node* deleteKey(Node* node, string key, int d); Trie () { root = new Node(); } ~Trie () { destroy(root); } Node* getRoot() { return root; } // delete and destroy a trie void destroy(Node* root); // insert a word iteratively void insert(string word, int val); // insert a word recursively Node* insert(Node* node, string word, int val, int d); // find the symbol value interatively, can return node by change the signature int search(string word); // find the symbol value recursively, can return node by change the signature int search(Node* node, string word, int d); // exists a key start with a prefix bool existsKeyStartsWith(string prefix); // get all the keys have a common prefix vector<string> keyStartWith(string prefix); // get all the keys that match a pattern (support wildcard '.') vector<string> keysThatMatch(string pattern); // get the longest key that matches the prefix of the target string keyWithLongestMatch(string target); // delete a key void deleteKey(string key); }; ``` Note The implementation can be found at here Problems \u00b6 208. Implement Trie (Prefix Tree) \u00b6 C++ class Node { public: bool isEnd; vector<Node*> next; Node () { next = vector<Node*>(26, nullptr); isEnd = false; } }; class Trie { Node* root; public: /** Initialize your data structure here. */ Trie() { root = new Node(); } ~Trie() { destroy(root); } void destroy(Node* root) { for (auto node : root->next) { if (node) { destroy(node); } } delete root; } /** Inserts a word into the trie. */ void insert(string word) { Node* p = root; for (char c : word) { if (p->next[c - 'a'] == nullptr) { p->next[c - 'a'] = new Node(); } p = p->next[c - 'a']; } p->isEnd = true; } /** Returns if the word is in the trie. */ bool search(string word) { Node* p = root; for (char c : word) { if (p != nullptr) { p = p->next[c - 'a']; } else { return false; } } return p != nullptr && p->isEnd; } /** Returns if there is any word in the trie that starts with the given prefix. */ bool startsWith(string prefix) { Node* p = root; for (char c : prefix) { if (p != nullptr) { p = p->next[c - 'a']; } } return p != nullptr; } }; /** * Your Trie object will be instantiated and called as such: * Trie obj = new Trie(); * obj.insert(word); * bool param_2 = obj.search(word); * bool param_3 = obj.startsWith(prefix); */ 212. Word Search II \u00b6 Solution 1 search the word square against the trie mark visited to avoid revisit. Use a set to remove duplicate word. need destructor for the trie. class TrieNode { public : bool isWord ; vector < TrieNode *> next ; TrieNode () { isWord = false ; next = vector < TrieNode *> ( 26 , nullptr ); } }; class Trie { private : TrieNode * root ; public : Trie () { root = new TrieNode (); } ~ Trie () { destroy ( root ); } void destroy ( TrieNode * root ) { for ( auto node : root -> next ) { if ( node ) { destroy ( node ); } } delete root ; } TrieNode * getRoot () { return root ; } void insert ( string s ) { TrieNode * p = root ; for ( int i = 0 ; i < s . length (); ++ i ) { if ( p -> next [ s [ i ] - 'a' ] == nullptr ) { p -> next [ s [ i ] - 'a' ] = new TrieNode (); } p = p -> next [ s [ i ] - 'a' ]; } p -> isWord = true ; } }; class Solution { public : vector < string > findWords ( vector < vector < char >>& board , vector < string >& words ) { Trie wordTrie ; TrieNode * root = wordTrie . getRoot (); for ( string word : words ) { wordTrie . insert ( word ); } unordered_set < string > res ; for ( int i = 0 ; i < board . size (); ++ i ) { for ( int j = 0 ; j < board [ 0 ]. size (); ++ j ) { string word ; helper ( board , i , j , root , word , res ); } } return vector < string > ( res . begin (), res . end ()); } void helper ( vector < vector < char >>& board , int i , int j , TrieNode * node , string word , unordered_set < string >& res ) { if ( i < 0 || i >= board . size () || j < 0 || j >= board [ 0 ]. size () || board [ i ][ j ] == '#' ) { return ; } if ( node -> next [ board [ i ][ j ] - 'a' ]) { word . push_back ( board [ i ][ j ]); node = node -> next [ board [ i ][ j ] - 'a' ]; if ( node -> isWord ) { res . insert ( word ); } char tmp = board [ i ][ j ]; board [ i ][ j ] = '#' ; helper ( board , i + 1 , j , node , word , res ); helper ( board , i - 1 , j , node , word , res ); helper ( board , i , j + 1 , node , word , res ); helper ( board , i , j - 1 , node , word , res ); board [ i ][ j ] = tmp ; } } }; 425. Word Squares \u00b6 search the trie row by row, first row include the start of the second row in WS. So we can build a prefix for next row from previous row. Use the prefix for current row, we can search the trie for a complete word for current row. The key to solve this problem is in a node to record all the index of the word in the given words that share a prefix. So that given the prefix we build for current row, we can retrive words with this prefix fast. The trie node definition doesn't have isEnd , because in this problem, we know the length of the words are fixed. if the char we found from the trie is exist they must be a valid word. The \"backtracking\" is necessary because the word we get from its prefix might not form a WS, we have to \"backtrack\" and try a different word. struct TrieNode { bool isEnd ; vector < int > indexes ; vector < TrieNode *> next ; TrieNode () { next = vector < TrieNode *> ( 26 , NULL ); } }; class Trie { TrieNode * root ; public : Trie () { root = new TrieNode (); } ~ Trie () { destroy ( root ); } void destroy ( TrieNode * root ) { for ( auto node : root -> next ) { if ( node ) { destroy ( node ); } } delete root ; } TrieNode * getRoot () { return root ; } void insert ( string s , int idx ) { TrieNode * p = root ; for ( int i = 0 ; i < s . length (); ++ i ) { if ( p -> next [ s [ i ] - 'a' ] == NULL ) { p -> next [ s [ i ] - 'a' ] = new TrieNode (); } p = p -> next [ s [ i ] - 'a' ]; p -> indexes . push_back ( idx ); } } }; class Solution { public : vector < vector < string >> wordSquares ( vector < string >& words ) { int m = words . size (); int n = m ? words [ 0 ]. size () : 0 ; vector < vector < string >> res ; Trie wordsTrie ; for ( int i = 0 ; i < m ; ++ i ) { wordsTrie . insert ( words [ i ], i ); } TrieNode * root = wordsTrie . getRoot (); for ( int i = 0 ; i < m ; ++ i ) { vector < string > wordsSquare ( n , \"\" ); wordsSquare [ 0 ] = words [ i ]; helper ( words , 1 , root , wordsSquare , res ); } return res ; } void helper ( vector < string >& words , int count , TrieNode * root , vector < string >& wordsSquare , vector < vector < string >>& res ) { if ( count == words [ 0 ]. size ()) { res . push_back ( wordsSquare ); return ; } string prefix = \"\" ; for ( int i = 0 ; i < count ; ++ i ) { prefix += wordsSquare [ i ][ count ]; } TrieNode * p = root ; for ( char c : prefix ) { if ( p ) { p = p -> next [ c - 'a' ]; } } if ( p == nullptr ) return ; for ( int idx : p -> indexes ) { wordsSquare [ count ] = words [ idx ]; helper ( words , count + 1 , root , wordsSquare , res ); } } };","title":"Trie"},{"location":"leetcode/trie/notes/#trie","text":"To implement a trie, you should first have the mental model of a trie node. Especially how a char is stored in the node. It is not explicitly stored in a node, but use the index of the pointer array to represent char (by pointing the pointer to a node). For build and query the trie, you can either implement recursive or iterative solution. Notice the deleteKey might be the hardest API to implement and track the correctness.","title":"Trie"},{"location":"leetcode/trie/notes/#trie-apis","text":"```C++ tab=\"C++ Trie APIs\" class Node { public: bool isEnd; int val; vector next; Node () { isEnd = false; val = INT_MIN; next = vector<Node*>(256, nullptr); } }; class Trie { public: Node* root; const int NUM_CHARS = 256; // recursive helper function to get all the keys with a common prefix void collectByPrefix(Node* node, string curr, vector<string>& keys); // recursive helper function to get all the keys that match a pattern void collectByPattern(Node* node, string curr, string pattern, vector<string>& keys); // recursive helper function to search the longest word in the trie that match target int search(Node* node, string target, int d, int length); // recursive helper function to detele a key and return the trie root Node* deleteKey(Node* node, string key, int d); Trie () { root = new Node(); } ~Trie () { destroy(root); } Node* getRoot() { return root; } // delete and destroy a trie void destroy(Node* root); // insert a word iteratively void insert(string word, int val); // insert a word recursively Node* insert(Node* node, string word, int val, int d); // find the symbol value interatively, can return node by change the signature int search(string word); // find the symbol value recursively, can return node by change the signature int search(Node* node, string word, int d); // exists a key start with a prefix bool existsKeyStartsWith(string prefix); // get all the keys have a common prefix vector<string> keyStartWith(string prefix); // get all the keys that match a pattern (support wildcard '.') vector<string> keysThatMatch(string pattern); // get the longest key that matches the prefix of the target string keyWithLongestMatch(string target); // delete a key void deleteKey(string key); }; ``` Note The implementation can be found at here","title":"Trie APIs"},{"location":"leetcode/trie/notes/#problems","text":"","title":"Problems"},{"location":"leetcode/trie/notes/#208-implement-trie-prefix-tree","text":"C++ class Node { public: bool isEnd; vector<Node*> next; Node () { next = vector<Node*>(26, nullptr); isEnd = false; } }; class Trie { Node* root; public: /** Initialize your data structure here. */ Trie() { root = new Node(); } ~Trie() { destroy(root); } void destroy(Node* root) { for (auto node : root->next) { if (node) { destroy(node); } } delete root; } /** Inserts a word into the trie. */ void insert(string word) { Node* p = root; for (char c : word) { if (p->next[c - 'a'] == nullptr) { p->next[c - 'a'] = new Node(); } p = p->next[c - 'a']; } p->isEnd = true; } /** Returns if the word is in the trie. */ bool search(string word) { Node* p = root; for (char c : word) { if (p != nullptr) { p = p->next[c - 'a']; } else { return false; } } return p != nullptr && p->isEnd; } /** Returns if there is any word in the trie that starts with the given prefix. */ bool startsWith(string prefix) { Node* p = root; for (char c : prefix) { if (p != nullptr) { p = p->next[c - 'a']; } } return p != nullptr; } }; /** * Your Trie object will be instantiated and called as such: * Trie obj = new Trie(); * obj.insert(word); * bool param_2 = obj.search(word); * bool param_3 = obj.startsWith(prefix); */","title":"208. Implement Trie (Prefix Tree)"},{"location":"leetcode/trie/notes/#212-word-search-ii","text":"Solution 1 search the word square against the trie mark visited to avoid revisit. Use a set to remove duplicate word. need destructor for the trie. class TrieNode { public : bool isWord ; vector < TrieNode *> next ; TrieNode () { isWord = false ; next = vector < TrieNode *> ( 26 , nullptr ); } }; class Trie { private : TrieNode * root ; public : Trie () { root = new TrieNode (); } ~ Trie () { destroy ( root ); } void destroy ( TrieNode * root ) { for ( auto node : root -> next ) { if ( node ) { destroy ( node ); } } delete root ; } TrieNode * getRoot () { return root ; } void insert ( string s ) { TrieNode * p = root ; for ( int i = 0 ; i < s . length (); ++ i ) { if ( p -> next [ s [ i ] - 'a' ] == nullptr ) { p -> next [ s [ i ] - 'a' ] = new TrieNode (); } p = p -> next [ s [ i ] - 'a' ]; } p -> isWord = true ; } }; class Solution { public : vector < string > findWords ( vector < vector < char >>& board , vector < string >& words ) { Trie wordTrie ; TrieNode * root = wordTrie . getRoot (); for ( string word : words ) { wordTrie . insert ( word ); } unordered_set < string > res ; for ( int i = 0 ; i < board . size (); ++ i ) { for ( int j = 0 ; j < board [ 0 ]. size (); ++ j ) { string word ; helper ( board , i , j , root , word , res ); } } return vector < string > ( res . begin (), res . end ()); } void helper ( vector < vector < char >>& board , int i , int j , TrieNode * node , string word , unordered_set < string >& res ) { if ( i < 0 || i >= board . size () || j < 0 || j >= board [ 0 ]. size () || board [ i ][ j ] == '#' ) { return ; } if ( node -> next [ board [ i ][ j ] - 'a' ]) { word . push_back ( board [ i ][ j ]); node = node -> next [ board [ i ][ j ] - 'a' ]; if ( node -> isWord ) { res . insert ( word ); } char tmp = board [ i ][ j ]; board [ i ][ j ] = '#' ; helper ( board , i + 1 , j , node , word , res ); helper ( board , i - 1 , j , node , word , res ); helper ( board , i , j + 1 , node , word , res ); helper ( board , i , j - 1 , node , word , res ); board [ i ][ j ] = tmp ; } } };","title":"212. Word Search II"},{"location":"leetcode/trie/notes/#425-word-squares","text":"search the trie row by row, first row include the start of the second row in WS. So we can build a prefix for next row from previous row. Use the prefix for current row, we can search the trie for a complete word for current row. The key to solve this problem is in a node to record all the index of the word in the given words that share a prefix. So that given the prefix we build for current row, we can retrive words with this prefix fast. The trie node definition doesn't have isEnd , because in this problem, we know the length of the words are fixed. if the char we found from the trie is exist they must be a valid word. The \"backtracking\" is necessary because the word we get from its prefix might not form a WS, we have to \"backtrack\" and try a different word. struct TrieNode { bool isEnd ; vector < int > indexes ; vector < TrieNode *> next ; TrieNode () { next = vector < TrieNode *> ( 26 , NULL ); } }; class Trie { TrieNode * root ; public : Trie () { root = new TrieNode (); } ~ Trie () { destroy ( root ); } void destroy ( TrieNode * root ) { for ( auto node : root -> next ) { if ( node ) { destroy ( node ); } } delete root ; } TrieNode * getRoot () { return root ; } void insert ( string s , int idx ) { TrieNode * p = root ; for ( int i = 0 ; i < s . length (); ++ i ) { if ( p -> next [ s [ i ] - 'a' ] == NULL ) { p -> next [ s [ i ] - 'a' ] = new TrieNode (); } p = p -> next [ s [ i ] - 'a' ]; p -> indexes . push_back ( idx ); } } }; class Solution { public : vector < vector < string >> wordSquares ( vector < string >& words ) { int m = words . size (); int n = m ? words [ 0 ]. size () : 0 ; vector < vector < string >> res ; Trie wordsTrie ; for ( int i = 0 ; i < m ; ++ i ) { wordsTrie . insert ( words [ i ], i ); } TrieNode * root = wordsTrie . getRoot (); for ( int i = 0 ; i < m ; ++ i ) { vector < string > wordsSquare ( n , \"\" ); wordsSquare [ 0 ] = words [ i ]; helper ( words , 1 , root , wordsSquare , res ); } return res ; } void helper ( vector < string >& words , int count , TrieNode * root , vector < string >& wordsSquare , vector < vector < string >>& res ) { if ( count == words [ 0 ]. size ()) { res . push_back ( wordsSquare ); return ; } string prefix = \"\" ; for ( int i = 0 ; i < count ; ++ i ) { prefix += wordsSquare [ i ][ count ]; } TrieNode * p = root ; for ( char c : prefix ) { if ( p ) { p = p -> next [ c - 'a' ]; } } if ( p == nullptr ) return ; for ( int idx : p -> indexes ) { wordsSquare [ count ] = words [ idx ]; helper ( words , count + 1 , root , wordsSquare , res ); } } };","title":"425. Word Squares"},{"location":"leetcode/two-pointers/notes/","text":"Two Pointers \u00b6 Key problem types \u00b6 Sliding window \u00b6 Problems \u00b6 \u00b6","title":"Two Pointers"},{"location":"leetcode/two-pointers/notes/#two-pointers","text":"","title":"Two Pointers"},{"location":"leetcode/two-pointers/notes/#key-problem-types","text":"","title":"Key problem types"},{"location":"leetcode/two-pointers/notes/#sliding-window","text":"","title":"Sliding window"},{"location":"leetcode/two-pointers/notes/#problems","text":"","title":"Problems"},{"location":"leetcode/two-pointers/notes/#_1","text":"","title":""},{"location":"leetcode/union-find/notes/","text":"Union Find \u00b6 Introduction \u00b6 Union Find problem can be used to model the dynamic connectivity problem. Briefly, given a set of N N objects, implement a union command to connect two objects, and a find (or connected ) command to query whether exist a path connecting the two objects. To model the connections, we assume the \u201cis connected to\u201d is reflective, symmetric, and transitive. Moreover, we define the connected components as a maximal set of objects that are mutually connected. We now implement the following operations. Find query, Check if two objects are in the same components. Union command. Replace components containing two objects with their union. Quick find \u00b6 Data structure \u00b6 integer array id[N] interpretation: p and q are connected iff they have the same id. i = 0 1 2 3 4 5 6 7 8 9 id=[0, 1, 1, 8, 8, 0, 0, 1, 8, 8] Find. Check if p and q have the same id. Union. Change the id of nodes have same id as p to the id of q . Implementation \u00b6 public class QuickFindUF { private int [] id ; public QuickFindUF ( int N ) { id = new int [ N ] ; for ( int i = 0 ; i < N ; i ++ ) id [ i ] = i ; } public boolean connected ( int p , int q ) { return id [ p ] == id [ q ] ; } public void union ( int p , int q ) { int pid = id [ p ] ; int qid = id [ q ] ; for ( int i = 0 ; i < id . length ; i ++ ) if ( id [ i ] == pid ) id [ i ] = qid ; } } Complexity \u00b6 Algorithm Initialization Find Union Quick Find N N 1 N N Quick find defect \u00b6 Union is too expensive ( O(N) O(N) ) Trees are flat, but too expensive to keep them flat. Quick union \u00b6 Data structure \u00b6 Integer array id[N] Interpretation: id[i] is parent of i . Root of i is id[id[id[...id[i]...]]] . (the root\u2019s parent is itself). i = 0 1 2 3 4 5 6 7 8 9 id=[0, 1, 9, 4, 9, 6, 6, 7, 8, 9] Find. Check if p and q have the same root. Union. set the id of p \u2019s root to the id of q \u2019s root. Implementation \u00b6 public class QucikUnionUF { private int [] id ; public QuickUnionUF ( int N ) { id = new int [ N ] ; for ( int i = 0 ; i < N ; i ++ ) id [ i ] = i ; } private int root ( int i ) { while ( i != id [ i ] ) i = id [ i ] ; return i ; } public boolean connected ( int p , int q ) { return root ( p ) == root ( q ); } public void union ( int p , int q ) { int i = root ( p ); int j = root ( q ); id [ i ] = j ; } } Complexity \u00b6 Algorithm Initialization Find Union Quick Union N N N N N N Quick union defect \u00b6 Trees can get tall Find is too expensive O(N) O(N) Improvement by weighting \u00b6 Principles \u00b6 Modify quick-union to avoid tall trees. Keep track of the size of each tree. Balance by linking root of the smaller tree to root of the large tree. Data structure \u00b6 Same as the quick union, but maintain extra array sz[i] to count the number of objects in the tree rooted at i. Find. Identical to the Quick Union Union. Link root of the smaller tree to root of the larger tree. Implementation \u00b6 /* weighted quick union */ public void union ( int p , int q ) { int i = root ( p ); int j = root ( q ); if ( i == j ) return ; if ( sz [ i ] < sz [ j ] ) { id [ i ] = j ; sz [ j ] += sz [ i ] ; } else { id [ j ] = i ; sz [ i ] += sz [ j ] ; } } Weighted quick-union complexity \u00b6 Find. takes time proportional to the depth of p and q . Union. Take constant time, given root. Algorithm Initialization Find Union Quick Find N N 1 N N Quick Union N N N N N N Weighted Quick Union N N lg(N) lg(N) lg(N) lg(N) Analysis \u00b6 Proposition. Depth of any node x x is at most lg(N) lg(N) Proof. When does depth of x x increse? The depth of x x in tree T_1 T_1 will increase 1 when it merged with a larger tree. How many merges it could possibly happen? Because each merge will double the size of the tree containing x x , it can double the tree at most lg(N) lg(N) times. We can prove that it can merge at most lg(N) lg(N) times. Quick union with path compression \u00b6 Principles \u00b6 After computing the root of the p , set the id of each examined node to point to that root. Implementation \u00b6 /* one-pass solution */ private int root ( in i ) { while ( i != id [ i ] ) { id [ i ] = id [ id [ i ]] ; /* object i's grad parent becomes its parent. */ i = id [ i ] ; } } Weighted quick-union with path compression \u00b6 Amortized analysis \u00b6 Proposition. Starting from an empty data structure, any sequence of M M union-find ops on N N objects makes \\leq c(N + Mlg^*N) \\leq c(N + Mlg^*N) array access. Analysis can be implemented to N + M \\alpha(M, N) N + M \\alpha(M, N) . The proof is too complex to be discussed here. Here is a notation of lg^* lg^* , which is the iterate log function. it growing very slow. In theory, the Weighted Quick Union Path Compression algorithm cost within a constant factor of reading in the data. But not truly linear. In practice, WQUPC is linear. Summary \u00b6 M M union-find operations on a set of N N objects Algorithm worst-case time quick find MN MN quick union MN MN weighted quick union N+MlgN N+MlgN quick union with path compression N+MlgN N+MlgN weighted quick union with path compression N+Mlg^*N N+Mlg^*N Problems \u00b6 Number of Islands \u00b6 Longest Consecutive Sequence \u00b6 Surrounded Regions \u00b6 Number of Islands II \u00b6 Graph Valid Tree \u00b6 Number of Connected Components in an Undirected Graph \u00b6 Friend Circles \u00b6 765. Couples Holding Hands \u00b6 Solution 1 Union Find Briefly, We can view each double seat couch as a node. Each couple in the same couch is a graph node. For those couple not seat in the same couch, we connect the two different couch together. We are looking for valid swaps that remove the inter-counch edge and create a new component in the graph. the conclusion is that we cannot remove two edges by a single swap. This should proof the greedy will work. C++ Union Find Solution 2 Greedy Notice there is a tick to use here which is XOR a number x with 1 will get you to either x + 1 or x - 1 . x^1 = x + 1, if x is even x^1 = x - 1, if x is odd C++ Greedy class Solution { public : int minSwapsCouples ( vector < int >& row ) { int res = 0 ; for ( int i = 0 ; i < row . size (); i += 2 ) { int x = row [ i ]; if (( x ^ 1 ) == row [ i + 1 ]) continue ; res ++ ; for ( int j = i + 1 ; j < row . size (); ++ j ) { if ( row [ j ] == ( x ^ 1 )) { row [ j ] = row [ i + 1 ]; row [ i + 1 ] = ( x ^ 1 ); break ; } } } return res ; } };","title":"Union Find"},{"location":"leetcode/union-find/notes/#union-find","text":"","title":"Union Find"},{"location":"leetcode/union-find/notes/#introduction","text":"Union Find problem can be used to model the dynamic connectivity problem. Briefly, given a set of N N objects, implement a union command to connect two objects, and a find (or connected ) command to query whether exist a path connecting the two objects. To model the connections, we assume the \u201cis connected to\u201d is reflective, symmetric, and transitive. Moreover, we define the connected components as a maximal set of objects that are mutually connected. We now implement the following operations. Find query, Check if two objects are in the same components. Union command. Replace components containing two objects with their union.","title":"Introduction"},{"location":"leetcode/union-find/notes/#quick-find","text":"","title":"Quick find"},{"location":"leetcode/union-find/notes/#data-structure","text":"integer array id[N] interpretation: p and q are connected iff they have the same id. i = 0 1 2 3 4 5 6 7 8 9 id=[0, 1, 1, 8, 8, 0, 0, 1, 8, 8] Find. Check if p and q have the same id. Union. Change the id of nodes have same id as p to the id of q .","title":"Data structure"},{"location":"leetcode/union-find/notes/#implementation","text":"public class QuickFindUF { private int [] id ; public QuickFindUF ( int N ) { id = new int [ N ] ; for ( int i = 0 ; i < N ; i ++ ) id [ i ] = i ; } public boolean connected ( int p , int q ) { return id [ p ] == id [ q ] ; } public void union ( int p , int q ) { int pid = id [ p ] ; int qid = id [ q ] ; for ( int i = 0 ; i < id . length ; i ++ ) if ( id [ i ] == pid ) id [ i ] = qid ; } }","title":"Implementation"},{"location":"leetcode/union-find/notes/#complexity","text":"Algorithm Initialization Find Union Quick Find N N 1 N N","title":"Complexity"},{"location":"leetcode/union-find/notes/#quick-find-defect","text":"Union is too expensive ( O(N) O(N) ) Trees are flat, but too expensive to keep them flat.","title":"Quick find defect"},{"location":"leetcode/union-find/notes/#quick-union","text":"","title":"Quick union"},{"location":"leetcode/union-find/notes/#data-structure_1","text":"Integer array id[N] Interpretation: id[i] is parent of i . Root of i is id[id[id[...id[i]...]]] . (the root\u2019s parent is itself). i = 0 1 2 3 4 5 6 7 8 9 id=[0, 1, 9, 4, 9, 6, 6, 7, 8, 9] Find. Check if p and q have the same root. Union. set the id of p \u2019s root to the id of q \u2019s root.","title":"Data structure"},{"location":"leetcode/union-find/notes/#implementation_1","text":"public class QucikUnionUF { private int [] id ; public QuickUnionUF ( int N ) { id = new int [ N ] ; for ( int i = 0 ; i < N ; i ++ ) id [ i ] = i ; } private int root ( int i ) { while ( i != id [ i ] ) i = id [ i ] ; return i ; } public boolean connected ( int p , int q ) { return root ( p ) == root ( q ); } public void union ( int p , int q ) { int i = root ( p ); int j = root ( q ); id [ i ] = j ; } }","title":"Implementation"},{"location":"leetcode/union-find/notes/#complexity_1","text":"Algorithm Initialization Find Union Quick Union N N N N N N","title":"Complexity"},{"location":"leetcode/union-find/notes/#quick-union-defect","text":"Trees can get tall Find is too expensive O(N) O(N)","title":"Quick union defect"},{"location":"leetcode/union-find/notes/#improvement-by-weighting","text":"","title":"Improvement by weighting"},{"location":"leetcode/union-find/notes/#principles","text":"Modify quick-union to avoid tall trees. Keep track of the size of each tree. Balance by linking root of the smaller tree to root of the large tree.","title":"Principles"},{"location":"leetcode/union-find/notes/#data-structure_2","text":"Same as the quick union, but maintain extra array sz[i] to count the number of objects in the tree rooted at i. Find. Identical to the Quick Union Union. Link root of the smaller tree to root of the larger tree.","title":"Data structure"},{"location":"leetcode/union-find/notes/#implementation_2","text":"/* weighted quick union */ public void union ( int p , int q ) { int i = root ( p ); int j = root ( q ); if ( i == j ) return ; if ( sz [ i ] < sz [ j ] ) { id [ i ] = j ; sz [ j ] += sz [ i ] ; } else { id [ j ] = i ; sz [ i ] += sz [ j ] ; } }","title":"Implementation"},{"location":"leetcode/union-find/notes/#weighted-quick-union-complexity","text":"Find. takes time proportional to the depth of p and q . Union. Take constant time, given root. Algorithm Initialization Find Union Quick Find N N 1 N N Quick Union N N N N N N Weighted Quick Union N N lg(N) lg(N) lg(N) lg(N)","title":"Weighted quick-union complexity"},{"location":"leetcode/union-find/notes/#analysis","text":"Proposition. Depth of any node x x is at most lg(N) lg(N) Proof. When does depth of x x increse? The depth of x x in tree T_1 T_1 will increase 1 when it merged with a larger tree. How many merges it could possibly happen? Because each merge will double the size of the tree containing x x , it can double the tree at most lg(N) lg(N) times. We can prove that it can merge at most lg(N) lg(N) times.","title":"Analysis"},{"location":"leetcode/union-find/notes/#quick-union-with-path-compression","text":"","title":"Quick union with path compression"},{"location":"leetcode/union-find/notes/#principles_1","text":"After computing the root of the p , set the id of each examined node to point to that root.","title":"Principles"},{"location":"leetcode/union-find/notes/#implementation_3","text":"/* one-pass solution */ private int root ( in i ) { while ( i != id [ i ] ) { id [ i ] = id [ id [ i ]] ; /* object i's grad parent becomes its parent. */ i = id [ i ] ; } }","title":"Implementation"},{"location":"leetcode/union-find/notes/#weighted-quick-union-with-path-compression","text":"","title":"Weighted quick-union with path compression"},{"location":"leetcode/union-find/notes/#amortized-analysis","text":"Proposition. Starting from an empty data structure, any sequence of M M union-find ops on N N objects makes \\leq c(N + Mlg^*N) \\leq c(N + Mlg^*N) array access. Analysis can be implemented to N + M \\alpha(M, N) N + M \\alpha(M, N) . The proof is too complex to be discussed here. Here is a notation of lg^* lg^* , which is the iterate log function. it growing very slow. In theory, the Weighted Quick Union Path Compression algorithm cost within a constant factor of reading in the data. But not truly linear. In practice, WQUPC is linear.","title":"Amortized analysis"},{"location":"leetcode/union-find/notes/#summary","text":"M M union-find operations on a set of N N objects Algorithm worst-case time quick find MN MN quick union MN MN weighted quick union N+MlgN N+MlgN quick union with path compression N+MlgN N+MlgN weighted quick union with path compression N+Mlg^*N N+Mlg^*N","title":"Summary"},{"location":"leetcode/union-find/notes/#problems","text":"","title":"Problems"},{"location":"leetcode/union-find/notes/#number-of-islands","text":"","title":"Number of Islands"},{"location":"leetcode/union-find/notes/#longest-consecutive-sequence","text":"","title":"Longest Consecutive Sequence"},{"location":"leetcode/union-find/notes/#surrounded-regions","text":"","title":"Surrounded Regions"},{"location":"leetcode/union-find/notes/#number-of-islands-ii","text":"","title":"Number of Islands II"},{"location":"leetcode/union-find/notes/#graph-valid-tree","text":"","title":"Graph Valid Tree"},{"location":"leetcode/union-find/notes/#number-of-connected-components-in-an-undirected-graph","text":"","title":"Number of Connected Components in an Undirected Graph"},{"location":"leetcode/union-find/notes/#friend-circles","text":"","title":"Friend Circles"},{"location":"leetcode/union-find/notes/#765-couples-holding-hands","text":"Solution 1 Union Find Briefly, We can view each double seat couch as a node. Each couple in the same couch is a graph node. For those couple not seat in the same couch, we connect the two different couch together. We are looking for valid swaps that remove the inter-counch edge and create a new component in the graph. the conclusion is that we cannot remove two edges by a single swap. This should proof the greedy will work. C++ Union Find Solution 2 Greedy Notice there is a tick to use here which is XOR a number x with 1 will get you to either x + 1 or x - 1 . x^1 = x + 1, if x is even x^1 = x - 1, if x is odd C++ Greedy class Solution { public : int minSwapsCouples ( vector < int >& row ) { int res = 0 ; for ( int i = 0 ; i < row . size (); i += 2 ) { int x = row [ i ]; if (( x ^ 1 ) == row [ i + 1 ]) continue ; res ++ ; for ( int j = i + 1 ; j < row . size (); ++ j ) { if ( row [ j ] == ( x ^ 1 )) { row [ j ] = row [ i + 1 ]; row [ i + 1 ] = ( x ^ 1 ); break ; } } } return res ; } };","title":"765. Couples Holding Hands"},{"location":"research/","text":"Research \u00b6 Paper Reading Coalition Game Contextual Multi-Armed Bandit TF-IDF and Scoring for Information Retrieval","title":"Index"},{"location":"research/#research","text":"Paper Reading Coalition Game Contextual Multi-Armed Bandit TF-IDF and Scoring for Information Retrieval","title":"Research"},{"location":"research/coalition-game/notes/","text":"Coalition Game Theory Notes \u00b6 https://wiki.rui-han.com/index.php/Private:Research/Coalition_Game","title":"Coalition Game"},{"location":"research/coalition-game/notes/#coalition-game-theory-notes","text":"https://wiki.rui-han.com/index.php/Private:Research/Coalition_Game","title":"Coalition Game Theory Notes"},{"location":"research/contextual-bandit/notes/","text":"Contextual Multi-Armed Bandit \u00b6 https://wiki.rui-han.com/index.php/Private:Research/Multi-Armed_Bandit Multi-armed bandit problems \u00b6 NB: action is equivalent to arm, action i i means play the i i -th arm T T : total rounds K K : number of arms G_t(k) G_t(k) : reward (gain) obtained by play the k k -th arm at the t t -th round R_t(k) R_t(k) : regret (loss) incurr by play the k k -th arm at the t t -th round \\mu^* \\mu^* : the optimal expected payoff for the best arm (action) Brute force algorithm \u00b6 This doesn't apply to the real setting, because in the real bandit problem you can only play one arm at each round. But it doesn't heart that we evaluate this scenario as a benchmark for other algorithms. In this algorithm, we basically evaluate all arms in every round, pretended we are in god's angle and know the underline distribution of each arm. Optimistic initialization \u00b6 This is what we do in the supervised training, Random selection algorithm \u00b6 Each step, you select one classifier uniform randomly and calculate the reward. Greedy ( \\epsilon \\epsilon -Greedy) algorithm \u00b6 In each round, switch to a random arm with probability \\epsilon \\epsilon , otherwise stick on the current optimal arm. The first few rounds are important. For example, if the optimal arm is selected in the beginning, this algorithm will achieve better performance; If the worst arm is selected in the beginning, it will stuck in getting the least reward. Upper Confidence bound \u00b6 Confidence bound is usually misunderstood by it's name. It is NOT the probability of certain outcome fall between the lower bound and upper bound (confidence interval). Confidence bound also called confidence interval, both the lower bound and upper bound are random variables. It describe the property of a method to come up a confidence bound in individual trails. It can be iterpreted as the probability that a realized confidence bound include the ture estimate. Notice UCB method is a classic statistic method. The expected reward \\mu_k \\mu_k is treated as a fixed value, not a random variable. In Bayesian bandit, the expected reward is measured as random variable. For arm a a , \\hat Q_t(a) \\hat Q_t(a) is the sample mean, \\hat U_t(a) \\hat U_t(a) is the upper confidence bound. Q_t(a) Q_t(a) is the true mean, then we have Q_t(a) \\le \\hat Q_t(a) + \\hat U_t(a) Q_t(a) \\le \\hat Q_t(a) + \\hat U_t(a) . \\hat U_t(a) \\hat U_t(a) is inverse proportional to N_t(a) N_t(a) , the total number of selection of arm a a , because the more we played the less uncertain we have for \\hat Q_t(a) \\hat Q_t(a) , thus the upper bound shrink as N_t(a) N_t(a) increases. In UCB method, we select the arm with highest upper confidence bound, this is because it is not played sufficient enough and we believe this arm have the high potential to be the optimal arm. Formally, UCB is equivalent to the following optimization problem: a_t = \\operatorname*{arg\\,max}_{a \\in A} \\hat Q_t(a) + \\hat U_t(a) a_t = \\operatorname*{arg\\,max}_{a \\in A} \\hat Q_t(a) + \\hat U_t(a) UCB1 \u00b6 Bayesian bandit (Thompson Sampling) \u00b6 Exploitation versus exploration \u00b6 Contextual multi-armed bandit \u00b6 Reference \u00b6","title":"Contextual Multi-Armed Bandit"},{"location":"research/contextual-bandit/notes/#contextual-multi-armed-bandit","text":"https://wiki.rui-han.com/index.php/Private:Research/Multi-Armed_Bandit","title":"Contextual Multi-Armed Bandit"},{"location":"research/contextual-bandit/notes/#multi-armed-bandit-problems","text":"NB: action is equivalent to arm, action i i means play the i i -th arm T T : total rounds K K : number of arms G_t(k) G_t(k) : reward (gain) obtained by play the k k -th arm at the t t -th round R_t(k) R_t(k) : regret (loss) incurr by play the k k -th arm at the t t -th round \\mu^* \\mu^* : the optimal expected payoff for the best arm (action)","title":"Multi-armed bandit problems"},{"location":"research/contextual-bandit/notes/#brute-force-algorithm","text":"This doesn't apply to the real setting, because in the real bandit problem you can only play one arm at each round. But it doesn't heart that we evaluate this scenario as a benchmark for other algorithms. In this algorithm, we basically evaluate all arms in every round, pretended we are in god's angle and know the underline distribution of each arm.","title":"Brute force algorithm"},{"location":"research/contextual-bandit/notes/#optimistic-initialization","text":"This is what we do in the supervised training,","title":"Optimistic initialization"},{"location":"research/contextual-bandit/notes/#random-selection-algorithm","text":"Each step, you select one classifier uniform randomly and calculate the reward.","title":"Random selection algorithm"},{"location":"research/contextual-bandit/notes/#greedy-epsilonepsilon-greedy-algorithm","text":"In each round, switch to a random arm with probability \\epsilon \\epsilon , otherwise stick on the current optimal arm. The first few rounds are important. For example, if the optimal arm is selected in the beginning, this algorithm will achieve better performance; If the worst arm is selected in the beginning, it will stuck in getting the least reward.","title":"Greedy (\\epsilon\\epsilon-Greedy) algorithm"},{"location":"research/contextual-bandit/notes/#upper-confidence-bound","text":"Confidence bound is usually misunderstood by it's name. It is NOT the probability of certain outcome fall between the lower bound and upper bound (confidence interval). Confidence bound also called confidence interval, both the lower bound and upper bound are random variables. It describe the property of a method to come up a confidence bound in individual trails. It can be iterpreted as the probability that a realized confidence bound include the ture estimate. Notice UCB method is a classic statistic method. The expected reward \\mu_k \\mu_k is treated as a fixed value, not a random variable. In Bayesian bandit, the expected reward is measured as random variable. For arm a a , \\hat Q_t(a) \\hat Q_t(a) is the sample mean, \\hat U_t(a) \\hat U_t(a) is the upper confidence bound. Q_t(a) Q_t(a) is the true mean, then we have Q_t(a) \\le \\hat Q_t(a) + \\hat U_t(a) Q_t(a) \\le \\hat Q_t(a) + \\hat U_t(a) . \\hat U_t(a) \\hat U_t(a) is inverse proportional to N_t(a) N_t(a) , the total number of selection of arm a a , because the more we played the less uncertain we have for \\hat Q_t(a) \\hat Q_t(a) , thus the upper bound shrink as N_t(a) N_t(a) increases. In UCB method, we select the arm with highest upper confidence bound, this is because it is not played sufficient enough and we believe this arm have the high potential to be the optimal arm. Formally, UCB is equivalent to the following optimization problem: a_t = \\operatorname*{arg\\,max}_{a \\in A} \\hat Q_t(a) + \\hat U_t(a) a_t = \\operatorname*{arg\\,max}_{a \\in A} \\hat Q_t(a) + \\hat U_t(a)","title":"Upper Confidence bound"},{"location":"research/contextual-bandit/notes/#ucb1","text":"","title":"UCB1"},{"location":"research/contextual-bandit/notes/#bayesian-bandit-thompson-sampling","text":"","title":"Bayesian bandit (Thompson Sampling)"},{"location":"research/contextual-bandit/notes/#exploitation-versus-exploration","text":"","title":"Exploitation versus exploration"},{"location":"research/contextual-bandit/notes/#contextual-multi-armed-bandit_1","text":"","title":"Contextual multi-armed bandit"},{"location":"research/contextual-bandit/notes/#reference","text":"","title":"Reference"},{"location":"research/paper-reading/notes/","text":"2020 Conference and Paper \u00b6 KDD 2020 \u00b6 MLHat @ KDD 2020 MLHat: The First International Workshop on Deployable Machine Learning for Security Defense \u00b6","title":"Paper Reading"},{"location":"research/paper-reading/notes/#2020-conference-and-paper","text":"","title":"2020 Conference and Paper"},{"location":"research/paper-reading/notes/#kdd-2020","text":"","title":"KDD 2020"},{"location":"research/paper-reading/notes/#mlhat-kdd-2020-mlhat-the-first-international-workshop-on-deployable-machine-learning-for-security-defense","text":"","title":"MLHat @ KDD 2020 MLHat: The First International Workshop on Deployable Machine Learning for Security Defense"},{"location":"research/tfidf-score/notes/","text":"TF-IDF and Scoring for Information Retrieval \u00b6 Youtube Lecture by Dr. Manning Book chapter tfidf slides term document incidence matrices \u00b6 not practical because of the sparsity. The inverted index \u00b6 token to document list mapping word -> [doc1, doc2, ...] UA -> [2020-04-01 13:03:23, 2020-04-01 13:03:23] Query processing \u00b6 If we want to query which document have the phrash \"information retrival\", we'd like to take the invert index postings list for \"information\" and \"retrival\", then find the document that have both word. (by run a merge algorithm on the two postings list) Boolean query \u00b6 Mainly leverage the boolean operation on the list merging algorithm. Phrase queries \u00b6 no longer suffice to store <term, doc> entries. First attempt, biword indexes. treat the two words phrase as a single phrase. For longer phrase queries, we can broken into the boolean query on biword. false positive, two words not adjacent index blowup due to big dictionary positional indexes, store the positions of each token occurs at a document. use a merge algorithm to recursively at the document level. Ranked retriveval \u00b6 \"How can we rank-order the documents in the collection with respect to a query?\" assign a score to each document - say in [0, 1] - to each document. This score measures how well document and query match. We need a way to assigning a score to a query/document pair. The more frequent a term appears in a document, the higher the score for the document. Scoring with the Jaccard Coefficient \u00b6 doesn't consider term frequency. wee need a better way of normalizing for length. (compare with cosine similarity) Term frequency weighting \u00b6 binary vector \\in {0, 1} \\in {0, 1} count vector bag of words model Tip We want to use TF when computing query-document match scores, but the relevance does not increase proportionally with term frequency Log-frequency weighting \u00b6 w_{t, d} = \\begin{cases} 1 + \\log_{10} (tf_{t,d}), & tf_{t,d} \\ge 0 \\\\ 0, & \\textrm{otherwise} \\end{cases} w_{t, d} = \\begin{cases} 1 + \\log_{10} (tf_{t,d}), & tf_{t,d} \\ge 0 \\\\ 0, & \\textrm{otherwise} \\end{cases} Score for a document-query pair: sum over terms in both q q and d d : \\textrm{score} = \\sum_{t\\in q \\cap d} (1 + \\log_{10} (tf_{t, d})) \\textrm{score} = \\sum_{t\\in q \\cap d} (1 + \\log_{10} (tf_{t, d})) The score is 0 0 if none of the terms is presented in the document. (Inverted) Document frequency weighting \u00b6 idea: Rare terms are more informative than frequent terms. df_t df_t is an inverse measure of informativeness of term t t . There is one idf value for each term t t in a collection. idf_t = \\log_{10}(N/df_t) idf_t = \\log_{10}(N/df_t) Question Does idf have an effect on ranking for one-term queries, like \"iPhone\"? The answer is no, but why? collection frequency vs document frequency \u00b6 TF-IDF weighting \u00b6 \\textrm{score}(q, d) = \\sum_{t\\in q \\cap d} tf_{t, d} \\times idf_{t} \\textrm{score}(q, d) = \\sum_{t\\in q \\cap d} tf_{t, d} \\times idf_{t} binary vector -> count vector -> weight matrix Vector Space Model \u00b6 Euclidean distance, not a good option. because different vector length can have large distance application for https request embedding \u00b6 If we can build a dataset, given a \"query\", it can efficiently retrive how many relevant \"document\" are in the past 2 hours? We will be able to tell wether the \"query\" is mallicious or not. in anti-abuse, we can use the current request as the query, all the reprevious request data that are indexed as the \"documents\". This will give us a score about the current request. This score can tell us the malliciousness of the request. problem \u00b6 Current distribution score cannot identify scraping that routates headers. How to up weight bad requests with certain patterns? Currently, no good way to block signals with low false positives. How to use score instead of text pattern matching to reduce false positives? Solution \u00b6 calculate tf-idf for individual header fields \u00b6 This solution models a collection of requests during a window as a document. term frequency = count of each UA, HC, CC, and RF in a time window document frequency = count of how many windows each UA, HC, CC, and RF occurs The window can be a time window or a fixed size window. In the fixed size window, instead of dividing by equal time slots, we divide by the fixed number of requests and treat each collection as a document. This model can also be used for the tokenized solution. tokenize all request facets \u00b6 This solution is to break down the header fields into tokens and use a set of tokens as the query. Compare this to a phrase retrival task. term frequency = count of each token Question Can we use the cosine similarity to compare two queries? Because the similarity measure described in the lectures is for query-document pair. Can we compare query-query pair? Whether the TF-IDF embedding can capture enough similarity info and used for clustering? Intuitions \u00b6 Using TF-IDF, we can measure the speed of damage from requests possess similar attributes (belongs to certain attacks). Specifically, given a request in realtime, it can check the current TF, and DF, and calculate the TF-IDF. All these three values are useful for us. TF - If TF is greater than a threshold setting. We can it is may be a high qps scraping. If TF is moderate or very small, we can check DF. DF - If DF is very large, the \"term\" maybe common to all sample in the space. If DF streak is not long enough in a series of previous documents. there might be a spike of increase for the term in some window before. If DF is very small, and the TF is large, we detected a spike. # TF DF result 1 large small bad v 2 large large bad/good x 3 small large bad/good x 4 small small good x TF and IDF used separately can help to observe bad signals, but they are not enough. We will need to use TF-IDF to help us to enhance our believe about the badness of a request. TF-IDF Practical consideration \u00b6 How to calculation TF-IDF in sliding windows? title: HTTP requests embedding to score and search bot traffic.","title":"TF-IDF for Information Retrieval"},{"location":"research/tfidf-score/notes/#tf-idf-and-scoring-for-information-retrieval","text":"Youtube Lecture by Dr. Manning Book chapter tfidf slides","title":"TF-IDF and Scoring for Information Retrieval"},{"location":"research/tfidf-score/notes/#term-document-incidence-matrices","text":"not practical because of the sparsity.","title":"term document incidence matrices"},{"location":"research/tfidf-score/notes/#the-inverted-index","text":"token to document list mapping word -> [doc1, doc2, ...] UA -> [2020-04-01 13:03:23, 2020-04-01 13:03:23]","title":"The inverted index"},{"location":"research/tfidf-score/notes/#query-processing","text":"If we want to query which document have the phrash \"information retrival\", we'd like to take the invert index postings list for \"information\" and \"retrival\", then find the document that have both word. (by run a merge algorithm on the two postings list)","title":"Query processing"},{"location":"research/tfidf-score/notes/#boolean-query","text":"Mainly leverage the boolean operation on the list merging algorithm.","title":"Boolean query"},{"location":"research/tfidf-score/notes/#phrase-queries","text":"no longer suffice to store <term, doc> entries. First attempt, biword indexes. treat the two words phrase as a single phrase. For longer phrase queries, we can broken into the boolean query on biword. false positive, two words not adjacent index blowup due to big dictionary positional indexes, store the positions of each token occurs at a document. use a merge algorithm to recursively at the document level.","title":"Phrase queries"},{"location":"research/tfidf-score/notes/#ranked-retriveval","text":"\"How can we rank-order the documents in the collection with respect to a query?\" assign a score to each document - say in [0, 1] - to each document. This score measures how well document and query match. We need a way to assigning a score to a query/document pair. The more frequent a term appears in a document, the higher the score for the document.","title":"Ranked retriveval"},{"location":"research/tfidf-score/notes/#scoring-with-the-jaccard-coefficient","text":"doesn't consider term frequency. wee need a better way of normalizing for length. (compare with cosine similarity)","title":"Scoring with the Jaccard Coefficient"},{"location":"research/tfidf-score/notes/#term-frequency-weighting","text":"binary vector \\in {0, 1} \\in {0, 1} count vector bag of words model Tip We want to use TF when computing query-document match scores, but the relevance does not increase proportionally with term frequency","title":"Term frequency weighting"},{"location":"research/tfidf-score/notes/#log-frequency-weighting","text":"w_{t, d} = \\begin{cases} 1 + \\log_{10} (tf_{t,d}), & tf_{t,d} \\ge 0 \\\\ 0, & \\textrm{otherwise} \\end{cases} w_{t, d} = \\begin{cases} 1 + \\log_{10} (tf_{t,d}), & tf_{t,d} \\ge 0 \\\\ 0, & \\textrm{otherwise} \\end{cases} Score for a document-query pair: sum over terms in both q q and d d : \\textrm{score} = \\sum_{t\\in q \\cap d} (1 + \\log_{10} (tf_{t, d})) \\textrm{score} = \\sum_{t\\in q \\cap d} (1 + \\log_{10} (tf_{t, d})) The score is 0 0 if none of the terms is presented in the document.","title":"Log-frequency weighting"},{"location":"research/tfidf-score/notes/#inverted-document-frequency-weighting","text":"idea: Rare terms are more informative than frequent terms. df_t df_t is an inverse measure of informativeness of term t t . There is one idf value for each term t t in a collection. idf_t = \\log_{10}(N/df_t) idf_t = \\log_{10}(N/df_t) Question Does idf have an effect on ranking for one-term queries, like \"iPhone\"? The answer is no, but why?","title":"(Inverted) Document frequency weighting"},{"location":"research/tfidf-score/notes/#collection-frequency-vs-document-frequency","text":"","title":"collection frequency vs document frequency"},{"location":"research/tfidf-score/notes/#tf-idf-weighting","text":"\\textrm{score}(q, d) = \\sum_{t\\in q \\cap d} tf_{t, d} \\times idf_{t} \\textrm{score}(q, d) = \\sum_{t\\in q \\cap d} tf_{t, d} \\times idf_{t} binary vector -> count vector -> weight matrix","title":"TF-IDF weighting"},{"location":"research/tfidf-score/notes/#vector-space-model","text":"Euclidean distance, not a good option. because different vector length can have large distance","title":"Vector Space Model"},{"location":"research/tfidf-score/notes/#application-for-https-request-embedding","text":"If we can build a dataset, given a \"query\", it can efficiently retrive how many relevant \"document\" are in the past 2 hours? We will be able to tell wether the \"query\" is mallicious or not. in anti-abuse, we can use the current request as the query, all the reprevious request data that are indexed as the \"documents\". This will give us a score about the current request. This score can tell us the malliciousness of the request.","title":"application for https request embedding"},{"location":"research/tfidf-score/notes/#problem","text":"Current distribution score cannot identify scraping that routates headers. How to up weight bad requests with certain patterns? Currently, no good way to block signals with low false positives. How to use score instead of text pattern matching to reduce false positives?","title":"problem"},{"location":"research/tfidf-score/notes/#solution","text":"","title":"Solution"},{"location":"research/tfidf-score/notes/#calculate-tf-idf-for-individual-header-fields","text":"This solution models a collection of requests during a window as a document. term frequency = count of each UA, HC, CC, and RF in a time window document frequency = count of how many windows each UA, HC, CC, and RF occurs The window can be a time window or a fixed size window. In the fixed size window, instead of dividing by equal time slots, we divide by the fixed number of requests and treat each collection as a document. This model can also be used for the tokenized solution.","title":"calculate tf-idf for individual header fields"},{"location":"research/tfidf-score/notes/#tokenize-all-request-facets","text":"This solution is to break down the header fields into tokens and use a set of tokens as the query. Compare this to a phrase retrival task. term frequency = count of each token Question Can we use the cosine similarity to compare two queries? Because the similarity measure described in the lectures is for query-document pair. Can we compare query-query pair? Whether the TF-IDF embedding can capture enough similarity info and used for clustering?","title":"tokenize all request facets"},{"location":"research/tfidf-score/notes/#intuitions","text":"Using TF-IDF, we can measure the speed of damage from requests possess similar attributes (belongs to certain attacks). Specifically, given a request in realtime, it can check the current TF, and DF, and calculate the TF-IDF. All these three values are useful for us. TF - If TF is greater than a threshold setting. We can it is may be a high qps scraping. If TF is moderate or very small, we can check DF. DF - If DF is very large, the \"term\" maybe common to all sample in the space. If DF streak is not long enough in a series of previous documents. there might be a spike of increase for the term in some window before. If DF is very small, and the TF is large, we detected a spike. # TF DF result 1 large small bad v 2 large large bad/good x 3 small large bad/good x 4 small small good x TF and IDF used separately can help to observe bad signals, but they are not enough. We will need to use TF-IDF to help us to enhance our believe about the badness of a request. TF-IDF","title":"Intuitions"},{"location":"research/tfidf-score/notes/#practical-consideration","text":"How to calculation TF-IDF in sliding windows? title: HTTP requests embedding to score and search bot traffic.","title":"Practical consideration"},{"location":"seedlabs/","text":"SEED Labs \u00b6 Applied Cryptograph Notes Public-Key Cryptography and PKI","title":"Index"},{"location":"seedlabs/#seed-labs","text":"Applied Cryptograph Notes Public-Key Cryptography and PKI","title":"SEED Labs"},{"location":"seedlabs/applied-crypto/notes/","text":"Applied Cryptograph Notes \u00b6 GCD \u00b6 Definition \u00b6 GCD of two integer is the largest integer that divide both given integers. Calculation \u00b6 Factorization method \u00b6 Find the integer factorization of the two integer first, find the common factors and multiply together. Euclidean Algorithm \u00b6 The key idea of Euclidean algorithm is to use the smaller integer to \"measure\" the larger integer, then use the reminder to \"measure\" the smaller integer, and so on, untill the reminder is 0. Generally, gcd(a, b) can be obtained by apply the following sequence of equations until r_k = 0 r_k = 0 . a = q_0 b + r_0 a = q_0 b + r_0 b = q_1 r_0 + r_1 b = q_1 r_0 + r_1 r_0 = q_2 r_1 + r_2 r_0 = q_2 r_1 + r_2 \\vdots \\vdots r_{k - 3} = q_{k-1} r_{k - 2} + r_{k - 1} r_{k - 3} = q_{k-1} r_{k - 2} + r_{k - 1} r_{k - 2} = q_{k-1} r_{k - 1} + r_{k} r_{k - 2} = q_{k-1} r_{k - 1} + r_{k} Because the qotient isn't needed in find the GCD. We can replace the iteration r_{k-2} = q_k r_{k-1} + r_k r_{k-2} = q_k r_{k-1} + r_k with modulo operations r_k = r_{k-2} \\mod r_{k-1} r_k = r_{k-2} \\mod r_{k-1} . Modular multiplicative inverse (modular division) \u00b6 Definition \u00b6 A modular multiplicative inverse of an integer a a is an integer x x such that the product ax ax is congruent to 1 with respect to the modulus m m . The congruence is written as ax \\equiv 1 \\pmod m ax \\equiv 1 \\pmod m . Two integers a a and b b are said to be congruent modulo m m if m m divides their differences, denoted by a \\equiv b \\pmod m a \\equiv b \\pmod m . Solution \u00b6 The modular multiplicative inverse x x of an integer a a may have zero, one or several solutions. But with the condition \\gcd(a, m) = 1 \\gcd(a, m) = 1 hold, there exists exactly one solution. This is the basis of the theory used to calculate the private key d d in RSA. Now we could use Extended Euclidean Algorithm to calculate d d . Extended Euclidean Algorithm \u00b6 def EEA ( r0 , r1 ): ''' Extended Euclidean algorithm''' # ensure r0 > r1 if r0 < r1 : r0 ^= r1 r1 ^= r0 r0 ^= r1 # init value s0 , s1 , t0 , t1 = 1 , 0 , 0 , 1 while True : ri = r0 % r1 q = ( r0 - ri ) / r1 si = s0 - q * s1 ti = t0 - q * t1 if ri == 0 : break ; r0 , r1 = r1 , ri s0 , s1 = s1 , si t0 , t1 = t1 , ti return r1 , s1 , t1 Euler's \\Phi \\Phi function \u00b6 Definition: The number of integers in \\mathbb{Z}_m \\mathbb{Z}_m relatively prime to m is denoted by \\Phi(m) \\Phi(m) . \\mathbb{Z}_m \\mathbb{Z}_m is interger set. \\mathbb{Z}_6 = \\{0,1,2,3,4,5\\} \\mathbb{Z}_6 = \\{0,1,2,3,4,5\\} . How to calculate \\Phi(m) \\Phi(m) \u00b6 Let m m have the folllowing canonical factorization m = p^{e_1}_{1} \\cdot p^{e_2}_{2} \\cdot, \\dots, \\cdot p^{e_n}_{n} m = p^{e_1}_{1} \\cdot p^{e_2}_{2} \\cdot, \\dots, \\cdot p^{e_n}_{n} Where the p_i p_i are distinct prime numbers and e_i e_i are positive integers, then \\Phi(m) = \\prod_{i = 1}^{n}{(p^{e_i}_{i} - p^{e_i - 1}_{i})}. \\Phi(m) = \\prod_{i = 1}^{n}{(p^{e_i}_{i} - p^{e_i - 1}_{i})}. For example: m = 240 = 2^4 \\cdot 3 \\cdot 5 m = 240 = 2^4 \\cdot 3 \\cdot 5 , \\Phi(240) = (2^4 - 2^3)\\cdot (3^1 - 3^0) \\cdot (5^1 - 5^0) = 8 \\cdot 2 \\cdot 4 = 64 \\Phi(240) = (2^4 - 2^3)\\cdot (3^1 - 3^0) \\cdot (5^1 - 5^0) = 8 \\cdot 2 \\cdot 4 = 64 m = 21 = 3 \\cdot 7 m = 21 = 3 \\cdot 7 , \\Phi(21) = (3^1 - 3^0)\\cdot (7^1 - 7^0) = 2\\cdot6 = 12 \\Phi(21) = (3^1 - 3^0)\\cdot (7^1 - 7^0) = 2\\cdot6 = 12 Remark 1: The security of RSA is based on the fact that Euler's function \\Phi(m) \\Phi(m) is hard to be calculated for large integers, as we know it is compuationally hard to do prime factorization for a large integer. Euler's Theorem \u00b6 Definition: Let a a and m m be integers with gcd(a, m) = 1, then: a^{\\Phi(m)} \\equiv1\\pmod m a^{\\Phi(m)} \\equiv1\\pmod m . Fermat's Little Theorem \u00b6 Definition: Let a a be an integer and p p be a prime, then: a^p \\equiv a \\pmod p a^p \\equiv a \\pmod p . It can also be stated in the form: a^{p-1} \\equiv 1 \\pmod p a^{p-1} \\equiv 1 \\pmod p . RSA Algorithm \u00b6 Key generation \u00b6 Choose large primes p p and q q . Calculate n = p\\cdot q n = p\\cdot q . Calculate \\Phi(n) = (p-1)(q-1) \\Phi(n) = (p-1)(q-1) . Choose public key K_{pub}=e \\in \\{1,2,\\cdots,\\Phi(n)-1\\} K_{pub}=e \\in \\{1,2,\\cdots,\\Phi(n)-1\\} such that \\gcd(e, \\Phi(n)) = 1 \\gcd(e, \\Phi(n)) = 1 ( e e and \\Phi(n) \\Phi(n) are co-prime). Compute the private key K_{priv}=d K_{priv}=d such that d\\cdot e \\equiv 1\\pmod {\\Phi(n)} d\\cdot e \\equiv 1\\pmod {\\Phi(n)} using the Extended Euclidean Algorithm. EEA: gcd(e, \\Phi(n)) = 1 \\Rightarrow gcd(e, \\Phi(n)) = d\\cdot e + t\\cdot \\Phi(n) \\Rightarrow d\\cdot e \\equiv 1\\pmod {\\Phi(n)} gcd(e, \\Phi(n)) = 1 \\Rightarrow gcd(e, \\Phi(n)) = d\\cdot e + t\\cdot \\Phi(n) \\Rightarrow d\\cdot e \\equiv 1\\pmod {\\Phi(n)} . Output of the routine: 1. K_{pub}=(n,e) K_{pub}=(n,e) , 2. K_{priv}=d K_{priv}=d Remark 1 The gcd condition is necessary to ensure the modular inverse of e e , which is d d , is unique. Remark 2 Eve have K_{pub} = (n, e) K_{pub} = (n, e) , but not \\Phi(n) \\Phi(n) . In order to compute the private key d d . He have to first compute \\Phi(n) \\Phi(n) . To compute \\Phi(n) \\Phi(n) , he need to compute the primie factorization of n n , which is very hard to compute when n is very large, such as a 2048-bit integer. This ensure the security of RSA. Encryption \u00b6 Given public key K_{pub} = (n, e) K_{pub} = (n, e) and the plain text message x \\in Z_n = \\{0, 1, \\cdots, n-1\\} x \\in Z_n = \\{0, 1, \\cdots, n-1\\} , the encrypted y y is computed by y = ENC_{k_{pub}}(x) \\equiv x^e\\bmod n y = ENC_{k_{pub}}(x) \\equiv x^e\\bmod n Decryption \u00b6 Given private key K_{priv}=d K_{priv}=d and the cipher text y \\in Z_n = \\{0, 1, \\cdots, n-1\\} y \\in Z_n = \\{0, 1, \\cdots, n-1\\} , the message can be decrypted as x = DEC_{k_{priv}}(y) \\equiv y^d\\bmod n x = DEC_{k_{priv}}(y) \\equiv y^d\\bmod n Fast exponentiation \u00b6 Note RSA one-way function, 1. encryption and decryption, 2. use n n and e e to compute d d .","title":"Applied Cryptograph Notes"},{"location":"seedlabs/applied-crypto/notes/#applied-cryptograph-notes","text":"","title":"Applied Cryptograph Notes"},{"location":"seedlabs/applied-crypto/notes/#gcd","text":"","title":"GCD"},{"location":"seedlabs/applied-crypto/notes/#definition","text":"GCD of two integer is the largest integer that divide both given integers.","title":"Definition"},{"location":"seedlabs/applied-crypto/notes/#calculation","text":"","title":"Calculation"},{"location":"seedlabs/applied-crypto/notes/#factorization-method","text":"Find the integer factorization of the two integer first, find the common factors and multiply together.","title":"Factorization method"},{"location":"seedlabs/applied-crypto/notes/#euclidean-algorithm","text":"The key idea of Euclidean algorithm is to use the smaller integer to \"measure\" the larger integer, then use the reminder to \"measure\" the smaller integer, and so on, untill the reminder is 0. Generally, gcd(a, b) can be obtained by apply the following sequence of equations until r_k = 0 r_k = 0 . a = q_0 b + r_0 a = q_0 b + r_0 b = q_1 r_0 + r_1 b = q_1 r_0 + r_1 r_0 = q_2 r_1 + r_2 r_0 = q_2 r_1 + r_2 \\vdots \\vdots r_{k - 3} = q_{k-1} r_{k - 2} + r_{k - 1} r_{k - 3} = q_{k-1} r_{k - 2} + r_{k - 1} r_{k - 2} = q_{k-1} r_{k - 1} + r_{k} r_{k - 2} = q_{k-1} r_{k - 1} + r_{k} Because the qotient isn't needed in find the GCD. We can replace the iteration r_{k-2} = q_k r_{k-1} + r_k r_{k-2} = q_k r_{k-1} + r_k with modulo operations r_k = r_{k-2} \\mod r_{k-1} r_k = r_{k-2} \\mod r_{k-1} .","title":"Euclidean Algorithm"},{"location":"seedlabs/applied-crypto/notes/#modular-multiplicative-inverse-modular-division","text":"","title":"Modular multiplicative inverse (modular division)"},{"location":"seedlabs/applied-crypto/notes/#definition_1","text":"A modular multiplicative inverse of an integer a a is an integer x x such that the product ax ax is congruent to 1 with respect to the modulus m m . The congruence is written as ax \\equiv 1 \\pmod m ax \\equiv 1 \\pmod m . Two integers a a and b b are said to be congruent modulo m m if m m divides their differences, denoted by a \\equiv b \\pmod m a \\equiv b \\pmod m .","title":"Definition"},{"location":"seedlabs/applied-crypto/notes/#solution","text":"The modular multiplicative inverse x x of an integer a a may have zero, one or several solutions. But with the condition \\gcd(a, m) = 1 \\gcd(a, m) = 1 hold, there exists exactly one solution. This is the basis of the theory used to calculate the private key d d in RSA. Now we could use Extended Euclidean Algorithm to calculate d d .","title":"Solution"},{"location":"seedlabs/applied-crypto/notes/#extended-euclidean-algorithm","text":"def EEA ( r0 , r1 ): ''' Extended Euclidean algorithm''' # ensure r0 > r1 if r0 < r1 : r0 ^= r1 r1 ^= r0 r0 ^= r1 # init value s0 , s1 , t0 , t1 = 1 , 0 , 0 , 1 while True : ri = r0 % r1 q = ( r0 - ri ) / r1 si = s0 - q * s1 ti = t0 - q * t1 if ri == 0 : break ; r0 , r1 = r1 , ri s0 , s1 = s1 , si t0 , t1 = t1 , ti return r1 , s1 , t1","title":"Extended Euclidean Algorithm"},{"location":"seedlabs/applied-crypto/notes/#eulers-phiphi-function","text":"Definition: The number of integers in \\mathbb{Z}_m \\mathbb{Z}_m relatively prime to m is denoted by \\Phi(m) \\Phi(m) . \\mathbb{Z}_m \\mathbb{Z}_m is interger set. \\mathbb{Z}_6 = \\{0,1,2,3,4,5\\} \\mathbb{Z}_6 = \\{0,1,2,3,4,5\\} .","title":"Euler's \\Phi\\Phi function"},{"location":"seedlabs/applied-crypto/notes/#how-to-calculate-phimphim","text":"Let m m have the folllowing canonical factorization m = p^{e_1}_{1} \\cdot p^{e_2}_{2} \\cdot, \\dots, \\cdot p^{e_n}_{n} m = p^{e_1}_{1} \\cdot p^{e_2}_{2} \\cdot, \\dots, \\cdot p^{e_n}_{n} Where the p_i p_i are distinct prime numbers and e_i e_i are positive integers, then \\Phi(m) = \\prod_{i = 1}^{n}{(p^{e_i}_{i} - p^{e_i - 1}_{i})}. \\Phi(m) = \\prod_{i = 1}^{n}{(p^{e_i}_{i} - p^{e_i - 1}_{i})}. For example: m = 240 = 2^4 \\cdot 3 \\cdot 5 m = 240 = 2^4 \\cdot 3 \\cdot 5 , \\Phi(240) = (2^4 - 2^3)\\cdot (3^1 - 3^0) \\cdot (5^1 - 5^0) = 8 \\cdot 2 \\cdot 4 = 64 \\Phi(240) = (2^4 - 2^3)\\cdot (3^1 - 3^0) \\cdot (5^1 - 5^0) = 8 \\cdot 2 \\cdot 4 = 64 m = 21 = 3 \\cdot 7 m = 21 = 3 \\cdot 7 , \\Phi(21) = (3^1 - 3^0)\\cdot (7^1 - 7^0) = 2\\cdot6 = 12 \\Phi(21) = (3^1 - 3^0)\\cdot (7^1 - 7^0) = 2\\cdot6 = 12 Remark 1: The security of RSA is based on the fact that Euler's function \\Phi(m) \\Phi(m) is hard to be calculated for large integers, as we know it is compuationally hard to do prime factorization for a large integer.","title":"How to calculate \\Phi(m)\\Phi(m)"},{"location":"seedlabs/applied-crypto/notes/#eulers-theorem","text":"Definition: Let a a and m m be integers with gcd(a, m) = 1, then: a^{\\Phi(m)} \\equiv1\\pmod m a^{\\Phi(m)} \\equiv1\\pmod m .","title":"Euler's Theorem"},{"location":"seedlabs/applied-crypto/notes/#fermats-little-theorem","text":"Definition: Let a a be an integer and p p be a prime, then: a^p \\equiv a \\pmod p a^p \\equiv a \\pmod p . It can also be stated in the form: a^{p-1} \\equiv 1 \\pmod p a^{p-1} \\equiv 1 \\pmod p .","title":"Fermat's Little Theorem"},{"location":"seedlabs/applied-crypto/notes/#rsa-algorithm","text":"","title":"RSA Algorithm"},{"location":"seedlabs/applied-crypto/notes/#key-generation","text":"Choose large primes p p and q q . Calculate n = p\\cdot q n = p\\cdot q . Calculate \\Phi(n) = (p-1)(q-1) \\Phi(n) = (p-1)(q-1) . Choose public key K_{pub}=e \\in \\{1,2,\\cdots,\\Phi(n)-1\\} K_{pub}=e \\in \\{1,2,\\cdots,\\Phi(n)-1\\} such that \\gcd(e, \\Phi(n)) = 1 \\gcd(e, \\Phi(n)) = 1 ( e e and \\Phi(n) \\Phi(n) are co-prime). Compute the private key K_{priv}=d K_{priv}=d such that d\\cdot e \\equiv 1\\pmod {\\Phi(n)} d\\cdot e \\equiv 1\\pmod {\\Phi(n)} using the Extended Euclidean Algorithm. EEA: gcd(e, \\Phi(n)) = 1 \\Rightarrow gcd(e, \\Phi(n)) = d\\cdot e + t\\cdot \\Phi(n) \\Rightarrow d\\cdot e \\equiv 1\\pmod {\\Phi(n)} gcd(e, \\Phi(n)) = 1 \\Rightarrow gcd(e, \\Phi(n)) = d\\cdot e + t\\cdot \\Phi(n) \\Rightarrow d\\cdot e \\equiv 1\\pmod {\\Phi(n)} . Output of the routine: 1. K_{pub}=(n,e) K_{pub}=(n,e) , 2. K_{priv}=d K_{priv}=d Remark 1 The gcd condition is necessary to ensure the modular inverse of e e , which is d d , is unique. Remark 2 Eve have K_{pub} = (n, e) K_{pub} = (n, e) , but not \\Phi(n) \\Phi(n) . In order to compute the private key d d . He have to first compute \\Phi(n) \\Phi(n) . To compute \\Phi(n) \\Phi(n) , he need to compute the primie factorization of n n , which is very hard to compute when n is very large, such as a 2048-bit integer. This ensure the security of RSA.","title":"Key generation"},{"location":"seedlabs/applied-crypto/notes/#encryption","text":"Given public key K_{pub} = (n, e) K_{pub} = (n, e) and the plain text message x \\in Z_n = \\{0, 1, \\cdots, n-1\\} x \\in Z_n = \\{0, 1, \\cdots, n-1\\} , the encrypted y y is computed by y = ENC_{k_{pub}}(x) \\equiv x^e\\bmod n y = ENC_{k_{pub}}(x) \\equiv x^e\\bmod n","title":"Encryption"},{"location":"seedlabs/applied-crypto/notes/#decryption","text":"Given private key K_{priv}=d K_{priv}=d and the cipher text y \\in Z_n = \\{0, 1, \\cdots, n-1\\} y \\in Z_n = \\{0, 1, \\cdots, n-1\\} , the message can be decrypted as x = DEC_{k_{priv}}(y) \\equiv y^d\\bmod n x = DEC_{k_{priv}}(y) \\equiv y^d\\bmod n","title":"Decryption"},{"location":"seedlabs/applied-crypto/notes/#fast-exponentiation","text":"Note RSA one-way function, 1. encryption and decryption, 2. use n n and e e to compute d d .","title":"Fast exponentiation"},{"location":"seedlabs/public-key-cryptography-and-pki/notes/","text":"Public-Key Cryptography and PKI \u00b6 Questions to answer \u00b6 What is a Certificate Authority (CA), and how to become one? How to create a certificate for a customer? (i.e. www.ruihan.org) How PKI is used for web site authentication and encryption? What are the necessary files of such an infrastructure? 0x1 Become a Certificate Authority (CA) \u00b6 CA is a trusted entity to issue digital certificates. The digital certificate certifies the ownership of a public key by the named subject of the certificate. Root CAs are a number of trusted commercial CAs. (VeriSign is one of the largest) However, we are not obligate to the commercial world, we are free to become a root CA, and then use this root CA to issue certificates to others. Remember that the root CAs are self-signed. root CA's certificates are pre-loaded into most OS, browsers, and other software that using PKI. Root CA's certificates are unconditionally trusted. To create a root CA, we use the following openssl command to generate a self-signed certificate for the CA. seed@ubuntu:~/SEED-Labs/PKI-Lab$ openssl req -new -x509 -keyout ca.key -out ca.crt -config openssl.cnf Generating a 1024 bit RSA private key ..........................++++++ ..++++++ unable to write 'random state' writing new private key to 'ca.key' Enter PEM pass phrase: Verifying - Enter PEM pass phrase: ----- You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter '.', the field will be left blank. ----- Country Name (2 letter code) [AU]:US State or Province Name (full name) [Some-State]:CA Locality Name (eg, city) []:San Jose Organization Name (eg, company) [Internet Widgits Pty Ltd]:PKILabServer Organizational Unit Name (eg, section) []: Common Name (e.g. server FQDN or YOUR name) []:PKILabServer.com Email Address []:rui@PKILabServer.com [11/17/2016 11:14] seed@ubuntu:~/SEED-Labs/PKI-Lab$ The Configure File openssl.conf is used by three OpenSSL command: ca , req , and x509 . To get a glimpse of it's purporse, here is partial of the openssl.conf (from [CA_Default} section). dir = ./demoCA # Where everything is kept certs = $dir/certs # Where the issued certs are kept crl_dir = $dir/crl # Where the issued crl are kept new_certs_dir = $dir/newcerts # default place for new certs database = $dir/index.txt # database index file serial = $dir/serial # The current serial number 0x2 Create a Certificate for PKILabServer.com \u00b6 When one becomes a certificate autority(CA), he/she is ready to sign digital certificates for his/her customers. PKILabServer.com is a company that tries to get a digital certificate from a CA. He needs to do the following steps: Generate RSA key pairs Generate a certificate signing request (CSR) Generate a certificate 0x21 Generate RSA Key Paris \u00b6 This step could be done using OpenSSL command line tool like this: seed@ubuntu:~/SEED-Labs/PKI-Lab$ openssl genrsa -aes128 -out server.key 1024 Generating RSA private key, 1024 bit long modulus ....++++++ .............++++++ unable to write 'random state' e is 65537 (0x10001) Enter pass phrase for server.key: Verifying - Enter pass phrase for server.key: [11/17/2016 11:16] seed@ubuntu:~/SEED-Labs/PKI-Lab$ The generated keys will be stored in the file server.key . This file is encrypted as indicated by the option -ase128 , which means using AES-128 encrytpion algorithm. The effect of this is you have to enter a password when executing the above command. To see the plan text content of the server.key file, you can run the following command: seed@ubuntu:~/SEED-Labs/PKI-Lab$ openssl rsa -in server.key -text Enter pass phrase for server.key: Private-Key: (1024 bit) modulus: 00:e1:23:f9:75:2d:2c:d9:21:36:6f:62:d6:28:6a: 2c:c6:1a:3f:d0:77:c2:8c:e0:49:1a:95:b8:99:6d: 7f:15:cb:db:9c:42:6e:a9:c7:fd:af:cd:94:3e:b5: d3:48:1a:14:11:ca:a2:63:d6:27:32:bc:db:75:d7: 5e:05:9a:8c:2b:7d:0d:fc:6f:61:77:69:c3:37:6b: 94:a3:d0:5b:70:0d:80:9a:89:ea:10:04:cd:6f:4a: 72:b9:e7:d6:1b:fa:ff:25:a1:10:0e:05:d5:14:c7: 34:97:78:55:0b:c7:ed:c4:16:7c:0b:fa:df:46:dd: 7f:35:00:7f:72:21:02:38:51 publicExponent: 65537 (0x10001) privateExponent: 00:bb:06:91:72:1f:c7:03:d0:ad:51:b6:7f:45:2f: 7e:66:4a:e3:fc:1f:a0:84:3a:c5:3f:ca:64:81:40: 0e:b4:62:96:da:a5:ed:1c:29:40:ba:cc:42:7c:1d: bb:98:47:e1:a8:2d:f6:dc:8c:c8:1f:43:f7:e3:5a: b7:86:5c:2e:57:5a:ea:fa:8b:48:28:6a:4c:35:6b: fe:80:83:1f:9b:44:80:73:98:62:e2:bc:7e:e1:f0: 15:dd:74:dc:05:e0:5d:a5:59:a4:71:5f:96:13:e5: 8f:a6:4c:c0:31:50:ca:b1:b9:e4:92:de:e1:e9:2a: 2b:3a:2e:07:1c:fb:dc:ac:01 prime1: 00:f9:bf:6b:f0:28:15:b5:ba:2e:5f:c1:62:63:d7: 53:b0:ab:9c:3f:53:e1:9b:76:1c:12:fb:b0:61:c4: 34:fa:28:d7:94:d2:d3:d9:fe:e5:84:57:60:77:23: af:82:75:50:d6:0c:2e:2e:23:55:82:8c:6f:5c:b4: 5a:2f:77:82:b1 prime2: 00:e6:c6:d9:bf:57:3b:a4:1e:b4:62:c9:fd:5f:e9: d8:cc:d7:49:9e:13:db:99:2c:99:2e:7e:78:0a:c2: b1:b4:c4:2f:08:fc:76:89:e2:60:12:7f:b7:47:bb: d2:98:1e:03:99:42:d7:ec:ab:0d:55:52:95:5e:b8: 77:c5:55:37:a1 exponent1: 63:09:e2:fe:f0:96:73:63:6a:a2:74:68:d5:18:fd: ca:30:b3:9c:75:62:21:29:3c:46:d6:e2:82:52:b1: 83:86:90:bf:26:bf:f6:51:db:a8:98:91:db:8d:1e: 3b:88:d4:4e:9d:b3:ee:7d:fa:99:f3:a0:f1:cd:5f: 7a:35:55:71 exponent2: 00:b9:05:4e:48:80:a8:b9:71:40:90:3f:7f:5b:a9: 81:7e:e1:50:0d:63:c0:58:f3:0f:b0:de:06:62:22: 2c:15:50:80:a1:44:bf:c8:d6:6c:ce:08:05:2a:86: e4:55:bf:22:85:7a:b8:e0:ef:56:d6:44:4a:ab:51: 5b:fd:22:d0:61 coefficient: 3d:ec:20:c8:23:3e:d9:f3:88:8f:03:9d:1b:57:c8: 9d:87:14:83:a2:a8:2c:bc:cc:e7:dd:2d:c4:ef:74: 2a:21:2b:5a:ef:fb:79:49:f0:bc:6f:4e:d3:0e:f7: d2:48:af:b1:12:ae:43:e6:1b:03:bb:f2:18:f1:61: 8b:7a:1e:8f writing RSA key -----BEGIN RSA PRIVATE KEY----- MIICXQIBAAKBgQDhI/l1LSzZITZvYtYoaizGGj/Qd8KM4EkalbiZbX8Vy9ucQm6p x/2vzZQ+tdNIGhQRyqJj1icyvNt1114FmowrfQ38b2F3acM3a5Sj0FtwDYCaieoQ BM1vSnK559Yb+v8loRAOBdUUxzSXeFULx+3EFnwL+t9G3X81AH9yIQI4UQIDAQAB AoGBALsGkXIfxwPQrVG2f0UvfmZK4/wfoIQ6xT/KZIFADrRiltql7RwpQLrMQnwd u5hH4agt9tyMyB9D9+Nat4ZcLlda6vqLSChqTDVr/oCDH5tEgHOYYuK8fuHwFd10 3AXgXaVZpHFflhPlj6ZMwDFQyrG55JLe4ekqKzouBxz73KwBAkEA+b9r8CgVtbou X8FiY9dTsKucP1Phm3YcEvuwYcQ0+ijXlNLT2f7lhFdgdyOvgnVQ1gwuLiNVgoxv XLRaL3eCsQJBAObG2b9XO6QetGLJ/V/p2MzXSZ4T25ksmS5+eArCsbTELwj8doni YBJ/t0e70pgeA5lC1+yrDVVSlV64d8VVN6ECQGMJ4v7wlnNjaqJ0aNUY/cows5x1 YiEpPEbW4oJSsYOGkL8mv/ZR26iYkduNHjuI1E6ds+59+pnzoPHNX3o1VXECQQC5 BU5IgKi5cUCQP39bqYF+4VANY8BY8w+w3gZiIiwVUIChRL/I1mzOCAUqhuRVvyKF erjg71bWREqrUVv9ItBhAkA97CDIIz7Z84iPA50bV8idhxSDoqgsvMzn3S3E73Qq ISta7/t5SfC8b07TDvfSSK+xEq5D5hsDu/IY8WGLeh6P -----END RSA PRIVATE KEY----- [11/17/2016 11:57] seed@ubuntu:~/SEED-Labs/PKI-Lab$ The password you entered works like a key to decrypt the ciphertext in the file. 0x22 Generate a certificate signing request (CSR) \u00b6 To request a certificate, the company need to include its public key in a certificate signing request (CSR) and send the CSR to the certificate authority. The command to generate the CSR using OpenSSL is: seed@ubuntu:~/SEED-Labs/PKI-Lab$ openssl req -new -key server.key -out server.csr -config openssl.cnf Enter pass phrase for server.key: You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter '.', the field will be left blank. ----- Country Name (2 letter code) [AU]:US State or Province Name (full name) [Some-State]:CA Locality Name (eg, city) []:San Jose Organization Name (eg, company) [Internet Widgits Pty Ltd]:PKILabServer Organizational Unit Name (eg, section) []: Common Name (e.g. server FQDN or YOUR name) []:PKILabServer.com Email Address []:admin@PKILabServer.com Please enter the following 'extra' attributes to be sent with your certificate request A challenge password []:server.csr An optional company name []: [11/17/2016 11:19] seed@ubuntu:~/SEED-Labs/PKI-Lab$ Note that this command is quite similar to the one we used in creating a self-signed certificate for the certificate authority previously. The only difference is the -x509 option. 0x23 Generate a certificate \u00b6 The CSR file needs to have the CA's signature to form a certificate. When received the CSR file, CA will first verify the identity information in the CSR. In reality, CA might contact the company through physical channel to do the verification. For example, through traditional mails or letters. For this experiment, we can assume we are the CA and will generate the certificate for the company. The following command turns the certificate signing request ( server.csr ) into an X509 certificate ( server.crt ), using the CA's ca.crt and ca.key : seed@ubuntu:~/SEED-Labs/PKI-Lab$ openssl ca -in server.csr -out server.crt -cert ca.crt -keyfile ca.key -config openssl.cnf Using configuration from openssl.cnf Enter pass phrase for ca.key: Check that the request matches the signature Signature ok Certificate Details: Serial Number: 4096 (0x1000) Validity Not Before: Nov 17 19:21:33 2016 GMT Not After : Nov 17 19:21:33 2017 GMT Subject: countryName = US stateOrProvinceName = CA localityName = San Jose organizationName = PKILabServer commonName = PKILabServer.com emailAddress = admin@PKILabServer.com X509v3 extensions: X509v3 Basic Constraints: CA:FALSE Netscape Comment: OpenSSL Generated Certificate X509v3 Subject Key Identifier: B4:F1:39:2A:56:C6:1F:1F:9C:49:AF:0B:45:FC:FD:79:66:90:F2:07 X509v3 Authority Key Identifier: keyid:1D:11:39:21:1C:E2:0D:8B:14:34:B8:36:C4:F5:34:27:09:9D:E8:7A Certificate is to be certified until Nov 17 19:21:33 2017 GMT (365 days) Sign the certificate? [y/n]:y 1 out of 1 certificate requests certified, commit? [y/n]y Write out database with 1 new entries Data Base Updated unable to write 'random state' [11/17/2016 11:22] seed@ubuntu:~/SEED-Labs/PKI-Lab$ If OpenSSL refuses to generate certificates, it is very likely that the names in your requests do not match with those of CA. (yes, think about you are creating a root CA for your company, and use it to sign certificates for different departments.) The matching rules are specified in the configuration file (at the [policy match] section). You can change the names of your requests to comply with the policy, or you can change the policy. The configuration file also includes another policy (called policy_anything ), which is less restrictive. You can choose that policy by changing the following line: \"policy = policy_match\" change to \"policy = policy_anything\". 0x3 Use PKI for Web Sites \u00b6 Following the lab manual didn't work out. While adding the ca.crt to Firefox, I run into the following error that the format isn't supported by Firefox. Failed to decode the file. Either it is not in PKCS #12 format, has been corrupted, or the password you entered was incorrect. Find this stackoverflow post . We should use the following command to combine the server.crt with the ca.crt . openssl pkcs12 -export -in server.crt -inkey server.key -out PKILabServerFirefox.pfx -certfile ca.crt Finally, we can import PKILabServerFirefox.pfx to Firefox and right click the certificate and select \"Edit Trust->this certificate can identify websites\" 0x4 Test the certificate \u00b6 start a openssl simple webserver: seed@ubuntu:~/SEED-Labs/PKI-Lab$ cp server.key server.pem seed@ubuntu:~/SEED-Labs/PKI-Lab$ cat server.crt >> server.pem seed@ubuntu:~/SEED-Labs/PKI-Lab$ openssl s_server -cert server.pem -www -accpet 6666 Now navigate to https://www.pkilabserver.com:6666 , the security verification is passed and show the openssl web server default page. s_server -cert server.pem -www -accept 3633 Ciphers supported in s_server binary TLSv1/SSLv3:ECDHE-RSA-AES256-GCM-SHA384TLSv1/SSLv3:ECDHE-ECDSA-AES256-GCM-SHA384 TLSv1/SSLv3:ECDHE-RSA-AES256-SHA384 TLSv1/SSLv3:ECDHE-ECDSA-AES256-SHA384 TLSv1/SSLv3:ECDHE-RSA-AES256-SHA TLSv1/SSLv3:ECDHE-ECDSA-AES256-SHA TLSv1/SSLv3:SRP-DSS-AES-256-CBC-SHA TLSv1/SSLv3:SRP-RSA-AES-256-CBC-SHA TLSv1/SSLv3:SRP-AES-256-CBC-SHA TLSv1/SSLv3:DHE-DSS-AES256-GCM-SHA384 TLSv1/SSLv3:DHE-RSA-AES256-GCM-SHA384TLSv1/SSLv3:DHE-RSA-AES256-SHA256 TLSv1/SSLv3:DHE-DSS-AES256-SHA256 TLSv1/SSLv3:DHE-RSA-AES256-SHA TLSv1/SSLv3:DHE-DSS-AES256-SHA TLSv1/SSLv3:DHE-RSA-CAMELLIA256-SHA TLSv1/SSLv3:DHE-DSS-CAMELLIA256-SHA TLSv1/SSLv3:ECDH-RSA-AES256-GCM-SHA384 TLSv1/SSLv3:ECDH-ECDSA-AES256-GCM-SHA384TLSv1/SSLv3:ECDH-RSA-AES256-SHA384 TLSv1/SSLv3:ECDH-ECDSA-AES256-SHA384 TLSv1/SSLv3:ECDH-RSA-AES256-SHA TLSv1/SSLv3:ECDH-ECDSA-AES256-SHA TLSv1/SSLv3:AES256-GCM-SHA384 TLSv1/SSLv3:AES256-SHA256 TLSv1/SSLv3:AES256-SHA TLSv1/SSLv3:CAMELLIA256-SHA TLSv1/SSLv3:PSK-AES256-CBC-SHA TLSv1/SSLv3:ECDHE-RSA-DES-CBC3-SHA TLSv1/SSLv3:ECDHE-ECDSA-DES-CBC3-SHA TLSv1/SSLv3:SRP-DSS-3DES-EDE-CBC-SHA TLSv1/SSLv3:SRP-RSA-3DES-EDE-CBC-SHA TLSv1/SSLv3:SRP-3DES-EDE-CBC-SHA TLSv1/SSLv3:EDH-RSA-DES-CBC3-SHA TLSv1/SSLv3:EDH-DSS-DES-CBC3-SHA TLSv1/SSLv3:ECDH-RSA-DES-CBC3-SHA TLSv1/SSLv3:ECDH-ECDSA-DES-CBC3-SHA TLSv1/SSLv3:DES-CBC3-SHA TLSv1/SSLv3:PSK-3DES-EDE-CBC-SHA TLSv1/SSLv3:ECDHE-RSA-AES128-GCM-SHA256 TLSv1/SSLv3:ECDHE-ECDSA-AES128-GCM-SHA256TLSv1/SSLv3:ECDHE-RSA-AES128-SHA256 TLSv1/SSLv3:ECDHE-ECDSA-AES128-SHA256TLSv1/SSLv3:ECDHE-RSA-AES128-SHA TLSv1/SSLv3:ECDHE-ECDSA-AES128-SHA TLSv1/SSLv3:SRP-DSS-AES-128-CBC-SHA TLSv1/SSLv3:SRP-RSA-AES-128-CBC-SHA TLSv1/SSLv3:SRP-AES-128-CBC-SHA TLSv1/SSLv3:DHE-DSS-AES128-GCM-SHA256TLSv1/SSLv3:DHE-RSA-AES128-GCM-SHA256 TLSv1/SSLv3:DHE-RSA-AES128-SHA256 TLSv1/SSLv3:DHE-DSS-AES128-SHA256 TLSv1/SSLv3:DHE-RSA-AES128-SHA TLSv1/SSLv3:DHE-DSS-AES128-SHA TLSv1/SSLv3:DHE-RSA-SEED-SHA TLSv1/SSLv3:DHE-DSS-SEED-SHA TLSv1/SSLv3:DHE-RSA-CAMELLIA128-SHA TLSv1/SSLv3:DHE-DSS-CAMELLIA128-SHA TLSv1/SSLv3:ECDH-RSA-AES128-GCM-SHA256TLSv1/SSLv3:ECDH-ECDSA-AES128-GCM-SHA256 TLSv1/SSLv3:ECDH-RSA-AES128-SHA256 TLSv1/SSLv3:ECDH-ECDSA-AES128-SHA256 TLSv1/SSLv3:ECDH-RSA-AES128-SHA TLSv1/SSLv3:ECDH-ECDSA-AES128-SHA TLSv1/SSLv3:AES128-GCM-SHA256 TLSv1/SSLv3:AES128-SHA256 TLSv1/SSLv3:AES128-SHA TLSv1/SSLv3:SEED-SHA TLSv1/SSLv3:CAMELLIA128-SHA TLSv1/SSLv3:PSK-AES128-CBC-SHA TLSv1/SSLv3:ECDHE-RSA-RC4-SHA TLSv1/SSLv3:ECDHE-ECDSA-RC4-SHA TLSv1/SSLv3:ECDH-RSA-RC4-SHA TLSv1/SSLv3:ECDH-ECDSA-RC4-SHA TLSv1/SSLv3:RC4-SHA TLSv1/SSLv3:RC4-MD5 TLSv1/SSLv3:PSK-RC4-SHA TLSv1/SSLv3:EDH-RSA-DES-CBC-SHA TLSv1/SSLv3:EDH-DSS-DES-CBC-SHA TLSv1/SSLv3:DES-CBC-SHA --- Reused, TLSv1/SSLv3, Cipher is ECDHE-RSA-AES256-SHA SSL-Session: Protocol : TLSv1 Cipher : ECDHE-RSA-AES256-SHA Session-ID: 124BFE9E98B4DF8E5DC0CC87DEB16DDE615FEC0B63A5FD687CD7033012639DD1 Session-ID-ctx: 01000000 Master-Key: A23B112D5A56B98709D5C140EEC5876155C6023CF8D5DCD81858B1EFF3B4E9CE8723E2D9703C217D8C517A762820B32C Key-Arg : None PSK identity: None PSK identity hint: None SRP username: None Start Time: 1479414975 Timeout : 300 (sec) Verify return code: 0 (ok) --- 0 items in the session cache 0 client connects (SSL_connect()) 0 client renegotiates (SSL_connect()) 0 client connects that finished 16 server accepts (SSL_accept()) 0 server renegotiates (SSL_accept()) 16 server accepts that finished 3 session cache hits 0 session cache misses 0 session cache timeouts 0 callback cache hits 0 cache full overflows (128 allowed) --- no client certificate available 0x5 RSA and AES encryption performace \u00b6 We use the following commands to evaluate RSA encryption and decryption: # generate private key openssl genrsa -out private-rsa.pem 1024 # generate public key openssl genrsa -in private-rsa.pem -pubout -out public-rsa.pem # encrypt using public key openssl rsautl -encrypt -pubin -inkey public-rsa.pem -in message.txt -out message_enc.txt # decrypt using private key openssl rsautl -decrypt -inkey public-rsa.pem -in message_enc.txt -out message_dec.txt Commands for AES encrytption and decryption TODO","title":"Public Key Cryptography and PKI"},{"location":"seedlabs/public-key-cryptography-and-pki/notes/#public-key-cryptography-and-pki","text":"","title":"Public-Key Cryptography and PKI"},{"location":"seedlabs/public-key-cryptography-and-pki/notes/#questions-to-answer","text":"What is a Certificate Authority (CA), and how to become one? How to create a certificate for a customer? (i.e. www.ruihan.org) How PKI is used for web site authentication and encryption? What are the necessary files of such an infrastructure?","title":"Questions to answer"},{"location":"seedlabs/public-key-cryptography-and-pki/notes/#0x1-become-a-certificate-authority-ca","text":"CA is a trusted entity to issue digital certificates. The digital certificate certifies the ownership of a public key by the named subject of the certificate. Root CAs are a number of trusted commercial CAs. (VeriSign is one of the largest) However, we are not obligate to the commercial world, we are free to become a root CA, and then use this root CA to issue certificates to others. Remember that the root CAs are self-signed. root CA's certificates are pre-loaded into most OS, browsers, and other software that using PKI. Root CA's certificates are unconditionally trusted. To create a root CA, we use the following openssl command to generate a self-signed certificate for the CA. seed@ubuntu:~/SEED-Labs/PKI-Lab$ openssl req -new -x509 -keyout ca.key -out ca.crt -config openssl.cnf Generating a 1024 bit RSA private key ..........................++++++ ..++++++ unable to write 'random state' writing new private key to 'ca.key' Enter PEM pass phrase: Verifying - Enter PEM pass phrase: ----- You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter '.', the field will be left blank. ----- Country Name (2 letter code) [AU]:US State or Province Name (full name) [Some-State]:CA Locality Name (eg, city) []:San Jose Organization Name (eg, company) [Internet Widgits Pty Ltd]:PKILabServer Organizational Unit Name (eg, section) []: Common Name (e.g. server FQDN or YOUR name) []:PKILabServer.com Email Address []:rui@PKILabServer.com [11/17/2016 11:14] seed@ubuntu:~/SEED-Labs/PKI-Lab$ The Configure File openssl.conf is used by three OpenSSL command: ca , req , and x509 . To get a glimpse of it's purporse, here is partial of the openssl.conf (from [CA_Default} section). dir = ./demoCA # Where everything is kept certs = $dir/certs # Where the issued certs are kept crl_dir = $dir/crl # Where the issued crl are kept new_certs_dir = $dir/newcerts # default place for new certs database = $dir/index.txt # database index file serial = $dir/serial # The current serial number","title":"0x1 Become a Certificate Authority (CA)"},{"location":"seedlabs/public-key-cryptography-and-pki/notes/#0x2-create-a-certificate-for-pkilabservercom","text":"When one becomes a certificate autority(CA), he/she is ready to sign digital certificates for his/her customers. PKILabServer.com is a company that tries to get a digital certificate from a CA. He needs to do the following steps: Generate RSA key pairs Generate a certificate signing request (CSR) Generate a certificate","title":"0x2 Create a Certificate for PKILabServer.com"},{"location":"seedlabs/public-key-cryptography-and-pki/notes/#0x21-generate-rsa-key-paris","text":"This step could be done using OpenSSL command line tool like this: seed@ubuntu:~/SEED-Labs/PKI-Lab$ openssl genrsa -aes128 -out server.key 1024 Generating RSA private key, 1024 bit long modulus ....++++++ .............++++++ unable to write 'random state' e is 65537 (0x10001) Enter pass phrase for server.key: Verifying - Enter pass phrase for server.key: [11/17/2016 11:16] seed@ubuntu:~/SEED-Labs/PKI-Lab$ The generated keys will be stored in the file server.key . This file is encrypted as indicated by the option -ase128 , which means using AES-128 encrytpion algorithm. The effect of this is you have to enter a password when executing the above command. To see the plan text content of the server.key file, you can run the following command: seed@ubuntu:~/SEED-Labs/PKI-Lab$ openssl rsa -in server.key -text Enter pass phrase for server.key: Private-Key: (1024 bit) modulus: 00:e1:23:f9:75:2d:2c:d9:21:36:6f:62:d6:28:6a: 2c:c6:1a:3f:d0:77:c2:8c:e0:49:1a:95:b8:99:6d: 7f:15:cb:db:9c:42:6e:a9:c7:fd:af:cd:94:3e:b5: d3:48:1a:14:11:ca:a2:63:d6:27:32:bc:db:75:d7: 5e:05:9a:8c:2b:7d:0d:fc:6f:61:77:69:c3:37:6b: 94:a3:d0:5b:70:0d:80:9a:89:ea:10:04:cd:6f:4a: 72:b9:e7:d6:1b:fa:ff:25:a1:10:0e:05:d5:14:c7: 34:97:78:55:0b:c7:ed:c4:16:7c:0b:fa:df:46:dd: 7f:35:00:7f:72:21:02:38:51 publicExponent: 65537 (0x10001) privateExponent: 00:bb:06:91:72:1f:c7:03:d0:ad:51:b6:7f:45:2f: 7e:66:4a:e3:fc:1f:a0:84:3a:c5:3f:ca:64:81:40: 0e:b4:62:96:da:a5:ed:1c:29:40:ba:cc:42:7c:1d: bb:98:47:e1:a8:2d:f6:dc:8c:c8:1f:43:f7:e3:5a: b7:86:5c:2e:57:5a:ea:fa:8b:48:28:6a:4c:35:6b: fe:80:83:1f:9b:44:80:73:98:62:e2:bc:7e:e1:f0: 15:dd:74:dc:05:e0:5d:a5:59:a4:71:5f:96:13:e5: 8f:a6:4c:c0:31:50:ca:b1:b9:e4:92:de:e1:e9:2a: 2b:3a:2e:07:1c:fb:dc:ac:01 prime1: 00:f9:bf:6b:f0:28:15:b5:ba:2e:5f:c1:62:63:d7: 53:b0:ab:9c:3f:53:e1:9b:76:1c:12:fb:b0:61:c4: 34:fa:28:d7:94:d2:d3:d9:fe:e5:84:57:60:77:23: af:82:75:50:d6:0c:2e:2e:23:55:82:8c:6f:5c:b4: 5a:2f:77:82:b1 prime2: 00:e6:c6:d9:bf:57:3b:a4:1e:b4:62:c9:fd:5f:e9: d8:cc:d7:49:9e:13:db:99:2c:99:2e:7e:78:0a:c2: b1:b4:c4:2f:08:fc:76:89:e2:60:12:7f:b7:47:bb: d2:98:1e:03:99:42:d7:ec:ab:0d:55:52:95:5e:b8: 77:c5:55:37:a1 exponent1: 63:09:e2:fe:f0:96:73:63:6a:a2:74:68:d5:18:fd: ca:30:b3:9c:75:62:21:29:3c:46:d6:e2:82:52:b1: 83:86:90:bf:26:bf:f6:51:db:a8:98:91:db:8d:1e: 3b:88:d4:4e:9d:b3:ee:7d:fa:99:f3:a0:f1:cd:5f: 7a:35:55:71 exponent2: 00:b9:05:4e:48:80:a8:b9:71:40:90:3f:7f:5b:a9: 81:7e:e1:50:0d:63:c0:58:f3:0f:b0:de:06:62:22: 2c:15:50:80:a1:44:bf:c8:d6:6c:ce:08:05:2a:86: e4:55:bf:22:85:7a:b8:e0:ef:56:d6:44:4a:ab:51: 5b:fd:22:d0:61 coefficient: 3d:ec:20:c8:23:3e:d9:f3:88:8f:03:9d:1b:57:c8: 9d:87:14:83:a2:a8:2c:bc:cc:e7:dd:2d:c4:ef:74: 2a:21:2b:5a:ef:fb:79:49:f0:bc:6f:4e:d3:0e:f7: d2:48:af:b1:12:ae:43:e6:1b:03:bb:f2:18:f1:61: 8b:7a:1e:8f writing RSA key -----BEGIN RSA PRIVATE KEY----- MIICXQIBAAKBgQDhI/l1LSzZITZvYtYoaizGGj/Qd8KM4EkalbiZbX8Vy9ucQm6p x/2vzZQ+tdNIGhQRyqJj1icyvNt1114FmowrfQ38b2F3acM3a5Sj0FtwDYCaieoQ BM1vSnK559Yb+v8loRAOBdUUxzSXeFULx+3EFnwL+t9G3X81AH9yIQI4UQIDAQAB AoGBALsGkXIfxwPQrVG2f0UvfmZK4/wfoIQ6xT/KZIFADrRiltql7RwpQLrMQnwd u5hH4agt9tyMyB9D9+Nat4ZcLlda6vqLSChqTDVr/oCDH5tEgHOYYuK8fuHwFd10 3AXgXaVZpHFflhPlj6ZMwDFQyrG55JLe4ekqKzouBxz73KwBAkEA+b9r8CgVtbou X8FiY9dTsKucP1Phm3YcEvuwYcQ0+ijXlNLT2f7lhFdgdyOvgnVQ1gwuLiNVgoxv XLRaL3eCsQJBAObG2b9XO6QetGLJ/V/p2MzXSZ4T25ksmS5+eArCsbTELwj8doni YBJ/t0e70pgeA5lC1+yrDVVSlV64d8VVN6ECQGMJ4v7wlnNjaqJ0aNUY/cows5x1 YiEpPEbW4oJSsYOGkL8mv/ZR26iYkduNHjuI1E6ds+59+pnzoPHNX3o1VXECQQC5 BU5IgKi5cUCQP39bqYF+4VANY8BY8w+w3gZiIiwVUIChRL/I1mzOCAUqhuRVvyKF erjg71bWREqrUVv9ItBhAkA97CDIIz7Z84iPA50bV8idhxSDoqgsvMzn3S3E73Qq ISta7/t5SfC8b07TDvfSSK+xEq5D5hsDu/IY8WGLeh6P -----END RSA PRIVATE KEY----- [11/17/2016 11:57] seed@ubuntu:~/SEED-Labs/PKI-Lab$ The password you entered works like a key to decrypt the ciphertext in the file.","title":"0x21 Generate RSA Key Paris"},{"location":"seedlabs/public-key-cryptography-and-pki/notes/#0x22-generate-a-certificate-signing-request-csr","text":"To request a certificate, the company need to include its public key in a certificate signing request (CSR) and send the CSR to the certificate authority. The command to generate the CSR using OpenSSL is: seed@ubuntu:~/SEED-Labs/PKI-Lab$ openssl req -new -key server.key -out server.csr -config openssl.cnf Enter pass phrase for server.key: You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter '.', the field will be left blank. ----- Country Name (2 letter code) [AU]:US State or Province Name (full name) [Some-State]:CA Locality Name (eg, city) []:San Jose Organization Name (eg, company) [Internet Widgits Pty Ltd]:PKILabServer Organizational Unit Name (eg, section) []: Common Name (e.g. server FQDN or YOUR name) []:PKILabServer.com Email Address []:admin@PKILabServer.com Please enter the following 'extra' attributes to be sent with your certificate request A challenge password []:server.csr An optional company name []: [11/17/2016 11:19] seed@ubuntu:~/SEED-Labs/PKI-Lab$ Note that this command is quite similar to the one we used in creating a self-signed certificate for the certificate authority previously. The only difference is the -x509 option.","title":"0x22 Generate a certificate signing request (CSR)"},{"location":"seedlabs/public-key-cryptography-and-pki/notes/#0x23-generate-a-certificate","text":"The CSR file needs to have the CA's signature to form a certificate. When received the CSR file, CA will first verify the identity information in the CSR. In reality, CA might contact the company through physical channel to do the verification. For example, through traditional mails or letters. For this experiment, we can assume we are the CA and will generate the certificate for the company. The following command turns the certificate signing request ( server.csr ) into an X509 certificate ( server.crt ), using the CA's ca.crt and ca.key : seed@ubuntu:~/SEED-Labs/PKI-Lab$ openssl ca -in server.csr -out server.crt -cert ca.crt -keyfile ca.key -config openssl.cnf Using configuration from openssl.cnf Enter pass phrase for ca.key: Check that the request matches the signature Signature ok Certificate Details: Serial Number: 4096 (0x1000) Validity Not Before: Nov 17 19:21:33 2016 GMT Not After : Nov 17 19:21:33 2017 GMT Subject: countryName = US stateOrProvinceName = CA localityName = San Jose organizationName = PKILabServer commonName = PKILabServer.com emailAddress = admin@PKILabServer.com X509v3 extensions: X509v3 Basic Constraints: CA:FALSE Netscape Comment: OpenSSL Generated Certificate X509v3 Subject Key Identifier: B4:F1:39:2A:56:C6:1F:1F:9C:49:AF:0B:45:FC:FD:79:66:90:F2:07 X509v3 Authority Key Identifier: keyid:1D:11:39:21:1C:E2:0D:8B:14:34:B8:36:C4:F5:34:27:09:9D:E8:7A Certificate is to be certified until Nov 17 19:21:33 2017 GMT (365 days) Sign the certificate? [y/n]:y 1 out of 1 certificate requests certified, commit? [y/n]y Write out database with 1 new entries Data Base Updated unable to write 'random state' [11/17/2016 11:22] seed@ubuntu:~/SEED-Labs/PKI-Lab$ If OpenSSL refuses to generate certificates, it is very likely that the names in your requests do not match with those of CA. (yes, think about you are creating a root CA for your company, and use it to sign certificates for different departments.) The matching rules are specified in the configuration file (at the [policy match] section). You can change the names of your requests to comply with the policy, or you can change the policy. The configuration file also includes another policy (called policy_anything ), which is less restrictive. You can choose that policy by changing the following line: \"policy = policy_match\" change to \"policy = policy_anything\".","title":"0x23 Generate a certificate"},{"location":"seedlabs/public-key-cryptography-and-pki/notes/#0x3-use-pki-for-web-sites","text":"Following the lab manual didn't work out. While adding the ca.crt to Firefox, I run into the following error that the format isn't supported by Firefox. Failed to decode the file. Either it is not in PKCS #12 format, has been corrupted, or the password you entered was incorrect. Find this stackoverflow post . We should use the following command to combine the server.crt with the ca.crt . openssl pkcs12 -export -in server.crt -inkey server.key -out PKILabServerFirefox.pfx -certfile ca.crt Finally, we can import PKILabServerFirefox.pfx to Firefox and right click the certificate and select \"Edit Trust->this certificate can identify websites\"","title":"0x3 Use PKI for Web Sites"},{"location":"seedlabs/public-key-cryptography-and-pki/notes/#0x4-test-the-certificate","text":"start a openssl simple webserver: seed@ubuntu:~/SEED-Labs/PKI-Lab$ cp server.key server.pem seed@ubuntu:~/SEED-Labs/PKI-Lab$ cat server.crt >> server.pem seed@ubuntu:~/SEED-Labs/PKI-Lab$ openssl s_server -cert server.pem -www -accpet 6666 Now navigate to https://www.pkilabserver.com:6666 , the security verification is passed and show the openssl web server default page. s_server -cert server.pem -www -accept 3633 Ciphers supported in s_server binary TLSv1/SSLv3:ECDHE-RSA-AES256-GCM-SHA384TLSv1/SSLv3:ECDHE-ECDSA-AES256-GCM-SHA384 TLSv1/SSLv3:ECDHE-RSA-AES256-SHA384 TLSv1/SSLv3:ECDHE-ECDSA-AES256-SHA384 TLSv1/SSLv3:ECDHE-RSA-AES256-SHA TLSv1/SSLv3:ECDHE-ECDSA-AES256-SHA TLSv1/SSLv3:SRP-DSS-AES-256-CBC-SHA TLSv1/SSLv3:SRP-RSA-AES-256-CBC-SHA TLSv1/SSLv3:SRP-AES-256-CBC-SHA TLSv1/SSLv3:DHE-DSS-AES256-GCM-SHA384 TLSv1/SSLv3:DHE-RSA-AES256-GCM-SHA384TLSv1/SSLv3:DHE-RSA-AES256-SHA256 TLSv1/SSLv3:DHE-DSS-AES256-SHA256 TLSv1/SSLv3:DHE-RSA-AES256-SHA TLSv1/SSLv3:DHE-DSS-AES256-SHA TLSv1/SSLv3:DHE-RSA-CAMELLIA256-SHA TLSv1/SSLv3:DHE-DSS-CAMELLIA256-SHA TLSv1/SSLv3:ECDH-RSA-AES256-GCM-SHA384 TLSv1/SSLv3:ECDH-ECDSA-AES256-GCM-SHA384TLSv1/SSLv3:ECDH-RSA-AES256-SHA384 TLSv1/SSLv3:ECDH-ECDSA-AES256-SHA384 TLSv1/SSLv3:ECDH-RSA-AES256-SHA TLSv1/SSLv3:ECDH-ECDSA-AES256-SHA TLSv1/SSLv3:AES256-GCM-SHA384 TLSv1/SSLv3:AES256-SHA256 TLSv1/SSLv3:AES256-SHA TLSv1/SSLv3:CAMELLIA256-SHA TLSv1/SSLv3:PSK-AES256-CBC-SHA TLSv1/SSLv3:ECDHE-RSA-DES-CBC3-SHA TLSv1/SSLv3:ECDHE-ECDSA-DES-CBC3-SHA TLSv1/SSLv3:SRP-DSS-3DES-EDE-CBC-SHA TLSv1/SSLv3:SRP-RSA-3DES-EDE-CBC-SHA TLSv1/SSLv3:SRP-3DES-EDE-CBC-SHA TLSv1/SSLv3:EDH-RSA-DES-CBC3-SHA TLSv1/SSLv3:EDH-DSS-DES-CBC3-SHA TLSv1/SSLv3:ECDH-RSA-DES-CBC3-SHA TLSv1/SSLv3:ECDH-ECDSA-DES-CBC3-SHA TLSv1/SSLv3:DES-CBC3-SHA TLSv1/SSLv3:PSK-3DES-EDE-CBC-SHA TLSv1/SSLv3:ECDHE-RSA-AES128-GCM-SHA256 TLSv1/SSLv3:ECDHE-ECDSA-AES128-GCM-SHA256TLSv1/SSLv3:ECDHE-RSA-AES128-SHA256 TLSv1/SSLv3:ECDHE-ECDSA-AES128-SHA256TLSv1/SSLv3:ECDHE-RSA-AES128-SHA TLSv1/SSLv3:ECDHE-ECDSA-AES128-SHA TLSv1/SSLv3:SRP-DSS-AES-128-CBC-SHA TLSv1/SSLv3:SRP-RSA-AES-128-CBC-SHA TLSv1/SSLv3:SRP-AES-128-CBC-SHA TLSv1/SSLv3:DHE-DSS-AES128-GCM-SHA256TLSv1/SSLv3:DHE-RSA-AES128-GCM-SHA256 TLSv1/SSLv3:DHE-RSA-AES128-SHA256 TLSv1/SSLv3:DHE-DSS-AES128-SHA256 TLSv1/SSLv3:DHE-RSA-AES128-SHA TLSv1/SSLv3:DHE-DSS-AES128-SHA TLSv1/SSLv3:DHE-RSA-SEED-SHA TLSv1/SSLv3:DHE-DSS-SEED-SHA TLSv1/SSLv3:DHE-RSA-CAMELLIA128-SHA TLSv1/SSLv3:DHE-DSS-CAMELLIA128-SHA TLSv1/SSLv3:ECDH-RSA-AES128-GCM-SHA256TLSv1/SSLv3:ECDH-ECDSA-AES128-GCM-SHA256 TLSv1/SSLv3:ECDH-RSA-AES128-SHA256 TLSv1/SSLv3:ECDH-ECDSA-AES128-SHA256 TLSv1/SSLv3:ECDH-RSA-AES128-SHA TLSv1/SSLv3:ECDH-ECDSA-AES128-SHA TLSv1/SSLv3:AES128-GCM-SHA256 TLSv1/SSLv3:AES128-SHA256 TLSv1/SSLv3:AES128-SHA TLSv1/SSLv3:SEED-SHA TLSv1/SSLv3:CAMELLIA128-SHA TLSv1/SSLv3:PSK-AES128-CBC-SHA TLSv1/SSLv3:ECDHE-RSA-RC4-SHA TLSv1/SSLv3:ECDHE-ECDSA-RC4-SHA TLSv1/SSLv3:ECDH-RSA-RC4-SHA TLSv1/SSLv3:ECDH-ECDSA-RC4-SHA TLSv1/SSLv3:RC4-SHA TLSv1/SSLv3:RC4-MD5 TLSv1/SSLv3:PSK-RC4-SHA TLSv1/SSLv3:EDH-RSA-DES-CBC-SHA TLSv1/SSLv3:EDH-DSS-DES-CBC-SHA TLSv1/SSLv3:DES-CBC-SHA --- Reused, TLSv1/SSLv3, Cipher is ECDHE-RSA-AES256-SHA SSL-Session: Protocol : TLSv1 Cipher : ECDHE-RSA-AES256-SHA Session-ID: 124BFE9E98B4DF8E5DC0CC87DEB16DDE615FEC0B63A5FD687CD7033012639DD1 Session-ID-ctx: 01000000 Master-Key: A23B112D5A56B98709D5C140EEC5876155C6023CF8D5DCD81858B1EFF3B4E9CE8723E2D9703C217D8C517A762820B32C Key-Arg : None PSK identity: None PSK identity hint: None SRP username: None Start Time: 1479414975 Timeout : 300 (sec) Verify return code: 0 (ok) --- 0 items in the session cache 0 client connects (SSL_connect()) 0 client renegotiates (SSL_connect()) 0 client connects that finished 16 server accepts (SSL_accept()) 0 server renegotiates (SSL_accept()) 16 server accepts that finished 3 session cache hits 0 session cache misses 0 session cache timeouts 0 callback cache hits 0 cache full overflows (128 allowed) --- no client certificate available","title":"0x4 Test the certificate"},{"location":"seedlabs/public-key-cryptography-and-pki/notes/#0x5-rsa-and-aes-encryption-performace","text":"We use the following commands to evaluate RSA encryption and decryption: # generate private key openssl genrsa -out private-rsa.pem 1024 # generate public key openssl genrsa -in private-rsa.pem -pubout -out public-rsa.pem # encrypt using public key openssl rsautl -encrypt -pubin -inkey public-rsa.pem -in message.txt -out message_enc.txt # decrypt using private key openssl rsautl -decrypt -inkey public-rsa.pem -in message_enc.txt -out message_dec.txt Commands for AES encrytption and decryption TODO","title":"0x5 RSA and AES encryption performace"},{"location":"system-design/","text":"System Design \u00b6 Concurrency and Synchronization \u00b6 Distributed System Concepts \u00b6 Design Patterns \u00b6 Design Messager \u00b6 Design TinyUrl \u00b6 Design Twitter \u00b6 Design Ticketmaster \u00b6 Design Crawler \u00b6 Design Scheduler \u00b6 Reference \u00b6 \u7cfb\u7edf\u8bbe\u8ba1\u9762\u8bd5\u9898\u7cbe\u9009 \u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u4e8b\u52a1\u5904\u7406","title":"Index"},{"location":"system-design/#system-design","text":"","title":"System Design"},{"location":"system-design/#concurrency-and-synchronization","text":"","title":"Concurrency and Synchronization"},{"location":"system-design/#distributed-system-concepts","text":"","title":"Distributed System Concepts"},{"location":"system-design/#design-patterns","text":"","title":"Design Patterns"},{"location":"system-design/#design-messager","text":"","title":"Design Messager"},{"location":"system-design/#design-tinyurl","text":"","title":"Design TinyUrl"},{"location":"system-design/#design-twitter","text":"","title":"Design Twitter"},{"location":"system-design/#design-ticketmaster","text":"","title":"Design Ticketmaster"},{"location":"system-design/#design-crawler","text":"","title":"Design Crawler"},{"location":"system-design/#design-scheduler","text":"","title":"Design Scheduler"},{"location":"system-design/#reference","text":"\u7cfb\u7edf\u8bbe\u8ba1\u9762\u8bd5\u9898\u7cbe\u9009 \u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u4e8b\u52a1\u5904\u7406","title":"Reference"},{"location":"system-design/concepts/notes/","text":"Distributed System Concepts \u00b6 Database normalization \u00b6 Database sharding \u00b6 ACID and BASE \u00b6 ACID\uff1aAtomicity, Consistency, Isolation, Durability; BASE\uff1aBasically Available, Soft-state, Eventually Consistent Databases are roughly divided into two types: ACID and BASE. Each type comes with a different set of features, pros, and cons. ACID databases are strict. Think of them as meticulous, no-nonsense librarians, constantly making sure that all the books stay organized. BASE databases are relaxed. Think of them as hands-off substitute teachers\u2014keeping things running but not maintaining strict classroom rules. When the ACID and BASE acronyms are expanded out, they describe the technical features each type of database provides. ACID databases provide: Atomicity: database updates either happen completely, or they don't happen at all. (Books are either checked in to the library or checked out; they can't be in a weird in-between state.) Note the difference between Atomicity in a database system and atomic operation in concurrent programming. Consistency: rules in the database are always enforced. (All the books are shelved correctly and in order.) Consistency is a property of the application, while atomicity, isolation, and durability are properties of the database. Isolation: concurrent updates happen as if they ran one after the other. Formalized as serializability or linearizability in some textbooks. (e.g. Everybody is quiet in the library; no other patrons are distracting you.) Durability: once an update finishes, it stays in the database, even if something fails later on. In a single-node database, durability typically means that the data has been written to non-volatile storage such as a hard drive or SSD. In a replicated database, durability may mean that the data has been successfully copied to some number of nodes. (The library's records stick around even if the power goes out.) BASE databases have fewer guarantees: Basically Available: the database usually works. (Class goes on as scheduled.) Soft State: different database replicas might not be in sync with each other. (Students can be working on different things at the same time.) Eventually Consistent: updates eventually make it to all replicas. (Someday students will figure out the lesson plan that should have been covered.) ACID databases provide more features, so they're better, right? Not necessarily. All those features don't come for free. ACID databases are difficult to scale\u2014if your database is huge, it's expensive to ensure everything stays consistent. It's especially tricky if your database spans multiple machines or data centers, since all your machines will need to be kept in sync. So, which type of database should you use? ACID or BASE? It depends on what you need for the system you're building. Does your system need consistency and data reliability? If so, go with an ACID database. Does your system need to be distributed and highly available? Then a BASE database may be a better choice. CAP Principle \u00b6 Consistency Availability Partition Tolerance Consistency model \u00b6 Week consistent Cache system Eventually consistent Mail, DNS, SMTP Strong consistent RDBMS \u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u4e8b\u52a1\u5904\u7406 Google I/O 2009 - Transactions Across Datacenters Transaction Across DataCenter Slides Consistent hashing \u00b6 Geo Hashing \u00b6 Geospatial Performance Improvements in MongoDB 3.2 Bloom filter \u00b6 Storage structures used in databases and distributed systems Counting Bloom filter \u00b6 Count\u2013min sketch \u00b6 Cuckoo filter \u00b6 Collaborate filtering \u00b6 Cassandra \u00b6 Dynamo \u00b6 [Dynamo: Amazon\u2019s Highly Available Key-value Store] Memcached \u00b6 Redis \u00b6 Cassandra vs MongoDB vs CouchDB vs Redis vs Riak vs HBase vs Couchbase vs OrientDB vs Aerospike vs Neo4j vs Hypertable vs ElasticSearch vs Accumulo vs VoltDB vs Scalaris vs RethinkDB comparison ZooKeepr \u00b6 ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services. For example Rest.li use Zookeeper to store the DNS and the naming information for its D2 (Dynamic Discovery). Zookeeper store information that in less than 1M size. The service itself is replicated over a set of machines that comprise the service. These machines maintain an in-memory image of the data tree along with a transaction logs and snapshots in a persistent store. Clients only connect to a single ZooKeeper server. When a client first connects to the ZooKeeper service, the first ZooKeeper server will setup a session for the client. If the client needs to connect to another server, this session will get reestablished with the new server. Read requests sent by a ZooKeeper client are processed locally at the ZooKeeper server to which the client is connected. Write requests are forwarded to other ZooKeeper servers and go through consensus before a response is generated. Sync requests are also forwarded to another server, but do not actually go through consensus to improve throughtput. Thus, the throughput of read requests scales with the number of servers and the throughput of write requests decreases with the number of servers. Apache ZooKeeper Project Description Kafka \u00b6 The Log: What every software engineer should know about real-time data's unifying abstraction Kafka Tutorial From Confluent Apache Kafka Internals CouchDB \u00b6 Neo4j \u00b6 k-d Tree \u00b6 Beam search \u00b6 A* search \u00b6 Minmax Algorithm \u00b6 Distributed System Design Process \u00b6 scalability (vertical sharding, horizontal sharding) fault tolerance (replication, auto failover) consistency (choose consistency models, ACID v.s BASE) performance (distributed cache system)","title":"Distributed System Concepts"},{"location":"system-design/concepts/notes/#distributed-system-concepts","text":"","title":"Distributed System Concepts"},{"location":"system-design/concepts/notes/#database-normalization","text":"","title":"Database normalization"},{"location":"system-design/concepts/notes/#database-sharding","text":"","title":"Database sharding"},{"location":"system-design/concepts/notes/#acid-and-base","text":"ACID\uff1aAtomicity, Consistency, Isolation, Durability; BASE\uff1aBasically Available, Soft-state, Eventually Consistent Databases are roughly divided into two types: ACID and BASE. Each type comes with a different set of features, pros, and cons. ACID databases are strict. Think of them as meticulous, no-nonsense librarians, constantly making sure that all the books stay organized. BASE databases are relaxed. Think of them as hands-off substitute teachers\u2014keeping things running but not maintaining strict classroom rules. When the ACID and BASE acronyms are expanded out, they describe the technical features each type of database provides. ACID databases provide: Atomicity: database updates either happen completely, or they don't happen at all. (Books are either checked in to the library or checked out; they can't be in a weird in-between state.) Note the difference between Atomicity in a database system and atomic operation in concurrent programming. Consistency: rules in the database are always enforced. (All the books are shelved correctly and in order.) Consistency is a property of the application, while atomicity, isolation, and durability are properties of the database. Isolation: concurrent updates happen as if they ran one after the other. Formalized as serializability or linearizability in some textbooks. (e.g. Everybody is quiet in the library; no other patrons are distracting you.) Durability: once an update finishes, it stays in the database, even if something fails later on. In a single-node database, durability typically means that the data has been written to non-volatile storage such as a hard drive or SSD. In a replicated database, durability may mean that the data has been successfully copied to some number of nodes. (The library's records stick around even if the power goes out.) BASE databases have fewer guarantees: Basically Available: the database usually works. (Class goes on as scheduled.) Soft State: different database replicas might not be in sync with each other. (Students can be working on different things at the same time.) Eventually Consistent: updates eventually make it to all replicas. (Someday students will figure out the lesson plan that should have been covered.) ACID databases provide more features, so they're better, right? Not necessarily. All those features don't come for free. ACID databases are difficult to scale\u2014if your database is huge, it's expensive to ensure everything stays consistent. It's especially tricky if your database spans multiple machines or data centers, since all your machines will need to be kept in sync. So, which type of database should you use? ACID or BASE? It depends on what you need for the system you're building. Does your system need consistency and data reliability? If so, go with an ACID database. Does your system need to be distributed and highly available? Then a BASE database may be a better choice.","title":"ACID and BASE"},{"location":"system-design/concepts/notes/#cap-principle","text":"Consistency Availability Partition Tolerance","title":"CAP Principle"},{"location":"system-design/concepts/notes/#consistency-model","text":"Week consistent Cache system Eventually consistent Mail, DNS, SMTP Strong consistent RDBMS \u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u4e8b\u52a1\u5904\u7406 Google I/O 2009 - Transactions Across Datacenters Transaction Across DataCenter Slides","title":"Consistency model"},{"location":"system-design/concepts/notes/#consistent-hashing","text":"","title":"Consistent hashing"},{"location":"system-design/concepts/notes/#geo-hashing","text":"Geospatial Performance Improvements in MongoDB 3.2","title":"Geo Hashing"},{"location":"system-design/concepts/notes/#bloom-filter","text":"Storage structures used in databases and distributed systems","title":"Bloom filter"},{"location":"system-design/concepts/notes/#counting-bloom-filter","text":"","title":"Counting Bloom filter"},{"location":"system-design/concepts/notes/#countmin-sketch","text":"","title":"Count\u2013min sketch"},{"location":"system-design/concepts/notes/#cuckoo-filter","text":"","title":"Cuckoo filter"},{"location":"system-design/concepts/notes/#collaborate-filtering","text":"","title":"Collaborate filtering"},{"location":"system-design/concepts/notes/#cassandra","text":"","title":"Cassandra"},{"location":"system-design/concepts/notes/#dynamo","text":"[Dynamo: Amazon\u2019s Highly Available Key-value Store]","title":"Dynamo"},{"location":"system-design/concepts/notes/#memcached","text":"","title":"Memcached"},{"location":"system-design/concepts/notes/#redis","text":"Cassandra vs MongoDB vs CouchDB vs Redis vs Riak vs HBase vs Couchbase vs OrientDB vs Aerospike vs Neo4j vs Hypertable vs ElasticSearch vs Accumulo vs VoltDB vs Scalaris vs RethinkDB comparison","title":"Redis"},{"location":"system-design/concepts/notes/#zookeepr","text":"ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services. For example Rest.li use Zookeeper to store the DNS and the naming information for its D2 (Dynamic Discovery). Zookeeper store information that in less than 1M size. The service itself is replicated over a set of machines that comprise the service. These machines maintain an in-memory image of the data tree along with a transaction logs and snapshots in a persistent store. Clients only connect to a single ZooKeeper server. When a client first connects to the ZooKeeper service, the first ZooKeeper server will setup a session for the client. If the client needs to connect to another server, this session will get reestablished with the new server. Read requests sent by a ZooKeeper client are processed locally at the ZooKeeper server to which the client is connected. Write requests are forwarded to other ZooKeeper servers and go through consensus before a response is generated. Sync requests are also forwarded to another server, but do not actually go through consensus to improve throughtput. Thus, the throughput of read requests scales with the number of servers and the throughput of write requests decreases with the number of servers. Apache ZooKeeper Project Description","title":"ZooKeepr"},{"location":"system-design/concepts/notes/#kafka","text":"The Log: What every software engineer should know about real-time data's unifying abstraction Kafka Tutorial From Confluent Apache Kafka Internals","title":"Kafka"},{"location":"system-design/concepts/notes/#couchdb","text":"","title":"CouchDB"},{"location":"system-design/concepts/notes/#neo4j","text":"","title":"Neo4j"},{"location":"system-design/concepts/notes/#k-d-tree","text":"","title":"k-d Tree"},{"location":"system-design/concepts/notes/#beam-search","text":"","title":"Beam search"},{"location":"system-design/concepts/notes/#a-search","text":"","title":"A* search"},{"location":"system-design/concepts/notes/#minmax-algorithm","text":"","title":"Minmax Algorithm"},{"location":"system-design/concepts/notes/#distributed-system-design-process","text":"scalability (vertical sharding, horizontal sharding) fault tolerance (replication, auto failover) consistency (choose consistency models, ACID v.s BASE) performance (distributed cache system)","title":"Distributed System Design Process"},{"location":"system-design/concurrency/notes/","text":"Concurrency and Synchronization \u00b6 Coroutine \u00b6 Thread and concurrency \u00b6 Thread memory model \u00b6 Multiple threads run within the context of a process. Each thread has its own separate thread context Thread ID, stack, stack pointer, PC, condition codes, and GP registers All threads share the remaining process context Code, data, heap, and shared library segments in the process virtual address space File descriptors and opened handles However, the above conceptual model is not strictly enforced in practice. Operationally, the registers are truly separated and protected, but any thread can read and write the stack of any other thread! Variable mapping in memory \u00b6 Global variables \u00b6 Definition: Variable declared outside of a function Virtual memory contains exactly one instance of any global variable. Local variables \u00b6 Definition: Variable declared inside a function without static keyword. Each thread stack contains one instant of each local variable. Local static variables \u00b6 Definition: Variable declared inside a function with static keyword. Virtual memory contains exactly one instance of any local static variable. While different threads try to access the same shared variable (data structure) at the same time, they face a synchronous problem. The operations involved have to pertain a certain order so as the variable value will be correct. We need to design a mechanism to prevent this type of synchronous problem from happening. We do so by ensuring mutual exclusive of the two events, namely, event A and event B must not happen at the same time. Synchronization basic concepts \u00b6 A critical section is a piece of code that accesses a shared resource, usually a variable or data structure. A race condition arises if multiple threads of execution enter the critical section at roughly the same time; both attempt to update the shared data structure, leading to a surprising (and perhaps undesirable) outcome. Mutual exclusive is the requirement one want to fulfill if multiple threads want to access/update/modify the same variable. This property guarantees that if one thread is executing within the critical section, the others will be prevented from doing so. Lock \u00b6 pthread example \u00b6 A lock is just like a variable, To use a lock, you first declare a lock variable, the variable hold the state of the lock across all instance of time. Its state is either in available (unlocked or free) or in acquired (locked or held). To use a lock, we add some code around the critical section like the following code. pthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER ; pthread_mutex_lock ( & lock ); balance = balance + 1 ; // critical section pthread_mutex_unlock ( & lock ); Implement a lock \u00b6 Many programming interfaces provide locking mechanisms through interfaces that are easy to use, but: How can we build an efficient lock? What hardware support is needs? What OS support is needed? Before building locks, let's summarize the properties a lock might have so that we can evaluate the lock we build. mutual exclusive . Basic task, make sure event A and event B cannot happen at the same time. fairness . Each thread competing for the lock should get a fair chance to acquire the lock once the lock has been released. performance . How much overhead is added by using the lock? There are three cases we have to consider: single thread using the lock. multiple threads contending for the lock and run on a single CPU. multiple threads contending for the lock and run on multiple CPUs. Controlling Interrupts \u00b6 pros simplicity cons performing operations on interrupts are privileged. OS need to fully trust the applications. Otherwise, abuses of the system facility may happen, such as monopolize the CPU or faulty applications go into an endless loop. doesn't work on multiprocessors. Turning off interrupts for extended periods of time can lead to interrupts becoming lost. e.g. missing a disk I/O interrupt may cause a process waiting for the completion of the file write never wakeup. Inefficient due to slow execution of the code that interrupts have been masked and unmasked. Test-and-set (automic exchange) \u00b6 One idea that could be potentially useful in implementing a lock is using a flag as a lock. Thread who acquires the lock will set the flag to 1 while entering the critical section and reset to 0 when finished the critical section. This idea will NOT work since if two threads attempt to acquire the lock at the same time, the operations may interleave thus result in acquiring the lock at the same time. The problem with the above simple idea is that the operations to test the flag and set it to 1 aren't atomic. Nowadays, most hardware architecture has instructions that support lock. In x86 platform, the atomic exchange instruction xchg enables us to implement a \"spin-lock\" using the above \"flag\" idea. Such hardware support instructions are generally referred to as \"Test-and-set\" operation. Below includes the C snippet of a \"Test-and-set\" operation. It is an atomic operation. Imagine it as an equivalent to the single instruction xchg . What it does is return the old value and set the old value to the new value in one operation. implement the spin lock using Test-and-set int TestAndSet ( int * old_ptr , int new_val ) { int old = * old_ptr ; //fetch old value at old_ptr * old_ptr = new_val ; //store new_val into old_ptr return old ; //return the old value } typedef struct __lock_t { int flag ; } lock_t ; void init ( lock_t * lock ) { // 0 indicate free, 1 indicate acquired lock -> flag = 0 ; } void lock ( lock_t * lock ) { while ( TestAndSet ( & lock -> flag , 1 ) == 1 ) ; //spin wait } void unlock ( lock_t * lock ) { lock -> flag = 0 ; } Evaluation of the spin lock using Test-and-set Correctness: provide mutual exclusive Fairness: not fair. Performance: spinning waste CPU cycles. Compare-and-swap \u00b6 Compare-and-swap is another hardware primitive that some system provided. Compare-and-swap on SPARC systems and compare-and-exchange on x86. Here is the C snippet of the operations. It is atomically accomplished in machine level instruction. Implement spin lock with Compare-and-swap int CompareAndSwap ( int * old_ptr , int expected , int new_val ) { int old = * old_ptr ; if ( * old_ptr == expected ) { * old_ptr = new_val ; } return old ; } void lock ( lock_t * lock ) { while ( CompareAndSwap ( & lock -> flag , 0 , 1 ) == 1 ) ; //spin wait } !Note \"\" Although we saw that Test-and-set and compare-and-swap atomic instructions are very similar in implementing spinlocks, the later is more powerful instruction. We will see some of the good properties of the compare-and-swap instruction in lock-free synchronization . Load-linked and store-conditional \u00b6 TBD Fetch-and-add \u00b6 TBD How to avoid spinning \u00b6 Hardware support gets enable us to implement locks with correctness, namely mutual exclusive. We could also rely on hardware support to provide fairness as we saw in the fetch-and-add primitives in implementing ticket lock. But hardware alone will not provide us an efficient lock implementation. We need OS support to make the lock we discussed have good performance. Our first try is to yield when a thread tries to acquire a lock that already held by others. Instead of spinning, it immediately yields the CPU to allow other threads to finish the critical section. If we have only two processes contending for the lock, this approach works pretty well. But if we have many threads lets say 100 contending for the lock, this approach will be problematic again. There might be 99 threads yielding and doing context switches, the work of which can be substantially wasteful. Another solution to reduce the spinning is using queues. Put the thread into sleeping in a queue instead of spinning. When the lock is about to be released, the thread wakes up one of the sleeping threads in the sleeping queue attempted to acquire the lock. What we do here is to exert some control over which thread next gets to acquire the lock after the current holder release it. This solved the problem in the previous approaches. Using a queue to track the thread who want to acquire the lock and the thread who finished the critical section will select from the queue the next thread to run. Previously we leave too much to opportunity. It is the scheduler who determines which thread to run next. No matter which thread to select, it leaves too much CPU time spinning or yielding. Solaris example, use park() , unpark , and setpark() Conditional variables \u00b6 Backgroud \u00b6 In the case that a thread wants to check whether a condition is true before continuing its execution, such as a parent thread may check whether its child has completed before continue its own execution. (i.e. join() ) But how to implement such a wait() ? Generally, a shared variable will work but not efficient enough because the parent have to spin. A not efficient implementation of wait(). Parent wait for child, spin based approach. volatile int done = 0 ; void * child ( void * arg ) { printf ( \"child \\n \" ); done = 1 ; return NULL ; } int main ( int argc , char * argv []) { printf ( \"parent: begin \\n \" ); pthread_t c ; pthread_create ( & c , NULL , child , NULL ); while ( done == 0 ) ; //spin wait printf ( \"parent: end \\n \" ); return 0 ; } Definition \u00b6 A conditional variable is an explicit queue that threads can put themselves in and sleep (wait for being notified to change to ready) when a condition has not been met; some other thread later changes the condition and then wake one of those sleeping threads from the queue to notify them to continue. C++ pthread routine \u00b6 pthread_cond_wait(pthread_cond_t *c, pthread_mutex_t *m); mutex m should be acquired before calling wait() nutex m should be released when enter the wait() nutex m should be re-acquired upon return to caller. pthread_cond_signal(pthread_cond_t *c); basic ideas \u00b6 The general idea is when a thread wants to access the critical section, it first checks a conditional variable (like a flag) to decide whether it should wait on the condition (using pthread_cond_wait();) or keep running (fall off toward the function end). Another thread will set the conditional variable and wake up (using pthread_cond_signal();) the sleeping thread from the queue. Example of using a conditional variable \u00b6 // Parent waiting for child: using a conditional variable int done = 0 pthread_mutex_t m = PTHREAD_MUTEX_INITIALIZER ; pthread_cond_t c = PTHREAD_COND_INITIALIZER ; void thr_exit () { pthread_mutex_lock ( & m ); done = 1 ; pthread_cond_signal ( & c ); pthread_mutex_unlock ( & m ); } void thr_join () { pthread_mutex_lock ( & m ); while ( done == 0 ) pthread_cond_wait ( & c , & m ); pthread_mutex_unlock ( & m ); } void * child ( void * arg ) { printf ( \"child \\n \" ); thr_exit (); return NULL ; } int main ( int argc , char * argv []) { printf ( \"parent: begin \\n \" ); pthread_t p ; pthread_create ( & p , NULL , child , NULL ); thr_join (); printf ( \"parent: end \\n \" ); return 0 ; } Analysis of the code snippet We interested in three variables in this program: int done pthread_mutex_t m pthread_cond_t c One may wondering whether the done and m state variables are a must. Let's see what if we don't have the done state variable. In this case, if the child run before the parent, the child will signal and there is no thread in the queue, child returns and then the parent start running, parent will wait if at this time the child is finished, no one will signal it. The parent just stuck! Let's see if we remove the lock m. It will introduce a race condition. Considering if the child interrupts the parent at the time right before the parent call pthread_cond_wait() , the child set the conditional variable and signal, there is no thread in the waiting queue, so child return. Then the parent goes to sleep, and not be able to wait up. Producers-consumers problem \u00b6 In this problem, we have to carefully synchronize the two (or more) threads in order for them to work correctly. Here are the put() to populate the buffer, and get() to read the buffer. Our first solution \u00b6 Producers and consumers synchronization example, int buffer ; int count = 0 ; void put ( int value ) { assert ( count == 0 ); count = 1 ; buffer = value ; } int get () { assert ( count == 1 ); count = 0 ; return buffer ; } pthread_cond_t cond ; pthread_mutex_t mutex ; void * producer ( void * arg ) { int i ; int loops = ( int ) arg ; for ( i = 0 ; i < loops ; i ++ ) { pthread_mutex_lock ( & mutex ); // p1 if ( count == 1 ) // or use while // p2 pthread_cond_wait ( & cond , & mutex ); // p3 put ( i ); // p4 pthread_cond_signal ( & cond ); // p5 pthread_mutex_unlock ( & mutex ); // p6 } } void * consumer ( void * arg ) { int i ; int loops = ( int ) arg ; for ( i = 0 ; i < loops ; i ++ ) { pthread_mutex_lock ( & mutex ); // c1 if ( count == 0 ) // or use while // c2 pthread_cond_wait ( & cond , & mutex ); // c3 int tmp = get (); // c4 pthread_cond_signal ( & cond ); // c5 pthread_mutex_unlock ( & mutex ); // c6 printf ( \"%d \\n \" , tmp ); } } Problem with this solution, It works ok if we have only one consumer and one producer. Problems could arise when we have more than one consumer. If we have more than one consumer, one of them might sneak in to consume the buffer before the other consumer return from pthread_cond_wait() . Producer-consumer using if \u00b6 Producer-consumer using while \u00b6 Mesa semantics and Hoare semantics \u00b6 The problem with the if statement is that signaling a thread only wakes them up; it is thus a hint that the state of the world has changed but there is no guarantee that when the woken thread runs. This interpretation of what a signal means is often referred to as Mesa semantics ; in contrast, referred to as Hoare semantics , is harder to build but provides a stronger guarantee that the woken thread will run immediately upon being woken. Single buffer producer/consumer solution \u00b6 We saw from the above discussion that even change the if statement to while, we are still facing some problem that all producer thread and consumer thread will sleep. This is because one consumer signaled another consumer and the producer never be able to run again. To solve this problem we'd better to add another conditional variable, one for the producer and another for the consumer so that the producer can only signal the consumer and the consumer can only signal the producer. This sounds most plausible and the problem will never occur. Finally here is our workable solution with two conditional variables. pthread_cond_t empty , fill ; pthread_mutex_t mutex ; void * producer ( void * arg ) { int i ; int loops = ( int ) arg ; for ( i = 0 ; i < loops ; i ++ ) { pthread_mutex_lock ( & mutex ); while ( count == 1 ) pthread_cond_wait ( & empty , & mutex ); put ( i ); pthread_cond_signal ( & fill ); pthread_mutex_unlock ( & mutex ); } } void * consumer ( void * arg ) { int i ; int loops = ( int ) arg ; for ( i = 0 ; i < loops ; i ++ ) { pthread_mutex_lock ( & mutex ); while ( count == 0 ) pthread_cond_wait ( & fill , & mutex ); int tmp = get (); pthread_cond_signal ( & empty ); pthread_mutex_unlock ( & mutex ); printf ( \"%d \\n \" , tmp ); } } Producer consumer solution \u00b6 Now let's change the one element buffer to a more general solution, which has more buffers slots to work on for the consumers and the producers. This solution even support concurrent producing and consuming for multiple threads. int buffer [ MAX ]; int fill_ptr = 0 ; int use_ptr = 0 ; int count = 0 ; void put ( int value ) { buffer [ fill_ptr ] = value ; fill_ptr = ( fill_ptr + 2 ) % MAX ; count ++ ; } int get () { int tmp = buffer [ use_ptr ]; use_ptr = ( use_ptr + 1 ) % MAX ; count -- ; return tmp ; } pthread_cond_t empty , fill ; pthread_mutex_t mutex ; void * producer ( void * arg ) { int i ; int loops = ( int ) arg ; for ( i = 0 ; i < loops ; i ++ ) { pthread_mutex_lock ( & mutex ); while ( count == MAX ) pthread_cond_wait ( & empty , & mutex ); put ( i ); pthread_cond_signal ( & fill ); pthread_mutex_unlock ( & mutex ); } } void * consumer ( void * arg ) { int i ; int loops = ( int ) arg ; for ( i = 0 ; i < loops ; i ++ ) { pthread_mutex_lock ( & mutex ); while ( count == 0 ) pthread_cond_wait ( & fill , & mutex ); int tmp = get (); pthread_cond_signal ( & empty ); pthread_mutex_unlock ( & mutex ); printf ( \"%d \\n \" , tmp ); } } Covering Conditions \u00b6 TBD Semaphore \u00b6 What is the definition of a semaphore? How can we use semaphores instead of locks and condition variables? What is a binary semaphore? Is it straightforward to build a semaphore out of locks and condition variables? to build locks and condition variables out of semaphores? Definition \u00b6 A semaphore is an object with an integer value that we can manipulate with two routines; in the POSIX standard, these routines are sem_wait() and sem_post() . int sem_wait ( sem_t * s ) { decrement the value of semaphore s by one wait if value of semaphore s is negative } int sem_post ( sem_t * s ) { increment the value of semaphore s by one if there are one or more threads waiting , wake one } Semaphore as a lock \u00b6 sem_t m ; sem_init ( & m , 0 , X ); // initialize semaphore to X; what should X be? sem_wait ( & m ); // critical section here sem_post ( & m ); Semaphores as conditional variables \u00b6 Parent waiting for its child Producers and consumers TBD Deadlock \u00b6 TBD Monitor \u00b6 TBD Dining philosophers problem \u00b6 TBD Applications \u00b6 A timer is a child thread that wakes up periodically to check whether an expiration happened or not. The maximum number of timers is limited by the OS because many systems support a small number of concurrent threads. To implement arbitrary many timers using software, we can use a timer queue to reach the goal. Don't expect too much accuracy for this timer implementation, it is not. However, this approach is acceptable in most of the projects. A good reference is here[1]. source code. Reference \u00b6 Operating Systems: Three Easy Pieces timer queue \u2013 a way to handle multiple timers using one thread COMP 322: Fundamentals of Parallel Programming","title":"Concurrency and Synchronization"},{"location":"system-design/concurrency/notes/#concurrency-and-synchronization","text":"","title":"Concurrency and Synchronization"},{"location":"system-design/concurrency/notes/#coroutine","text":"","title":"Coroutine"},{"location":"system-design/concurrency/notes/#thread-and-concurrency","text":"","title":"Thread and concurrency"},{"location":"system-design/concurrency/notes/#thread-memory-model","text":"Multiple threads run within the context of a process. Each thread has its own separate thread context Thread ID, stack, stack pointer, PC, condition codes, and GP registers All threads share the remaining process context Code, data, heap, and shared library segments in the process virtual address space File descriptors and opened handles However, the above conceptual model is not strictly enforced in practice. Operationally, the registers are truly separated and protected, but any thread can read and write the stack of any other thread!","title":"Thread memory model"},{"location":"system-design/concurrency/notes/#variable-mapping-in-memory","text":"","title":"Variable mapping in memory"},{"location":"system-design/concurrency/notes/#global-variables","text":"Definition: Variable declared outside of a function Virtual memory contains exactly one instance of any global variable.","title":"Global variables"},{"location":"system-design/concurrency/notes/#local-variables","text":"Definition: Variable declared inside a function without static keyword. Each thread stack contains one instant of each local variable.","title":"Local variables"},{"location":"system-design/concurrency/notes/#local-static-variables","text":"Definition: Variable declared inside a function with static keyword. Virtual memory contains exactly one instance of any local static variable. While different threads try to access the same shared variable (data structure) at the same time, they face a synchronous problem. The operations involved have to pertain a certain order so as the variable value will be correct. We need to design a mechanism to prevent this type of synchronous problem from happening. We do so by ensuring mutual exclusive of the two events, namely, event A and event B must not happen at the same time.","title":"Local static variables"},{"location":"system-design/concurrency/notes/#synchronization-basic-concepts","text":"A critical section is a piece of code that accesses a shared resource, usually a variable or data structure. A race condition arises if multiple threads of execution enter the critical section at roughly the same time; both attempt to update the shared data structure, leading to a surprising (and perhaps undesirable) outcome. Mutual exclusive is the requirement one want to fulfill if multiple threads want to access/update/modify the same variable. This property guarantees that if one thread is executing within the critical section, the others will be prevented from doing so.","title":"Synchronization basic concepts"},{"location":"system-design/concurrency/notes/#lock","text":"","title":"Lock"},{"location":"system-design/concurrency/notes/#pthread-example","text":"A lock is just like a variable, To use a lock, you first declare a lock variable, the variable hold the state of the lock across all instance of time. Its state is either in available (unlocked or free) or in acquired (locked or held). To use a lock, we add some code around the critical section like the following code. pthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER ; pthread_mutex_lock ( & lock ); balance = balance + 1 ; // critical section pthread_mutex_unlock ( & lock );","title":"pthread example"},{"location":"system-design/concurrency/notes/#implement-a-lock","text":"Many programming interfaces provide locking mechanisms through interfaces that are easy to use, but: How can we build an efficient lock? What hardware support is needs? What OS support is needed? Before building locks, let's summarize the properties a lock might have so that we can evaluate the lock we build. mutual exclusive . Basic task, make sure event A and event B cannot happen at the same time. fairness . Each thread competing for the lock should get a fair chance to acquire the lock once the lock has been released. performance . How much overhead is added by using the lock? There are three cases we have to consider: single thread using the lock. multiple threads contending for the lock and run on a single CPU. multiple threads contending for the lock and run on multiple CPUs.","title":"Implement a lock"},{"location":"system-design/concurrency/notes/#controlling-interrupts","text":"pros simplicity cons performing operations on interrupts are privileged. OS need to fully trust the applications. Otherwise, abuses of the system facility may happen, such as monopolize the CPU or faulty applications go into an endless loop. doesn't work on multiprocessors. Turning off interrupts for extended periods of time can lead to interrupts becoming lost. e.g. missing a disk I/O interrupt may cause a process waiting for the completion of the file write never wakeup. Inefficient due to slow execution of the code that interrupts have been masked and unmasked.","title":"Controlling Interrupts"},{"location":"system-design/concurrency/notes/#test-and-set-automic-exchange","text":"One idea that could be potentially useful in implementing a lock is using a flag as a lock. Thread who acquires the lock will set the flag to 1 while entering the critical section and reset to 0 when finished the critical section. This idea will NOT work since if two threads attempt to acquire the lock at the same time, the operations may interleave thus result in acquiring the lock at the same time. The problem with the above simple idea is that the operations to test the flag and set it to 1 aren't atomic. Nowadays, most hardware architecture has instructions that support lock. In x86 platform, the atomic exchange instruction xchg enables us to implement a \"spin-lock\" using the above \"flag\" idea. Such hardware support instructions are generally referred to as \"Test-and-set\" operation. Below includes the C snippet of a \"Test-and-set\" operation. It is an atomic operation. Imagine it as an equivalent to the single instruction xchg . What it does is return the old value and set the old value to the new value in one operation. implement the spin lock using Test-and-set int TestAndSet ( int * old_ptr , int new_val ) { int old = * old_ptr ; //fetch old value at old_ptr * old_ptr = new_val ; //store new_val into old_ptr return old ; //return the old value } typedef struct __lock_t { int flag ; } lock_t ; void init ( lock_t * lock ) { // 0 indicate free, 1 indicate acquired lock -> flag = 0 ; } void lock ( lock_t * lock ) { while ( TestAndSet ( & lock -> flag , 1 ) == 1 ) ; //spin wait } void unlock ( lock_t * lock ) { lock -> flag = 0 ; } Evaluation of the spin lock using Test-and-set Correctness: provide mutual exclusive Fairness: not fair. Performance: spinning waste CPU cycles.","title":"Test-and-set (automic exchange)"},{"location":"system-design/concurrency/notes/#compare-and-swap","text":"Compare-and-swap is another hardware primitive that some system provided. Compare-and-swap on SPARC systems and compare-and-exchange on x86. Here is the C snippet of the operations. It is atomically accomplished in machine level instruction. Implement spin lock with Compare-and-swap int CompareAndSwap ( int * old_ptr , int expected , int new_val ) { int old = * old_ptr ; if ( * old_ptr == expected ) { * old_ptr = new_val ; } return old ; } void lock ( lock_t * lock ) { while ( CompareAndSwap ( & lock -> flag , 0 , 1 ) == 1 ) ; //spin wait } !Note \"\" Although we saw that Test-and-set and compare-and-swap atomic instructions are very similar in implementing spinlocks, the later is more powerful instruction. We will see some of the good properties of the compare-and-swap instruction in lock-free synchronization .","title":"Compare-and-swap"},{"location":"system-design/concurrency/notes/#load-linked-and-store-conditional","text":"TBD","title":"Load-linked and store-conditional"},{"location":"system-design/concurrency/notes/#fetch-and-add","text":"TBD","title":"Fetch-and-add"},{"location":"system-design/concurrency/notes/#how-to-avoid-spinning","text":"Hardware support gets enable us to implement locks with correctness, namely mutual exclusive. We could also rely on hardware support to provide fairness as we saw in the fetch-and-add primitives in implementing ticket lock. But hardware alone will not provide us an efficient lock implementation. We need OS support to make the lock we discussed have good performance. Our first try is to yield when a thread tries to acquire a lock that already held by others. Instead of spinning, it immediately yields the CPU to allow other threads to finish the critical section. If we have only two processes contending for the lock, this approach works pretty well. But if we have many threads lets say 100 contending for the lock, this approach will be problematic again. There might be 99 threads yielding and doing context switches, the work of which can be substantially wasteful. Another solution to reduce the spinning is using queues. Put the thread into sleeping in a queue instead of spinning. When the lock is about to be released, the thread wakes up one of the sleeping threads in the sleeping queue attempted to acquire the lock. What we do here is to exert some control over which thread next gets to acquire the lock after the current holder release it. This solved the problem in the previous approaches. Using a queue to track the thread who want to acquire the lock and the thread who finished the critical section will select from the queue the next thread to run. Previously we leave too much to opportunity. It is the scheduler who determines which thread to run next. No matter which thread to select, it leaves too much CPU time spinning or yielding. Solaris example, use park() , unpark , and setpark()","title":"How to avoid spinning"},{"location":"system-design/concurrency/notes/#conditional-variables","text":"","title":"Conditional variables"},{"location":"system-design/concurrency/notes/#backgroud","text":"In the case that a thread wants to check whether a condition is true before continuing its execution, such as a parent thread may check whether its child has completed before continue its own execution. (i.e. join() ) But how to implement such a wait() ? Generally, a shared variable will work but not efficient enough because the parent have to spin. A not efficient implementation of wait(). Parent wait for child, spin based approach. volatile int done = 0 ; void * child ( void * arg ) { printf ( \"child \\n \" ); done = 1 ; return NULL ; } int main ( int argc , char * argv []) { printf ( \"parent: begin \\n \" ); pthread_t c ; pthread_create ( & c , NULL , child , NULL ); while ( done == 0 ) ; //spin wait printf ( \"parent: end \\n \" ); return 0 ; }","title":"Backgroud"},{"location":"system-design/concurrency/notes/#definition","text":"A conditional variable is an explicit queue that threads can put themselves in and sleep (wait for being notified to change to ready) when a condition has not been met; some other thread later changes the condition and then wake one of those sleeping threads from the queue to notify them to continue.","title":"Definition"},{"location":"system-design/concurrency/notes/#c-pthread-routine","text":"pthread_cond_wait(pthread_cond_t *c, pthread_mutex_t *m); mutex m should be acquired before calling wait() nutex m should be released when enter the wait() nutex m should be re-acquired upon return to caller. pthread_cond_signal(pthread_cond_t *c);","title":"C++ pthread routine"},{"location":"system-design/concurrency/notes/#basic-ideas","text":"The general idea is when a thread wants to access the critical section, it first checks a conditional variable (like a flag) to decide whether it should wait on the condition (using pthread_cond_wait();) or keep running (fall off toward the function end). Another thread will set the conditional variable and wake up (using pthread_cond_signal();) the sleeping thread from the queue.","title":"basic ideas"},{"location":"system-design/concurrency/notes/#example-of-using-a-conditional-variable","text":"// Parent waiting for child: using a conditional variable int done = 0 pthread_mutex_t m = PTHREAD_MUTEX_INITIALIZER ; pthread_cond_t c = PTHREAD_COND_INITIALIZER ; void thr_exit () { pthread_mutex_lock ( & m ); done = 1 ; pthread_cond_signal ( & c ); pthread_mutex_unlock ( & m ); } void thr_join () { pthread_mutex_lock ( & m ); while ( done == 0 ) pthread_cond_wait ( & c , & m ); pthread_mutex_unlock ( & m ); } void * child ( void * arg ) { printf ( \"child \\n \" ); thr_exit (); return NULL ; } int main ( int argc , char * argv []) { printf ( \"parent: begin \\n \" ); pthread_t p ; pthread_create ( & p , NULL , child , NULL ); thr_join (); printf ( \"parent: end \\n \" ); return 0 ; } Analysis of the code snippet We interested in three variables in this program: int done pthread_mutex_t m pthread_cond_t c One may wondering whether the done and m state variables are a must. Let's see what if we don't have the done state variable. In this case, if the child run before the parent, the child will signal and there is no thread in the queue, child returns and then the parent start running, parent will wait if at this time the child is finished, no one will signal it. The parent just stuck! Let's see if we remove the lock m. It will introduce a race condition. Considering if the child interrupts the parent at the time right before the parent call pthread_cond_wait() , the child set the conditional variable and signal, there is no thread in the waiting queue, so child return. Then the parent goes to sleep, and not be able to wait up.","title":"Example of using a conditional variable"},{"location":"system-design/concurrency/notes/#producers-consumers-problem","text":"In this problem, we have to carefully synchronize the two (or more) threads in order for them to work correctly. Here are the put() to populate the buffer, and get() to read the buffer.","title":"Producers-consumers problem"},{"location":"system-design/concurrency/notes/#our-first-solution","text":"Producers and consumers synchronization example, int buffer ; int count = 0 ; void put ( int value ) { assert ( count == 0 ); count = 1 ; buffer = value ; } int get () { assert ( count == 1 ); count = 0 ; return buffer ; } pthread_cond_t cond ; pthread_mutex_t mutex ; void * producer ( void * arg ) { int i ; int loops = ( int ) arg ; for ( i = 0 ; i < loops ; i ++ ) { pthread_mutex_lock ( & mutex ); // p1 if ( count == 1 ) // or use while // p2 pthread_cond_wait ( & cond , & mutex ); // p3 put ( i ); // p4 pthread_cond_signal ( & cond ); // p5 pthread_mutex_unlock ( & mutex ); // p6 } } void * consumer ( void * arg ) { int i ; int loops = ( int ) arg ; for ( i = 0 ; i < loops ; i ++ ) { pthread_mutex_lock ( & mutex ); // c1 if ( count == 0 ) // or use while // c2 pthread_cond_wait ( & cond , & mutex ); // c3 int tmp = get (); // c4 pthread_cond_signal ( & cond ); // c5 pthread_mutex_unlock ( & mutex ); // c6 printf ( \"%d \\n \" , tmp ); } } Problem with this solution, It works ok if we have only one consumer and one producer. Problems could arise when we have more than one consumer. If we have more than one consumer, one of them might sneak in to consume the buffer before the other consumer return from pthread_cond_wait() .","title":"Our first solution"},{"location":"system-design/concurrency/notes/#producer-consumer-using-if","text":"","title":"Producer-consumer using if"},{"location":"system-design/concurrency/notes/#producer-consumer-using-while","text":"","title":"Producer-consumer using while"},{"location":"system-design/concurrency/notes/#mesa-semantics-and-hoare-semantics","text":"The problem with the if statement is that signaling a thread only wakes them up; it is thus a hint that the state of the world has changed but there is no guarantee that when the woken thread runs. This interpretation of what a signal means is often referred to as Mesa semantics ; in contrast, referred to as Hoare semantics , is harder to build but provides a stronger guarantee that the woken thread will run immediately upon being woken.","title":"Mesa semantics and Hoare semantics"},{"location":"system-design/concurrency/notes/#single-buffer-producerconsumer-solution","text":"We saw from the above discussion that even change the if statement to while, we are still facing some problem that all producer thread and consumer thread will sleep. This is because one consumer signaled another consumer and the producer never be able to run again. To solve this problem we'd better to add another conditional variable, one for the producer and another for the consumer so that the producer can only signal the consumer and the consumer can only signal the producer. This sounds most plausible and the problem will never occur. Finally here is our workable solution with two conditional variables. pthread_cond_t empty , fill ; pthread_mutex_t mutex ; void * producer ( void * arg ) { int i ; int loops = ( int ) arg ; for ( i = 0 ; i < loops ; i ++ ) { pthread_mutex_lock ( & mutex ); while ( count == 1 ) pthread_cond_wait ( & empty , & mutex ); put ( i ); pthread_cond_signal ( & fill ); pthread_mutex_unlock ( & mutex ); } } void * consumer ( void * arg ) { int i ; int loops = ( int ) arg ; for ( i = 0 ; i < loops ; i ++ ) { pthread_mutex_lock ( & mutex ); while ( count == 0 ) pthread_cond_wait ( & fill , & mutex ); int tmp = get (); pthread_cond_signal ( & empty ); pthread_mutex_unlock ( & mutex ); printf ( \"%d \\n \" , tmp ); } }","title":"Single buffer producer/consumer solution"},{"location":"system-design/concurrency/notes/#producer-consumer-solution","text":"Now let's change the one element buffer to a more general solution, which has more buffers slots to work on for the consumers and the producers. This solution even support concurrent producing and consuming for multiple threads. int buffer [ MAX ]; int fill_ptr = 0 ; int use_ptr = 0 ; int count = 0 ; void put ( int value ) { buffer [ fill_ptr ] = value ; fill_ptr = ( fill_ptr + 2 ) % MAX ; count ++ ; } int get () { int tmp = buffer [ use_ptr ]; use_ptr = ( use_ptr + 1 ) % MAX ; count -- ; return tmp ; } pthread_cond_t empty , fill ; pthread_mutex_t mutex ; void * producer ( void * arg ) { int i ; int loops = ( int ) arg ; for ( i = 0 ; i < loops ; i ++ ) { pthread_mutex_lock ( & mutex ); while ( count == MAX ) pthread_cond_wait ( & empty , & mutex ); put ( i ); pthread_cond_signal ( & fill ); pthread_mutex_unlock ( & mutex ); } } void * consumer ( void * arg ) { int i ; int loops = ( int ) arg ; for ( i = 0 ; i < loops ; i ++ ) { pthread_mutex_lock ( & mutex ); while ( count == 0 ) pthread_cond_wait ( & fill , & mutex ); int tmp = get (); pthread_cond_signal ( & empty ); pthread_mutex_unlock ( & mutex ); printf ( \"%d \\n \" , tmp ); } }","title":"Producer consumer solution"},{"location":"system-design/concurrency/notes/#covering-conditions","text":"TBD","title":"Covering Conditions"},{"location":"system-design/concurrency/notes/#semaphore","text":"What is the definition of a semaphore? How can we use semaphores instead of locks and condition variables? What is a binary semaphore? Is it straightforward to build a semaphore out of locks and condition variables? to build locks and condition variables out of semaphores?","title":"Semaphore"},{"location":"system-design/concurrency/notes/#definition_1","text":"A semaphore is an object with an integer value that we can manipulate with two routines; in the POSIX standard, these routines are sem_wait() and sem_post() . int sem_wait ( sem_t * s ) { decrement the value of semaphore s by one wait if value of semaphore s is negative } int sem_post ( sem_t * s ) { increment the value of semaphore s by one if there are one or more threads waiting , wake one }","title":"Definition"},{"location":"system-design/concurrency/notes/#semaphore-as-a-lock","text":"sem_t m ; sem_init ( & m , 0 , X ); // initialize semaphore to X; what should X be? sem_wait ( & m ); // critical section here sem_post ( & m );","title":"Semaphore as a lock"},{"location":"system-design/concurrency/notes/#semaphores-as-conditional-variables","text":"Parent waiting for its child Producers and consumers TBD","title":"Semaphores as conditional variables"},{"location":"system-design/concurrency/notes/#deadlock","text":"TBD","title":"Deadlock"},{"location":"system-design/concurrency/notes/#monitor","text":"TBD","title":"Monitor"},{"location":"system-design/concurrency/notes/#dining-philosophers-problem","text":"TBD","title":"Dining philosophers problem"},{"location":"system-design/concurrency/notes/#applications","text":"A timer is a child thread that wakes up periodically to check whether an expiration happened or not. The maximum number of timers is limited by the OS because many systems support a small number of concurrent threads. To implement arbitrary many timers using software, we can use a timer queue to reach the goal. Don't expect too much accuracy for this timer implementation, it is not. However, this approach is acceptable in most of the projects. A good reference is here[1]. source code.","title":"Applications"},{"location":"system-design/concurrency/notes/#reference","text":"Operating Systems: Three Easy Pieces timer queue \u2013 a way to handle multiple timers using one thread COMP 322: Fundamentals of Parallel Programming","title":"Reference"},{"location":"system-design/patterns/notes/","text":"Design Patterns \u00b6 Abstract Factory \u00b6 Builder \u00b6 Factory Method \u00b6","title":"Design Patterns"},{"location":"system-design/patterns/notes/#design-patterns","text":"","title":"Design Patterns"},{"location":"system-design/patterns/notes/#abstract-factory","text":"","title":"Abstract Factory"},{"location":"system-design/patterns/notes/#builder","text":"","title":"Builder"},{"location":"system-design/patterns/notes/#factory-method","text":"","title":"Factory Method"},{"location":"system-design/problems/crawler/notes/","text":"Design a Web Crawler \u00b6 Asynchronous programming \u00b6 Async IO async \u5f02\u6b65\u7f16\u7a0b","title":"Design Crawler"},{"location":"system-design/problems/crawler/notes/#design-a-web-crawler","text":"","title":"Design a Web Crawler"},{"location":"system-design/problems/crawler/notes/#asynchronous-programming","text":"Async IO async \u5f02\u6b65\u7f16\u7a0b","title":"Asynchronous programming"},{"location":"system-design/problems/messager/notes/","text":"Design Messager App \u00b6 Functional requirement \u00b6 Real-time messaging upload image/video/text files channel/group online status read reciept HTTP Pulling \u00b6 HTTP long pulling \u00b6 Websocket \u00b6 full duplex connection Message service (message queue) \u00b6 Notification \u00b6 Pulling v.s. pushing Reference \u00b6 System Design Mock Interview: Design Facebook Messenger WebSockets - A Conceptual Deep Dive","title":"Design Messager"},{"location":"system-design/problems/messager/notes/#design-messager-app","text":"","title":"Design Messager App"},{"location":"system-design/problems/messager/notes/#functional-requirement","text":"Real-time messaging upload image/video/text files channel/group online status read reciept","title":"Functional requirement"},{"location":"system-design/problems/messager/notes/#http-pulling","text":"","title":"HTTP Pulling"},{"location":"system-design/problems/messager/notes/#http-long-pulling","text":"","title":"HTTP long pulling"},{"location":"system-design/problems/messager/notes/#websocket","text":"full duplex connection","title":"Websocket"},{"location":"system-design/problems/messager/notes/#message-service-message-queue","text":"","title":"Message service (message queue)"},{"location":"system-design/problems/messager/notes/#notification","text":"Pulling v.s. pushing","title":"Notification"},{"location":"system-design/problems/messager/notes/#reference","text":"System Design Mock Interview: Design Facebook Messenger WebSockets - A Conceptual Deep Dive","title":"Reference"},{"location":"system-design/problems/payment/notes/","text":"Payment platform \u00b6 Uber Payment Platform \u00b6 Youtube focus on externally facing systems Disbursement Collection Payment Service Provider (PSP) (JSON API, one transaction per request/resposne) Bank (File-based API, multiple transactions per request/response) things responsible for preventing lack of payment duplicate payments Incorrect currency conversion Incorrect payment Dangling authorization Storage for multiplexing and retries idempotency/deduplication track each request/response before we make the call to PSP strong consistency Generate unique external ID ID generation clients (allocate batch of IDs to client) serve IDs in memory (backed up) Can only advance Deterministic Batching Same group of transaction for every try GreenField MVP \u00b6 Authorization \u00b6 client -> Merchant(POS) -> Acquirer -> Network (MasterCard) -> issuer (bank) Shopify eCommerce \u00b6 Keywords \u00b6 Customers Products Orders Payment Shop owners Isolation of items Plugins Storefront v.s. backend API Write ahead log (WAL) \u00b6 The payment transaction need to be logged before the API call is made.","title":"Design Payment System"},{"location":"system-design/problems/payment/notes/#payment-platform","text":"","title":"Payment platform"},{"location":"system-design/problems/payment/notes/#uber-payment-platform","text":"Youtube focus on externally facing systems Disbursement Collection Payment Service Provider (PSP) (JSON API, one transaction per request/resposne) Bank (File-based API, multiple transactions per request/response) things responsible for preventing lack of payment duplicate payments Incorrect currency conversion Incorrect payment Dangling authorization Storage for multiplexing and retries idempotency/deduplication track each request/response before we make the call to PSP strong consistency Generate unique external ID ID generation clients (allocate batch of IDs to client) serve IDs in memory (backed up) Can only advance Deterministic Batching Same group of transaction for every try","title":"Uber Payment Platform"},{"location":"system-design/problems/payment/notes/#greenfield-mvp","text":"","title":"GreenField MVP"},{"location":"system-design/problems/payment/notes/#authorization","text":"client -> Merchant(POS) -> Acquirer -> Network (MasterCard) -> issuer (bank)","title":"Authorization"},{"location":"system-design/problems/payment/notes/#shopify-ecommerce","text":"","title":"Shopify eCommerce"},{"location":"system-design/problems/payment/notes/#keywords","text":"Customers Products Orders Payment Shop owners Isolation of items Plugins Storefront v.s. backend API","title":"Keywords"},{"location":"system-design/problems/payment/notes/#write-ahead-log-wal","text":"The payment transaction need to be logged before the API call is made.","title":"Write ahead log (WAL)"},{"location":"system-design/problems/scheduler/notes/","text":"Design a Job Scheduler \u00b6 Job scheduler could be of many types, such as to schedule CPU jobs (multiprocessing), web server jobs (data application), Hadoop job scheduler, etc. Celery like job scheduler \u00b6 Celery pick up tasks from the web server (web application) manage the tasks using a broker service like message queue (Redis) to schedule the individual task in distributed workers. The tasks usually run in a first come and first serve manner. Celery Tutorial Redis in Action - 6.4 Task queues PagerDuty Scheduler Design Airflow like data job scheduler \u00b6 Azkaban Airflow","title":"Deisgn Scheduler"},{"location":"system-design/problems/scheduler/notes/#design-a-job-scheduler","text":"Job scheduler could be of many types, such as to schedule CPU jobs (multiprocessing), web server jobs (data application), Hadoop job scheduler, etc.","title":"Design a Job Scheduler"},{"location":"system-design/problems/scheduler/notes/#celery-like-job-scheduler","text":"Celery pick up tasks from the web server (web application) manage the tasks using a broker service like message queue (Redis) to schedule the individual task in distributed workers. The tasks usually run in a first come and first serve manner. Celery Tutorial Redis in Action - 6.4 Task queues PagerDuty Scheduler Design","title":"Celery like job scheduler"},{"location":"system-design/problems/scheduler/notes/#airflow-like-data-job-scheduler","text":"Azkaban Airflow","title":"Airflow like data job scheduler"},{"location":"system-design/problems/ticketmaster/notes/","text":"How to Design a Ticket System \u00b6 Similar design questions How to design a coupon system (ticketmaster) System Requirements \u00b6 What's type of coupon? Free or non free? (payment system) Can it be shared? Does it have an expiration? What's other features it should support beside distributing coupons? Users clients, consume coupon/tickets vendors, produce coupon/tickets, need to have a coupon management system to do that. coupon \"transactions\" coupon verification, barcode, etc. Availablility Fault torlerance Scalability Core features \u00b6 Browse coupon by shop, product/service types (categories). Claim a coupon, put into their bag. how to get a coupon? Who and how to produce a coupon? user profile/account? Handle failure Handle coupon timeout Horizontal scale Data structure and calculations \u00b6 How many products, how many coupons for each product? Time range we are publishing coupones? (1 month, 1 week?) How many coupon in total we need to tracking at the same time? distributed queue data structure elements in the queue should be a coupon object (with type, counter, expiration, etc.) May need to have different queue to keep coupon in different stages (e.g. created, published (display), reserved, claimed, expired). MVP \u00b6 overall system architecture, database Schema design (the data model and data size ) coupon (128 bytes) product (128 bytes) ticket event table event seats table user table purchase info table How to handle two people try to purchase the same ticket? How to store ticket purchase information in the database? can store with seat table. can store the credit card within the user profile. can have a seperate table for purchase, flexible in handling return and refund. How to handle guest purchase? Is register required? anonymous with purchase info, events, tickets, etc. Walk thorugh a simple use case to check whether we missed any core feature. Scale it \u00b6 reflect on the QPS calculation, identify bottlenecks and issues for high QPS. load balancer cache (CDN) how to handle different people by the same ticket? draw a timeline to show the potentiall issue (race condition) how to ensure there is no race condition? check in each step whether the seat is available or not? (browse, add payment, submit order) ACID database with transaction support (who ever comes first get the ticket) how to improve the user experience by deciding when to lock the seat. whoever first select the seat will be sure they will get it, but bad guys might never buy but just select. How can we solve the issue? timeout, after timeout, if not paid, go back to the queue so that others can buy. Database performance Read v.s write If read heavy, memory cache can be handy (Memcached) Frontend performance autoscaler (auto add additional webserver) load balancer (horizontal scale) cache static html file (precompute the main listing page cache it in memory or CDN) Reliability identify Single point of failure add backups/replica/failovers (loadbalancer, database, distributed cache) RAM volatile memory, lost data on power outage. (Uninterruptible power supply (UPS)) Bonus \u00b6 support return and refund? handle peak traffic incase a popolar event coming up. How to design firewall/routing policy, and how to secure user password and information Enable SSL for secure client server data transfer. Reference \u00b6 How We Developed Scalable Coupon Management System In Node?","title":"Design Ticketmaster"},{"location":"system-design/problems/ticketmaster/notes/#how-to-design-a-ticket-system","text":"Similar design questions How to design a coupon system (ticketmaster)","title":"How to Design a Ticket System"},{"location":"system-design/problems/ticketmaster/notes/#system-requirements","text":"What's type of coupon? Free or non free? (payment system) Can it be shared? Does it have an expiration? What's other features it should support beside distributing coupons? Users clients, consume coupon/tickets vendors, produce coupon/tickets, need to have a coupon management system to do that. coupon \"transactions\" coupon verification, barcode, etc. Availablility Fault torlerance Scalability","title":"System Requirements"},{"location":"system-design/problems/ticketmaster/notes/#core-features","text":"Browse coupon by shop, product/service types (categories). Claim a coupon, put into their bag. how to get a coupon? Who and how to produce a coupon? user profile/account? Handle failure Handle coupon timeout Horizontal scale","title":"Core features"},{"location":"system-design/problems/ticketmaster/notes/#data-structure-and-calculations","text":"How many products, how many coupons for each product? Time range we are publishing coupones? (1 month, 1 week?) How many coupon in total we need to tracking at the same time? distributed queue data structure elements in the queue should be a coupon object (with type, counter, expiration, etc.) May need to have different queue to keep coupon in different stages (e.g. created, published (display), reserved, claimed, expired).","title":"Data structure and calculations"},{"location":"system-design/problems/ticketmaster/notes/#mvp","text":"overall system architecture, database Schema design (the data model and data size ) coupon (128 bytes) product (128 bytes) ticket event table event seats table user table purchase info table How to handle two people try to purchase the same ticket? How to store ticket purchase information in the database? can store with seat table. can store the credit card within the user profile. can have a seperate table for purchase, flexible in handling return and refund. How to handle guest purchase? Is register required? anonymous with purchase info, events, tickets, etc. Walk thorugh a simple use case to check whether we missed any core feature.","title":"MVP"},{"location":"system-design/problems/ticketmaster/notes/#scale-it","text":"reflect on the QPS calculation, identify bottlenecks and issues for high QPS. load balancer cache (CDN) how to handle different people by the same ticket? draw a timeline to show the potentiall issue (race condition) how to ensure there is no race condition? check in each step whether the seat is available or not? (browse, add payment, submit order) ACID database with transaction support (who ever comes first get the ticket) how to improve the user experience by deciding when to lock the seat. whoever first select the seat will be sure they will get it, but bad guys might never buy but just select. How can we solve the issue? timeout, after timeout, if not paid, go back to the queue so that others can buy. Database performance Read v.s write If read heavy, memory cache can be handy (Memcached) Frontend performance autoscaler (auto add additional webserver) load balancer (horizontal scale) cache static html file (precompute the main listing page cache it in memory or CDN) Reliability identify Single point of failure add backups/replica/failovers (loadbalancer, database, distributed cache) RAM volatile memory, lost data on power outage. (Uninterruptible power supply (UPS))","title":"Scale it"},{"location":"system-design/problems/ticketmaster/notes/#bonus","text":"support return and refund? handle peak traffic incase a popolar event coming up. How to design firewall/routing policy, and how to secure user password and information Enable SSL for secure client server data transfer.","title":"Bonus"},{"location":"system-design/problems/ticketmaster/notes/#reference","text":"How We Developed Scalable Coupon Management System In Node?","title":"Reference"},{"location":"system-design/problems/tinyurl/notes/","text":"Design a TinyUrl application \u00b6","title":"Design TinyUrl"},{"location":"system-design/problems/tinyurl/notes/#design-a-tinyurl-application","text":"","title":"Design a TinyUrl application"},{"location":"system-design/problems/twitter/notes/","text":"","title":"Design Twitter"}]}